I0130 11:17:32.956158      23 e2e.go:126] Starting e2e run "195808ce-feca-480c-8c47-644dc9db1d74" on Ginkgo node 1
Jan 30 11:17:32.969: INFO: Enabling in-tree volume drivers
Running Suite: Kubernetes e2e suite - /usr/local/bin
====================================================
Random Seed: 1675077452 - will randomize all specs

Will run 368 of 7069 specs
------------------------------
[SynchronizedBeforeSuite] 
test/e2e/e2e.go:77
[SynchronizedBeforeSuite] TOP-LEVEL
  test/e2e/e2e.go:77
Jan 30 11:17:33.069: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
E0130 11:17:33.070351      23 progress.go:80] Failed to post progress update to http://localhost:8099/progress: Post "http://localhost:8099/progress": dial tcp 127.0.0.1:8099: connect: connection refused
Jan 30 11:17:33.070: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Jan 30 11:17:33.078: INFO: Unschedulable nodes= 1, maximum value for starting tests= 0
Jan 30 11:17:33.078: INFO: 	-> Node pubt2-nks-for-dev1.dg.163.org [[[ Ready=true, Network(available)=true, Taints=[{node.netease.com/phase checking NoSchedule <nil>}], NonblockingTaints=node-role.kubernetes.io/control-plane,node-role.kubernetes.io/master ]]]
Jan 30 11:17:33.078: INFO: ==== node wait: 1 out of 2 nodes are ready, max notReady allowed 0.  Need 1 more before starting.
Jan 30 11:18:03.082: INFO: Unschedulable nodes= 1, maximum value for starting tests= 0
Jan 30 11:18:03.082: INFO: 	-> Node pubt2-nks-for-dev1.dg.163.org [[[ Ready=true, Network(available)=true, Taints=[{node.netease.com/phase checking NoSchedule <nil>}], NonblockingTaints=node-role.kubernetes.io/control-plane,node-role.kubernetes.io/master ]]]
Jan 30 11:18:03.082: INFO: ==== node wait: 1 out of 2 nodes are ready, max notReady allowed 0.  Need 1 more before starting.
Jan 30 11:18:33.082: INFO: Unschedulable nodes= 1, maximum value for starting tests= 0
Jan 30 11:18:33.082: INFO: 	-> Node pubt2-nks-for-dev1.dg.163.org [[[ Ready=true, Network(available)=true, Taints=[{node.netease.com/phase checking NoSchedule <nil>}], NonblockingTaints=node-role.kubernetes.io/control-plane,node-role.kubernetes.io/master ]]]
Jan 30 11:18:33.082: INFO: ==== node wait: 1 out of 2 nodes are ready, max notReady allowed 0.  Need 1 more before starting.
Jan 30 11:19:03.081: INFO: Unschedulable nodes= 1, maximum value for starting tests= 0
Jan 30 11:19:03.081: INFO: 	-> Node pubt2-nks-for-dev1.dg.163.org [[[ Ready=true, Network(available)=true, Taints=[{node.netease.com/phase checking NoSchedule <nil>}], NonblockingTaints=node-role.kubernetes.io/control-plane,node-role.kubernetes.io/master ]]]
Jan 30 11:19:03.081: INFO: ==== node wait: 1 out of 2 nodes are ready, max notReady allowed 0.  Need 1 more before starting.
Jan 30 11:19:33.083: INFO: Unschedulable nodes= 1, maximum value for starting tests= 0
Jan 30 11:19:33.083: INFO: 	-> Node pubt2-nks-for-dev1.dg.163.org [[[ Ready=true, Network(available)=true, Taints=[{node.netease.com/phase checking NoSchedule <nil>}], NonblockingTaints=node-role.kubernetes.io/control-plane,node-role.kubernetes.io/master ]]]
Jan 30 11:19:33.083: INFO: ==== node wait: 1 out of 2 nodes are ready, max notReady allowed 0.  Need 1 more before starting.
Jan 30 11:20:03.082: INFO: Unschedulable nodes= 1, maximum value for starting tests= 0
Jan 30 11:20:03.082: INFO: 	-> Node pubt2-nks-for-dev1.dg.163.org [[[ Ready=true, Network(available)=true, Taints=[{node.netease.com/phase checking NoSchedule <nil>}], NonblockingTaints=node-role.kubernetes.io/control-plane,node-role.kubernetes.io/master ]]]
Jan 30 11:20:03.082: INFO: ==== node wait: 1 out of 2 nodes are ready, max notReady allowed 0.  Need 1 more before starting.
Jan 30 11:20:33.081: INFO: Unschedulable nodes= 1, maximum value for starting tests= 0
Jan 30 11:20:33.081: INFO: 	-> Node pubt2-nks-for-dev1.dg.163.org [[[ Ready=true, Network(available)=true, Taints=[{node.netease.com/phase checking NoSchedule <nil>}], NonblockingTaints=node-role.kubernetes.io/control-plane,node-role.kubernetes.io/master ]]]
Jan 30 11:20:33.081: INFO: ==== node wait: 1 out of 2 nodes are ready, max notReady allowed 0.  Need 1 more before starting.
Jan 30 11:21:03.082: INFO: Unschedulable nodes= 1, maximum value for starting tests= 0
Jan 30 11:21:03.082: INFO: 	-> Node pubt2-nks-for-dev1.dg.163.org [[[ Ready=true, Network(available)=true, Taints=[{node.netease.com/phase checking NoSchedule <nil>}], NonblockingTaints=node-role.kubernetes.io/control-plane,node-role.kubernetes.io/master ]]]
Jan 30 11:21:03.082: INFO: ==== node wait: 1 out of 2 nodes are ready, max notReady allowed 0.  Need 1 more before starting.
Jan 30 11:21:33.085: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Jan 30 11:21:33.109: INFO: 15 / 15 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Jan 30 11:21:33.109: INFO: expected 3 pod replicas in namespace 'kube-system', 3 are Running and Ready.
Jan 30 11:21:33.109: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Jan 30 11:21:33.113: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'calico-node' (0 seconds elapsed)
Jan 30 11:21:33.113: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'cleanlog' (0 seconds elapsed)
Jan 30 11:21:33.113: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
Jan 30 11:21:33.113: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'node-local-dns' (0 seconds elapsed)
Jan 30 11:21:33.113: INFO: e2e test version: v1.26.1
Jan 30 11:21:33.114: INFO: kube-apiserver version: v1.26.1-nks.1
[SynchronizedBeforeSuite] TOP-LEVEL
  test/e2e/e2e.go:77
Jan 30 11:21:33.114: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
Jan 30 11:21:33.117: INFO: Cluster IP family: ipv4
------------------------------
[SynchronizedBeforeSuite] PASSED [240.048 seconds]
[SynchronizedBeforeSuite] 
test/e2e/e2e.go:77

  Begin Captured GinkgoWriter Output >>
    [SynchronizedBeforeSuite] TOP-LEVEL
      test/e2e/e2e.go:77
    Jan 30 11:17:33.069: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    E0130 11:17:33.070351      23 progress.go:80] Failed to post progress update to http://localhost:8099/progress: Post "http://localhost:8099/progress": dial tcp 127.0.0.1:8099: connect: connection refused
    Jan 30 11:17:33.070: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
    Jan 30 11:17:33.078: INFO: Unschedulable nodes= 1, maximum value for starting tests= 0
    Jan 30 11:17:33.078: INFO: 	-> Node pubt2-nks-for-dev1.dg.163.org [[[ Ready=true, Network(available)=true, Taints=[{node.netease.com/phase checking NoSchedule <nil>}], NonblockingTaints=node-role.kubernetes.io/control-plane,node-role.kubernetes.io/master ]]]
    Jan 30 11:17:33.078: INFO: ==== node wait: 1 out of 2 nodes are ready, max notReady allowed 0.  Need 1 more before starting.
    Jan 30 11:18:03.082: INFO: Unschedulable nodes= 1, maximum value for starting tests= 0
    Jan 30 11:18:03.082: INFO: 	-> Node pubt2-nks-for-dev1.dg.163.org [[[ Ready=true, Network(available)=true, Taints=[{node.netease.com/phase checking NoSchedule <nil>}], NonblockingTaints=node-role.kubernetes.io/control-plane,node-role.kubernetes.io/master ]]]
    Jan 30 11:18:03.082: INFO: ==== node wait: 1 out of 2 nodes are ready, max notReady allowed 0.  Need 1 more before starting.
    Jan 30 11:18:33.082: INFO: Unschedulable nodes= 1, maximum value for starting tests= 0
    Jan 30 11:18:33.082: INFO: 	-> Node pubt2-nks-for-dev1.dg.163.org [[[ Ready=true, Network(available)=true, Taints=[{node.netease.com/phase checking NoSchedule <nil>}], NonblockingTaints=node-role.kubernetes.io/control-plane,node-role.kubernetes.io/master ]]]
    Jan 30 11:18:33.082: INFO: ==== node wait: 1 out of 2 nodes are ready, max notReady allowed 0.  Need 1 more before starting.
    Jan 30 11:19:03.081: INFO: Unschedulable nodes= 1, maximum value for starting tests= 0
    Jan 30 11:19:03.081: INFO: 	-> Node pubt2-nks-for-dev1.dg.163.org [[[ Ready=true, Network(available)=true, Taints=[{node.netease.com/phase checking NoSchedule <nil>}], NonblockingTaints=node-role.kubernetes.io/control-plane,node-role.kubernetes.io/master ]]]
    Jan 30 11:19:03.081: INFO: ==== node wait: 1 out of 2 nodes are ready, max notReady allowed 0.  Need 1 more before starting.
    Jan 30 11:19:33.083: INFO: Unschedulable nodes= 1, maximum value for starting tests= 0
    Jan 30 11:19:33.083: INFO: 	-> Node pubt2-nks-for-dev1.dg.163.org [[[ Ready=true, Network(available)=true, Taints=[{node.netease.com/phase checking NoSchedule <nil>}], NonblockingTaints=node-role.kubernetes.io/control-plane,node-role.kubernetes.io/master ]]]
    Jan 30 11:19:33.083: INFO: ==== node wait: 1 out of 2 nodes are ready, max notReady allowed 0.  Need 1 more before starting.
    Jan 30 11:20:03.082: INFO: Unschedulable nodes= 1, maximum value for starting tests= 0
    Jan 30 11:20:03.082: INFO: 	-> Node pubt2-nks-for-dev1.dg.163.org [[[ Ready=true, Network(available)=true, Taints=[{node.netease.com/phase checking NoSchedule <nil>}], NonblockingTaints=node-role.kubernetes.io/control-plane,node-role.kubernetes.io/master ]]]
    Jan 30 11:20:03.082: INFO: ==== node wait: 1 out of 2 nodes are ready, max notReady allowed 0.  Need 1 more before starting.
    Jan 30 11:20:33.081: INFO: Unschedulable nodes= 1, maximum value for starting tests= 0
    Jan 30 11:20:33.081: INFO: 	-> Node pubt2-nks-for-dev1.dg.163.org [[[ Ready=true, Network(available)=true, Taints=[{node.netease.com/phase checking NoSchedule <nil>}], NonblockingTaints=node-role.kubernetes.io/control-plane,node-role.kubernetes.io/master ]]]
    Jan 30 11:20:33.081: INFO: ==== node wait: 1 out of 2 nodes are ready, max notReady allowed 0.  Need 1 more before starting.
    Jan 30 11:21:03.082: INFO: Unschedulable nodes= 1, maximum value for starting tests= 0
    Jan 30 11:21:03.082: INFO: 	-> Node pubt2-nks-for-dev1.dg.163.org [[[ Ready=true, Network(available)=true, Taints=[{node.netease.com/phase checking NoSchedule <nil>}], NonblockingTaints=node-role.kubernetes.io/control-plane,node-role.kubernetes.io/master ]]]
    Jan 30 11:21:03.082: INFO: ==== node wait: 1 out of 2 nodes are ready, max notReady allowed 0.  Need 1 more before starting.
    Jan 30 11:21:33.085: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
    Jan 30 11:21:33.109: INFO: 15 / 15 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
    Jan 30 11:21:33.109: INFO: expected 3 pod replicas in namespace 'kube-system', 3 are Running and Ready.
    Jan 30 11:21:33.109: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
    Jan 30 11:21:33.113: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'calico-node' (0 seconds elapsed)
    Jan 30 11:21:33.113: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'cleanlog' (0 seconds elapsed)
    Jan 30 11:21:33.113: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
    Jan 30 11:21:33.113: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'node-local-dns' (0 seconds elapsed)
    Jan 30 11:21:33.113: INFO: e2e test version: v1.26.1
    Jan 30 11:21:33.114: INFO: kube-apiserver version: v1.26.1-nks.1
    [SynchronizedBeforeSuite] TOP-LEVEL
      test/e2e/e2e.go:77
    Jan 30 11:21:33.114: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    Jan 30 11:21:33.117: INFO: Cluster IP family: ipv4
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Downward API volume
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:261
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 11:21:33.136
Jan 30 11:21:33.136: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename downward-api 01/30/23 11:21:33.137
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:21:33.144
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:21:33.147
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:261
STEP: Creating a pod to test downward API volume plugin 01/30/23 11:21:33.149
Jan 30 11:21:33.153: INFO: Waiting up to 5m0s for pod "downwardapi-volume-81adea40-82c8-4b9d-993a-de4f8e34781b" in namespace "downward-api-9015" to be "Succeeded or Failed"
Jan 30 11:21:33.155: INFO: Pod "downwardapi-volume-81adea40-82c8-4b9d-993a-de4f8e34781b": Phase="Pending", Reason="", readiness=false. Elapsed: 1.612829ms
Jan 30 11:21:35.158: INFO: Pod "downwardapi-volume-81adea40-82c8-4b9d-993a-de4f8e34781b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005018206s
Jan 30 11:21:37.158: INFO: Pod "downwardapi-volume-81adea40-82c8-4b9d-993a-de4f8e34781b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.004776972s
Jan 30 11:21:39.158: INFO: Pod "downwardapi-volume-81adea40-82c8-4b9d-993a-de4f8e34781b": Phase="Pending", Reason="", readiness=false. Elapsed: 6.005195899s
Jan 30 11:21:41.158: INFO: Pod "downwardapi-volume-81adea40-82c8-4b9d-993a-de4f8e34781b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.005065512s
STEP: Saw pod success 01/30/23 11:21:41.158
Jan 30 11:21:41.159: INFO: Pod "downwardapi-volume-81adea40-82c8-4b9d-993a-de4f8e34781b" satisfied condition "Succeeded or Failed"
Jan 30 11:21:41.160: INFO: Trying to get logs from node pubt2-nks-for-dev1.dg.163.org pod downwardapi-volume-81adea40-82c8-4b9d-993a-de4f8e34781b container client-container: <nil>
STEP: delete the pod 01/30/23 11:21:41.174
Jan 30 11:21:41.178: INFO: Waiting for pod downwardapi-volume-81adea40-82c8-4b9d-993a-de4f8e34781b to disappear
Jan 30 11:21:41.180: INFO: Pod downwardapi-volume-81adea40-82c8-4b9d-993a-de4f8e34781b no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Jan 30 11:21:41.180: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-9015" for this suite. 01/30/23 11:21:41.182
------------------------------
• [SLOW TEST] [8.048 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:261

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 11:21:33.136
    Jan 30 11:21:33.136: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename downward-api 01/30/23 11:21:33.137
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:21:33.144
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:21:33.147
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:261
    STEP: Creating a pod to test downward API volume plugin 01/30/23 11:21:33.149
    Jan 30 11:21:33.153: INFO: Waiting up to 5m0s for pod "downwardapi-volume-81adea40-82c8-4b9d-993a-de4f8e34781b" in namespace "downward-api-9015" to be "Succeeded or Failed"
    Jan 30 11:21:33.155: INFO: Pod "downwardapi-volume-81adea40-82c8-4b9d-993a-de4f8e34781b": Phase="Pending", Reason="", readiness=false. Elapsed: 1.612829ms
    Jan 30 11:21:35.158: INFO: Pod "downwardapi-volume-81adea40-82c8-4b9d-993a-de4f8e34781b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005018206s
    Jan 30 11:21:37.158: INFO: Pod "downwardapi-volume-81adea40-82c8-4b9d-993a-de4f8e34781b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.004776972s
    Jan 30 11:21:39.158: INFO: Pod "downwardapi-volume-81adea40-82c8-4b9d-993a-de4f8e34781b": Phase="Pending", Reason="", readiness=false. Elapsed: 6.005195899s
    Jan 30 11:21:41.158: INFO: Pod "downwardapi-volume-81adea40-82c8-4b9d-993a-de4f8e34781b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.005065512s
    STEP: Saw pod success 01/30/23 11:21:41.158
    Jan 30 11:21:41.159: INFO: Pod "downwardapi-volume-81adea40-82c8-4b9d-993a-de4f8e34781b" satisfied condition "Succeeded or Failed"
    Jan 30 11:21:41.160: INFO: Trying to get logs from node pubt2-nks-for-dev1.dg.163.org pod downwardapi-volume-81adea40-82c8-4b9d-993a-de4f8e34781b container client-container: <nil>
    STEP: delete the pod 01/30/23 11:21:41.174
    Jan 30 11:21:41.178: INFO: Waiting for pod downwardapi-volume-81adea40-82c8-4b9d-993a-de4f8e34781b to disappear
    Jan 30 11:21:41.180: INFO: Pod downwardapi-volume-81adea40-82c8-4b9d-993a-de4f8e34781b no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Jan 30 11:21:41.180: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-9015" for this suite. 01/30/23 11:21:41.182
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:152
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 11:21:41.188
Jan 30 11:21:41.188: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename container-probe 01/30/23 11:21:41.189
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:21:41.195
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:21:41.197
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:152
STEP: Creating pod busybox-0197adc8-48f5-4904-b57a-6ecf7b3dbecb in namespace container-probe-7097 01/30/23 11:21:41.199
Jan 30 11:21:41.204: INFO: Waiting up to 5m0s for pod "busybox-0197adc8-48f5-4904-b57a-6ecf7b3dbecb" in namespace "container-probe-7097" to be "not pending"
Jan 30 11:21:41.206: INFO: Pod "busybox-0197adc8-48f5-4904-b57a-6ecf7b3dbecb": Phase="Pending", Reason="", readiness=false. Elapsed: 1.702806ms
Jan 30 11:21:43.208: INFO: Pod "busybox-0197adc8-48f5-4904-b57a-6ecf7b3dbecb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004142174s
Jan 30 11:21:45.209: INFO: Pod "busybox-0197adc8-48f5-4904-b57a-6ecf7b3dbecb": Phase="Running", Reason="", readiness=true. Elapsed: 4.004995557s
Jan 30 11:21:45.209: INFO: Pod "busybox-0197adc8-48f5-4904-b57a-6ecf7b3dbecb" satisfied condition "not pending"
Jan 30 11:21:45.209: INFO: Started pod busybox-0197adc8-48f5-4904-b57a-6ecf7b3dbecb in namespace container-probe-7097
STEP: checking the pod's current state and verifying that restartCount is present 01/30/23 11:21:45.209
Jan 30 11:21:45.211: INFO: Initial restart count of pod busybox-0197adc8-48f5-4904-b57a-6ecf7b3dbecb is 0
STEP: deleting the pod 01/30/23 11:25:45.574
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Jan 30 11:25:45.579: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-7097" for this suite. 01/30/23 11:25:45.581
------------------------------
• [SLOW TEST] [244.396 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:152

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 11:21:41.188
    Jan 30 11:21:41.188: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename container-probe 01/30/23 11:21:41.189
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:21:41.195
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:21:41.197
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:152
    STEP: Creating pod busybox-0197adc8-48f5-4904-b57a-6ecf7b3dbecb in namespace container-probe-7097 01/30/23 11:21:41.199
    Jan 30 11:21:41.204: INFO: Waiting up to 5m0s for pod "busybox-0197adc8-48f5-4904-b57a-6ecf7b3dbecb" in namespace "container-probe-7097" to be "not pending"
    Jan 30 11:21:41.206: INFO: Pod "busybox-0197adc8-48f5-4904-b57a-6ecf7b3dbecb": Phase="Pending", Reason="", readiness=false. Elapsed: 1.702806ms
    Jan 30 11:21:43.208: INFO: Pod "busybox-0197adc8-48f5-4904-b57a-6ecf7b3dbecb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004142174s
    Jan 30 11:21:45.209: INFO: Pod "busybox-0197adc8-48f5-4904-b57a-6ecf7b3dbecb": Phase="Running", Reason="", readiness=true. Elapsed: 4.004995557s
    Jan 30 11:21:45.209: INFO: Pod "busybox-0197adc8-48f5-4904-b57a-6ecf7b3dbecb" satisfied condition "not pending"
    Jan 30 11:21:45.209: INFO: Started pod busybox-0197adc8-48f5-4904-b57a-6ecf7b3dbecb in namespace container-probe-7097
    STEP: checking the pod's current state and verifying that restartCount is present 01/30/23 11:21:45.209
    Jan 30 11:21:45.211: INFO: Initial restart count of pod busybox-0197adc8-48f5-4904-b57a-6ecf7b3dbecb is 0
    STEP: deleting the pod 01/30/23 11:25:45.574
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Jan 30 11:25:45.579: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-7097" for this suite. 01/30/23 11:25:45.581
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:145
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 11:25:45.585
Jan 30 11:25:45.585: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename custom-resource-definition 01/30/23 11:25:45.586
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:25:45.592
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:25:45.594
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:145
Jan 30 11:25:45.596: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 30 11:25:46.118: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-3131" for this suite. 01/30/23 11:25:46.12
------------------------------
• [0.538 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    getting/updating/patching custom resource definition status sub-resource works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:145

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 11:25:45.585
    Jan 30 11:25:45.585: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename custom-resource-definition 01/30/23 11:25:45.586
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:25:45.592
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:25:45.594
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:145
    Jan 30 11:25:45.596: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 30 11:25:46.118: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-3131" for this suite. 01/30/23 11:25:46.12
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] KubeletManagedEtcHosts
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63
[BeforeEach] [sig-node] KubeletManagedEtcHosts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 11:25:46.124
Jan 30 11:25:46.124: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts 01/30/23 11:25:46.124
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:25:46.13
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:25:46.132
[BeforeEach] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/metrics/init/init.go:31
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63
STEP: Setting up the test 01/30/23 11:25:46.134
STEP: Creating hostNetwork=false pod 01/30/23 11:25:46.134
Jan 30 11:25:46.138: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "e2e-kubelet-etc-hosts-2497" to be "running and ready"
Jan 30 11:25:46.140: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 1.671034ms
Jan 30 11:25:46.140: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
Jan 30 11:25:48.142: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.003896885s
Jan 30 11:25:48.142: INFO: The phase of Pod test-pod is Running (Ready = true)
Jan 30 11:25:48.142: INFO: Pod "test-pod" satisfied condition "running and ready"
STEP: Creating hostNetwork=true pod 01/30/23 11:25:48.144
Jan 30 11:25:48.147: INFO: Waiting up to 5m0s for pod "test-host-network-pod" in namespace "e2e-kubelet-etc-hosts-2497" to be "running and ready"
Jan 30 11:25:48.148: INFO: Pod "test-host-network-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 1.560193ms
Jan 30 11:25:48.148: INFO: The phase of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
Jan 30 11:25:50.152: INFO: Pod "test-host-network-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.004960628s
Jan 30 11:25:50.152: INFO: The phase of Pod test-host-network-pod is Running (Ready = true)
Jan 30 11:25:50.152: INFO: Pod "test-host-network-pod" satisfied condition "running and ready"
STEP: Running the test 01/30/23 11:25:50.154
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false 01/30/23 11:25:50.154
Jan 30 11:25:50.154: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2497 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 30 11:25:50.154: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
Jan 30 11:25:50.154: INFO: ExecWithOptions: Clientset creation
Jan 30 11:25:50.154: INFO: ExecWithOptions: execute(POST https://10.178.64.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-2497/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Jan 30 11:25:50.253: INFO: Exec stderr: ""
Jan 30 11:25:50.254: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2497 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 30 11:25:50.254: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
Jan 30 11:25:50.254: INFO: ExecWithOptions: Clientset creation
Jan 30 11:25:50.254: INFO: ExecWithOptions: execute(POST https://10.178.64.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-2497/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Jan 30 11:25:50.330: INFO: Exec stderr: ""
Jan 30 11:25:50.330: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2497 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 30 11:25:50.330: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
Jan 30 11:25:50.331: INFO: ExecWithOptions: Clientset creation
Jan 30 11:25:50.331: INFO: ExecWithOptions: execute(POST https://10.178.64.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-2497/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Jan 30 11:25:50.414: INFO: Exec stderr: ""
Jan 30 11:25:50.414: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2497 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 30 11:25:50.414: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
Jan 30 11:25:50.414: INFO: ExecWithOptions: Clientset creation
Jan 30 11:25:50.414: INFO: ExecWithOptions: execute(POST https://10.178.64.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-2497/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Jan 30 11:25:50.486: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount 01/30/23 11:25:50.486
Jan 30 11:25:50.486: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2497 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 30 11:25:50.486: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
Jan 30 11:25:50.487: INFO: ExecWithOptions: Clientset creation
Jan 30 11:25:50.487: INFO: ExecWithOptions: execute(POST https://10.178.64.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-2497/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
Jan 30 11:25:50.553: INFO: Exec stderr: ""
Jan 30 11:25:50.553: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2497 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 30 11:25:50.553: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
Jan 30 11:25:50.554: INFO: ExecWithOptions: Clientset creation
Jan 30 11:25:50.554: INFO: ExecWithOptions: execute(POST https://10.178.64.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-2497/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
Jan 30 11:25:50.617: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true 01/30/23 11:25:50.617
Jan 30 11:25:50.617: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2497 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 30 11:25:50.617: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
Jan 30 11:25:50.617: INFO: ExecWithOptions: Clientset creation
Jan 30 11:25:50.617: INFO: ExecWithOptions: execute(POST https://10.178.64.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-2497/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Jan 30 11:25:50.682: INFO: Exec stderr: ""
Jan 30 11:25:50.682: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2497 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 30 11:25:50.682: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
Jan 30 11:25:50.682: INFO: ExecWithOptions: Clientset creation
Jan 30 11:25:50.682: INFO: ExecWithOptions: execute(POST https://10.178.64.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-2497/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Jan 30 11:25:50.774: INFO: Exec stderr: ""
Jan 30 11:25:50.774: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2497 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 30 11:25:50.774: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
Jan 30 11:25:50.774: INFO: ExecWithOptions: Clientset creation
Jan 30 11:25:50.774: INFO: ExecWithOptions: execute(POST https://10.178.64.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-2497/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Jan 30 11:25:50.846: INFO: Exec stderr: ""
Jan 30 11:25:50.846: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2497 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 30 11:25:50.846: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
Jan 30 11:25:50.847: INFO: ExecWithOptions: Clientset creation
Jan 30 11:25:50.847: INFO: ExecWithOptions: execute(POST https://10.178.64.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-2497/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Jan 30 11:25:50.913: INFO: Exec stderr: ""
[AfterEach] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/node/init/init.go:32
Jan 30 11:25:50.913: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
  tear down framework | framework.go:193
STEP: Destroying namespace "e2e-kubelet-etc-hosts-2497" for this suite. 01/30/23 11:25:50.915
------------------------------
• [4.794 seconds]
[sig-node] KubeletManagedEtcHosts
test/e2e/common/node/framework.go:23
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] KubeletManagedEtcHosts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 11:25:46.124
    Jan 30 11:25:46.124: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts 01/30/23 11:25:46.124
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:25:46.13
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:25:46.132
    [BeforeEach] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/metrics/init/init.go:31
    [It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet_etc_hosts.go:63
    STEP: Setting up the test 01/30/23 11:25:46.134
    STEP: Creating hostNetwork=false pod 01/30/23 11:25:46.134
    Jan 30 11:25:46.138: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "e2e-kubelet-etc-hosts-2497" to be "running and ready"
    Jan 30 11:25:46.140: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 1.671034ms
    Jan 30 11:25:46.140: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
    Jan 30 11:25:48.142: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.003896885s
    Jan 30 11:25:48.142: INFO: The phase of Pod test-pod is Running (Ready = true)
    Jan 30 11:25:48.142: INFO: Pod "test-pod" satisfied condition "running and ready"
    STEP: Creating hostNetwork=true pod 01/30/23 11:25:48.144
    Jan 30 11:25:48.147: INFO: Waiting up to 5m0s for pod "test-host-network-pod" in namespace "e2e-kubelet-etc-hosts-2497" to be "running and ready"
    Jan 30 11:25:48.148: INFO: Pod "test-host-network-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 1.560193ms
    Jan 30 11:25:48.148: INFO: The phase of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
    Jan 30 11:25:50.152: INFO: Pod "test-host-network-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.004960628s
    Jan 30 11:25:50.152: INFO: The phase of Pod test-host-network-pod is Running (Ready = true)
    Jan 30 11:25:50.152: INFO: Pod "test-host-network-pod" satisfied condition "running and ready"
    STEP: Running the test 01/30/23 11:25:50.154
    STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false 01/30/23 11:25:50.154
    Jan 30 11:25:50.154: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2497 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 30 11:25:50.154: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    Jan 30 11:25:50.154: INFO: ExecWithOptions: Clientset creation
    Jan 30 11:25:50.154: INFO: ExecWithOptions: execute(POST https://10.178.64.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-2497/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Jan 30 11:25:50.253: INFO: Exec stderr: ""
    Jan 30 11:25:50.254: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2497 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 30 11:25:50.254: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    Jan 30 11:25:50.254: INFO: ExecWithOptions: Clientset creation
    Jan 30 11:25:50.254: INFO: ExecWithOptions: execute(POST https://10.178.64.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-2497/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Jan 30 11:25:50.330: INFO: Exec stderr: ""
    Jan 30 11:25:50.330: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2497 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 30 11:25:50.330: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    Jan 30 11:25:50.331: INFO: ExecWithOptions: Clientset creation
    Jan 30 11:25:50.331: INFO: ExecWithOptions: execute(POST https://10.178.64.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-2497/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Jan 30 11:25:50.414: INFO: Exec stderr: ""
    Jan 30 11:25:50.414: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2497 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 30 11:25:50.414: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    Jan 30 11:25:50.414: INFO: ExecWithOptions: Clientset creation
    Jan 30 11:25:50.414: INFO: ExecWithOptions: execute(POST https://10.178.64.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-2497/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Jan 30 11:25:50.486: INFO: Exec stderr: ""
    STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount 01/30/23 11:25:50.486
    Jan 30 11:25:50.486: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2497 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 30 11:25:50.486: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    Jan 30 11:25:50.487: INFO: ExecWithOptions: Clientset creation
    Jan 30 11:25:50.487: INFO: ExecWithOptions: execute(POST https://10.178.64.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-2497/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
    Jan 30 11:25:50.553: INFO: Exec stderr: ""
    Jan 30 11:25:50.553: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2497 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 30 11:25:50.553: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    Jan 30 11:25:50.554: INFO: ExecWithOptions: Clientset creation
    Jan 30 11:25:50.554: INFO: ExecWithOptions: execute(POST https://10.178.64.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-2497/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
    Jan 30 11:25:50.617: INFO: Exec stderr: ""
    STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true 01/30/23 11:25:50.617
    Jan 30 11:25:50.617: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2497 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 30 11:25:50.617: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    Jan 30 11:25:50.617: INFO: ExecWithOptions: Clientset creation
    Jan 30 11:25:50.617: INFO: ExecWithOptions: execute(POST https://10.178.64.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-2497/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Jan 30 11:25:50.682: INFO: Exec stderr: ""
    Jan 30 11:25:50.682: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2497 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 30 11:25:50.682: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    Jan 30 11:25:50.682: INFO: ExecWithOptions: Clientset creation
    Jan 30 11:25:50.682: INFO: ExecWithOptions: execute(POST https://10.178.64.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-2497/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Jan 30 11:25:50.774: INFO: Exec stderr: ""
    Jan 30 11:25:50.774: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2497 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 30 11:25:50.774: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    Jan 30 11:25:50.774: INFO: ExecWithOptions: Clientset creation
    Jan 30 11:25:50.774: INFO: ExecWithOptions: execute(POST https://10.178.64.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-2497/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Jan 30 11:25:50.846: INFO: Exec stderr: ""
    Jan 30 11:25:50.846: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2497 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 30 11:25:50.846: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    Jan 30 11:25:50.847: INFO: ExecWithOptions: Clientset creation
    Jan 30 11:25:50.847: INFO: ExecWithOptions: execute(POST https://10.178.64.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-2497/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Jan 30 11:25:50.913: INFO: Exec stderr: ""
    [AfterEach] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/node/init/init.go:32
    Jan 30 11:25:50.913: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
      tear down framework | framework.go:193
    STEP: Destroying namespace "e2e-kubelet-etc-hosts-2497" for this suite. 01/30/23 11:25:50.915
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:46
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 11:25:50.918
Jan 30 11:25:50.918: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename projected 01/30/23 11:25:50.919
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:25:50.925
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:25:50.927
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:46
STEP: Creating projection with secret that has name projected-secret-test-4d77cb19-3ff5-44cc-ba01-807a4a6de649 01/30/23 11:25:50.929
STEP: Creating a pod to test consume secrets 01/30/23 11:25:50.931
Jan 30 11:25:50.935: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-f9c6908b-a43e-4b17-b2c2-a3d551b0c527" in namespace "projected-675" to be "Succeeded or Failed"
Jan 30 11:25:50.937: INFO: Pod "pod-projected-secrets-f9c6908b-a43e-4b17-b2c2-a3d551b0c527": Phase="Pending", Reason="", readiness=false. Elapsed: 1.755415ms
Jan 30 11:25:52.940: INFO: Pod "pod-projected-secrets-f9c6908b-a43e-4b17-b2c2-a3d551b0c527": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004898196s
Jan 30 11:25:54.939: INFO: Pod "pod-projected-secrets-f9c6908b-a43e-4b17-b2c2-a3d551b0c527": Phase="Pending", Reason="", readiness=false. Elapsed: 4.004516126s
Jan 30 11:25:56.940: INFO: Pod "pod-projected-secrets-f9c6908b-a43e-4b17-b2c2-a3d551b0c527": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.004752347s
STEP: Saw pod success 01/30/23 11:25:56.94
Jan 30 11:25:56.940: INFO: Pod "pod-projected-secrets-f9c6908b-a43e-4b17-b2c2-a3d551b0c527" satisfied condition "Succeeded or Failed"
Jan 30 11:25:56.941: INFO: Trying to get logs from node pubt2-nks-for-dev1.dg.163.org pod pod-projected-secrets-f9c6908b-a43e-4b17-b2c2-a3d551b0c527 container projected-secret-volume-test: <nil>
STEP: delete the pod 01/30/23 11:25:56.954
Jan 30 11:25:56.958: INFO: Waiting for pod pod-projected-secrets-f9c6908b-a43e-4b17-b2c2-a3d551b0c527 to disappear
Jan 30 11:25:56.960: INFO: Pod pod-projected-secrets-f9c6908b-a43e-4b17-b2c2-a3d551b0c527 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Jan 30 11:25:56.960: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-675" for this suite. 01/30/23 11:25:56.962
------------------------------
• [SLOW TEST] [6.046 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 11:25:50.918
    Jan 30 11:25:50.918: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename projected 01/30/23 11:25:50.919
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:25:50.925
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:25:50.927
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:46
    STEP: Creating projection with secret that has name projected-secret-test-4d77cb19-3ff5-44cc-ba01-807a4a6de649 01/30/23 11:25:50.929
    STEP: Creating a pod to test consume secrets 01/30/23 11:25:50.931
    Jan 30 11:25:50.935: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-f9c6908b-a43e-4b17-b2c2-a3d551b0c527" in namespace "projected-675" to be "Succeeded or Failed"
    Jan 30 11:25:50.937: INFO: Pod "pod-projected-secrets-f9c6908b-a43e-4b17-b2c2-a3d551b0c527": Phase="Pending", Reason="", readiness=false. Elapsed: 1.755415ms
    Jan 30 11:25:52.940: INFO: Pod "pod-projected-secrets-f9c6908b-a43e-4b17-b2c2-a3d551b0c527": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004898196s
    Jan 30 11:25:54.939: INFO: Pod "pod-projected-secrets-f9c6908b-a43e-4b17-b2c2-a3d551b0c527": Phase="Pending", Reason="", readiness=false. Elapsed: 4.004516126s
    Jan 30 11:25:56.940: INFO: Pod "pod-projected-secrets-f9c6908b-a43e-4b17-b2c2-a3d551b0c527": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.004752347s
    STEP: Saw pod success 01/30/23 11:25:56.94
    Jan 30 11:25:56.940: INFO: Pod "pod-projected-secrets-f9c6908b-a43e-4b17-b2c2-a3d551b0c527" satisfied condition "Succeeded or Failed"
    Jan 30 11:25:56.941: INFO: Trying to get logs from node pubt2-nks-for-dev1.dg.163.org pod pod-projected-secrets-f9c6908b-a43e-4b17-b2c2-a3d551b0c527 container projected-secret-volume-test: <nil>
    STEP: delete the pod 01/30/23 11:25:56.954
    Jan 30 11:25:56.958: INFO: Waiting for pod pod-projected-secrets-f9c6908b-a43e-4b17-b2c2-a3d551b0c527 to disappear
    Jan 30 11:25:56.960: INFO: Pod pod-projected-secrets-f9c6908b-a43e-4b17-b2c2-a3d551b0c527 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Jan 30 11:25:56.960: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-675" for this suite. 01/30/23 11:25:56.962
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3244
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 11:25:56.966
Jan 30 11:25:56.966: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename services 01/30/23 11:25:56.967
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:25:56.973
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:25:56.975
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3244
STEP: creating an Endpoint 01/30/23 11:25:56.979
STEP: waiting for available Endpoint 01/30/23 11:25:56.981
STEP: listing all Endpoints 01/30/23 11:25:56.982
STEP: updating the Endpoint 01/30/23 11:25:56.984
STEP: fetching the Endpoint 01/30/23 11:25:56.987
STEP: patching the Endpoint 01/30/23 11:25:56.988
STEP: fetching the Endpoint 01/30/23 11:25:56.994
STEP: deleting the Endpoint by Collection 01/30/23 11:25:56.997
STEP: waiting for Endpoint deletion 01/30/23 11:25:56.999
STEP: fetching the Endpoint 01/30/23 11:25:57
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jan 30 11:25:57.002: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-9175" for this suite. 01/30/23 11:25:57.004
------------------------------
• [0.040 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3244

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 11:25:56.966
    Jan 30 11:25:56.966: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename services 01/30/23 11:25:56.967
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:25:56.973
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:25:56.975
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should test the lifecycle of an Endpoint [Conformance]
      test/e2e/network/service.go:3244
    STEP: creating an Endpoint 01/30/23 11:25:56.979
    STEP: waiting for available Endpoint 01/30/23 11:25:56.981
    STEP: listing all Endpoints 01/30/23 11:25:56.982
    STEP: updating the Endpoint 01/30/23 11:25:56.984
    STEP: fetching the Endpoint 01/30/23 11:25:56.987
    STEP: patching the Endpoint 01/30/23 11:25:56.988
    STEP: fetching the Endpoint 01/30/23 11:25:56.994
    STEP: deleting the Endpoint by Collection 01/30/23 11:25:56.997
    STEP: waiting for Endpoint deletion 01/30/23 11:25:56.999
    STEP: fetching the Endpoint 01/30/23 11:25:57
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jan 30 11:25:57.002: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-9175" for this suite. 01/30/23 11:25:57.004
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with projected pod [Conformance]
  test/e2e/storage/subpath.go:106
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 11:25:57.007
Jan 30 11:25:57.007: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename subpath 01/30/23 11:25:57.007
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:25:57.013
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:25:57.015
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 01/30/23 11:25:57.017
[It] should support subpaths with projected pod [Conformance]
  test/e2e/storage/subpath.go:106
STEP: Creating pod pod-subpath-test-projected-cq4z 01/30/23 11:25:57.021
STEP: Creating a pod to test atomic-volume-subpath 01/30/23 11:25:57.021
Jan 30 11:25:57.025: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-cq4z" in namespace "subpath-2630" to be "Succeeded or Failed"
Jan 30 11:25:57.027: INFO: Pod "pod-subpath-test-projected-cq4z": Phase="Pending", Reason="", readiness=false. Elapsed: 1.631939ms
Jan 30 11:25:59.029: INFO: Pod "pod-subpath-test-projected-cq4z": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004161713s
Jan 30 11:26:01.030: INFO: Pod "pod-subpath-test-projected-cq4z": Phase="Running", Reason="", readiness=true. Elapsed: 4.005156707s
Jan 30 11:26:03.030: INFO: Pod "pod-subpath-test-projected-cq4z": Phase="Running", Reason="", readiness=true. Elapsed: 6.004802197s
Jan 30 11:26:05.031: INFO: Pod "pod-subpath-test-projected-cq4z": Phase="Running", Reason="", readiness=true. Elapsed: 8.005947597s
Jan 30 11:26:07.029: INFO: Pod "pod-subpath-test-projected-cq4z": Phase="Running", Reason="", readiness=true. Elapsed: 10.004123864s
Jan 30 11:26:09.029: INFO: Pod "pod-subpath-test-projected-cq4z": Phase="Running", Reason="", readiness=true. Elapsed: 12.004033578s
Jan 30 11:26:11.030: INFO: Pod "pod-subpath-test-projected-cq4z": Phase="Running", Reason="", readiness=true. Elapsed: 14.004911917s
Jan 30 11:26:13.030: INFO: Pod "pod-subpath-test-projected-cq4z": Phase="Running", Reason="", readiness=true. Elapsed: 16.004446678s
Jan 30 11:26:15.031: INFO: Pod "pod-subpath-test-projected-cq4z": Phase="Running", Reason="", readiness=true. Elapsed: 18.005369141s
Jan 30 11:26:17.030: INFO: Pod "pod-subpath-test-projected-cq4z": Phase="Running", Reason="", readiness=true. Elapsed: 20.004556444s
Jan 30 11:26:19.030: INFO: Pod "pod-subpath-test-projected-cq4z": Phase="Running", Reason="", readiness=true. Elapsed: 22.004875308s
Jan 30 11:26:21.029: INFO: Pod "pod-subpath-test-projected-cq4z": Phase="Running", Reason="", readiness=false. Elapsed: 24.0042357s
Jan 30 11:26:23.029: INFO: Pod "pod-subpath-test-projected-cq4z": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.004186173s
STEP: Saw pod success 01/30/23 11:26:23.029
Jan 30 11:26:23.030: INFO: Pod "pod-subpath-test-projected-cq4z" satisfied condition "Succeeded or Failed"
Jan 30 11:26:23.032: INFO: Trying to get logs from node pubt2-nks-for-dev1.dg.163.org pod pod-subpath-test-projected-cq4z container test-container-subpath-projected-cq4z: <nil>
STEP: delete the pod 01/30/23 11:26:23.037
Jan 30 11:26:23.042: INFO: Waiting for pod pod-subpath-test-projected-cq4z to disappear
Jan 30 11:26:23.043: INFO: Pod pod-subpath-test-projected-cq4z no longer exists
STEP: Deleting pod pod-subpath-test-projected-cq4z 01/30/23 11:26:23.043
Jan 30 11:26:23.043: INFO: Deleting pod "pod-subpath-test-projected-cq4z" in namespace "subpath-2630"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Jan 30 11:26:23.044: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-2630" for this suite. 01/30/23 11:26:23.047
------------------------------
• [SLOW TEST] [26.042 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with projected pod [Conformance]
    test/e2e/storage/subpath.go:106

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 11:25:57.007
    Jan 30 11:25:57.007: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename subpath 01/30/23 11:25:57.007
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:25:57.013
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:25:57.015
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 01/30/23 11:25:57.017
    [It] should support subpaths with projected pod [Conformance]
      test/e2e/storage/subpath.go:106
    STEP: Creating pod pod-subpath-test-projected-cq4z 01/30/23 11:25:57.021
    STEP: Creating a pod to test atomic-volume-subpath 01/30/23 11:25:57.021
    Jan 30 11:25:57.025: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-cq4z" in namespace "subpath-2630" to be "Succeeded or Failed"
    Jan 30 11:25:57.027: INFO: Pod "pod-subpath-test-projected-cq4z": Phase="Pending", Reason="", readiness=false. Elapsed: 1.631939ms
    Jan 30 11:25:59.029: INFO: Pod "pod-subpath-test-projected-cq4z": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004161713s
    Jan 30 11:26:01.030: INFO: Pod "pod-subpath-test-projected-cq4z": Phase="Running", Reason="", readiness=true. Elapsed: 4.005156707s
    Jan 30 11:26:03.030: INFO: Pod "pod-subpath-test-projected-cq4z": Phase="Running", Reason="", readiness=true. Elapsed: 6.004802197s
    Jan 30 11:26:05.031: INFO: Pod "pod-subpath-test-projected-cq4z": Phase="Running", Reason="", readiness=true. Elapsed: 8.005947597s
    Jan 30 11:26:07.029: INFO: Pod "pod-subpath-test-projected-cq4z": Phase="Running", Reason="", readiness=true. Elapsed: 10.004123864s
    Jan 30 11:26:09.029: INFO: Pod "pod-subpath-test-projected-cq4z": Phase="Running", Reason="", readiness=true. Elapsed: 12.004033578s
    Jan 30 11:26:11.030: INFO: Pod "pod-subpath-test-projected-cq4z": Phase="Running", Reason="", readiness=true. Elapsed: 14.004911917s
    Jan 30 11:26:13.030: INFO: Pod "pod-subpath-test-projected-cq4z": Phase="Running", Reason="", readiness=true. Elapsed: 16.004446678s
    Jan 30 11:26:15.031: INFO: Pod "pod-subpath-test-projected-cq4z": Phase="Running", Reason="", readiness=true. Elapsed: 18.005369141s
    Jan 30 11:26:17.030: INFO: Pod "pod-subpath-test-projected-cq4z": Phase="Running", Reason="", readiness=true. Elapsed: 20.004556444s
    Jan 30 11:26:19.030: INFO: Pod "pod-subpath-test-projected-cq4z": Phase="Running", Reason="", readiness=true. Elapsed: 22.004875308s
    Jan 30 11:26:21.029: INFO: Pod "pod-subpath-test-projected-cq4z": Phase="Running", Reason="", readiness=false. Elapsed: 24.0042357s
    Jan 30 11:26:23.029: INFO: Pod "pod-subpath-test-projected-cq4z": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.004186173s
    STEP: Saw pod success 01/30/23 11:26:23.029
    Jan 30 11:26:23.030: INFO: Pod "pod-subpath-test-projected-cq4z" satisfied condition "Succeeded or Failed"
    Jan 30 11:26:23.032: INFO: Trying to get logs from node pubt2-nks-for-dev1.dg.163.org pod pod-subpath-test-projected-cq4z container test-container-subpath-projected-cq4z: <nil>
    STEP: delete the pod 01/30/23 11:26:23.037
    Jan 30 11:26:23.042: INFO: Waiting for pod pod-subpath-test-projected-cq4z to disappear
    Jan 30 11:26:23.043: INFO: Pod pod-subpath-test-projected-cq4z no longer exists
    STEP: Deleting pod pod-subpath-test-projected-cq4z 01/30/23 11:26:23.043
    Jan 30 11:26:23.043: INFO: Deleting pod "pod-subpath-test-projected-cq4z" in namespace "subpath-2630"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Jan 30 11:26:23.044: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-2630" for this suite. 01/30/23 11:26:23.047
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:215
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 11:26:23.05
Jan 30 11:26:23.050: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename projected 01/30/23 11:26:23.051
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:26:23.057
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:26:23.059
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:215
STEP: Creating secret with name s-test-opt-del-22dcfb58-72a0-43dd-966b-4ba6ad3f8f5e 01/30/23 11:26:23.063
STEP: Creating secret with name s-test-opt-upd-05302419-ef5a-425c-b070-4c00d37cd5a2 01/30/23 11:26:23.065
STEP: Creating the pod 01/30/23 11:26:23.069
Jan 30 11:26:23.074: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-ac7626f2-65b5-4d0c-a18e-03ac37c4b5c2" in namespace "projected-8896" to be "running and ready"
Jan 30 11:26:23.075: INFO: Pod "pod-projected-secrets-ac7626f2-65b5-4d0c-a18e-03ac37c4b5c2": Phase="Pending", Reason="", readiness=false. Elapsed: 1.565185ms
Jan 30 11:26:23.075: INFO: The phase of Pod pod-projected-secrets-ac7626f2-65b5-4d0c-a18e-03ac37c4b5c2 is Pending, waiting for it to be Running (with Ready = true)
Jan 30 11:26:25.079: INFO: Pod "pod-projected-secrets-ac7626f2-65b5-4d0c-a18e-03ac37c4b5c2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004675671s
Jan 30 11:26:25.079: INFO: The phase of Pod pod-projected-secrets-ac7626f2-65b5-4d0c-a18e-03ac37c4b5c2 is Pending, waiting for it to be Running (with Ready = true)
Jan 30 11:26:27.078: INFO: Pod "pod-projected-secrets-ac7626f2-65b5-4d0c-a18e-03ac37c4b5c2": Phase="Running", Reason="", readiness=true. Elapsed: 4.004076064s
Jan 30 11:26:27.078: INFO: The phase of Pod pod-projected-secrets-ac7626f2-65b5-4d0c-a18e-03ac37c4b5c2 is Running (Ready = true)
Jan 30 11:26:27.078: INFO: Pod "pod-projected-secrets-ac7626f2-65b5-4d0c-a18e-03ac37c4b5c2" satisfied condition "running and ready"
STEP: Deleting secret s-test-opt-del-22dcfb58-72a0-43dd-966b-4ba6ad3f8f5e 01/30/23 11:26:27.092
STEP: Updating secret s-test-opt-upd-05302419-ef5a-425c-b070-4c00d37cd5a2 01/30/23 11:26:27.094
STEP: Creating secret with name s-test-opt-create-b5c908e1-2f28-4e99-9492-93218ef9e1e2 01/30/23 11:26:27.096
STEP: waiting to observe update in volume 01/30/23 11:26:27.098
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Jan 30 11:27:41.365: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-8896" for this suite. 01/30/23 11:27:41.368
------------------------------
• [SLOW TEST] [78.320 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:215

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 11:26:23.05
    Jan 30 11:26:23.050: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename projected 01/30/23 11:26:23.051
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:26:23.057
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:26:23.059
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:215
    STEP: Creating secret with name s-test-opt-del-22dcfb58-72a0-43dd-966b-4ba6ad3f8f5e 01/30/23 11:26:23.063
    STEP: Creating secret with name s-test-opt-upd-05302419-ef5a-425c-b070-4c00d37cd5a2 01/30/23 11:26:23.065
    STEP: Creating the pod 01/30/23 11:26:23.069
    Jan 30 11:26:23.074: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-ac7626f2-65b5-4d0c-a18e-03ac37c4b5c2" in namespace "projected-8896" to be "running and ready"
    Jan 30 11:26:23.075: INFO: Pod "pod-projected-secrets-ac7626f2-65b5-4d0c-a18e-03ac37c4b5c2": Phase="Pending", Reason="", readiness=false. Elapsed: 1.565185ms
    Jan 30 11:26:23.075: INFO: The phase of Pod pod-projected-secrets-ac7626f2-65b5-4d0c-a18e-03ac37c4b5c2 is Pending, waiting for it to be Running (with Ready = true)
    Jan 30 11:26:25.079: INFO: Pod "pod-projected-secrets-ac7626f2-65b5-4d0c-a18e-03ac37c4b5c2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004675671s
    Jan 30 11:26:25.079: INFO: The phase of Pod pod-projected-secrets-ac7626f2-65b5-4d0c-a18e-03ac37c4b5c2 is Pending, waiting for it to be Running (with Ready = true)
    Jan 30 11:26:27.078: INFO: Pod "pod-projected-secrets-ac7626f2-65b5-4d0c-a18e-03ac37c4b5c2": Phase="Running", Reason="", readiness=true. Elapsed: 4.004076064s
    Jan 30 11:26:27.078: INFO: The phase of Pod pod-projected-secrets-ac7626f2-65b5-4d0c-a18e-03ac37c4b5c2 is Running (Ready = true)
    Jan 30 11:26:27.078: INFO: Pod "pod-projected-secrets-ac7626f2-65b5-4d0c-a18e-03ac37c4b5c2" satisfied condition "running and ready"
    STEP: Deleting secret s-test-opt-del-22dcfb58-72a0-43dd-966b-4ba6ad3f8f5e 01/30/23 11:26:27.092
    STEP: Updating secret s-test-opt-upd-05302419-ef5a-425c-b070-4c00d37cd5a2 01/30/23 11:26:27.094
    STEP: Creating secret with name s-test-opt-create-b5c908e1-2f28-4e99-9492-93218ef9e1e2 01/30/23 11:26:27.096
    STEP: waiting to observe update in volume 01/30/23 11:26:27.098
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Jan 30 11:27:41.365: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-8896" for this suite. 01/30/23 11:27:41.368
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
  should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 11:27:41.371
Jan 30 11:27:41.371: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename runtimeclass 01/30/23 11:27:41.372
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:27:41.378
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:27:41.38
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It] should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104
Jan 30 11:27:41.387: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-4172 to be scheduled
Jan 30 11:27:41.389: INFO: 1 pods are not scheduled: [runtimeclass-4172/test-runtimeclass-runtimeclass-4172-preconfigured-handler-zlgld(009a1ada-4c88-4eca-a023-3e6d7b5bc814)]
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Jan 30 11:27:43.394: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-4172" for this suite. 01/30/23 11:27:43.396
------------------------------
• [2.027 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 11:27:41.371
    Jan 30 11:27:41.371: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename runtimeclass 01/30/23 11:27:41.372
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:27:41.378
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:27:41.38
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It] should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:104
    Jan 30 11:27:41.387: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-4172 to be scheduled
    Jan 30 11:27:41.389: INFO: 1 pods are not scheduled: [runtimeclass-4172/test-runtimeclass-runtimeclass-4172-preconfigured-handler-zlgld(009a1ada-4c88-4eca-a023-3e6d7b5bc814)]
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Jan 30 11:27:43.394: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-4172" for this suite. 01/30/23 11:27:43.396
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-network] Ingress API
  should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552
[BeforeEach] [sig-network] Ingress API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 11:27:43.399
Jan 30 11:27:43.399: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename ingress 01/30/23 11:27:43.4
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:27:43.405
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:27:43.407
[BeforeEach] [sig-network] Ingress API
  test/e2e/framework/metrics/init/init.go:31
[It] should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552
STEP: getting /apis 01/30/23 11:27:43.409
STEP: getting /apis/networking.k8s.io 01/30/23 11:27:43.41
STEP: getting /apis/networking.k8s.iov1 01/30/23 11:27:43.411
STEP: creating 01/30/23 11:27:43.412
STEP: getting 01/30/23 11:27:43.419
STEP: listing 01/30/23 11:27:43.42
STEP: watching 01/30/23 11:27:43.422
Jan 30 11:27:43.422: INFO: starting watch
STEP: cluster-wide listing 01/30/23 11:27:43.423
STEP: cluster-wide watching 01/30/23 11:27:43.424
Jan 30 11:27:43.424: INFO: starting watch
STEP: patching 01/30/23 11:27:43.425
STEP: updating 01/30/23 11:27:43.428
Jan 30 11:27:43.432: INFO: waiting for watch events with expected annotations
Jan 30 11:27:43.432: INFO: saw patched and updated annotations
STEP: patching /status 01/30/23 11:27:43.432
STEP: updating /status 01/30/23 11:27:43.435
STEP: get /status 01/30/23 11:27:43.439
STEP: deleting 01/30/23 11:27:43.441
STEP: deleting a collection 01/30/23 11:27:43.446
[AfterEach] [sig-network] Ingress API
  test/e2e/framework/node/init/init.go:32
Jan 30 11:27:43.451: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Ingress API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Ingress API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Ingress API
  tear down framework | framework.go:193
STEP: Destroying namespace "ingress-5726" for this suite. 01/30/23 11:27:43.453
------------------------------
• [0.057 seconds]
[sig-network] Ingress API
test/e2e/network/common/framework.go:23
  should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Ingress API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 11:27:43.399
    Jan 30 11:27:43.399: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename ingress 01/30/23 11:27:43.4
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:27:43.405
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:27:43.407
    [BeforeEach] [sig-network] Ingress API
      test/e2e/framework/metrics/init/init.go:31
    [It] should support creating Ingress API operations [Conformance]
      test/e2e/network/ingress.go:552
    STEP: getting /apis 01/30/23 11:27:43.409
    STEP: getting /apis/networking.k8s.io 01/30/23 11:27:43.41
    STEP: getting /apis/networking.k8s.iov1 01/30/23 11:27:43.411
    STEP: creating 01/30/23 11:27:43.412
    STEP: getting 01/30/23 11:27:43.419
    STEP: listing 01/30/23 11:27:43.42
    STEP: watching 01/30/23 11:27:43.422
    Jan 30 11:27:43.422: INFO: starting watch
    STEP: cluster-wide listing 01/30/23 11:27:43.423
    STEP: cluster-wide watching 01/30/23 11:27:43.424
    Jan 30 11:27:43.424: INFO: starting watch
    STEP: patching 01/30/23 11:27:43.425
    STEP: updating 01/30/23 11:27:43.428
    Jan 30 11:27:43.432: INFO: waiting for watch events with expected annotations
    Jan 30 11:27:43.432: INFO: saw patched and updated annotations
    STEP: patching /status 01/30/23 11:27:43.432
    STEP: updating /status 01/30/23 11:27:43.435
    STEP: get /status 01/30/23 11:27:43.439
    STEP: deleting 01/30/23 11:27:43.441
    STEP: deleting a collection 01/30/23 11:27:43.446
    [AfterEach] [sig-network] Ingress API
      test/e2e/framework/node/init/init.go:32
    Jan 30 11:27:43.451: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Ingress API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Ingress API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Ingress API
      tear down framework | framework.go:193
    STEP: Destroying namespace "ingress-5726" for this suite. 01/30/23 11:27:43.453
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:458
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 11:27:43.456
Jan 30 11:27:43.456: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename init-container 01/30/23 11:27:43.456
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:27:43.462
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:27:43.464
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:165
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:458
STEP: creating the pod 01/30/23 11:27:43.466
Jan 30 11:27:43.466: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Jan 30 11:27:47.878: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "init-container-2803" for this suite. 01/30/23 11:27:47.881
------------------------------
• [4.427 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:458

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 11:27:43.456
    Jan 30 11:27:43.456: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename init-container 01/30/23 11:27:43.456
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:27:43.462
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:27:43.464
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:165
    [It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
      test/e2e/common/node/init_container.go:458
    STEP: creating the pod 01/30/23 11:27:43.466
    Jan 30 11:27:43.466: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Jan 30 11:27:47.878: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "init-container-2803" for this suite. 01/30/23 11:27:47.881
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  test/e2e/apps/statefulset.go:697
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 11:27:47.884
Jan 30 11:27:47.884: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename statefulset 01/30/23 11:27:47.884
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:27:47.89
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:27:47.892
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-7432 01/30/23 11:27:47.894
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  test/e2e/apps/statefulset.go:697
STEP: Creating stateful set ss in namespace statefulset-7432 01/30/23 11:27:47.896
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-7432 01/30/23 11:27:47.899
Jan 30 11:27:47.901: INFO: Found 0 stateful pods, waiting for 1
Jan 30 11:27:57.904: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod 01/30/23 11:27:57.904
Jan 30 11:27:57.907: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=statefulset-7432 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 30 11:27:58.061: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 30 11:27:58.061: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 30 11:27:58.061: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 30 11:27:58.064: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Jan 30 11:28:08.067: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jan 30 11:28:08.067: INFO: Waiting for statefulset status.replicas updated to 0
Jan 30 11:28:08.076: INFO: POD   NODE                           PHASE    GRACE  CONDITIONS
Jan 30 11:28:08.076: INFO: ss-0  pubt2-nks-for-dev1.dg.163.org  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-30 11:27:47 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-30 11:27:58 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-30 11:27:58 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-30 11:27:47 +0000 UTC  }]
Jan 30 11:28:08.076: INFO: 
Jan 30 11:28:08.076: INFO: StatefulSet ss has not reached scale 3, at 1
Jan 30 11:28:09.079: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.997917248s
Jan 30 11:28:10.083: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.993834585s
Jan 30 11:28:11.086: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.990798877s
Jan 30 11:28:12.090: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.987597855s
Jan 30 11:28:13.093: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.983857084s
Jan 30 11:28:14.097: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.980065819s
Jan 30 11:28:15.100: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.976663602s
Jan 30 11:28:16.103: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.973998626s
Jan 30 11:28:17.106: INFO: Verifying statefulset ss doesn't scale past 3 for another 971.204833ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-7432 01/30/23 11:28:18.107
Jan 30 11:28:18.110: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=statefulset-7432 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 30 11:28:18.270: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan 30 11:28:18.270: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 30 11:28:18.270: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 30 11:28:18.270: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=statefulset-7432 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 30 11:28:18.438: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Jan 30 11:28:18.438: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 30 11:28:18.438: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 30 11:28:18.438: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=statefulset-7432 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 30 11:28:18.564: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Jan 30 11:28:18.564: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 30 11:28:18.564: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 30 11:28:18.566: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
Jan 30 11:28:28.569: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jan 30 11:28:28.569: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Jan 30 11:28:28.569: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod 01/30/23 11:28:28.569
Jan 30 11:28:28.571: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=statefulset-7432 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 30 11:28:28.728: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 30 11:28:28.728: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 30 11:28:28.728: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 30 11:28:28.728: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=statefulset-7432 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 30 11:28:28.875: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 30 11:28:28.875: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 30 11:28:28.875: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 30 11:28:28.875: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=statefulset-7432 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 30 11:28:29.008: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 30 11:28:29.008: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 30 11:28:29.008: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 30 11:28:29.008: INFO: Waiting for statefulset status.replicas updated to 0
Jan 30 11:28:29.010: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Jan 30 11:28:39.015: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jan 30 11:28:39.015: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Jan 30 11:28:39.015: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Jan 30 11:28:39.022: INFO: POD   NODE                           PHASE    GRACE  CONDITIONS
Jan 30 11:28:39.022: INFO: ss-0  pubt2-nks-for-dev1.dg.163.org  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-30 11:27:47 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-30 11:28:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-30 11:28:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-30 11:27:47 +0000 UTC  }]
Jan 30 11:28:39.022: INFO: ss-1  pubt2-nks-for-dev3.dg.163.org  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-30 11:28:08 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-30 11:28:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-30 11:28:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-30 11:28:08 +0000 UTC  }]
Jan 30 11:28:39.022: INFO: ss-2  pubt2-nks-for-dev1.dg.163.org  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-30 11:28:08 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-30 11:28:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-30 11:28:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-30 11:28:08 +0000 UTC  }]
Jan 30 11:28:39.022: INFO: 
Jan 30 11:28:39.022: INFO: StatefulSet ss has not reached scale 0, at 3
Jan 30 11:28:40.026: INFO: POD   NODE                           PHASE    GRACE  CONDITIONS
Jan 30 11:28:40.026: INFO: ss-0  pubt2-nks-for-dev1.dg.163.org  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-30 11:27:47 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-30 11:28:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-30 11:28:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-30 11:27:47 +0000 UTC  }]
Jan 30 11:28:40.026: INFO: ss-1  pubt2-nks-for-dev3.dg.163.org  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-30 11:28:08 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-30 11:28:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-30 11:28:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-30 11:28:08 +0000 UTC  }]
Jan 30 11:28:40.026: INFO: ss-2  pubt2-nks-for-dev1.dg.163.org  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-30 11:28:08 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-30 11:28:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-30 11:28:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-30 11:28:08 +0000 UTC  }]
Jan 30 11:28:40.026: INFO: 
Jan 30 11:28:40.026: INFO: StatefulSet ss has not reached scale 0, at 3
Jan 30 11:28:41.028: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.9938859s
Jan 30 11:28:42.030: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.992067729s
Jan 30 11:28:43.032: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.98990562s
Jan 30 11:28:44.034: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.988023167s
Jan 30 11:28:45.036: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.985698s
Jan 30 11:28:46.039: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.98282365s
Jan 30 11:28:47.041: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.980692069s
Jan 30 11:28:48.043: INFO: Verifying statefulset ss doesn't scale past 0 for another 978.856726ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-7432 01/30/23 11:28:49.043
Jan 30 11:28:49.045: INFO: Scaling statefulset ss to 0
Jan 30 11:28:49.052: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Jan 30 11:28:49.054: INFO: Deleting all statefulset in ns statefulset-7432
Jan 30 11:28:49.055: INFO: Scaling statefulset ss to 0
Jan 30 11:28:49.061: INFO: Waiting for statefulset status.replicas updated to 0
Jan 30 11:28:49.063: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Jan 30 11:28:49.068: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-7432" for this suite. 01/30/23 11:28:49.07
------------------------------
• [SLOW TEST] [61.189 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    test/e2e/apps/statefulset.go:697

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 11:27:47.884
    Jan 30 11:27:47.884: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename statefulset 01/30/23 11:27:47.884
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:27:47.89
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:27:47.892
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-7432 01/30/23 11:27:47.894
    [It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
      test/e2e/apps/statefulset.go:697
    STEP: Creating stateful set ss in namespace statefulset-7432 01/30/23 11:27:47.896
    STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-7432 01/30/23 11:27:47.899
    Jan 30 11:27:47.901: INFO: Found 0 stateful pods, waiting for 1
    Jan 30 11:27:57.904: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod 01/30/23 11:27:57.904
    Jan 30 11:27:57.907: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=statefulset-7432 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan 30 11:27:58.061: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan 30 11:27:58.061: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan 30 11:27:58.061: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jan 30 11:27:58.064: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
    Jan 30 11:28:08.067: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Jan 30 11:28:08.067: INFO: Waiting for statefulset status.replicas updated to 0
    Jan 30 11:28:08.076: INFO: POD   NODE                           PHASE    GRACE  CONDITIONS
    Jan 30 11:28:08.076: INFO: ss-0  pubt2-nks-for-dev1.dg.163.org  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-30 11:27:47 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-30 11:27:58 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-30 11:27:58 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-30 11:27:47 +0000 UTC  }]
    Jan 30 11:28:08.076: INFO: 
    Jan 30 11:28:08.076: INFO: StatefulSet ss has not reached scale 3, at 1
    Jan 30 11:28:09.079: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.997917248s
    Jan 30 11:28:10.083: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.993834585s
    Jan 30 11:28:11.086: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.990798877s
    Jan 30 11:28:12.090: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.987597855s
    Jan 30 11:28:13.093: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.983857084s
    Jan 30 11:28:14.097: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.980065819s
    Jan 30 11:28:15.100: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.976663602s
    Jan 30 11:28:16.103: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.973998626s
    Jan 30 11:28:17.106: INFO: Verifying statefulset ss doesn't scale past 3 for another 971.204833ms
    STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-7432 01/30/23 11:28:18.107
    Jan 30 11:28:18.110: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=statefulset-7432 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan 30 11:28:18.270: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Jan 30 11:28:18.270: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jan 30 11:28:18.270: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jan 30 11:28:18.270: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=statefulset-7432 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan 30 11:28:18.438: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
    Jan 30 11:28:18.438: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jan 30 11:28:18.438: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jan 30 11:28:18.438: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=statefulset-7432 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan 30 11:28:18.564: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
    Jan 30 11:28:18.564: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jan 30 11:28:18.564: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jan 30 11:28:18.566: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
    Jan 30 11:28:28.569: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Jan 30 11:28:28.569: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
    Jan 30 11:28:28.569: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Scale down will not halt with unhealthy stateful pod 01/30/23 11:28:28.569
    Jan 30 11:28:28.571: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=statefulset-7432 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan 30 11:28:28.728: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan 30 11:28:28.728: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan 30 11:28:28.728: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jan 30 11:28:28.728: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=statefulset-7432 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan 30 11:28:28.875: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan 30 11:28:28.875: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan 30 11:28:28.875: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jan 30 11:28:28.875: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=statefulset-7432 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan 30 11:28:29.008: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan 30 11:28:29.008: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan 30 11:28:29.008: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jan 30 11:28:29.008: INFO: Waiting for statefulset status.replicas updated to 0
    Jan 30 11:28:29.010: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
    Jan 30 11:28:39.015: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Jan 30 11:28:39.015: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
    Jan 30 11:28:39.015: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
    Jan 30 11:28:39.022: INFO: POD   NODE                           PHASE    GRACE  CONDITIONS
    Jan 30 11:28:39.022: INFO: ss-0  pubt2-nks-for-dev1.dg.163.org  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-30 11:27:47 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-30 11:28:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-30 11:28:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-30 11:27:47 +0000 UTC  }]
    Jan 30 11:28:39.022: INFO: ss-1  pubt2-nks-for-dev3.dg.163.org  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-30 11:28:08 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-30 11:28:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-30 11:28:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-30 11:28:08 +0000 UTC  }]
    Jan 30 11:28:39.022: INFO: ss-2  pubt2-nks-for-dev1.dg.163.org  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-30 11:28:08 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-30 11:28:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-30 11:28:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-30 11:28:08 +0000 UTC  }]
    Jan 30 11:28:39.022: INFO: 
    Jan 30 11:28:39.022: INFO: StatefulSet ss has not reached scale 0, at 3
    Jan 30 11:28:40.026: INFO: POD   NODE                           PHASE    GRACE  CONDITIONS
    Jan 30 11:28:40.026: INFO: ss-0  pubt2-nks-for-dev1.dg.163.org  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-30 11:27:47 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-30 11:28:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-30 11:28:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-30 11:27:47 +0000 UTC  }]
    Jan 30 11:28:40.026: INFO: ss-1  pubt2-nks-for-dev3.dg.163.org  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-30 11:28:08 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-30 11:28:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-30 11:28:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-30 11:28:08 +0000 UTC  }]
    Jan 30 11:28:40.026: INFO: ss-2  pubt2-nks-for-dev1.dg.163.org  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-30 11:28:08 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-30 11:28:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-30 11:28:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-30 11:28:08 +0000 UTC  }]
    Jan 30 11:28:40.026: INFO: 
    Jan 30 11:28:40.026: INFO: StatefulSet ss has not reached scale 0, at 3
    Jan 30 11:28:41.028: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.9938859s
    Jan 30 11:28:42.030: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.992067729s
    Jan 30 11:28:43.032: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.98990562s
    Jan 30 11:28:44.034: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.988023167s
    Jan 30 11:28:45.036: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.985698s
    Jan 30 11:28:46.039: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.98282365s
    Jan 30 11:28:47.041: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.980692069s
    Jan 30 11:28:48.043: INFO: Verifying statefulset ss doesn't scale past 0 for another 978.856726ms
    STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-7432 01/30/23 11:28:49.043
    Jan 30 11:28:49.045: INFO: Scaling statefulset ss to 0
    Jan 30 11:28:49.052: INFO: Waiting for statefulset status.replicas updated to 0
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Jan 30 11:28:49.054: INFO: Deleting all statefulset in ns statefulset-7432
    Jan 30 11:28:49.055: INFO: Scaling statefulset ss to 0
    Jan 30 11:28:49.061: INFO: Waiting for statefulset status.replicas updated to 0
    Jan 30 11:28:49.063: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Jan 30 11:28:49.068: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-7432" for this suite. 01/30/23 11:28:49.07
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:92
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 11:28:49.073
Jan 30 11:28:49.073: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename replication-controller 01/30/23 11:28:49.074
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:28:49.082
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:28:49.084
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:92
STEP: Given a Pod with a 'name' label pod-adoption is created 01/30/23 11:28:49.086
Jan 30 11:28:49.090: INFO: Waiting up to 5m0s for pod "pod-adoption" in namespace "replication-controller-4960" to be "running and ready"
Jan 30 11:28:49.092: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 1.536397ms
Jan 30 11:28:49.092: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
Jan 30 11:28:51.095: INFO: Pod "pod-adoption": Phase="Running", Reason="", readiness=true. Elapsed: 2.004557038s
Jan 30 11:28:51.095: INFO: The phase of Pod pod-adoption is Running (Ready = true)
Jan 30 11:28:51.095: INFO: Pod "pod-adoption" satisfied condition "running and ready"
STEP: When a replication controller with a matching selector is created 01/30/23 11:28:51.097
STEP: Then the orphan pod is adopted 01/30/23 11:28:51.1
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Jan 30 11:28:52.103: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-4960" for this suite. 01/30/23 11:28:52.106
------------------------------
• [3.035 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 11:28:49.073
    Jan 30 11:28:49.073: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename replication-controller 01/30/23 11:28:49.074
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:28:49.082
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:28:49.084
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should adopt matching pods on creation [Conformance]
      test/e2e/apps/rc.go:92
    STEP: Given a Pod with a 'name' label pod-adoption is created 01/30/23 11:28:49.086
    Jan 30 11:28:49.090: INFO: Waiting up to 5m0s for pod "pod-adoption" in namespace "replication-controller-4960" to be "running and ready"
    Jan 30 11:28:49.092: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 1.536397ms
    Jan 30 11:28:49.092: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
    Jan 30 11:28:51.095: INFO: Pod "pod-adoption": Phase="Running", Reason="", readiness=true. Elapsed: 2.004557038s
    Jan 30 11:28:51.095: INFO: The phase of Pod pod-adoption is Running (Ready = true)
    Jan 30 11:28:51.095: INFO: Pod "pod-adoption" satisfied condition "running and ready"
    STEP: When a replication controller with a matching selector is created 01/30/23 11:28:51.097
    STEP: Then the orphan pod is adopted 01/30/23 11:28:51.1
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Jan 30 11:28:52.103: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-4960" for this suite. 01/30/23 11:28:52.106
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 11:28:52.109
Jan 30 11:28:52.109: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename replicaset 01/30/23 11:28:52.11
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:28:52.116
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:28:52.118
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165
STEP: Create a ReplicaSet 01/30/23 11:28:52.12
STEP: Verify that the required pods have come up 01/30/23 11:28:52.122
Jan 30 11:28:52.124: INFO: Pod name sample-pod: Found 0 pods out of 3
Jan 30 11:28:57.127: INFO: Pod name sample-pod: Found 3 pods out of 3
STEP: ensuring each pod is running 01/30/23 11:28:57.127
Jan 30 11:28:57.129: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
STEP: Listing all ReplicaSets 01/30/23 11:28:57.129
STEP: DeleteCollection of the ReplicaSets 01/30/23 11:28:57.131
STEP: After DeleteCollection verify that ReplicaSets have been deleted 01/30/23 11:28:57.134
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Jan 30 11:28:57.136: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-7697" for this suite. 01/30/23 11:28:57.138
------------------------------
• [SLOW TEST] [5.031 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 11:28:52.109
    Jan 30 11:28:52.109: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename replicaset 01/30/23 11:28:52.11
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:28:52.116
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:28:52.118
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] should list and delete a collection of ReplicaSets [Conformance]
      test/e2e/apps/replica_set.go:165
    STEP: Create a ReplicaSet 01/30/23 11:28:52.12
    STEP: Verify that the required pods have come up 01/30/23 11:28:52.122
    Jan 30 11:28:52.124: INFO: Pod name sample-pod: Found 0 pods out of 3
    Jan 30 11:28:57.127: INFO: Pod name sample-pod: Found 3 pods out of 3
    STEP: ensuring each pod is running 01/30/23 11:28:57.127
    Jan 30 11:28:57.129: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
    STEP: Listing all ReplicaSets 01/30/23 11:28:57.129
    STEP: DeleteCollection of the ReplicaSets 01/30/23 11:28:57.131
    STEP: After DeleteCollection verify that ReplicaSets have been deleted 01/30/23 11:28:57.134
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Jan 30 11:28:57.136: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-7697" for this suite. 01/30/23 11:28:57.138
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 11:28:57.141
Jan 30 11:28:57.141: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename deployment 01/30/23 11:28:57.142
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:28:57.15
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:28:57.152
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479
STEP: creating a Deployment 01/30/23 11:28:57.156
Jan 30 11:28:57.156: INFO: Creating simple deployment test-deployment-44lv8
Jan 30 11:28:57.161: INFO: new replicaset for deployment "test-deployment-44lv8" is yet to be created
STEP: Getting /status 01/30/23 11:28:59.169
Jan 30 11:28:59.173: INFO: Deployment test-deployment-44lv8 has Conditions: [{Available True 2023-01-30 11:28:58 +0000 UTC 2023-01-30 11:28:58 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2023-01-30 11:28:58 +0000 UTC 2023-01-30 11:28:57 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-44lv8-66b965764f" has successfully progressed.}]
STEP: updating Deployment Status 01/30/23 11:28:59.173
Jan 30 11:28:59.178: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 30, 11, 28, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 30, 11, 28, 58, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 30, 11, 28, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 30, 11, 28, 57, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-44lv8-66b965764f\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Deployment status to be updated 01/30/23 11:28:59.178
Jan 30 11:28:59.180: INFO: Observed &Deployment event: ADDED
Jan 30 11:28:59.180: INFO: Observed Deployment test-deployment-44lv8 in namespace deployment-8438 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-30 11:28:57 +0000 UTC 2023-01-30 11:28:57 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-44lv8-66b965764f"}
Jan 30 11:28:59.180: INFO: Observed &Deployment event: MODIFIED
Jan 30 11:28:59.180: INFO: Observed Deployment test-deployment-44lv8 in namespace deployment-8438 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-30 11:28:57 +0000 UTC 2023-01-30 11:28:57 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-44lv8-66b965764f"}
Jan 30 11:28:59.180: INFO: Observed Deployment test-deployment-44lv8 in namespace deployment-8438 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-01-30 11:28:57 +0000 UTC 2023-01-30 11:28:57 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Jan 30 11:28:59.180: INFO: Observed &Deployment event: MODIFIED
Jan 30 11:28:59.180: INFO: Observed Deployment test-deployment-44lv8 in namespace deployment-8438 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-01-30 11:28:57 +0000 UTC 2023-01-30 11:28:57 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Jan 30 11:28:59.180: INFO: Observed Deployment test-deployment-44lv8 in namespace deployment-8438 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-30 11:28:57 +0000 UTC 2023-01-30 11:28:57 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-44lv8-66b965764f" is progressing.}
Jan 30 11:28:59.180: INFO: Observed &Deployment event: MODIFIED
Jan 30 11:28:59.180: INFO: Observed Deployment test-deployment-44lv8 in namespace deployment-8438 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-01-30 11:28:58 +0000 UTC 2023-01-30 11:28:58 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Jan 30 11:28:59.180: INFO: Observed Deployment test-deployment-44lv8 in namespace deployment-8438 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-30 11:28:58 +0000 UTC 2023-01-30 11:28:57 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-44lv8-66b965764f" has successfully progressed.}
Jan 30 11:28:59.180: INFO: Observed &Deployment event: MODIFIED
Jan 30 11:28:59.180: INFO: Observed Deployment test-deployment-44lv8 in namespace deployment-8438 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-01-30 11:28:58 +0000 UTC 2023-01-30 11:28:58 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Jan 30 11:28:59.180: INFO: Observed Deployment test-deployment-44lv8 in namespace deployment-8438 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-30 11:28:58 +0000 UTC 2023-01-30 11:28:57 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-44lv8-66b965764f" has successfully progressed.}
Jan 30 11:28:59.180: INFO: Found Deployment test-deployment-44lv8 in namespace deployment-8438 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Jan 30 11:28:59.180: INFO: Deployment test-deployment-44lv8 has an updated status
STEP: patching the Statefulset Status 01/30/23 11:28:59.18
Jan 30 11:28:59.180: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Jan 30 11:28:59.185: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Deployment status to be patched 01/30/23 11:28:59.185
Jan 30 11:28:59.187: INFO: Observed &Deployment event: ADDED
Jan 30 11:28:59.187: INFO: Observed deployment test-deployment-44lv8 in namespace deployment-8438 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-30 11:28:57 +0000 UTC 2023-01-30 11:28:57 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-44lv8-66b965764f"}
Jan 30 11:28:59.187: INFO: Observed &Deployment event: MODIFIED
Jan 30 11:28:59.187: INFO: Observed deployment test-deployment-44lv8 in namespace deployment-8438 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-30 11:28:57 +0000 UTC 2023-01-30 11:28:57 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-44lv8-66b965764f"}
Jan 30 11:28:59.187: INFO: Observed deployment test-deployment-44lv8 in namespace deployment-8438 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-01-30 11:28:57 +0000 UTC 2023-01-30 11:28:57 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Jan 30 11:28:59.187: INFO: Observed &Deployment event: MODIFIED
Jan 30 11:28:59.187: INFO: Observed deployment test-deployment-44lv8 in namespace deployment-8438 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-01-30 11:28:57 +0000 UTC 2023-01-30 11:28:57 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Jan 30 11:28:59.187: INFO: Observed deployment test-deployment-44lv8 in namespace deployment-8438 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-30 11:28:57 +0000 UTC 2023-01-30 11:28:57 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-44lv8-66b965764f" is progressing.}
Jan 30 11:28:59.187: INFO: Observed &Deployment event: MODIFIED
Jan 30 11:28:59.187: INFO: Observed deployment test-deployment-44lv8 in namespace deployment-8438 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-01-30 11:28:58 +0000 UTC 2023-01-30 11:28:58 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Jan 30 11:28:59.187: INFO: Observed deployment test-deployment-44lv8 in namespace deployment-8438 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-30 11:28:58 +0000 UTC 2023-01-30 11:28:57 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-44lv8-66b965764f" has successfully progressed.}
Jan 30 11:28:59.187: INFO: Observed &Deployment event: MODIFIED
Jan 30 11:28:59.187: INFO: Observed deployment test-deployment-44lv8 in namespace deployment-8438 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-01-30 11:28:58 +0000 UTC 2023-01-30 11:28:58 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Jan 30 11:28:59.187: INFO: Observed deployment test-deployment-44lv8 in namespace deployment-8438 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-30 11:28:58 +0000 UTC 2023-01-30 11:28:57 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-44lv8-66b965764f" has successfully progressed.}
Jan 30 11:28:59.187: INFO: Observed deployment test-deployment-44lv8 in namespace deployment-8438 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Jan 30 11:28:59.187: INFO: Observed &Deployment event: MODIFIED
Jan 30 11:28:59.187: INFO: Found deployment test-deployment-44lv8 in namespace deployment-8438 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
Jan 30 11:28:59.187: INFO: Deployment test-deployment-44lv8 has a patched status
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan 30 11:28:59.189: INFO: Deployment "test-deployment-44lv8":
&Deployment{ObjectMeta:{test-deployment-44lv8  deployment-8438  aa9bfebf-9e59-4ab9-990c-f635bdec3481 11171 1 2023-01-30 11:28:57 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-01-30 11:28:57 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-30 11:28:58 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status} {e2e.test Update apps/v1 2023-01-30 11:28:59 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] [] []} {[] [] [{httpd harbor.cloud.netease.com/qzprod-k8s/e2e/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00496b0c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jan 30 11:28:59.191: INFO: New ReplicaSet "test-deployment-44lv8-66b965764f" of Deployment "test-deployment-44lv8":
&ReplicaSet{ObjectMeta:{test-deployment-44lv8-66b965764f  deployment-8438  e4e598aa-1436-48f1-a080-f3e23eaa6171 11165 1 2023-01-30 11:28:57 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:66b965764f] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-44lv8 aa9bfebf-9e59-4ab9-990c-f635bdec3481 0xc004781410 0xc004781411}] [] [{kube-controller-manager Update apps/v1 2023-01-30 11:28:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"aa9bfebf-9e59-4ab9-990c-f635bdec3481\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-30 11:28:58 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 66b965764f,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:66b965764f] map[] [] [] []} {[] [] [{httpd harbor.cloud.netease.com/qzprod-k8s/e2e/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0047814b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jan 30 11:28:59.193: INFO: Pod "test-deployment-44lv8-66b965764f-2h554" is available:
&Pod{ObjectMeta:{test-deployment-44lv8-66b965764f-2h554 test-deployment-44lv8-66b965764f- deployment-8438  4e2fdaa5-e76b-43b6-89ac-9b586d81c913 11164 0 2023-01-30 11:28:57 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:66b965764f] map[cni.projectcalico.org/podIP:10.178.151.14/32 cni.projectcalico.org/podIPs:10.178.151.14/32] [{apps/v1 ReplicaSet test-deployment-44lv8-66b965764f e4e598aa-1436-48f1-a080-f3e23eaa6171 0xc000b55530 0xc000b55531}] [] [{kube-controller-manager Update v1 2023-01-30 11:28:57 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e4e598aa-1436-48f1-a080-f3e23eaa6171\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-30 11:28:58 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-30 11:28:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.178.151.14\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rr9xb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:harbor.cloud.netease.com/qzprod-k8s/e2e/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rr9xb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pubt2-nks-for-dev1.dg.163.org,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 11:28:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 11:28:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 11:28:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 11:28:57 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.182.0.82,PodIP:10.178.151.14,StartTime:2023-01-30 11:28:57 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-30 11:28:58 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:harbor.cloud.netease.com/qzprod-k8s/e2e/httpd:2.4.38-4,ImageID:docker-pullable://harbor.cloud.netease.com/qzprod-k8s/e2e/httpd@sha256:8b745b6e9cb6a6184353110aabe28d8d6f6145d3fe7c0e1899aa70c8228c0fd3,ContainerID:docker://71626d502b7c95fef6ab29e3083a2220ad9131f02249c26ddaf7f27f252089a3,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.178.151.14,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Jan 30 11:28:59.193: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-8438" for this suite. 01/30/23 11:28:59.195
------------------------------
• [2.056 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 11:28:57.141
    Jan 30 11:28:57.141: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename deployment 01/30/23 11:28:57.142
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:28:57.15
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:28:57.152
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] should validate Deployment Status endpoints [Conformance]
      test/e2e/apps/deployment.go:479
    STEP: creating a Deployment 01/30/23 11:28:57.156
    Jan 30 11:28:57.156: INFO: Creating simple deployment test-deployment-44lv8
    Jan 30 11:28:57.161: INFO: new replicaset for deployment "test-deployment-44lv8" is yet to be created
    STEP: Getting /status 01/30/23 11:28:59.169
    Jan 30 11:28:59.173: INFO: Deployment test-deployment-44lv8 has Conditions: [{Available True 2023-01-30 11:28:58 +0000 UTC 2023-01-30 11:28:58 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2023-01-30 11:28:58 +0000 UTC 2023-01-30 11:28:57 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-44lv8-66b965764f" has successfully progressed.}]
    STEP: updating Deployment Status 01/30/23 11:28:59.173
    Jan 30 11:28:59.178: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 30, 11, 28, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 30, 11, 28, 58, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 30, 11, 28, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 30, 11, 28, 57, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-44lv8-66b965764f\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the Deployment status to be updated 01/30/23 11:28:59.178
    Jan 30 11:28:59.180: INFO: Observed &Deployment event: ADDED
    Jan 30 11:28:59.180: INFO: Observed Deployment test-deployment-44lv8 in namespace deployment-8438 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-30 11:28:57 +0000 UTC 2023-01-30 11:28:57 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-44lv8-66b965764f"}
    Jan 30 11:28:59.180: INFO: Observed &Deployment event: MODIFIED
    Jan 30 11:28:59.180: INFO: Observed Deployment test-deployment-44lv8 in namespace deployment-8438 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-30 11:28:57 +0000 UTC 2023-01-30 11:28:57 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-44lv8-66b965764f"}
    Jan 30 11:28:59.180: INFO: Observed Deployment test-deployment-44lv8 in namespace deployment-8438 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-01-30 11:28:57 +0000 UTC 2023-01-30 11:28:57 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Jan 30 11:28:59.180: INFO: Observed &Deployment event: MODIFIED
    Jan 30 11:28:59.180: INFO: Observed Deployment test-deployment-44lv8 in namespace deployment-8438 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-01-30 11:28:57 +0000 UTC 2023-01-30 11:28:57 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Jan 30 11:28:59.180: INFO: Observed Deployment test-deployment-44lv8 in namespace deployment-8438 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-30 11:28:57 +0000 UTC 2023-01-30 11:28:57 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-44lv8-66b965764f" is progressing.}
    Jan 30 11:28:59.180: INFO: Observed &Deployment event: MODIFIED
    Jan 30 11:28:59.180: INFO: Observed Deployment test-deployment-44lv8 in namespace deployment-8438 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-01-30 11:28:58 +0000 UTC 2023-01-30 11:28:58 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Jan 30 11:28:59.180: INFO: Observed Deployment test-deployment-44lv8 in namespace deployment-8438 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-30 11:28:58 +0000 UTC 2023-01-30 11:28:57 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-44lv8-66b965764f" has successfully progressed.}
    Jan 30 11:28:59.180: INFO: Observed &Deployment event: MODIFIED
    Jan 30 11:28:59.180: INFO: Observed Deployment test-deployment-44lv8 in namespace deployment-8438 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-01-30 11:28:58 +0000 UTC 2023-01-30 11:28:58 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Jan 30 11:28:59.180: INFO: Observed Deployment test-deployment-44lv8 in namespace deployment-8438 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-30 11:28:58 +0000 UTC 2023-01-30 11:28:57 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-44lv8-66b965764f" has successfully progressed.}
    Jan 30 11:28:59.180: INFO: Found Deployment test-deployment-44lv8 in namespace deployment-8438 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Jan 30 11:28:59.180: INFO: Deployment test-deployment-44lv8 has an updated status
    STEP: patching the Statefulset Status 01/30/23 11:28:59.18
    Jan 30 11:28:59.180: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Jan 30 11:28:59.185: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Deployment status to be patched 01/30/23 11:28:59.185
    Jan 30 11:28:59.187: INFO: Observed &Deployment event: ADDED
    Jan 30 11:28:59.187: INFO: Observed deployment test-deployment-44lv8 in namespace deployment-8438 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-30 11:28:57 +0000 UTC 2023-01-30 11:28:57 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-44lv8-66b965764f"}
    Jan 30 11:28:59.187: INFO: Observed &Deployment event: MODIFIED
    Jan 30 11:28:59.187: INFO: Observed deployment test-deployment-44lv8 in namespace deployment-8438 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-30 11:28:57 +0000 UTC 2023-01-30 11:28:57 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-44lv8-66b965764f"}
    Jan 30 11:28:59.187: INFO: Observed deployment test-deployment-44lv8 in namespace deployment-8438 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-01-30 11:28:57 +0000 UTC 2023-01-30 11:28:57 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Jan 30 11:28:59.187: INFO: Observed &Deployment event: MODIFIED
    Jan 30 11:28:59.187: INFO: Observed deployment test-deployment-44lv8 in namespace deployment-8438 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-01-30 11:28:57 +0000 UTC 2023-01-30 11:28:57 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Jan 30 11:28:59.187: INFO: Observed deployment test-deployment-44lv8 in namespace deployment-8438 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-30 11:28:57 +0000 UTC 2023-01-30 11:28:57 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-44lv8-66b965764f" is progressing.}
    Jan 30 11:28:59.187: INFO: Observed &Deployment event: MODIFIED
    Jan 30 11:28:59.187: INFO: Observed deployment test-deployment-44lv8 in namespace deployment-8438 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-01-30 11:28:58 +0000 UTC 2023-01-30 11:28:58 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Jan 30 11:28:59.187: INFO: Observed deployment test-deployment-44lv8 in namespace deployment-8438 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-30 11:28:58 +0000 UTC 2023-01-30 11:28:57 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-44lv8-66b965764f" has successfully progressed.}
    Jan 30 11:28:59.187: INFO: Observed &Deployment event: MODIFIED
    Jan 30 11:28:59.187: INFO: Observed deployment test-deployment-44lv8 in namespace deployment-8438 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-01-30 11:28:58 +0000 UTC 2023-01-30 11:28:58 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Jan 30 11:28:59.187: INFO: Observed deployment test-deployment-44lv8 in namespace deployment-8438 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-30 11:28:58 +0000 UTC 2023-01-30 11:28:57 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-44lv8-66b965764f" has successfully progressed.}
    Jan 30 11:28:59.187: INFO: Observed deployment test-deployment-44lv8 in namespace deployment-8438 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Jan 30 11:28:59.187: INFO: Observed &Deployment event: MODIFIED
    Jan 30 11:28:59.187: INFO: Found deployment test-deployment-44lv8 in namespace deployment-8438 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
    Jan 30 11:28:59.187: INFO: Deployment test-deployment-44lv8 has a patched status
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jan 30 11:28:59.189: INFO: Deployment "test-deployment-44lv8":
    &Deployment{ObjectMeta:{test-deployment-44lv8  deployment-8438  aa9bfebf-9e59-4ab9-990c-f635bdec3481 11171 1 2023-01-30 11:28:57 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-01-30 11:28:57 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-30 11:28:58 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status} {e2e.test Update apps/v1 2023-01-30 11:28:59 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] [] []} {[] [] [{httpd harbor.cloud.netease.com/qzprod-k8s/e2e/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00496b0c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Jan 30 11:28:59.191: INFO: New ReplicaSet "test-deployment-44lv8-66b965764f" of Deployment "test-deployment-44lv8":
    &ReplicaSet{ObjectMeta:{test-deployment-44lv8-66b965764f  deployment-8438  e4e598aa-1436-48f1-a080-f3e23eaa6171 11165 1 2023-01-30 11:28:57 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:66b965764f] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-44lv8 aa9bfebf-9e59-4ab9-990c-f635bdec3481 0xc004781410 0xc004781411}] [] [{kube-controller-manager Update apps/v1 2023-01-30 11:28:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"aa9bfebf-9e59-4ab9-990c-f635bdec3481\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-30 11:28:58 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 66b965764f,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:66b965764f] map[] [] [] []} {[] [] [{httpd harbor.cloud.netease.com/qzprod-k8s/e2e/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0047814b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Jan 30 11:28:59.193: INFO: Pod "test-deployment-44lv8-66b965764f-2h554" is available:
    &Pod{ObjectMeta:{test-deployment-44lv8-66b965764f-2h554 test-deployment-44lv8-66b965764f- deployment-8438  4e2fdaa5-e76b-43b6-89ac-9b586d81c913 11164 0 2023-01-30 11:28:57 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:66b965764f] map[cni.projectcalico.org/podIP:10.178.151.14/32 cni.projectcalico.org/podIPs:10.178.151.14/32] [{apps/v1 ReplicaSet test-deployment-44lv8-66b965764f e4e598aa-1436-48f1-a080-f3e23eaa6171 0xc000b55530 0xc000b55531}] [] [{kube-controller-manager Update v1 2023-01-30 11:28:57 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e4e598aa-1436-48f1-a080-f3e23eaa6171\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-30 11:28:58 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-30 11:28:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.178.151.14\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rr9xb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:harbor.cloud.netease.com/qzprod-k8s/e2e/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rr9xb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pubt2-nks-for-dev1.dg.163.org,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 11:28:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 11:28:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 11:28:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 11:28:57 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.182.0.82,PodIP:10.178.151.14,StartTime:2023-01-30 11:28:57 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-30 11:28:58 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:harbor.cloud.netease.com/qzprod-k8s/e2e/httpd:2.4.38-4,ImageID:docker-pullable://harbor.cloud.netease.com/qzprod-k8s/e2e/httpd@sha256:8b745b6e9cb6a6184353110aabe28d8d6f6145d3fe7c0e1899aa70c8228c0fd3,ContainerID:docker://71626d502b7c95fef6ab29e3083a2220ad9131f02249c26ddaf7f27f252089a3,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.178.151.14,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Jan 30 11:28:59.193: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-8438" for this suite. 01/30/23 11:28:59.195
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 11:28:59.198
Jan 30 11:28:59.198: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename watch 01/30/23 11:28:59.198
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:28:59.204
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:28:59.206
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142
STEP: creating a new configmap 01/30/23 11:28:59.208
STEP: modifying the configmap once 01/30/23 11:28:59.21
STEP: modifying the configmap a second time 01/30/23 11:28:59.213
STEP: deleting the configmap 01/30/23 11:28:59.216
STEP: creating a watch on configmaps from the resource version returned by the first update 01/30/23 11:28:59.219
STEP: Expecting to observe notifications for all changes to the configmap after the first update 01/30/23 11:28:59.219
Jan 30 11:28:59.220: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-6288  59037813-f74f-4a03-89ee-6825ac1e4623 11180 0 2023-01-30 11:28:59 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-01-30 11:28:59 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 30 11:28:59.220: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-6288  59037813-f74f-4a03-89ee-6825ac1e4623 11181 0 2023-01-30 11:28:59 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-01-30 11:28:59 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Jan 30 11:28:59.220: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-6288" for this suite. 01/30/23 11:28:59.222
------------------------------
• [0.026 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 11:28:59.198
    Jan 30 11:28:59.198: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename watch 01/30/23 11:28:59.198
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:28:59.204
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:28:59.206
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to start watching from a specific resource version [Conformance]
      test/e2e/apimachinery/watch.go:142
    STEP: creating a new configmap 01/30/23 11:28:59.208
    STEP: modifying the configmap once 01/30/23 11:28:59.21
    STEP: modifying the configmap a second time 01/30/23 11:28:59.213
    STEP: deleting the configmap 01/30/23 11:28:59.216
    STEP: creating a watch on configmaps from the resource version returned by the first update 01/30/23 11:28:59.219
    STEP: Expecting to observe notifications for all changes to the configmap after the first update 01/30/23 11:28:59.219
    Jan 30 11:28:59.220: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-6288  59037813-f74f-4a03-89ee-6825ac1e4623 11180 0 2023-01-30 11:28:59 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-01-30 11:28:59 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan 30 11:28:59.220: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-6288  59037813-f74f-4a03-89ee-6825ac1e4623 11181 0 2023-01-30 11:28:59 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-01-30 11:28:59 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Jan 30 11:28:59.220: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-6288" for this suite. 01/30/23 11:28:59.222
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:107
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 11:28:59.224
Jan 30 11:28:59.224: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename emptydir 01/30/23 11:28:59.225
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:28:59.23
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:28:59.232
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:107
STEP: Creating a pod to test emptydir 0666 on tmpfs 01/30/23 11:28:59.234
Jan 30 11:28:59.238: INFO: Waiting up to 5m0s for pod "pod-3b7e4e83-cf95-47e6-b508-125b607f8878" in namespace "emptydir-8244" to be "Succeeded or Failed"
Jan 30 11:28:59.239: INFO: Pod "pod-3b7e4e83-cf95-47e6-b508-125b607f8878": Phase="Pending", Reason="", readiness=false. Elapsed: 1.479775ms
Jan 30 11:29:01.242: INFO: Pod "pod-3b7e4e83-cf95-47e6-b508-125b607f8878": Phase="Running", Reason="", readiness=true. Elapsed: 2.004518854s
Jan 30 11:29:03.241: INFO: Pod "pod-3b7e4e83-cf95-47e6-b508-125b607f8878": Phase="Running", Reason="", readiness=false. Elapsed: 4.003498559s
Jan 30 11:29:05.243: INFO: Pod "pod-3b7e4e83-cf95-47e6-b508-125b607f8878": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.005502873s
STEP: Saw pod success 01/30/23 11:29:05.243
Jan 30 11:29:05.243: INFO: Pod "pod-3b7e4e83-cf95-47e6-b508-125b607f8878" satisfied condition "Succeeded or Failed"
Jan 30 11:29:05.245: INFO: Trying to get logs from node pubt2-nks-for-dev1.dg.163.org pod pod-3b7e4e83-cf95-47e6-b508-125b607f8878 container test-container: <nil>
STEP: delete the pod 01/30/23 11:29:05.25
Jan 30 11:29:05.254: INFO: Waiting for pod pod-3b7e4e83-cf95-47e6-b508-125b607f8878 to disappear
Jan 30 11:29:05.255: INFO: Pod pod-3b7e4e83-cf95-47e6-b508-125b607f8878 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jan 30 11:29:05.255: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-8244" for this suite. 01/30/23 11:29:05.258
------------------------------
• [SLOW TEST] [6.036 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:107

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 11:28:59.224
    Jan 30 11:28:59.224: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename emptydir 01/30/23 11:28:59.225
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:28:59.23
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:28:59.232
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:107
    STEP: Creating a pod to test emptydir 0666 on tmpfs 01/30/23 11:28:59.234
    Jan 30 11:28:59.238: INFO: Waiting up to 5m0s for pod "pod-3b7e4e83-cf95-47e6-b508-125b607f8878" in namespace "emptydir-8244" to be "Succeeded or Failed"
    Jan 30 11:28:59.239: INFO: Pod "pod-3b7e4e83-cf95-47e6-b508-125b607f8878": Phase="Pending", Reason="", readiness=false. Elapsed: 1.479775ms
    Jan 30 11:29:01.242: INFO: Pod "pod-3b7e4e83-cf95-47e6-b508-125b607f8878": Phase="Running", Reason="", readiness=true. Elapsed: 2.004518854s
    Jan 30 11:29:03.241: INFO: Pod "pod-3b7e4e83-cf95-47e6-b508-125b607f8878": Phase="Running", Reason="", readiness=false. Elapsed: 4.003498559s
    Jan 30 11:29:05.243: INFO: Pod "pod-3b7e4e83-cf95-47e6-b508-125b607f8878": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.005502873s
    STEP: Saw pod success 01/30/23 11:29:05.243
    Jan 30 11:29:05.243: INFO: Pod "pod-3b7e4e83-cf95-47e6-b508-125b607f8878" satisfied condition "Succeeded or Failed"
    Jan 30 11:29:05.245: INFO: Trying to get logs from node pubt2-nks-for-dev1.dg.163.org pod pod-3b7e4e83-cf95-47e6-b508-125b607f8878 container test-container: <nil>
    STEP: delete the pod 01/30/23 11:29:05.25
    Jan 30 11:29:05.254: INFO: Waiting for pod pod-3b7e4e83-cf95-47e6-b508-125b607f8878 to disappear
    Jan 30 11:29:05.255: INFO: Pod pod-3b7e4e83-cf95-47e6-b508-125b607f8878 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jan 30 11:29:05.255: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-8244" for this suite. 01/30/23 11:29:05.258
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:39
[BeforeEach] [sig-node] Containers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 11:29:05.261
Jan 30 11:29:05.261: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename containers 01/30/23 11:29:05.261
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:29:05.267
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:29:05.269
[BeforeEach] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:31
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:39
Jan 30 11:29:05.274: INFO: Waiting up to 5m0s for pod "client-containers-18e4a8fa-f28c-4e35-af18-997a837fda93" in namespace "containers-6017" to be "running"
Jan 30 11:29:05.276: INFO: Pod "client-containers-18e4a8fa-f28c-4e35-af18-997a837fda93": Phase="Pending", Reason="", readiness=false. Elapsed: 1.424795ms
Jan 30 11:29:07.278: INFO: Pod "client-containers-18e4a8fa-f28c-4e35-af18-997a837fda93": Phase="Running", Reason="", readiness=true. Elapsed: 2.004016339s
Jan 30 11:29:07.278: INFO: Pod "client-containers-18e4a8fa-f28c-4e35-af18-997a837fda93" satisfied condition "running"
[AfterEach] [sig-node] Containers
  test/e2e/framework/node/init/init.go:32
Jan 30 11:29:07.283: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Containers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Containers
  tear down framework | framework.go:193
STEP: Destroying namespace "containers-6017" for this suite. 01/30/23 11:29:07.285
------------------------------
• [2.027 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:39

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 11:29:05.261
    Jan 30 11:29:05.261: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename containers 01/30/23 11:29:05.261
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:29:05.267
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:29:05.269
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:31
    [It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:39
    Jan 30 11:29:05.274: INFO: Waiting up to 5m0s for pod "client-containers-18e4a8fa-f28c-4e35-af18-997a837fda93" in namespace "containers-6017" to be "running"
    Jan 30 11:29:05.276: INFO: Pod "client-containers-18e4a8fa-f28c-4e35-af18-997a837fda93": Phase="Pending", Reason="", readiness=false. Elapsed: 1.424795ms
    Jan 30 11:29:07.278: INFO: Pod "client-containers-18e4a8fa-f28c-4e35-af18-997a837fda93": Phase="Running", Reason="", readiness=true. Elapsed: 2.004016339s
    Jan 30 11:29:07.278: INFO: Pod "client-containers-18e4a8fa-f28c-4e35-af18-997a837fda93" satisfied condition "running"
    [AfterEach] [sig-node] Containers
      test/e2e/framework/node/init/init.go:32
    Jan 30 11:29:07.283: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Containers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Containers
      tear down framework | framework.go:193
    STEP: Destroying namespace "containers-6017" for this suite. 01/30/23 11:29:07.285
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Containers
  should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:59
[BeforeEach] [sig-node] Containers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 11:29:07.288
Jan 30 11:29:07.288: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename containers 01/30/23 11:29:07.288
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:29:07.295
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:29:07.297
[BeforeEach] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:59
STEP: Creating a pod to test override arguments 01/30/23 11:29:07.299
Jan 30 11:29:07.303: INFO: Waiting up to 5m0s for pod "client-containers-efc37a81-6a25-4360-a9f2-75ac87abb9c3" in namespace "containers-1989" to be "Succeeded or Failed"
Jan 30 11:29:07.305: INFO: Pod "client-containers-efc37a81-6a25-4360-a9f2-75ac87abb9c3": Phase="Pending", Reason="", readiness=false. Elapsed: 1.590461ms
Jan 30 11:29:09.308: INFO: Pod "client-containers-efc37a81-6a25-4360-a9f2-75ac87abb9c3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004436589s
Jan 30 11:29:11.307: INFO: Pod "client-containers-efc37a81-6a25-4360-a9f2-75ac87abb9c3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004340326s
STEP: Saw pod success 01/30/23 11:29:11.307
Jan 30 11:29:11.308: INFO: Pod "client-containers-efc37a81-6a25-4360-a9f2-75ac87abb9c3" satisfied condition "Succeeded or Failed"
Jan 30 11:29:11.309: INFO: Trying to get logs from node pubt2-nks-for-dev1.dg.163.org pod client-containers-efc37a81-6a25-4360-a9f2-75ac87abb9c3 container agnhost-container: <nil>
STEP: delete the pod 01/30/23 11:29:11.314
Jan 30 11:29:11.318: INFO: Waiting for pod client-containers-efc37a81-6a25-4360-a9f2-75ac87abb9c3 to disappear
Jan 30 11:29:11.319: INFO: Pod client-containers-efc37a81-6a25-4360-a9f2-75ac87abb9c3 no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/node/init/init.go:32
Jan 30 11:29:11.319: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Containers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Containers
  tear down framework | framework.go:193
STEP: Destroying namespace "containers-1989" for this suite. 01/30/23 11:29:11.321
------------------------------
• [4.036 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:59

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 11:29:07.288
    Jan 30 11:29:07.288: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename containers 01/30/23 11:29:07.288
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:29:07.295
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:29:07.297
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:59
    STEP: Creating a pod to test override arguments 01/30/23 11:29:07.299
    Jan 30 11:29:07.303: INFO: Waiting up to 5m0s for pod "client-containers-efc37a81-6a25-4360-a9f2-75ac87abb9c3" in namespace "containers-1989" to be "Succeeded or Failed"
    Jan 30 11:29:07.305: INFO: Pod "client-containers-efc37a81-6a25-4360-a9f2-75ac87abb9c3": Phase="Pending", Reason="", readiness=false. Elapsed: 1.590461ms
    Jan 30 11:29:09.308: INFO: Pod "client-containers-efc37a81-6a25-4360-a9f2-75ac87abb9c3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004436589s
    Jan 30 11:29:11.307: INFO: Pod "client-containers-efc37a81-6a25-4360-a9f2-75ac87abb9c3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004340326s
    STEP: Saw pod success 01/30/23 11:29:11.307
    Jan 30 11:29:11.308: INFO: Pod "client-containers-efc37a81-6a25-4360-a9f2-75ac87abb9c3" satisfied condition "Succeeded or Failed"
    Jan 30 11:29:11.309: INFO: Trying to get logs from node pubt2-nks-for-dev1.dg.163.org pod client-containers-efc37a81-6a25-4360-a9f2-75ac87abb9c3 container agnhost-container: <nil>
    STEP: delete the pod 01/30/23 11:29:11.314
    Jan 30 11:29:11.318: INFO: Waiting for pod client-containers-efc37a81-6a25-4360-a9f2-75ac87abb9c3 to disappear
    Jan 30 11:29:11.319: INFO: Pod client-containers-efc37a81-6a25-4360-a9f2-75ac87abb9c3 no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/node/init/init.go:32
    Jan 30 11:29:11.319: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Containers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Containers
      tear down framework | framework.go:193
    STEP: Destroying namespace "containers-1989" for this suite. 01/30/23 11:29:11.321
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:391
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 11:29:11.324
Jan 30 11:29:11.324: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename crd-publish-openapi 01/30/23 11:29:11.325
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:29:11.331
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:29:11.333
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:391
STEP: set up a multi version CRD 01/30/23 11:29:11.335
Jan 30 11:29:11.335: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: rename a version 01/30/23 11:29:15.521
STEP: check the new version name is served 01/30/23 11:29:15.534
STEP: check the old version name is removed 01/30/23 11:29:16.844
STEP: check the other version is not changed 01/30/23 11:29:17.631
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 30 11:29:20.981: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-3247" for this suite. 01/30/23 11:29:20.987
------------------------------
• [SLOW TEST] [9.665 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:391

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 11:29:11.324
    Jan 30 11:29:11.324: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename crd-publish-openapi 01/30/23 11:29:11.325
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:29:11.331
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:29:11.333
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] updates the published spec when one version gets renamed [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:391
    STEP: set up a multi version CRD 01/30/23 11:29:11.335
    Jan 30 11:29:11.335: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: rename a version 01/30/23 11:29:15.521
    STEP: check the new version name is served 01/30/23 11:29:15.534
    STEP: check the old version name is removed 01/30/23 11:29:16.844
    STEP: check the other version is not changed 01/30/23 11:29:17.631
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 30 11:29:20.981: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-3247" for this suite. 01/30/23 11:29:20.987
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl version
  should check is all data is printed  [Conformance]
  test/e2e/kubectl/kubectl.go:1685
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 11:29:20.99
Jan 30 11:29:20.990: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename kubectl 01/30/23 11:29:20.991
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:29:20.997
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:29:20.999
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check is all data is printed  [Conformance]
  test/e2e/kubectl/kubectl.go:1685
Jan 30 11:29:21.001: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=kubectl-4975 version'
Jan 30 11:29:21.052: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
Jan 30 11:29:21.052: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"26\", GitVersion:\"v1.26.1\", GitCommit:\"8f94681cd294aa8cfd3407b8191f6c70214973a4\", GitTreeState:\"clean\", BuildDate:\"2023-01-18T15:58:16Z\", GoVersion:\"go1.19.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v4.5.7\nServer Version: version.Info{Major:\"1\", Minor:\"26+\", GitVersion:\"v1.26.1-nks.1\", GitCommit:\"376f673f9ef53ab21d3d8cc1e5cb2b2dc0f06a2a\", GitTreeState:\"clean\", BuildDate:\"2023-01-30T03:20:11Z\", GoVersion:\"go1.19.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jan 30 11:29:21.052: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-4975" for this suite. 01/30/23 11:29:21.055
------------------------------
• [0.067 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl version
  test/e2e/kubectl/kubectl.go:1679
    should check is all data is printed  [Conformance]
    test/e2e/kubectl/kubectl.go:1685

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 11:29:20.99
    Jan 30 11:29:20.990: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename kubectl 01/30/23 11:29:20.991
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:29:20.997
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:29:20.999
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check is all data is printed  [Conformance]
      test/e2e/kubectl/kubectl.go:1685
    Jan 30 11:29:21.001: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=kubectl-4975 version'
    Jan 30 11:29:21.052: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
    Jan 30 11:29:21.052: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"26\", GitVersion:\"v1.26.1\", GitCommit:\"8f94681cd294aa8cfd3407b8191f6c70214973a4\", GitTreeState:\"clean\", BuildDate:\"2023-01-18T15:58:16Z\", GoVersion:\"go1.19.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v4.5.7\nServer Version: version.Info{Major:\"1\", Minor:\"26+\", GitVersion:\"v1.26.1-nks.1\", GitCommit:\"376f673f9ef53ab21d3d8cc1e5cb2b2dc0f06a2a\", GitTreeState:\"clean\", BuildDate:\"2023-01-30T03:20:11Z\", GoVersion:\"go1.19.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jan 30 11:29:21.052: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-4975" for this suite. 01/30/23 11:29:21.055
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should list, patch and delete a collection of StatefulSets [Conformance]
  test/e2e/apps/statefulset.go:908
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 11:29:21.059
Jan 30 11:29:21.059: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename statefulset 01/30/23 11:29:21.059
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:29:21.065
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:29:21.067
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-7125 01/30/23 11:29:21.07
[It] should list, patch and delete a collection of StatefulSets [Conformance]
  test/e2e/apps/statefulset.go:908
Jan 30 11:29:21.079: INFO: Found 0 stateful pods, waiting for 1
Jan 30 11:29:31.083: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: patching the StatefulSet 01/30/23 11:29:31.086
W0130 11:29:31.095527      23 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Jan 30 11:29:31.098: INFO: Found 1 stateful pods, waiting for 2
Jan 30 11:29:41.104: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jan 30 11:29:41.104: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
STEP: Listing all StatefulSets 01/30/23 11:29:41.108
STEP: Delete all of the StatefulSets 01/30/23 11:29:41.11
STEP: Verify that StatefulSets have been deleted 01/30/23 11:29:41.116
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Jan 30 11:29:41.118: INFO: Deleting all statefulset in ns statefulset-7125
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Jan 30 11:29:41.123: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-7125" for this suite. 01/30/23 11:29:41.125
------------------------------
• [SLOW TEST] [20.069 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should list, patch and delete a collection of StatefulSets [Conformance]
    test/e2e/apps/statefulset.go:908

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 11:29:21.059
    Jan 30 11:29:21.059: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename statefulset 01/30/23 11:29:21.059
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:29:21.065
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:29:21.067
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-7125 01/30/23 11:29:21.07
    [It] should list, patch and delete a collection of StatefulSets [Conformance]
      test/e2e/apps/statefulset.go:908
    Jan 30 11:29:21.079: INFO: Found 0 stateful pods, waiting for 1
    Jan 30 11:29:31.083: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: patching the StatefulSet 01/30/23 11:29:31.086
    W0130 11:29:31.095527      23 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Jan 30 11:29:31.098: INFO: Found 1 stateful pods, waiting for 2
    Jan 30 11:29:41.104: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Jan 30 11:29:41.104: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Listing all StatefulSets 01/30/23 11:29:41.108
    STEP: Delete all of the StatefulSets 01/30/23 11:29:41.11
    STEP: Verify that StatefulSets have been deleted 01/30/23 11:29:41.116
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Jan 30 11:29:41.118: INFO: Deleting all statefulset in ns statefulset-7125
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Jan 30 11:29:41.123: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-7125" for this suite. 01/30/23 11:29:41.125
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints
  verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  test/e2e/scheduling/preemption.go:814
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 11:29:41.128
Jan 30 11:29:41.128: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename sched-preemption 01/30/23 11:29:41.129
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:29:41.135
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:29:41.137
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:97
Jan 30 11:29:41.145: INFO: Waiting up to 1m0s for all nodes to be ready
Jan 30 11:30:41.164: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PriorityClass endpoints
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 11:30:41.166
Jan 30 11:30:41.166: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename sched-preemption-path 01/30/23 11:30:41.167
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:30:41.173
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:30:41.175
[BeforeEach] PriorityClass endpoints
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] PriorityClass endpoints
  test/e2e/scheduling/preemption.go:771
[It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  test/e2e/scheduling/preemption.go:814
Jan 30 11:30:41.183: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
Jan 30 11:30:41.185: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
[AfterEach] PriorityClass endpoints
  test/e2e/framework/node/init/init.go:32
Jan 30 11:30:41.193: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] PriorityClass endpoints
  test/e2e/scheduling/preemption.go:787
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/node/init/init.go:32
Jan 30 11:30:41.199: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:84
[DeferCleanup (Each)] PriorityClass endpoints
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] PriorityClass endpoints
  dump namespaces | framework.go:196
[DeferCleanup (Each)] PriorityClass endpoints
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-path-5324" for this suite. 01/30/23 11:30:41.222
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-747" for this suite. 01/30/23 11:30:41.225
------------------------------
• [SLOW TEST] [60.099 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  PriorityClass endpoints
  test/e2e/scheduling/preemption.go:764
    verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
    test/e2e/scheduling/preemption.go:814

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 11:29:41.128
    Jan 30 11:29:41.128: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename sched-preemption 01/30/23 11:29:41.129
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:29:41.135
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:29:41.137
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:97
    Jan 30 11:29:41.145: INFO: Waiting up to 1m0s for all nodes to be ready
    Jan 30 11:30:41.164: INFO: Waiting for terminating namespaces to be deleted...
    [BeforeEach] PriorityClass endpoints
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 11:30:41.166
    Jan 30 11:30:41.166: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename sched-preemption-path 01/30/23 11:30:41.167
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:30:41.173
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:30:41.175
    [BeforeEach] PriorityClass endpoints
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] PriorityClass endpoints
      test/e2e/scheduling/preemption.go:771
    [It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
      test/e2e/scheduling/preemption.go:814
    Jan 30 11:30:41.183: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
    Jan 30 11:30:41.185: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
    [AfterEach] PriorityClass endpoints
      test/e2e/framework/node/init/init.go:32
    Jan 30 11:30:41.193: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] PriorityClass endpoints
      test/e2e/scheduling/preemption.go:787
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan 30 11:30:41.199: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:84
    [DeferCleanup (Each)] PriorityClass endpoints
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] PriorityClass endpoints
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] PriorityClass endpoints
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-path-5324" for this suite. 01/30/23 11:30:41.222
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-747" for this suite. 01/30/23 11:30:41.225
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1438
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 11:30:41.227
Jan 30 11:30:41.227: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename services 01/30/23 11:30:41.228
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:30:41.233
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:30:41.235
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1438
STEP: creating a service externalname-service with the type=ExternalName in namespace services-8274 01/30/23 11:30:41.237
STEP: changing the ExternalName service to type=ClusterIP 01/30/23 11:30:41.239
STEP: creating replication controller externalname-service in namespace services-8274 01/30/23 11:30:41.244
I0130 11:30:41.247152      23 runners.go:193] Created replication controller with name: externalname-service, namespace: services-8274, replica count: 2
I0130 11:30:44.298540      23 runners.go:193] externalname-service Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0130 11:30:47.299384      23 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 30 11:30:47.299: INFO: Creating new exec pod
Jan 30 11:30:47.302: INFO: Waiting up to 5m0s for pod "execpodqx8cd" in namespace "services-8274" to be "running"
Jan 30 11:30:47.315: INFO: Pod "execpodqx8cd": Phase="Pending", Reason="", readiness=false. Elapsed: 13.154463ms
Jan 30 11:30:49.317: INFO: Pod "execpodqx8cd": Phase="Running", Reason="", readiness=true. Elapsed: 2.015408561s
Jan 30 11:30:49.317: INFO: Pod "execpodqx8cd" satisfied condition "running"
Jan 30 11:30:50.317: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=services-8274 exec execpodqx8cd -- /bin/sh -x -c nc -v -z -w 2 externalname-service 80'
Jan 30 11:30:50.476: INFO: stderr: "+ nc -v -z -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Jan 30 11:30:50.476: INFO: stdout: ""
Jan 30 11:30:50.476: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=services-8274 exec execpodqx8cd -- /bin/sh -x -c nc -v -z -w 2 10.178.66.254 80'
Jan 30 11:30:50.603: INFO: stderr: "+ nc -v -z -w 2 10.178.66.254 80\nConnection to 10.178.66.254 80 port [tcp/http] succeeded!\n"
Jan 30 11:30:50.603: INFO: stdout: ""
Jan 30 11:30:50.603: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jan 30 11:30:50.608: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-8274" for this suite. 01/30/23 11:30:50.611
------------------------------
• [SLOW TEST] [9.386 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1438

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 11:30:41.227
    Jan 30 11:30:41.227: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename services 01/30/23 11:30:41.228
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:30:41.233
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:30:41.235
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to change the type from ExternalName to ClusterIP [Conformance]
      test/e2e/network/service.go:1438
    STEP: creating a service externalname-service with the type=ExternalName in namespace services-8274 01/30/23 11:30:41.237
    STEP: changing the ExternalName service to type=ClusterIP 01/30/23 11:30:41.239
    STEP: creating replication controller externalname-service in namespace services-8274 01/30/23 11:30:41.244
    I0130 11:30:41.247152      23 runners.go:193] Created replication controller with name: externalname-service, namespace: services-8274, replica count: 2
    I0130 11:30:44.298540      23 runners.go:193] externalname-service Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0130 11:30:47.299384      23 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan 30 11:30:47.299: INFO: Creating new exec pod
    Jan 30 11:30:47.302: INFO: Waiting up to 5m0s for pod "execpodqx8cd" in namespace "services-8274" to be "running"
    Jan 30 11:30:47.315: INFO: Pod "execpodqx8cd": Phase="Pending", Reason="", readiness=false. Elapsed: 13.154463ms
    Jan 30 11:30:49.317: INFO: Pod "execpodqx8cd": Phase="Running", Reason="", readiness=true. Elapsed: 2.015408561s
    Jan 30 11:30:49.317: INFO: Pod "execpodqx8cd" satisfied condition "running"
    Jan 30 11:30:50.317: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=services-8274 exec execpodqx8cd -- /bin/sh -x -c nc -v -z -w 2 externalname-service 80'
    Jan 30 11:30:50.476: INFO: stderr: "+ nc -v -z -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Jan 30 11:30:50.476: INFO: stdout: ""
    Jan 30 11:30:50.476: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=services-8274 exec execpodqx8cd -- /bin/sh -x -c nc -v -z -w 2 10.178.66.254 80'
    Jan 30 11:30:50.603: INFO: stderr: "+ nc -v -z -w 2 10.178.66.254 80\nConnection to 10.178.66.254 80 port [tcp/http] succeeded!\n"
    Jan 30 11:30:50.603: INFO: stdout: ""
    Jan 30 11:30:50.603: INFO: Cleaning up the ExternalName to ClusterIP test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jan 30 11:30:50.608: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-8274" for this suite. 01/30/23 11:30:50.611
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] CSIStorageCapacity
   should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49
[BeforeEach] [sig-storage] CSIStorageCapacity
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 11:30:50.613
Jan 30 11:30:50.614: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename csistoragecapacity 01/30/23 11:30:50.614
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:30:50.62
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:30:50.622
[BeforeEach] [sig-storage] CSIStorageCapacity
  test/e2e/framework/metrics/init/init.go:31
[It]  should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49
STEP: getting /apis 01/30/23 11:30:50.624
STEP: getting /apis/storage.k8s.io 01/30/23 11:30:50.625
STEP: getting /apis/storage.k8s.io/v1 01/30/23 11:30:50.626
STEP: creating 01/30/23 11:30:50.627
STEP: watching 01/30/23 11:30:50.633
Jan 30 11:30:50.633: INFO: starting watch
STEP: getting 01/30/23 11:30:50.637
STEP: listing in namespace 01/30/23 11:30:50.638
STEP: listing across namespaces 01/30/23 11:30:50.64
STEP: patching 01/30/23 11:30:50.641
STEP: updating 01/30/23 11:30:50.644
Jan 30 11:30:50.646: INFO: waiting for watch events with expected annotations in namespace
Jan 30 11:30:50.646: INFO: waiting for watch events with expected annotations across namespace
STEP: deleting 01/30/23 11:30:50.646
STEP: deleting a collection 01/30/23 11:30:50.651
[AfterEach] [sig-storage] CSIStorageCapacity
  test/e2e/framework/node/init/init.go:32
Jan 30 11:30:50.656: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
  tear down framework | framework.go:193
STEP: Destroying namespace "csistoragecapacity-698" for this suite. 01/30/23 11:30:50.658
------------------------------
• [0.047 seconds]
[sig-storage] CSIStorageCapacity
test/e2e/storage/utils/framework.go:23
   should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] CSIStorageCapacity
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 11:30:50.613
    Jan 30 11:30:50.614: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename csistoragecapacity 01/30/23 11:30:50.614
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:30:50.62
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:30:50.622
    [BeforeEach] [sig-storage] CSIStorageCapacity
      test/e2e/framework/metrics/init/init.go:31
    [It]  should support CSIStorageCapacities API operations [Conformance]
      test/e2e/storage/csistoragecapacity.go:49
    STEP: getting /apis 01/30/23 11:30:50.624
    STEP: getting /apis/storage.k8s.io 01/30/23 11:30:50.625
    STEP: getting /apis/storage.k8s.io/v1 01/30/23 11:30:50.626
    STEP: creating 01/30/23 11:30:50.627
    STEP: watching 01/30/23 11:30:50.633
    Jan 30 11:30:50.633: INFO: starting watch
    STEP: getting 01/30/23 11:30:50.637
    STEP: listing in namespace 01/30/23 11:30:50.638
    STEP: listing across namespaces 01/30/23 11:30:50.64
    STEP: patching 01/30/23 11:30:50.641
    STEP: updating 01/30/23 11:30:50.644
    Jan 30 11:30:50.646: INFO: waiting for watch events with expected annotations in namespace
    Jan 30 11:30:50.646: INFO: waiting for watch events with expected annotations across namespace
    STEP: deleting 01/30/23 11:30:50.646
    STEP: deleting a collection 01/30/23 11:30:50.651
    [AfterEach] [sig-storage] CSIStorageCapacity
      test/e2e/framework/node/init/init.go:32
    Jan 30 11:30:50.656: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
      tear down framework | framework.go:193
    STEP: Destroying namespace "csistoragecapacity-698" for this suite. 01/30/23 11:30:50.658
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod
  should create a pod from an image when restart is Never  [Conformance]
  test/e2e/kubectl/kubectl.go:1713
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 11:30:50.661
Jan 30 11:30:50.661: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename kubectl 01/30/23 11:30:50.661
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:30:50.669
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:30:50.671
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Kubectl run pod
  test/e2e/kubectl/kubectl.go:1700
[It] should create a pod from an image when restart is Never  [Conformance]
  test/e2e/kubectl/kubectl.go:1713
STEP: running the image harbor.cloud.netease.com/qzprod-k8s/e2e/httpd:2.4.38-4 01/30/23 11:30:50.673
Jan 30 11:30:50.673: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=kubectl-8235 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=harbor.cloud.netease.com/qzprod-k8s/e2e/httpd:2.4.38-4'
Jan 30 11:30:50.735: INFO: stderr: ""
Jan 30 11:30:50.735: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created 01/30/23 11:30:50.735
[AfterEach] Kubectl run pod
  test/e2e/kubectl/kubectl.go:1704
Jan 30 11:30:50.737: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=kubectl-8235 delete pods e2e-test-httpd-pod'
Jan 30 11:30:53.011: INFO: stderr: ""
Jan 30 11:30:53.011: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jan 30 11:30:53.011: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-8235" for this suite. 01/30/23 11:30:53.014
------------------------------
• [2.356 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl run pod
  test/e2e/kubectl/kubectl.go:1697
    should create a pod from an image when restart is Never  [Conformance]
    test/e2e/kubectl/kubectl.go:1713

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 11:30:50.661
    Jan 30 11:30:50.661: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename kubectl 01/30/23 11:30:50.661
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:30:50.669
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:30:50.671
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Kubectl run pod
      test/e2e/kubectl/kubectl.go:1700
    [It] should create a pod from an image when restart is Never  [Conformance]
      test/e2e/kubectl/kubectl.go:1713
    STEP: running the image harbor.cloud.netease.com/qzprod-k8s/e2e/httpd:2.4.38-4 01/30/23 11:30:50.673
    Jan 30 11:30:50.673: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=kubectl-8235 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=harbor.cloud.netease.com/qzprod-k8s/e2e/httpd:2.4.38-4'
    Jan 30 11:30:50.735: INFO: stderr: ""
    Jan 30 11:30:50.735: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: verifying the pod e2e-test-httpd-pod was created 01/30/23 11:30:50.735
    [AfterEach] Kubectl run pod
      test/e2e/kubectl/kubectl.go:1704
    Jan 30 11:30:50.737: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=kubectl-8235 delete pods e2e-test-httpd-pod'
    Jan 30 11:30:53.011: INFO: stderr: ""
    Jan 30 11:30:53.011: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jan 30 11:30:53.011: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-8235" for this suite. 01/30/23 11:30:53.014
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:151
[BeforeEach] [sig-node] Container Lifecycle Hook
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 11:30:53.017
Jan 30 11:30:53.017: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename container-lifecycle-hook 01/30/23 11:30:53.019
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:30:53.027
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:30:53.029
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:77
STEP: create the container to handle the HTTPGet hook request. 01/30/23 11:30:53.033
Jan 30 11:30:53.037: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-801" to be "running and ready"
Jan 30 11:30:53.039: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 1.582155ms
Jan 30 11:30:53.039: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jan 30 11:30:55.043: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005342386s
Jan 30 11:30:55.043: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jan 30 11:30:57.042: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 4.004139093s
Jan 30 11:30:57.042: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Jan 30 11:30:57.042: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:151
STEP: create the pod with lifecycle hook 01/30/23 11:30:57.043
Jan 30 11:30:57.047: INFO: Waiting up to 5m0s for pod "pod-with-prestop-exec-hook" in namespace "container-lifecycle-hook-801" to be "running and ready"
Jan 30 11:30:57.049: INFO: Pod "pod-with-prestop-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 1.671926ms
Jan 30 11:30:57.049: INFO: The phase of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Jan 30 11:30:59.051: INFO: Pod "pod-with-prestop-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00403066s
Jan 30 11:30:59.051: INFO: The phase of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Jan 30 11:31:01.052: INFO: Pod "pod-with-prestop-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 4.004260616s
Jan 30 11:31:01.052: INFO: The phase of Pod pod-with-prestop-exec-hook is Running (Ready = true)
Jan 30 11:31:01.052: INFO: Pod "pod-with-prestop-exec-hook" satisfied condition "running and ready"
STEP: delete the pod with lifecycle hook 01/30/23 11:31:01.053
Jan 30 11:31:01.056: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jan 30 11:31:01.058: INFO: Pod pod-with-prestop-exec-hook still exists
Jan 30 11:31:03.058: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jan 30 11:31:03.060: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook 01/30/23 11:31:03.06
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/node/init/init.go:32
Jan 30 11:31:03.073: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  tear down framework | framework.go:193
STEP: Destroying namespace "container-lifecycle-hook-801" for this suite. 01/30/23 11:31:03.075
------------------------------
• [SLOW TEST] [10.060 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:151

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 11:30:53.017
    Jan 30 11:30:53.017: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename container-lifecycle-hook 01/30/23 11:30:53.019
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:30:53.027
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:30:53.029
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:77
    STEP: create the container to handle the HTTPGet hook request. 01/30/23 11:30:53.033
    Jan 30 11:30:53.037: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-801" to be "running and ready"
    Jan 30 11:30:53.039: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 1.582155ms
    Jan 30 11:30:53.039: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Jan 30 11:30:55.043: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005342386s
    Jan 30 11:30:55.043: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Jan 30 11:30:57.042: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 4.004139093s
    Jan 30 11:30:57.042: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Jan 30 11:30:57.042: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute prestop exec hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:151
    STEP: create the pod with lifecycle hook 01/30/23 11:30:57.043
    Jan 30 11:30:57.047: INFO: Waiting up to 5m0s for pod "pod-with-prestop-exec-hook" in namespace "container-lifecycle-hook-801" to be "running and ready"
    Jan 30 11:30:57.049: INFO: Pod "pod-with-prestop-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 1.671926ms
    Jan 30 11:30:57.049: INFO: The phase of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
    Jan 30 11:30:59.051: INFO: Pod "pod-with-prestop-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00403066s
    Jan 30 11:30:59.051: INFO: The phase of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
    Jan 30 11:31:01.052: INFO: Pod "pod-with-prestop-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 4.004260616s
    Jan 30 11:31:01.052: INFO: The phase of Pod pod-with-prestop-exec-hook is Running (Ready = true)
    Jan 30 11:31:01.052: INFO: Pod "pod-with-prestop-exec-hook" satisfied condition "running and ready"
    STEP: delete the pod with lifecycle hook 01/30/23 11:31:01.053
    Jan 30 11:31:01.056: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Jan 30 11:31:01.058: INFO: Pod pod-with-prestop-exec-hook still exists
    Jan 30 11:31:03.058: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Jan 30 11:31:03.060: INFO: Pod pod-with-prestop-exec-hook no longer exists
    STEP: check prestop hook 01/30/23 11:31:03.06
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/node/init/init.go:32
    Jan 30 11:31:03.073: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-lifecycle-hook-801" for this suite. 01/30/23 11:31:03.075
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Secrets
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:95
[BeforeEach] [sig-node] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 11:31:03.078
Jan 30 11:31:03.078: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename secrets 01/30/23 11:31:03.079
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:31:03.086
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:31:03.087
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:95
STEP: creating secret secrets-9339/secret-test-9e1daf33-53a5-4300-983b-5570faa783e5 01/30/23 11:31:03.09
STEP: Creating a pod to test consume secrets 01/30/23 11:31:03.092
Jan 30 11:31:03.096: INFO: Waiting up to 5m0s for pod "pod-configmaps-5a45e9b7-2d4a-46d7-9f6d-65f3d167ca3f" in namespace "secrets-9339" to be "Succeeded or Failed"
Jan 30 11:31:03.097: INFO: Pod "pod-configmaps-5a45e9b7-2d4a-46d7-9f6d-65f3d167ca3f": Phase="Pending", Reason="", readiness=false. Elapsed: 1.627721ms
Jan 30 11:31:05.100: INFO: Pod "pod-configmaps-5a45e9b7-2d4a-46d7-9f6d-65f3d167ca3f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004712497s
Jan 30 11:31:07.101: INFO: Pod "pod-configmaps-5a45e9b7-2d4a-46d7-9f6d-65f3d167ca3f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005754598s
STEP: Saw pod success 01/30/23 11:31:07.102
Jan 30 11:31:07.102: INFO: Pod "pod-configmaps-5a45e9b7-2d4a-46d7-9f6d-65f3d167ca3f" satisfied condition "Succeeded or Failed"
Jan 30 11:31:07.103: INFO: Trying to get logs from node pubt2-nks-for-dev1.dg.163.org pod pod-configmaps-5a45e9b7-2d4a-46d7-9f6d-65f3d167ca3f container env-test: <nil>
STEP: delete the pod 01/30/23 11:31:07.109
Jan 30 11:31:07.116: INFO: Waiting for pod pod-configmaps-5a45e9b7-2d4a-46d7-9f6d-65f3d167ca3f to disappear
Jan 30 11:31:07.117: INFO: Pod pod-configmaps-5a45e9b7-2d4a-46d7-9f6d-65f3d167ca3f no longer exists
[AfterEach] [sig-node] Secrets
  test/e2e/framework/node/init/init.go:32
Jan 30 11:31:07.117: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-9339" for this suite. 01/30/23 11:31:07.119
------------------------------
• [4.044 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:95

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 11:31:03.078
    Jan 30 11:31:03.078: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename secrets 01/30/23 11:31:03.079
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:31:03.086
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:31:03.087
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable via the environment [NodeConformance] [Conformance]
      test/e2e/common/node/secrets.go:95
    STEP: creating secret secrets-9339/secret-test-9e1daf33-53a5-4300-983b-5570faa783e5 01/30/23 11:31:03.09
    STEP: Creating a pod to test consume secrets 01/30/23 11:31:03.092
    Jan 30 11:31:03.096: INFO: Waiting up to 5m0s for pod "pod-configmaps-5a45e9b7-2d4a-46d7-9f6d-65f3d167ca3f" in namespace "secrets-9339" to be "Succeeded or Failed"
    Jan 30 11:31:03.097: INFO: Pod "pod-configmaps-5a45e9b7-2d4a-46d7-9f6d-65f3d167ca3f": Phase="Pending", Reason="", readiness=false. Elapsed: 1.627721ms
    Jan 30 11:31:05.100: INFO: Pod "pod-configmaps-5a45e9b7-2d4a-46d7-9f6d-65f3d167ca3f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004712497s
    Jan 30 11:31:07.101: INFO: Pod "pod-configmaps-5a45e9b7-2d4a-46d7-9f6d-65f3d167ca3f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005754598s
    STEP: Saw pod success 01/30/23 11:31:07.102
    Jan 30 11:31:07.102: INFO: Pod "pod-configmaps-5a45e9b7-2d4a-46d7-9f6d-65f3d167ca3f" satisfied condition "Succeeded or Failed"
    Jan 30 11:31:07.103: INFO: Trying to get logs from node pubt2-nks-for-dev1.dg.163.org pod pod-configmaps-5a45e9b7-2d4a-46d7-9f6d-65f3d167ca3f container env-test: <nil>
    STEP: delete the pod 01/30/23 11:31:07.109
    Jan 30 11:31:07.116: INFO: Waiting for pod pod-configmaps-5a45e9b7-2d4a-46d7-9f6d-65f3d167ca3f to disappear
    Jan 30 11:31:07.117: INFO: Pod pod-configmaps-5a45e9b7-2d4a-46d7-9f6d-65f3d167ca3f no longer exists
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/node/init/init.go:32
    Jan 30 11:31:07.117: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-9339" for this suite. 01/30/23 11:31:07.119
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] Security Context When creating a container with runAsUser
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:347
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 11:31:07.122
Jan 30 11:31:07.122: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename security-context-test 01/30/23 11:31:07.123
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:31:07.129
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:31:07.131
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:50
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:347
Jan 30 11:31:07.137: INFO: Waiting up to 5m0s for pod "busybox-user-65534-81095911-b6ef-4dcc-9e55-d4608fa7f6c2" in namespace "security-context-test-4808" to be "Succeeded or Failed"
Jan 30 11:31:07.138: INFO: Pod "busybox-user-65534-81095911-b6ef-4dcc-9e55-d4608fa7f6c2": Phase="Pending", Reason="", readiness=false. Elapsed: 1.481424ms
Jan 30 11:31:09.141: INFO: Pod "busybox-user-65534-81095911-b6ef-4dcc-9e55-d4608fa7f6c2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004305302s
Jan 30 11:31:11.142: INFO: Pod "busybox-user-65534-81095911-b6ef-4dcc-9e55-d4608fa7f6c2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005094371s
Jan 30 11:31:11.142: INFO: Pod "busybox-user-65534-81095911-b6ef-4dcc-9e55-d4608fa7f6c2" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Jan 30 11:31:11.142: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-test-4808" for this suite. 01/30/23 11:31:11.144
------------------------------
• [4.024 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a container with runAsUser
  test/e2e/common/node/security_context.go:309
    should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:347

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 11:31:07.122
    Jan 30 11:31:07.122: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename security-context-test 01/30/23 11:31:07.123
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:31:07.129
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:31:07.131
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:50
    [It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:347
    Jan 30 11:31:07.137: INFO: Waiting up to 5m0s for pod "busybox-user-65534-81095911-b6ef-4dcc-9e55-d4608fa7f6c2" in namespace "security-context-test-4808" to be "Succeeded or Failed"
    Jan 30 11:31:07.138: INFO: Pod "busybox-user-65534-81095911-b6ef-4dcc-9e55-d4608fa7f6c2": Phase="Pending", Reason="", readiness=false. Elapsed: 1.481424ms
    Jan 30 11:31:09.141: INFO: Pod "busybox-user-65534-81095911-b6ef-4dcc-9e55-d4608fa7f6c2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004305302s
    Jan 30 11:31:11.142: INFO: Pod "busybox-user-65534-81095911-b6ef-4dcc-9e55-d4608fa7f6c2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005094371s
    Jan 30 11:31:11.142: INFO: Pod "busybox-user-65534-81095911-b6ef-4dcc-9e55-d4608fa7f6c2" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Jan 30 11:31:11.142: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-test-4808" for this suite. 01/30/23 11:31:11.144
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:141
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 11:31:11.148
Jan 30 11:31:11.148: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename disruption 01/30/23 11:31:11.148
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:31:11.155
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:31:11.157
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[It] should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:141
STEP: Waiting for the pdb to be processed 01/30/23 11:31:11.161
STEP: Waiting for all pods to be running 01/30/23 11:31:13.176
Jan 30 11:31:13.178: INFO: running pods: 0 < 3
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Jan 30 11:31:15.183: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-552" for this suite. 01/30/23 11:31:15.185
------------------------------
• [4.040 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:141

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 11:31:11.148
    Jan 30 11:31:11.148: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename disruption 01/30/23 11:31:11.148
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:31:11.155
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:31:11.157
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [It] should observe PodDisruptionBudget status updated [Conformance]
      test/e2e/apps/disruption.go:141
    STEP: Waiting for the pdb to be processed 01/30/23 11:31:11.161
    STEP: Waiting for all pods to be running 01/30/23 11:31:13.176
    Jan 30 11:31:13.178: INFO: running pods: 0 < 3
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Jan 30 11:31:15.183: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-552" for this suite. 01/30/23 11:31:15.185
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:236
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 11:31:15.189
Jan 30 11:31:15.189: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename crd-publish-openapi 01/30/23 11:31:15.19
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:31:15.196
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:31:15.198
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:236
Jan 30 11:31:15.200: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 01/30/23 11:31:17.532
Jan 30 11:31:17.532: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=crd-publish-openapi-5100 --namespace=crd-publish-openapi-5100 create -f -'
Jan 30 11:31:18.669: INFO: stderr: ""
Jan 30 11:31:18.669: INFO: stdout: "e2e-test-crd-publish-openapi-66-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Jan 30 11:31:18.669: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=crd-publish-openapi-5100 --namespace=crd-publish-openapi-5100 delete e2e-test-crd-publish-openapi-66-crds test-cr'
Jan 30 11:31:18.732: INFO: stderr: ""
Jan 30 11:31:18.732: INFO: stdout: "e2e-test-crd-publish-openapi-66-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Jan 30 11:31:18.732: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=crd-publish-openapi-5100 --namespace=crd-publish-openapi-5100 apply -f -'
Jan 30 11:31:18.900: INFO: stderr: ""
Jan 30 11:31:18.900: INFO: stdout: "e2e-test-crd-publish-openapi-66-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Jan 30 11:31:18.901: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=crd-publish-openapi-5100 --namespace=crd-publish-openapi-5100 delete e2e-test-crd-publish-openapi-66-crds test-cr'
Jan 30 11:31:18.962: INFO: stderr: ""
Jan 30 11:31:18.962: INFO: stdout: "e2e-test-crd-publish-openapi-66-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR 01/30/23 11:31:18.962
Jan 30 11:31:18.962: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=crd-publish-openapi-5100 explain e2e-test-crd-publish-openapi-66-crds'
Jan 30 11:31:19.124: INFO: stderr: ""
Jan 30 11:31:19.124: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-66-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 30 11:31:20.955: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-5100" for this suite. 01/30/23 11:31:20.962
------------------------------
• [SLOW TEST] [5.775 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:236

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 11:31:15.189
    Jan 30 11:31:15.189: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename crd-publish-openapi 01/30/23 11:31:15.19
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:31:15.196
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:31:15.198
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for CRD preserving unknown fields in an embedded object [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:236
    Jan 30 11:31:15.200: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 01/30/23 11:31:17.532
    Jan 30 11:31:17.532: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=crd-publish-openapi-5100 --namespace=crd-publish-openapi-5100 create -f -'
    Jan 30 11:31:18.669: INFO: stderr: ""
    Jan 30 11:31:18.669: INFO: stdout: "e2e-test-crd-publish-openapi-66-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
    Jan 30 11:31:18.669: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=crd-publish-openapi-5100 --namespace=crd-publish-openapi-5100 delete e2e-test-crd-publish-openapi-66-crds test-cr'
    Jan 30 11:31:18.732: INFO: stderr: ""
    Jan 30 11:31:18.732: INFO: stdout: "e2e-test-crd-publish-openapi-66-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
    Jan 30 11:31:18.732: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=crd-publish-openapi-5100 --namespace=crd-publish-openapi-5100 apply -f -'
    Jan 30 11:31:18.900: INFO: stderr: ""
    Jan 30 11:31:18.900: INFO: stdout: "e2e-test-crd-publish-openapi-66-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
    Jan 30 11:31:18.901: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=crd-publish-openapi-5100 --namespace=crd-publish-openapi-5100 delete e2e-test-crd-publish-openapi-66-crds test-cr'
    Jan 30 11:31:18.962: INFO: stderr: ""
    Jan 30 11:31:18.962: INFO: stdout: "e2e-test-crd-publish-openapi-66-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR 01/30/23 11:31:18.962
    Jan 30 11:31:18.962: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=crd-publish-openapi-5100 explain e2e-test-crd-publish-openapi-66-crds'
    Jan 30 11:31:19.124: INFO: stderr: ""
    Jan 30 11:31:19.124: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-66-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 30 11:31:20.955: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-5100" for this suite. 01/30/23 11:31:20.962
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should apply an update to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:366
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 11:31:20.965
Jan 30 11:31:20.965: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename namespaces 01/30/23 11:31:20.966
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:31:20.973
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:31:20.975
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should apply an update to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:366
STEP: Updating Namespace "namespaces-5472" 01/30/23 11:31:20.977
Jan 30 11:31:20.981: INFO: Namespace "namespaces-5472" now has labels, map[string]string{"e2e-framework":"namespaces", "e2e-run":"195808ce-feca-480c-8c47-644dc9db1d74", "kubernetes.io/metadata.name":"namespaces-5472", "namespaces-5472":"updated", "pod-security.kubernetes.io/enforce":"baseline"}
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Jan 30 11:31:20.981: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-5472" for this suite. 01/30/23 11:31:20.983
------------------------------
• [0.020 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should apply an update to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:366

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 11:31:20.965
    Jan 30 11:31:20.965: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename namespaces 01/30/23 11:31:20.966
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:31:20.973
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:31:20.975
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply an update to a Namespace [Conformance]
      test/e2e/apimachinery/namespace.go:366
    STEP: Updating Namespace "namespaces-5472" 01/30/23 11:31:20.977
    Jan 30 11:31:20.981: INFO: Namespace "namespaces-5472" now has labels, map[string]string{"e2e-framework":"namespaces", "e2e-run":"195808ce-feca-480c-8c47-644dc9db1d74", "kubernetes.io/metadata.name":"namespaces-5472", "namespaces-5472":"updated", "pod-security.kubernetes.io/enforce":"baseline"}
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan 30 11:31:20.981: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-5472" for this suite. 01/30/23 11:31:20.983
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:426
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 11:31:20.986
Jan 30 11:31:20.986: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename job 01/30/23 11:31:20.987
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:31:20.992
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:31:20.994
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:426
STEP: Creating a job 01/30/23 11:31:20.996
STEP: Ensuring job reaches completions 01/30/23 11:31:21
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Jan 30 11:31:33.002: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-3591" for this suite. 01/30/23 11:31:33.004
------------------------------
• [SLOW TEST] [12.021 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:426

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 11:31:20.986
    Jan 30 11:31:20.986: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename job 01/30/23 11:31:20.987
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:31:20.992
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:31:20.994
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
      test/e2e/apps/job.go:426
    STEP: Creating a job 01/30/23 11:31:20.996
    STEP: Ensuring job reaches completions 01/30/23 11:31:21
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Jan 30 11:31:33.002: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-3591" for this suite. 01/30/23 11:31:33.004
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:194
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 11:31:33.008
Jan 30 11:31:33.008: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename daemonsets 01/30/23 11:31:33.008
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:31:33.015
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:31:33.017
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:194
Jan 30 11:31:33.027: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes. 01/30/23 11:31:33.03
Jan 30 11:31:33.032: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 30 11:31:33.032: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Change node label to blue, check that daemon pod is launched. 01/30/23 11:31:33.032
Jan 30 11:31:33.049: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 30 11:31:33.049: INFO: Node pubt2-nks-for-dev1.dg.163.org is running 0 daemon pod, expected 1
Jan 30 11:31:34.052: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 30 11:31:34.052: INFO: Node pubt2-nks-for-dev1.dg.163.org is running 0 daemon pod, expected 1
Jan 30 11:31:35.051: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 30 11:31:35.051: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
STEP: Update the node label to green, and wait for daemons to be unscheduled 01/30/23 11:31:35.053
Jan 30 11:31:35.064: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 30 11:31:35.064: INFO: Number of running nodes: 0, number of available pods: 1 in daemonset daemon-set
Jan 30 11:31:36.066: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 30 11:31:36.066: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate 01/30/23 11:31:36.066
Jan 30 11:31:36.072: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 30 11:31:36.072: INFO: Node pubt2-nks-for-dev1.dg.163.org is running 0 daemon pod, expected 1
Jan 30 11:31:37.075: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 30 11:31:37.075: INFO: Node pubt2-nks-for-dev1.dg.163.org is running 0 daemon pod, expected 1
Jan 30 11:31:38.074: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 30 11:31:38.074: INFO: Node pubt2-nks-for-dev1.dg.163.org is running 0 daemon pod, expected 1
Jan 30 11:31:39.075: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 30 11:31:39.075: INFO: Node pubt2-nks-for-dev1.dg.163.org is running 0 daemon pod, expected 1
Jan 30 11:31:40.074: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 30 11:31:40.074: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 01/30/23 11:31:40.077
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1086, will wait for the garbage collector to delete the pods 01/30/23 11:31:40.078
Jan 30 11:31:40.132: INFO: Deleting DaemonSet.extensions daemon-set took: 2.454619ms
Jan 30 11:31:40.232: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.116876ms
Jan 30 11:31:43.134: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 30 11:31:43.134: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jan 30 11:31:43.136: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"12392"},"items":null}

Jan 30 11:31:43.137: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"12392"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Jan 30 11:31:43.153: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-1086" for this suite. 01/30/23 11:31:43.155
------------------------------
• [SLOW TEST] [10.150 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:194

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 11:31:33.008
    Jan 30 11:31:33.008: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename daemonsets 01/30/23 11:31:33.008
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:31:33.015
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:31:33.017
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should run and stop complex daemon [Conformance]
      test/e2e/apps/daemon_set.go:194
    Jan 30 11:31:33.027: INFO: Creating daemon "daemon-set" with a node selector
    STEP: Initially, daemon pods should not be running on any nodes. 01/30/23 11:31:33.03
    Jan 30 11:31:33.032: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 30 11:31:33.032: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    STEP: Change node label to blue, check that daemon pod is launched. 01/30/23 11:31:33.032
    Jan 30 11:31:33.049: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 30 11:31:33.049: INFO: Node pubt2-nks-for-dev1.dg.163.org is running 0 daemon pod, expected 1
    Jan 30 11:31:34.052: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 30 11:31:34.052: INFO: Node pubt2-nks-for-dev1.dg.163.org is running 0 daemon pod, expected 1
    Jan 30 11:31:35.051: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan 30 11:31:35.051: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
    STEP: Update the node label to green, and wait for daemons to be unscheduled 01/30/23 11:31:35.053
    Jan 30 11:31:35.064: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan 30 11:31:35.064: INFO: Number of running nodes: 0, number of available pods: 1 in daemonset daemon-set
    Jan 30 11:31:36.066: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 30 11:31:36.066: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate 01/30/23 11:31:36.066
    Jan 30 11:31:36.072: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 30 11:31:36.072: INFO: Node pubt2-nks-for-dev1.dg.163.org is running 0 daemon pod, expected 1
    Jan 30 11:31:37.075: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 30 11:31:37.075: INFO: Node pubt2-nks-for-dev1.dg.163.org is running 0 daemon pod, expected 1
    Jan 30 11:31:38.074: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 30 11:31:38.074: INFO: Node pubt2-nks-for-dev1.dg.163.org is running 0 daemon pod, expected 1
    Jan 30 11:31:39.075: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 30 11:31:39.075: INFO: Node pubt2-nks-for-dev1.dg.163.org is running 0 daemon pod, expected 1
    Jan 30 11:31:40.074: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan 30 11:31:40.074: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 01/30/23 11:31:40.077
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1086, will wait for the garbage collector to delete the pods 01/30/23 11:31:40.078
    Jan 30 11:31:40.132: INFO: Deleting DaemonSet.extensions daemon-set took: 2.454619ms
    Jan 30 11:31:40.232: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.116876ms
    Jan 30 11:31:43.134: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 30 11:31:43.134: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Jan 30 11:31:43.136: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"12392"},"items":null}

    Jan 30 11:31:43.137: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"12392"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan 30 11:31:43.153: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-1086" for this suite. 01/30/23 11:31:43.155
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] Variable Expansion
  should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:297
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 11:31:43.158
Jan 30 11:31:43.158: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename var-expansion 01/30/23 11:31:43.159
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:31:43.167
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:31:43.182
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:297
STEP: creating the pod 01/30/23 11:31:43.184
STEP: waiting for pod running 01/30/23 11:31:43.19
Jan 30 11:31:43.190: INFO: Waiting up to 2m0s for pod "var-expansion-1e134aae-3e86-4c31-856f-a00ac679297f" in namespace "var-expansion-3229" to be "running"
Jan 30 11:31:43.192: INFO: Pod "var-expansion-1e134aae-3e86-4c31-856f-a00ac679297f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.042804ms
Jan 30 11:31:45.195: INFO: Pod "var-expansion-1e134aae-3e86-4c31-856f-a00ac679297f": Phase="Running", Reason="", readiness=true. Elapsed: 2.005184769s
Jan 30 11:31:45.195: INFO: Pod "var-expansion-1e134aae-3e86-4c31-856f-a00ac679297f" satisfied condition "running"
STEP: creating a file in subpath 01/30/23 11:31:45.195
Jan 30 11:31:45.197: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-3229 PodName:var-expansion-1e134aae-3e86-4c31-856f-a00ac679297f ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 30 11:31:45.197: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
Jan 30 11:31:45.197: INFO: ExecWithOptions: Clientset creation
Jan 30 11:31:45.197: INFO: ExecWithOptions: execute(POST https://10.178.64.1:443/api/v1/namespaces/var-expansion-3229/pods/var-expansion-1e134aae-3e86-4c31-856f-a00ac679297f/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
STEP: test for file in mounted path 01/30/23 11:31:45.278
Jan 30 11:31:45.279: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-3229 PodName:var-expansion-1e134aae-3e86-4c31-856f-a00ac679297f ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 30 11:31:45.279: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
Jan 30 11:31:45.280: INFO: ExecWithOptions: Clientset creation
Jan 30 11:31:45.280: INFO: ExecWithOptions: execute(POST https://10.178.64.1:443/api/v1/namespaces/var-expansion-3229/pods/var-expansion-1e134aae-3e86-4c31-856f-a00ac679297f/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
STEP: updating the annotation value 01/30/23 11:31:45.358
Jan 30 11:31:45.867: INFO: Successfully updated pod "var-expansion-1e134aae-3e86-4c31-856f-a00ac679297f"
STEP: waiting for annotated pod running 01/30/23 11:31:45.867
Jan 30 11:31:45.867: INFO: Waiting up to 2m0s for pod "var-expansion-1e134aae-3e86-4c31-856f-a00ac679297f" in namespace "var-expansion-3229" to be "running"
Jan 30 11:31:45.869: INFO: Pod "var-expansion-1e134aae-3e86-4c31-856f-a00ac679297f": Phase="Running", Reason="", readiness=true. Elapsed: 1.772921ms
Jan 30 11:31:45.869: INFO: Pod "var-expansion-1e134aae-3e86-4c31-856f-a00ac679297f" satisfied condition "running"
STEP: deleting the pod gracefully 01/30/23 11:31:45.869
Jan 30 11:31:45.869: INFO: Deleting pod "var-expansion-1e134aae-3e86-4c31-856f-a00ac679297f" in namespace "var-expansion-3229"
Jan 30 11:31:45.872: INFO: Wait up to 5m0s for pod "var-expansion-1e134aae-3e86-4c31-856f-a00ac679297f" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Jan 30 11:32:19.876: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-3229" for this suite. 01/30/23 11:32:19.878
------------------------------
• [SLOW TEST] [36.722 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:297

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 11:31:43.158
    Jan 30 11:31:43.158: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename var-expansion 01/30/23 11:31:43.159
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:31:43.167
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:31:43.182
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should succeed in writing subpaths in container [Slow] [Conformance]
      test/e2e/common/node/expansion.go:297
    STEP: creating the pod 01/30/23 11:31:43.184
    STEP: waiting for pod running 01/30/23 11:31:43.19
    Jan 30 11:31:43.190: INFO: Waiting up to 2m0s for pod "var-expansion-1e134aae-3e86-4c31-856f-a00ac679297f" in namespace "var-expansion-3229" to be "running"
    Jan 30 11:31:43.192: INFO: Pod "var-expansion-1e134aae-3e86-4c31-856f-a00ac679297f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.042804ms
    Jan 30 11:31:45.195: INFO: Pod "var-expansion-1e134aae-3e86-4c31-856f-a00ac679297f": Phase="Running", Reason="", readiness=true. Elapsed: 2.005184769s
    Jan 30 11:31:45.195: INFO: Pod "var-expansion-1e134aae-3e86-4c31-856f-a00ac679297f" satisfied condition "running"
    STEP: creating a file in subpath 01/30/23 11:31:45.195
    Jan 30 11:31:45.197: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-3229 PodName:var-expansion-1e134aae-3e86-4c31-856f-a00ac679297f ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 30 11:31:45.197: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    Jan 30 11:31:45.197: INFO: ExecWithOptions: Clientset creation
    Jan 30 11:31:45.197: INFO: ExecWithOptions: execute(POST https://10.178.64.1:443/api/v1/namespaces/var-expansion-3229/pods/var-expansion-1e134aae-3e86-4c31-856f-a00ac679297f/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
    STEP: test for file in mounted path 01/30/23 11:31:45.278
    Jan 30 11:31:45.279: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-3229 PodName:var-expansion-1e134aae-3e86-4c31-856f-a00ac679297f ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 30 11:31:45.279: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    Jan 30 11:31:45.280: INFO: ExecWithOptions: Clientset creation
    Jan 30 11:31:45.280: INFO: ExecWithOptions: execute(POST https://10.178.64.1:443/api/v1/namespaces/var-expansion-3229/pods/var-expansion-1e134aae-3e86-4c31-856f-a00ac679297f/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
    STEP: updating the annotation value 01/30/23 11:31:45.358
    Jan 30 11:31:45.867: INFO: Successfully updated pod "var-expansion-1e134aae-3e86-4c31-856f-a00ac679297f"
    STEP: waiting for annotated pod running 01/30/23 11:31:45.867
    Jan 30 11:31:45.867: INFO: Waiting up to 2m0s for pod "var-expansion-1e134aae-3e86-4c31-856f-a00ac679297f" in namespace "var-expansion-3229" to be "running"
    Jan 30 11:31:45.869: INFO: Pod "var-expansion-1e134aae-3e86-4c31-856f-a00ac679297f": Phase="Running", Reason="", readiness=true. Elapsed: 1.772921ms
    Jan 30 11:31:45.869: INFO: Pod "var-expansion-1e134aae-3e86-4c31-856f-a00ac679297f" satisfied condition "running"
    STEP: deleting the pod gracefully 01/30/23 11:31:45.869
    Jan 30 11:31:45.869: INFO: Deleting pod "var-expansion-1e134aae-3e86-4c31-856f-a00ac679297f" in namespace "var-expansion-3229"
    Jan 30 11:31:45.872: INFO: Wait up to 5m0s for pod "var-expansion-1e134aae-3e86-4c31-856f-a00ac679297f" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Jan 30 11:32:19.876: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-3229" for this suite. 01/30/23 11:32:19.878
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:119
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 11:32:19.881
Jan 30 11:32:19.881: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename projected 01/30/23 11:32:19.882
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:32:19.888
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:32:19.89
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:119
STEP: Creating secret with name projected-secret-test-0d4d7379-ade1-4aa0-8aba-63f9229f442b 01/30/23 11:32:19.892
STEP: Creating a pod to test consume secrets 01/30/23 11:32:19.894
Jan 30 11:32:19.899: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-dae2c82b-9c76-400c-822d-337c27db05a1" in namespace "projected-5217" to be "Succeeded or Failed"
Jan 30 11:32:19.900: INFO: Pod "pod-projected-secrets-dae2c82b-9c76-400c-822d-337c27db05a1": Phase="Pending", Reason="", readiness=false. Elapsed: 1.613735ms
Jan 30 11:32:21.903: INFO: Pod "pod-projected-secrets-dae2c82b-9c76-400c-822d-337c27db05a1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004603228s
Jan 30 11:32:23.904: INFO: Pod "pod-projected-secrets-dae2c82b-9c76-400c-822d-337c27db05a1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005199872s
STEP: Saw pod success 01/30/23 11:32:23.904
Jan 30 11:32:23.904: INFO: Pod "pod-projected-secrets-dae2c82b-9c76-400c-822d-337c27db05a1" satisfied condition "Succeeded or Failed"
Jan 30 11:32:23.906: INFO: Trying to get logs from node pubt2-nks-for-dev1.dg.163.org pod pod-projected-secrets-dae2c82b-9c76-400c-822d-337c27db05a1 container secret-volume-test: <nil>
STEP: delete the pod 01/30/23 11:32:23.915
Jan 30 11:32:23.919: INFO: Waiting for pod pod-projected-secrets-dae2c82b-9c76-400c-822d-337c27db05a1 to disappear
Jan 30 11:32:23.921: INFO: Pod pod-projected-secrets-dae2c82b-9c76-400c-822d-337c27db05a1 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Jan 30 11:32:23.921: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-5217" for this suite. 01/30/23 11:32:23.926
------------------------------
• [4.047 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:119

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 11:32:19.881
    Jan 30 11:32:19.881: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename projected 01/30/23 11:32:19.882
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:32:19.888
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:32:19.89
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:119
    STEP: Creating secret with name projected-secret-test-0d4d7379-ade1-4aa0-8aba-63f9229f442b 01/30/23 11:32:19.892
    STEP: Creating a pod to test consume secrets 01/30/23 11:32:19.894
    Jan 30 11:32:19.899: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-dae2c82b-9c76-400c-822d-337c27db05a1" in namespace "projected-5217" to be "Succeeded or Failed"
    Jan 30 11:32:19.900: INFO: Pod "pod-projected-secrets-dae2c82b-9c76-400c-822d-337c27db05a1": Phase="Pending", Reason="", readiness=false. Elapsed: 1.613735ms
    Jan 30 11:32:21.903: INFO: Pod "pod-projected-secrets-dae2c82b-9c76-400c-822d-337c27db05a1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004603228s
    Jan 30 11:32:23.904: INFO: Pod "pod-projected-secrets-dae2c82b-9c76-400c-822d-337c27db05a1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005199872s
    STEP: Saw pod success 01/30/23 11:32:23.904
    Jan 30 11:32:23.904: INFO: Pod "pod-projected-secrets-dae2c82b-9c76-400c-822d-337c27db05a1" satisfied condition "Succeeded or Failed"
    Jan 30 11:32:23.906: INFO: Trying to get logs from node pubt2-nks-for-dev1.dg.163.org pod pod-projected-secrets-dae2c82b-9c76-400c-822d-337c27db05a1 container secret-volume-test: <nil>
    STEP: delete the pod 01/30/23 11:32:23.915
    Jan 30 11:32:23.919: INFO: Waiting for pod pod-projected-secrets-dae2c82b-9c76-400c-822d-337c27db05a1 to disappear
    Jan 30 11:32:23.921: INFO: Pod pod-projected-secrets-dae2c82b-9c76-400c-822d-337c27db05a1 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Jan 30 11:32:23.921: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-5217" for this suite. 01/30/23 11:32:23.926
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-auth] ServiceAccounts
  should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:161
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 11:32:23.928
Jan 30 11:32:23.928: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename svcaccounts 01/30/23 11:32:23.929
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:32:23.935
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:32:23.938
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:161
Jan 30 11:32:23.947: INFO: created pod pod-service-account-defaultsa
Jan 30 11:32:23.947: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Jan 30 11:32:23.950: INFO: created pod pod-service-account-mountsa
Jan 30 11:32:23.950: INFO: pod pod-service-account-mountsa service account token volume mount: true
Jan 30 11:32:23.952: INFO: created pod pod-service-account-nomountsa
Jan 30 11:32:23.952: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Jan 30 11:32:23.954: INFO: created pod pod-service-account-defaultsa-mountspec
Jan 30 11:32:23.954: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Jan 30 11:32:23.957: INFO: created pod pod-service-account-mountsa-mountspec
Jan 30 11:32:23.957: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Jan 30 11:32:23.960: INFO: created pod pod-service-account-nomountsa-mountspec
Jan 30 11:32:23.960: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Jan 30 11:32:23.962: INFO: created pod pod-service-account-defaultsa-nomountspec
Jan 30 11:32:23.962: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Jan 30 11:32:23.964: INFO: created pod pod-service-account-mountsa-nomountspec
Jan 30 11:32:23.964: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Jan 30 11:32:23.967: INFO: created pod pod-service-account-nomountsa-nomountspec
Jan 30 11:32:23.967: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Jan 30 11:32:23.967: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-8505" for this suite. 01/30/23 11:32:23.969
------------------------------
• [0.043 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:161

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 11:32:23.928
    Jan 30 11:32:23.928: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename svcaccounts 01/30/23 11:32:23.929
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:32:23.935
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:32:23.938
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow opting out of API token automount  [Conformance]
      test/e2e/auth/service_accounts.go:161
    Jan 30 11:32:23.947: INFO: created pod pod-service-account-defaultsa
    Jan 30 11:32:23.947: INFO: pod pod-service-account-defaultsa service account token volume mount: true
    Jan 30 11:32:23.950: INFO: created pod pod-service-account-mountsa
    Jan 30 11:32:23.950: INFO: pod pod-service-account-mountsa service account token volume mount: true
    Jan 30 11:32:23.952: INFO: created pod pod-service-account-nomountsa
    Jan 30 11:32:23.952: INFO: pod pod-service-account-nomountsa service account token volume mount: false
    Jan 30 11:32:23.954: INFO: created pod pod-service-account-defaultsa-mountspec
    Jan 30 11:32:23.954: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
    Jan 30 11:32:23.957: INFO: created pod pod-service-account-mountsa-mountspec
    Jan 30 11:32:23.957: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
    Jan 30 11:32:23.960: INFO: created pod pod-service-account-nomountsa-mountspec
    Jan 30 11:32:23.960: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
    Jan 30 11:32:23.962: INFO: created pod pod-service-account-defaultsa-nomountspec
    Jan 30 11:32:23.962: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
    Jan 30 11:32:23.964: INFO: created pod pod-service-account-mountsa-nomountspec
    Jan 30 11:32:23.964: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
    Jan 30 11:32:23.967: INFO: created pod pod-service-account-nomountsa-nomountspec
    Jan 30 11:32:23.967: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Jan 30 11:32:23.967: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-8505" for this suite. 01/30/23 11:32:23.969
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test when starting a container that exits
  should run with the expected status [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:52
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 11:32:23.972
Jan 30 11:32:23.972: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename container-runtime 01/30/23 11:32:23.972
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:32:23.978
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:32:23.98
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should run with the expected status [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:52
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount' 01/30/23 11:32:23.986
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase' 01/30/23 11:32:45.036
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition 01/30/23 11:32:45.038
STEP: Container 'terminate-cmd-rpa': should get the expected 'State' 01/30/23 11:32:45.041
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance] 01/30/23 11:32:45.041
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount' 01/30/23 11:32:45.05
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase' 01/30/23 11:32:48.059
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition 01/30/23 11:32:50.065
STEP: Container 'terminate-cmd-rpof': should get the expected 'State' 01/30/23 11:32:50.068
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance] 01/30/23 11:32:50.068
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount' 01/30/23 11:32:50.077
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase' 01/30/23 11:32:51.081
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition 01/30/23 11:32:54.09
STEP: Container 'terminate-cmd-rpn': should get the expected 'State' 01/30/23 11:32:54.093
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance] 01/30/23 11:32:54.093
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Jan 30 11:32:54.103: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-2535" for this suite. 01/30/23 11:32:54.106
------------------------------
• [SLOW TEST] [30.136 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    when starting a container that exits
    test/e2e/common/node/runtime.go:45
      should run with the expected status [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:52

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 11:32:23.972
    Jan 30 11:32:23.972: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename container-runtime 01/30/23 11:32:23.972
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:32:23.978
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:32:23.98
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should run with the expected status [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:52
    STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount' 01/30/23 11:32:23.986
    STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase' 01/30/23 11:32:45.036
    STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition 01/30/23 11:32:45.038
    STEP: Container 'terminate-cmd-rpa': should get the expected 'State' 01/30/23 11:32:45.041
    STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance] 01/30/23 11:32:45.041
    STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount' 01/30/23 11:32:45.05
    STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase' 01/30/23 11:32:48.059
    STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition 01/30/23 11:32:50.065
    STEP: Container 'terminate-cmd-rpof': should get the expected 'State' 01/30/23 11:32:50.068
    STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance] 01/30/23 11:32:50.068
    STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount' 01/30/23 11:32:50.077
    STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase' 01/30/23 11:32:51.081
    STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition 01/30/23 11:32:54.09
    STEP: Container 'terminate-cmd-rpn': should get the expected 'State' 01/30/23 11:32:54.093
    STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance] 01/30/23 11:32:54.093
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Jan 30 11:32:54.103: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-2535" for this suite. 01/30/23 11:32:54.106
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Single Pod [Serial]
  removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:293
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 11:32:54.108
Jan 30 11:32:54.109: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename taint-single-pod 01/30/23 11:32:54.109
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:32:54.116
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:32:54.118
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/node/taints.go:170
Jan 30 11:32:54.120: INFO: Waiting up to 1m0s for all nodes to be ready
Jan 30 11:33:54.143: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:293
Jan 30 11:33:54.145: INFO: Starting informer...
STEP: Starting pod... 01/30/23 11:33:54.145
Jan 30 11:33:54.354: INFO: Pod is running on pubt2-nks-for-dev1.dg.163.org. Tainting Node
STEP: Trying to apply a taint on the Node 01/30/23 11:33:54.354
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 01/30/23 11:33:54.363
STEP: Waiting short time to make sure Pod is queued for deletion 01/30/23 11:33:54.365
Jan 30 11:33:54.365: INFO: Pod wasn't evicted. Proceeding
Jan 30 11:33:54.365: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 01/30/23 11:33:54.377
STEP: Waiting some time to make sure that toleration time passed. 01/30/23 11:33:54.379
Jan 30 11:35:09.380: INFO: Pod wasn't evicted. Test successful
[AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/node/init/init.go:32
Jan 30 11:35:09.380: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "taint-single-pod-982" for this suite. 01/30/23 11:35:09.384
------------------------------
• [SLOW TEST] [135.278 seconds]
[sig-node] NoExecuteTaintManager Single Pod [Serial]
test/e2e/node/framework.go:23
  removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:293

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 11:32:54.108
    Jan 30 11:32:54.109: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename taint-single-pod 01/30/23 11:32:54.109
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:32:54.116
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:32:54.118
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/node/taints.go:170
    Jan 30 11:32:54.120: INFO: Waiting up to 1m0s for all nodes to be ready
    Jan 30 11:33:54.143: INFO: Waiting for terminating namespaces to be deleted...
    [It] removing taint cancels eviction [Disruptive] [Conformance]
      test/e2e/node/taints.go:293
    Jan 30 11:33:54.145: INFO: Starting informer...
    STEP: Starting pod... 01/30/23 11:33:54.145
    Jan 30 11:33:54.354: INFO: Pod is running on pubt2-nks-for-dev1.dg.163.org. Tainting Node
    STEP: Trying to apply a taint on the Node 01/30/23 11:33:54.354
    STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 01/30/23 11:33:54.363
    STEP: Waiting short time to make sure Pod is queued for deletion 01/30/23 11:33:54.365
    Jan 30 11:33:54.365: INFO: Pod wasn't evicted. Proceeding
    Jan 30 11:33:54.365: INFO: Removing taint from Node
    STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 01/30/23 11:33:54.377
    STEP: Waiting some time to make sure that toleration time passed. 01/30/23 11:33:54.379
    Jan 30 11:35:09.380: INFO: Pod wasn't evicted. Test successful
    [AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan 30 11:35:09.380: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "taint-single-pod-982" for this suite. 01/30/23 11:35:09.384
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-auth] ServiceAccounts
  should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:275
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 11:35:09.387
Jan 30 11:35:09.387: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename svcaccounts 01/30/23 11:35:09.387
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:35:09.394
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:35:09.396
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:275
STEP: Creating a pod to test service account token:  01/30/23 11:35:09.398
Jan 30 11:35:09.402: INFO: Waiting up to 5m0s for pod "test-pod-0d6f56ad-2384-4618-b6a4-ae0ea9c5844c" in namespace "svcaccounts-833" to be "Succeeded or Failed"
Jan 30 11:35:09.403: INFO: Pod "test-pod-0d6f56ad-2384-4618-b6a4-ae0ea9c5844c": Phase="Pending", Reason="", readiness=false. Elapsed: 1.524187ms
Jan 30 11:35:11.406: INFO: Pod "test-pod-0d6f56ad-2384-4618-b6a4-ae0ea9c5844c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00380297s
Jan 30 11:35:13.406: INFO: Pod "test-pod-0d6f56ad-2384-4618-b6a4-ae0ea9c5844c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004398902s
STEP: Saw pod success 01/30/23 11:35:13.406
Jan 30 11:35:13.406: INFO: Pod "test-pod-0d6f56ad-2384-4618-b6a4-ae0ea9c5844c" satisfied condition "Succeeded or Failed"
Jan 30 11:35:13.408: INFO: Trying to get logs from node pubt2-nks-for-dev1.dg.163.org pod test-pod-0d6f56ad-2384-4618-b6a4-ae0ea9c5844c container agnhost-container: <nil>
STEP: delete the pod 01/30/23 11:35:13.42
Jan 30 11:35:13.424: INFO: Waiting for pod test-pod-0d6f56ad-2384-4618-b6a4-ae0ea9c5844c to disappear
Jan 30 11:35:13.426: INFO: Pod test-pod-0d6f56ad-2384-4618-b6a4-ae0ea9c5844c no longer exists
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Jan 30 11:35:13.426: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-833" for this suite. 01/30/23 11:35:13.428
------------------------------
• [4.044 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:275

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 11:35:09.387
    Jan 30 11:35:09.387: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename svcaccounts 01/30/23 11:35:09.387
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:35:09.394
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:35:09.396
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should mount projected service account token [Conformance]
      test/e2e/auth/service_accounts.go:275
    STEP: Creating a pod to test service account token:  01/30/23 11:35:09.398
    Jan 30 11:35:09.402: INFO: Waiting up to 5m0s for pod "test-pod-0d6f56ad-2384-4618-b6a4-ae0ea9c5844c" in namespace "svcaccounts-833" to be "Succeeded or Failed"
    Jan 30 11:35:09.403: INFO: Pod "test-pod-0d6f56ad-2384-4618-b6a4-ae0ea9c5844c": Phase="Pending", Reason="", readiness=false. Elapsed: 1.524187ms
    Jan 30 11:35:11.406: INFO: Pod "test-pod-0d6f56ad-2384-4618-b6a4-ae0ea9c5844c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00380297s
    Jan 30 11:35:13.406: INFO: Pod "test-pod-0d6f56ad-2384-4618-b6a4-ae0ea9c5844c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004398902s
    STEP: Saw pod success 01/30/23 11:35:13.406
    Jan 30 11:35:13.406: INFO: Pod "test-pod-0d6f56ad-2384-4618-b6a4-ae0ea9c5844c" satisfied condition "Succeeded or Failed"
    Jan 30 11:35:13.408: INFO: Trying to get logs from node pubt2-nks-for-dev1.dg.163.org pod test-pod-0d6f56ad-2384-4618-b6a4-ae0ea9c5844c container agnhost-container: <nil>
    STEP: delete the pod 01/30/23 11:35:13.42
    Jan 30 11:35:13.424: INFO: Waiting for pod test-pod-0d6f56ad-2384-4618-b6a4-ae0ea9c5844c to disappear
    Jan 30 11:35:13.426: INFO: Pod test-pod-0d6f56ad-2384-4618-b6a4-ae0ea9c5844c no longer exists
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Jan 30 11:35:13.426: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-833" for this suite. 01/30/23 11:35:13.428
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 11:35:13.432
Jan 30 11:35:13.432: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename deployment 01/30/23 11:35:13.433
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:35:13.439
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:35:13.441
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105
Jan 30 11:35:13.443: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Jan 30 11:35:13.447: INFO: Pod name sample-pod: Found 0 pods out of 1
Jan 30 11:35:18.450: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 01/30/23 11:35:18.451
Jan 30 11:35:18.451: INFO: Creating deployment "test-rolling-update-deployment"
Jan 30 11:35:18.455: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Jan 30 11:35:18.459: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Jan 30 11:35:20.465: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Jan 30 11:35:20.466: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 30, 11, 35, 18, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 30, 11, 35, 18, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 30, 11, 35, 18, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 30, 11, 35, 18, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-69759bb4\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 30 11:35:22.469: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan 30 11:35:22.474: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-734  0dd2eba9-63f9-4483-89a0-1240e89055f2 13225 1 2023-01-30 11:35:18 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] [] [{e2e.test Update apps/v1 2023-01-30 11:35:18 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-30 11:35:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] [] []} {[] [] [{agnhost harbor.cloud.netease.com/qzprod-k8s/e2e/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004793a18 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-01-30 11:35:18 +0000 UTC,LastTransitionTime:2023-01-30 11:35:18 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-69759bb4" has successfully progressed.,LastUpdateTime:2023-01-30 11:35:20 +0000 UTC,LastTransitionTime:2023-01-30 11:35:18 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jan 30 11:35:22.476: INFO: New ReplicaSet "test-rolling-update-deployment-69759bb4" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-69759bb4  deployment-734  6f9db062-0d9d-4132-852a-c18b6c91c8de 13215 1 2023-01-30 11:35:18 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:69759bb4] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 0dd2eba9-63f9-4483-89a0-1240e89055f2 0xc002410b7f 0xc002410b90}] [] [{kube-controller-manager Update apps/v1 2023-01-30 11:35:18 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0dd2eba9-63f9-4483-89a0-1240e89055f2\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-30 11:35:20 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 69759bb4,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:69759bb4] map[] [] [] []} {[] [] [{agnhost harbor.cloud.netease.com/qzprod-k8s/e2e/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002410c38 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jan 30 11:35:22.476: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Jan 30 11:35:22.476: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-734  336a9e4c-13c7-4599-945b-fdb4c1fd22ab 13224 2 2023-01-30 11:35:13 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 0dd2eba9-63f9-4483-89a0-1240e89055f2 0xc002410a67 0xc002410a68}] [] [{e2e.test Update apps/v1 2023-01-30 11:35:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-30 11:35:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0dd2eba9-63f9-4483-89a0-1240e89055f2\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-01-30 11:35:20 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] [] []} {[] [] [{httpd harbor.cloud.netease.com/qzprod-k8s/e2e/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc002410b28 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan 30 11:35:22.478: INFO: Pod "test-rolling-update-deployment-69759bb4-xhmr5" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-69759bb4-xhmr5 test-rolling-update-deployment-69759bb4- deployment-734  e512f065-d273-43d0-82d0-0ee2267e0310 13214 0 2023-01-30 11:35:18 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:69759bb4] map[cni.projectcalico.org/podIP:10.178.151.49/32 cni.projectcalico.org/podIPs:10.178.151.49/32] [{apps/v1 ReplicaSet test-rolling-update-deployment-69759bb4 6f9db062-0d9d-4132-852a-c18b6c91c8de 0xc006e60b2f 0xc006e60b60}] [] [{kube-controller-manager Update v1 2023-01-30 11:35:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6f9db062-0d9d-4132-852a-c18b6c91c8de\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-30 11:35:19 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-30 11:35:20 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.178.151.49\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mbrl8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:harbor.cloud.netease.com/qzprod-k8s/e2e/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mbrl8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pubt2-nks-for-dev1.dg.163.org,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 11:35:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 11:35:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 11:35:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 11:35:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.182.0.82,PodIP:10.178.151.49,StartTime:2023-01-30 11:35:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-30 11:35:19 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:harbor.cloud.netease.com/qzprod-k8s/e2e/agnhost:2.43,ImageID:docker-pullable://harbor.cloud.netease.com/qzprod-k8s/e2e/agnhost@sha256:c338e9c1470d7e0e40b21e2fe191b99c1855ce0a6c265a2eacf70f7ed6a979f6,ContainerID:docker://13f06bcab7a2bf2cdd2da43a769283a1bab541e51647b48be4321974e69f9bbe,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.178.151.49,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Jan 30 11:35:22.478: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-734" for this suite. 01/30/23 11:35:22.48
------------------------------
• [SLOW TEST] [9.050 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 11:35:13.432
    Jan 30 11:35:13.432: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename deployment 01/30/23 11:35:13.433
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:35:13.439
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:35:13.441
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
      test/e2e/apps/deployment.go:105
    Jan 30 11:35:13.443: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
    Jan 30 11:35:13.447: INFO: Pod name sample-pod: Found 0 pods out of 1
    Jan 30 11:35:18.450: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 01/30/23 11:35:18.451
    Jan 30 11:35:18.451: INFO: Creating deployment "test-rolling-update-deployment"
    Jan 30 11:35:18.455: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
    Jan 30 11:35:18.459: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
    Jan 30 11:35:20.465: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
    Jan 30 11:35:20.466: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 30, 11, 35, 18, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 30, 11, 35, 18, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 30, 11, 35, 18, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 30, 11, 35, 18, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-69759bb4\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 30 11:35:22.469: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jan 30 11:35:22.474: INFO: Deployment "test-rolling-update-deployment":
    &Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-734  0dd2eba9-63f9-4483-89a0-1240e89055f2 13225 1 2023-01-30 11:35:18 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] [] [{e2e.test Update apps/v1 2023-01-30 11:35:18 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-30 11:35:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] [] []} {[] [] [{agnhost harbor.cloud.netease.com/qzprod-k8s/e2e/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004793a18 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-01-30 11:35:18 +0000 UTC,LastTransitionTime:2023-01-30 11:35:18 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-69759bb4" has successfully progressed.,LastUpdateTime:2023-01-30 11:35:20 +0000 UTC,LastTransitionTime:2023-01-30 11:35:18 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Jan 30 11:35:22.476: INFO: New ReplicaSet "test-rolling-update-deployment-69759bb4" of Deployment "test-rolling-update-deployment":
    &ReplicaSet{ObjectMeta:{test-rolling-update-deployment-69759bb4  deployment-734  6f9db062-0d9d-4132-852a-c18b6c91c8de 13215 1 2023-01-30 11:35:18 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:69759bb4] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 0dd2eba9-63f9-4483-89a0-1240e89055f2 0xc002410b7f 0xc002410b90}] [] [{kube-controller-manager Update apps/v1 2023-01-30 11:35:18 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0dd2eba9-63f9-4483-89a0-1240e89055f2\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-30 11:35:20 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 69759bb4,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:69759bb4] map[] [] [] []} {[] [] [{agnhost harbor.cloud.netease.com/qzprod-k8s/e2e/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002410c38 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Jan 30 11:35:22.476: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
    Jan 30 11:35:22.476: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-734  336a9e4c-13c7-4599-945b-fdb4c1fd22ab 13224 2 2023-01-30 11:35:13 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 0dd2eba9-63f9-4483-89a0-1240e89055f2 0xc002410a67 0xc002410a68}] [] [{e2e.test Update apps/v1 2023-01-30 11:35:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-30 11:35:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0dd2eba9-63f9-4483-89a0-1240e89055f2\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-01-30 11:35:20 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] [] []} {[] [] [{httpd harbor.cloud.netease.com/qzprod-k8s/e2e/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc002410b28 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Jan 30 11:35:22.478: INFO: Pod "test-rolling-update-deployment-69759bb4-xhmr5" is available:
    &Pod{ObjectMeta:{test-rolling-update-deployment-69759bb4-xhmr5 test-rolling-update-deployment-69759bb4- deployment-734  e512f065-d273-43d0-82d0-0ee2267e0310 13214 0 2023-01-30 11:35:18 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:69759bb4] map[cni.projectcalico.org/podIP:10.178.151.49/32 cni.projectcalico.org/podIPs:10.178.151.49/32] [{apps/v1 ReplicaSet test-rolling-update-deployment-69759bb4 6f9db062-0d9d-4132-852a-c18b6c91c8de 0xc006e60b2f 0xc006e60b60}] [] [{kube-controller-manager Update v1 2023-01-30 11:35:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6f9db062-0d9d-4132-852a-c18b6c91c8de\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-30 11:35:19 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-30 11:35:20 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.178.151.49\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mbrl8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:harbor.cloud.netease.com/qzprod-k8s/e2e/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mbrl8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pubt2-nks-for-dev1.dg.163.org,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 11:35:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 11:35:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 11:35:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 11:35:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.182.0.82,PodIP:10.178.151.49,StartTime:2023-01-30 11:35:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-30 11:35:19 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:harbor.cloud.netease.com/qzprod-k8s/e2e/agnhost:2.43,ImageID:docker-pullable://harbor.cloud.netease.com/qzprod-k8s/e2e/agnhost@sha256:c338e9c1470d7e0e40b21e2fe191b99c1855ce0a6c265a2eacf70f7ed6a979f6,ContainerID:docker://13f06bcab7a2bf2cdd2da43a769283a1bab541e51647b48be4321974e69f9bbe,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.178.151.49,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Jan 30 11:35:22.478: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-734" for this suite. 01/30/23 11:35:22.48
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 11:35:22.483
Jan 30 11:35:22.483: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename deployment 01/30/23 11:35:22.484
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:35:22.489
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:35:22.491
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132
Jan 30 11:35:22.497: INFO: Pod name rollover-pod: Found 0 pods out of 1
Jan 30 11:35:27.503: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 01/30/23 11:35:27.503
Jan 30 11:35:27.503: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Jan 30 11:35:29.506: INFO: Creating deployment "test-rollover-deployment"
Jan 30 11:35:29.511: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Jan 30 11:35:31.515: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Jan 30 11:35:31.519: INFO: Ensure that both replica sets have 1 created replica
Jan 30 11:35:31.522: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Jan 30 11:35:31.527: INFO: Updating deployment test-rollover-deployment
Jan 30 11:35:31.528: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Jan 30 11:35:33.531: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Jan 30 11:35:33.535: INFO: Make sure deployment "test-rollover-deployment" is complete
Jan 30 11:35:33.538: INFO: all replica sets need to contain the pod-template-hash label
Jan 30 11:35:33.538: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 30, 11, 35, 29, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 30, 11, 35, 29, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 30, 11, 35, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 30, 11, 35, 29, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-68cc5554d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 30 11:35:35.543: INFO: all replica sets need to contain the pod-template-hash label
Jan 30 11:35:35.543: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 30, 11, 35, 29, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 30, 11, 35, 29, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 30, 11, 35, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 30, 11, 35, 29, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-68cc5554d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 30 11:35:37.544: INFO: all replica sets need to contain the pod-template-hash label
Jan 30 11:35:37.544: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 30, 11, 35, 29, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 30, 11, 35, 29, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 30, 11, 35, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 30, 11, 35, 29, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-68cc5554d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 30 11:35:39.543: INFO: all replica sets need to contain the pod-template-hash label
Jan 30 11:35:39.543: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 30, 11, 35, 29, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 30, 11, 35, 29, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 30, 11, 35, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 30, 11, 35, 29, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-68cc5554d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 30 11:35:41.545: INFO: all replica sets need to contain the pod-template-hash label
Jan 30 11:35:41.545: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 30, 11, 35, 29, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 30, 11, 35, 29, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 30, 11, 35, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 30, 11, 35, 29, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-68cc5554d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 30 11:35:43.542: INFO: 
Jan 30 11:35:43.542: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan 30 11:35:43.547: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-345  e5fcfba6-69a9-4911-84f7-a2217d93fb12 13375 2 2023-01-30 11:35:29 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-01-30 11:35:31 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-30 11:35:43 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] [] []} {[] [] [{agnhost harbor.cloud.netease.com/qzprod-k8s/e2e/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00389fbe8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-01-30 11:35:29 +0000 UTC,LastTransitionTime:2023-01-30 11:35:29 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-68cc5554d9" has successfully progressed.,LastUpdateTime:2023-01-30 11:35:43 +0000 UTC,LastTransitionTime:2023-01-30 11:35:29 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jan 30 11:35:43.549: INFO: New ReplicaSet "test-rollover-deployment-68cc5554d9" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-68cc5554d9  deployment-345  f98aaae4-4275-4712-a88e-d757cffac939 13365 2 2023-01-30 11:35:31 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:68cc5554d9] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment e5fcfba6-69a9-4911-84f7-a2217d93fb12 0xc006bdb0d7 0xc006bdb0d8}] [] [{kube-controller-manager Update apps/v1 2023-01-30 11:35:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e5fcfba6-69a9-4911-84f7-a2217d93fb12\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-30 11:35:43 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 68cc5554d9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:68cc5554d9] map[] [] [] []} {[] [] [{agnhost harbor.cloud.netease.com/qzprod-k8s/e2e/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc006bdb188 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jan 30 11:35:43.549: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Jan 30 11:35:43.549: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-345  0021f19f-67a6-4a70-8cc3-8441365f3737 13374 2 2023-01-30 11:35:22 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment e5fcfba6-69a9-4911-84f7-a2217d93fb12 0xc006bdafa7 0xc006bdafa8}] [] [{e2e.test Update apps/v1 2023-01-30 11:35:22 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-30 11:35:43 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e5fcfba6-69a9-4911-84f7-a2217d93fb12\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-01-30 11:35:43 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] [] []} {[] [] [{httpd harbor.cloud.netease.com/qzprod-k8s/e2e/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc006bdb068 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan 30 11:35:43.549: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-768dcbc65b  deployment-345  f68a2155-0626-4dec-879d-405e138dbb06 13327 2 2023-01-30 11:35:29 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:768dcbc65b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment e5fcfba6-69a9-4911-84f7-a2217d93fb12 0xc006bdb1f7 0xc006bdb1f8}] [] [{kube-controller-manager Update apps/v1 2023-01-30 11:35:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e5fcfba6-69a9-4911-84f7-a2217d93fb12\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-30 11:35:31 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 768dcbc65b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:768dcbc65b] map[] [] [] []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc006bdb2a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan 30 11:35:43.551: INFO: Pod "test-rollover-deployment-68cc5554d9-zncls" is available:
&Pod{ObjectMeta:{test-rollover-deployment-68cc5554d9-zncls test-rollover-deployment-68cc5554d9- deployment-345  0c28314c-fa26-4038-880a-56424373d5b5 13346 0 2023-01-30 11:35:31 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:68cc5554d9] map[cni.projectcalico.org/podIP:10.178.151.52/32 cni.projectcalico.org/podIPs:10.178.151.52/32] [{apps/v1 ReplicaSet test-rollover-deployment-68cc5554d9 f98aaae4-4275-4712-a88e-d757cffac939 0xc00389ff87 0xc00389ff88}] [] [{kube-controller-manager Update v1 2023-01-30 11:35:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f98aaae4-4275-4712-a88e-d757cffac939\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-30 11:35:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-30 11:35:33 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.178.151.52\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-57vxn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:harbor.cloud.netease.com/qzprod-k8s/e2e/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-57vxn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pubt2-nks-for-dev1.dg.163.org,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 11:35:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 11:35:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 11:35:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 11:35:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.182.0.82,PodIP:10.178.151.52,StartTime:2023-01-30 11:35:31 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-30 11:35:32 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:harbor.cloud.netease.com/qzprod-k8s/e2e/agnhost:2.43,ImageID:docker-pullable://harbor.cloud.netease.com/qzprod-k8s/e2e/agnhost@sha256:c338e9c1470d7e0e40b21e2fe191b99c1855ce0a6c265a2eacf70f7ed6a979f6,ContainerID:docker://6fbf706467327e2fbcfc32f4af9affbfa0c16e14621c11579854cc84f99dfd47,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.178.151.52,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Jan 30 11:35:43.551: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-345" for this suite. 01/30/23 11:35:43.553
------------------------------
• [SLOW TEST] [21.072 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 11:35:22.483
    Jan 30 11:35:22.483: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename deployment 01/30/23 11:35:22.484
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:35:22.489
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:35:22.491
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should support rollover [Conformance]
      test/e2e/apps/deployment.go:132
    Jan 30 11:35:22.497: INFO: Pod name rollover-pod: Found 0 pods out of 1
    Jan 30 11:35:27.503: INFO: Pod name rollover-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 01/30/23 11:35:27.503
    Jan 30 11:35:27.503: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
    Jan 30 11:35:29.506: INFO: Creating deployment "test-rollover-deployment"
    Jan 30 11:35:29.511: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
    Jan 30 11:35:31.515: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
    Jan 30 11:35:31.519: INFO: Ensure that both replica sets have 1 created replica
    Jan 30 11:35:31.522: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
    Jan 30 11:35:31.527: INFO: Updating deployment test-rollover-deployment
    Jan 30 11:35:31.528: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
    Jan 30 11:35:33.531: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
    Jan 30 11:35:33.535: INFO: Make sure deployment "test-rollover-deployment" is complete
    Jan 30 11:35:33.538: INFO: all replica sets need to contain the pod-template-hash label
    Jan 30 11:35:33.538: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 30, 11, 35, 29, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 30, 11, 35, 29, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 30, 11, 35, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 30, 11, 35, 29, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-68cc5554d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 30 11:35:35.543: INFO: all replica sets need to contain the pod-template-hash label
    Jan 30 11:35:35.543: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 30, 11, 35, 29, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 30, 11, 35, 29, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 30, 11, 35, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 30, 11, 35, 29, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-68cc5554d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 30 11:35:37.544: INFO: all replica sets need to contain the pod-template-hash label
    Jan 30 11:35:37.544: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 30, 11, 35, 29, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 30, 11, 35, 29, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 30, 11, 35, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 30, 11, 35, 29, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-68cc5554d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 30 11:35:39.543: INFO: all replica sets need to contain the pod-template-hash label
    Jan 30 11:35:39.543: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 30, 11, 35, 29, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 30, 11, 35, 29, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 30, 11, 35, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 30, 11, 35, 29, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-68cc5554d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 30 11:35:41.545: INFO: all replica sets need to contain the pod-template-hash label
    Jan 30 11:35:41.545: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 30, 11, 35, 29, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 30, 11, 35, 29, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 30, 11, 35, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 30, 11, 35, 29, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-68cc5554d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 30 11:35:43.542: INFO: 
    Jan 30 11:35:43.542: INFO: Ensure that both old replica sets have no replicas
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jan 30 11:35:43.547: INFO: Deployment "test-rollover-deployment":
    &Deployment{ObjectMeta:{test-rollover-deployment  deployment-345  e5fcfba6-69a9-4911-84f7-a2217d93fb12 13375 2 2023-01-30 11:35:29 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-01-30 11:35:31 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-30 11:35:43 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] [] []} {[] [] [{agnhost harbor.cloud.netease.com/qzprod-k8s/e2e/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00389fbe8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-01-30 11:35:29 +0000 UTC,LastTransitionTime:2023-01-30 11:35:29 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-68cc5554d9" has successfully progressed.,LastUpdateTime:2023-01-30 11:35:43 +0000 UTC,LastTransitionTime:2023-01-30 11:35:29 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Jan 30 11:35:43.549: INFO: New ReplicaSet "test-rollover-deployment-68cc5554d9" of Deployment "test-rollover-deployment":
    &ReplicaSet{ObjectMeta:{test-rollover-deployment-68cc5554d9  deployment-345  f98aaae4-4275-4712-a88e-d757cffac939 13365 2 2023-01-30 11:35:31 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:68cc5554d9] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment e5fcfba6-69a9-4911-84f7-a2217d93fb12 0xc006bdb0d7 0xc006bdb0d8}] [] [{kube-controller-manager Update apps/v1 2023-01-30 11:35:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e5fcfba6-69a9-4911-84f7-a2217d93fb12\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-30 11:35:43 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 68cc5554d9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:68cc5554d9] map[] [] [] []} {[] [] [{agnhost harbor.cloud.netease.com/qzprod-k8s/e2e/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc006bdb188 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Jan 30 11:35:43.549: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
    Jan 30 11:35:43.549: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-345  0021f19f-67a6-4a70-8cc3-8441365f3737 13374 2 2023-01-30 11:35:22 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment e5fcfba6-69a9-4911-84f7-a2217d93fb12 0xc006bdafa7 0xc006bdafa8}] [] [{e2e.test Update apps/v1 2023-01-30 11:35:22 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-30 11:35:43 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e5fcfba6-69a9-4911-84f7-a2217d93fb12\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-01-30 11:35:43 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] [] []} {[] [] [{httpd harbor.cloud.netease.com/qzprod-k8s/e2e/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc006bdb068 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Jan 30 11:35:43.549: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-768dcbc65b  deployment-345  f68a2155-0626-4dec-879d-405e138dbb06 13327 2 2023-01-30 11:35:29 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:768dcbc65b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment e5fcfba6-69a9-4911-84f7-a2217d93fb12 0xc006bdb1f7 0xc006bdb1f8}] [] [{kube-controller-manager Update apps/v1 2023-01-30 11:35:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e5fcfba6-69a9-4911-84f7-a2217d93fb12\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-30 11:35:31 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 768dcbc65b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:768dcbc65b] map[] [] [] []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc006bdb2a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Jan 30 11:35:43.551: INFO: Pod "test-rollover-deployment-68cc5554d9-zncls" is available:
    &Pod{ObjectMeta:{test-rollover-deployment-68cc5554d9-zncls test-rollover-deployment-68cc5554d9- deployment-345  0c28314c-fa26-4038-880a-56424373d5b5 13346 0 2023-01-30 11:35:31 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:68cc5554d9] map[cni.projectcalico.org/podIP:10.178.151.52/32 cni.projectcalico.org/podIPs:10.178.151.52/32] [{apps/v1 ReplicaSet test-rollover-deployment-68cc5554d9 f98aaae4-4275-4712-a88e-d757cffac939 0xc00389ff87 0xc00389ff88}] [] [{kube-controller-manager Update v1 2023-01-30 11:35:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f98aaae4-4275-4712-a88e-d757cffac939\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-30 11:35:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-30 11:35:33 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.178.151.52\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-57vxn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:harbor.cloud.netease.com/qzprod-k8s/e2e/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-57vxn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pubt2-nks-for-dev1.dg.163.org,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 11:35:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 11:35:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 11:35:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 11:35:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.182.0.82,PodIP:10.178.151.52,StartTime:2023-01-30 11:35:31 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-30 11:35:32 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:harbor.cloud.netease.com/qzprod-k8s/e2e/agnhost:2.43,ImageID:docker-pullable://harbor.cloud.netease.com/qzprod-k8s/e2e/agnhost@sha256:c338e9c1470d7e0e40b21e2fe191b99c1855ce0a6c265a2eacf70f7ed6a979f6,ContainerID:docker://6fbf706467327e2fbcfc32f4af9affbfa0c16e14621c11579854cc84f99dfd47,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.178.151.52,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Jan 30 11:35:43.551: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-345" for this suite. 01/30/23 11:35:43.553
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should have a working scale subresource [Conformance]
  test/e2e/apps/statefulset.go:848
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 11:35:43.556
Jan 30 11:35:43.556: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename statefulset 01/30/23 11:35:43.556
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:35:43.563
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:35:43.564
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-2870 01/30/23 11:35:43.566
[It] should have a working scale subresource [Conformance]
  test/e2e/apps/statefulset.go:848
STEP: Creating statefulset ss in namespace statefulset-2870 01/30/23 11:35:43.569
Jan 30 11:35:43.573: INFO: Found 0 stateful pods, waiting for 1
Jan 30 11:35:53.576: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource 01/30/23 11:35:53.579
STEP: updating a scale subresource 01/30/23 11:35:53.581
STEP: verifying the statefulset Spec.Replicas was modified 01/30/23 11:35:53.584
STEP: Patch a scale subresource 01/30/23 11:35:53.586
STEP: verifying the statefulset Spec.Replicas was modified 01/30/23 11:35:53.591
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Jan 30 11:35:53.592: INFO: Deleting all statefulset in ns statefulset-2870
Jan 30 11:35:53.594: INFO: Scaling statefulset ss to 0
Jan 30 11:36:03.603: INFO: Waiting for statefulset status.replicas updated to 0
Jan 30 11:36:03.605: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Jan 30 11:36:03.611: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-2870" for this suite. 01/30/23 11:36:03.613
------------------------------
• [SLOW TEST] [20.059 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should have a working scale subresource [Conformance]
    test/e2e/apps/statefulset.go:848

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 11:35:43.556
    Jan 30 11:35:43.556: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename statefulset 01/30/23 11:35:43.556
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:35:43.563
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:35:43.564
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-2870 01/30/23 11:35:43.566
    [It] should have a working scale subresource [Conformance]
      test/e2e/apps/statefulset.go:848
    STEP: Creating statefulset ss in namespace statefulset-2870 01/30/23 11:35:43.569
    Jan 30 11:35:43.573: INFO: Found 0 stateful pods, waiting for 1
    Jan 30 11:35:53.576: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: getting scale subresource 01/30/23 11:35:53.579
    STEP: updating a scale subresource 01/30/23 11:35:53.581
    STEP: verifying the statefulset Spec.Replicas was modified 01/30/23 11:35:53.584
    STEP: Patch a scale subresource 01/30/23 11:35:53.586
    STEP: verifying the statefulset Spec.Replicas was modified 01/30/23 11:35:53.591
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Jan 30 11:35:53.592: INFO: Deleting all statefulset in ns statefulset-2870
    Jan 30 11:35:53.594: INFO: Scaling statefulset ss to 0
    Jan 30 11:36:03.603: INFO: Waiting for statefulset status.replicas updated to 0
    Jan 30 11:36:03.605: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Jan 30 11:36:03.611: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-2870" for this suite. 01/30/23 11:36:03.613
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:264
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 11:36:03.615
Jan 30 11:36:03.615: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename webhook 01/30/23 11:36:03.616
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:36:03.622
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:36:03.624
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 01/30/23 11:36:03.633
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/30/23 11:36:03.86
STEP: Deploying the webhook pod 01/30/23 11:36:03.864
STEP: Wait for the deployment to be ready 01/30/23 11:36:03.87
Jan 30 11:36:03.873: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 01/30/23 11:36:05.879
STEP: Verifying the service has paired with the endpoint 01/30/23 11:36:05.882
Jan 30 11:36:06.883: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:264
STEP: Registering the mutating pod webhook via the AdmissionRegistration API 01/30/23 11:36:06.884
STEP: create a pod that should be updated by the webhook 01/30/23 11:36:06.895
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 30 11:36:06.908: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-1272" for this suite. 01/30/23 11:36:06.922
STEP: Destroying namespace "webhook-1272-markers" for this suite. 01/30/23 11:36:06.924
------------------------------
• [3.311 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:264

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 11:36:03.615
    Jan 30 11:36:03.615: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename webhook 01/30/23 11:36:03.616
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:36:03.622
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:36:03.624
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 01/30/23 11:36:03.633
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/30/23 11:36:03.86
    STEP: Deploying the webhook pod 01/30/23 11:36:03.864
    STEP: Wait for the deployment to be ready 01/30/23 11:36:03.87
    Jan 30 11:36:03.873: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 01/30/23 11:36:05.879
    STEP: Verifying the service has paired with the endpoint 01/30/23 11:36:05.882
    Jan 30 11:36:06.883: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate pod and apply defaults after mutation [Conformance]
      test/e2e/apimachinery/webhook.go:264
    STEP: Registering the mutating pod webhook via the AdmissionRegistration API 01/30/23 11:36:06.884
    STEP: create a pod that should be updated by the webhook 01/30/23 11:36:06.895
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 30 11:36:06.908: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-1272" for this suite. 01/30/23 11:36:06.922
    STEP: Destroying namespace "webhook-1272-markers" for this suite. 01/30/23 11:36:06.924
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 11:36:06.927
Jan 30 11:36:06.927: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename dns 01/30/23 11:36:06.928
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:36:06.934
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:36:06.935
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333
STEP: Creating a test externalName service 01/30/23 11:36:06.937
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-2616.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-2616.svc.cluster.local; sleep 1; done
 01/30/23 11:36:06.939
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-2616.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-2616.svc.cluster.local; sleep 1; done
 01/30/23 11:36:06.939
STEP: creating a pod to probe DNS 01/30/23 11:36:06.94
STEP: submitting the pod to kubernetes 01/30/23 11:36:06.94
Jan 30 11:36:06.944: INFO: Waiting up to 15m0s for pod "dns-test-021287f9-c8c6-46c4-a724-492e21d6045f" in namespace "dns-2616" to be "running"
Jan 30 11:36:06.945: INFO: Pod "dns-test-021287f9-c8c6-46c4-a724-492e21d6045f": Phase="Pending", Reason="", readiness=false. Elapsed: 1.471086ms
Jan 30 11:36:08.947: INFO: Pod "dns-test-021287f9-c8c6-46c4-a724-492e21d6045f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003632835s
Jan 30 11:36:10.949: INFO: Pod "dns-test-021287f9-c8c6-46c4-a724-492e21d6045f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.004807592s
Jan 30 11:36:12.948: INFO: Pod "dns-test-021287f9-c8c6-46c4-a724-492e21d6045f": Phase="Pending", Reason="", readiness=false. Elapsed: 6.004456392s
Jan 30 11:36:14.949: INFO: Pod "dns-test-021287f9-c8c6-46c4-a724-492e21d6045f": Phase="Pending", Reason="", readiness=false. Elapsed: 8.005099373s
Jan 30 11:36:16.949: INFO: Pod "dns-test-021287f9-c8c6-46c4-a724-492e21d6045f": Phase="Pending", Reason="", readiness=false. Elapsed: 10.004833759s
Jan 30 11:36:18.948: INFO: Pod "dns-test-021287f9-c8c6-46c4-a724-492e21d6045f": Phase="Running", Reason="", readiness=true. Elapsed: 12.004390973s
Jan 30 11:36:18.948: INFO: Pod "dns-test-021287f9-c8c6-46c4-a724-492e21d6045f" satisfied condition "running"
STEP: retrieving the pod 01/30/23 11:36:18.948
STEP: looking for the results for each expected name from probers 01/30/23 11:36:18.95
Jan 30 11:36:18.955: INFO: DNS probes using dns-test-021287f9-c8c6-46c4-a724-492e21d6045f succeeded

STEP: deleting the pod 01/30/23 11:36:18.955
STEP: changing the externalName to bar.example.com 01/30/23 11:36:18.96
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-2616.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-2616.svc.cluster.local; sleep 1; done
 01/30/23 11:36:18.964
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-2616.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-2616.svc.cluster.local; sleep 1; done
 01/30/23 11:36:18.964
STEP: creating a second pod to probe DNS 01/30/23 11:36:18.964
STEP: submitting the pod to kubernetes 01/30/23 11:36:18.964
Jan 30 11:36:18.967: INFO: Waiting up to 15m0s for pod "dns-test-bac3f8b1-3aaa-4fd0-8b4e-1b60a76758f9" in namespace "dns-2616" to be "running"
Jan 30 11:36:18.969: INFO: Pod "dns-test-bac3f8b1-3aaa-4fd0-8b4e-1b60a76758f9": Phase="Pending", Reason="", readiness=false. Elapsed: 1.580358ms
Jan 30 11:36:20.972: INFO: Pod "dns-test-bac3f8b1-3aaa-4fd0-8b4e-1b60a76758f9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004630836s
Jan 30 11:36:22.972: INFO: Pod "dns-test-bac3f8b1-3aaa-4fd0-8b4e-1b60a76758f9": Phase="Running", Reason="", readiness=true. Elapsed: 4.004637313s
Jan 30 11:36:22.972: INFO: Pod "dns-test-bac3f8b1-3aaa-4fd0-8b4e-1b60a76758f9" satisfied condition "running"
STEP: retrieving the pod 01/30/23 11:36:22.972
STEP: looking for the results for each expected name from probers 01/30/23 11:36:22.977
Jan 30 11:36:22.982: INFO: DNS probes using dns-test-bac3f8b1-3aaa-4fd0-8b4e-1b60a76758f9 succeeded

STEP: deleting the pod 01/30/23 11:36:22.982
STEP: changing the service to type=ClusterIP 01/30/23 11:36:22.987
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-2616.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-2616.svc.cluster.local; sleep 1; done
 01/30/23 11:36:22.992
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-2616.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-2616.svc.cluster.local; sleep 1; done
 01/30/23 11:36:22.992
STEP: creating a third pod to probe DNS 01/30/23 11:36:22.993
STEP: submitting the pod to kubernetes 01/30/23 11:36:22.994
Jan 30 11:36:22.997: INFO: Waiting up to 15m0s for pod "dns-test-8e9364a0-1ee6-4f9e-a00a-63c8a721831a" in namespace "dns-2616" to be "running"
Jan 30 11:36:22.999: INFO: Pod "dns-test-8e9364a0-1ee6-4f9e-a00a-63c8a721831a": Phase="Pending", Reason="", readiness=false. Elapsed: 1.8062ms
Jan 30 11:36:25.002: INFO: Pod "dns-test-8e9364a0-1ee6-4f9e-a00a-63c8a721831a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005599431s
Jan 30 11:36:27.002: INFO: Pod "dns-test-8e9364a0-1ee6-4f9e-a00a-63c8a721831a": Phase="Running", Reason="", readiness=true. Elapsed: 4.005721535s
Jan 30 11:36:27.002: INFO: Pod "dns-test-8e9364a0-1ee6-4f9e-a00a-63c8a721831a" satisfied condition "running"
STEP: retrieving the pod 01/30/23 11:36:27.002
STEP: looking for the results for each expected name from probers 01/30/23 11:36:27.004
Jan 30 11:36:27.009: INFO: DNS probes using dns-test-8e9364a0-1ee6-4f9e-a00a-63c8a721831a succeeded

STEP: deleting the pod 01/30/23 11:36:27.009
STEP: deleting the test externalName service 01/30/23 11:36:27.014
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Jan 30 11:36:27.019: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-2616" for this suite. 01/30/23 11:36:27.021
------------------------------
• [SLOW TEST] [20.096 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 11:36:06.927
    Jan 30 11:36:06.927: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename dns 01/30/23 11:36:06.928
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:36:06.934
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:36:06.935
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for ExternalName services [Conformance]
      test/e2e/network/dns.go:333
    STEP: Creating a test externalName service 01/30/23 11:36:06.937
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-2616.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-2616.svc.cluster.local; sleep 1; done
     01/30/23 11:36:06.939
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-2616.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-2616.svc.cluster.local; sleep 1; done
     01/30/23 11:36:06.939
    STEP: creating a pod to probe DNS 01/30/23 11:36:06.94
    STEP: submitting the pod to kubernetes 01/30/23 11:36:06.94
    Jan 30 11:36:06.944: INFO: Waiting up to 15m0s for pod "dns-test-021287f9-c8c6-46c4-a724-492e21d6045f" in namespace "dns-2616" to be "running"
    Jan 30 11:36:06.945: INFO: Pod "dns-test-021287f9-c8c6-46c4-a724-492e21d6045f": Phase="Pending", Reason="", readiness=false. Elapsed: 1.471086ms
    Jan 30 11:36:08.947: INFO: Pod "dns-test-021287f9-c8c6-46c4-a724-492e21d6045f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003632835s
    Jan 30 11:36:10.949: INFO: Pod "dns-test-021287f9-c8c6-46c4-a724-492e21d6045f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.004807592s
    Jan 30 11:36:12.948: INFO: Pod "dns-test-021287f9-c8c6-46c4-a724-492e21d6045f": Phase="Pending", Reason="", readiness=false. Elapsed: 6.004456392s
    Jan 30 11:36:14.949: INFO: Pod "dns-test-021287f9-c8c6-46c4-a724-492e21d6045f": Phase="Pending", Reason="", readiness=false. Elapsed: 8.005099373s
    Jan 30 11:36:16.949: INFO: Pod "dns-test-021287f9-c8c6-46c4-a724-492e21d6045f": Phase="Pending", Reason="", readiness=false. Elapsed: 10.004833759s
    Jan 30 11:36:18.948: INFO: Pod "dns-test-021287f9-c8c6-46c4-a724-492e21d6045f": Phase="Running", Reason="", readiness=true. Elapsed: 12.004390973s
    Jan 30 11:36:18.948: INFO: Pod "dns-test-021287f9-c8c6-46c4-a724-492e21d6045f" satisfied condition "running"
    STEP: retrieving the pod 01/30/23 11:36:18.948
    STEP: looking for the results for each expected name from probers 01/30/23 11:36:18.95
    Jan 30 11:36:18.955: INFO: DNS probes using dns-test-021287f9-c8c6-46c4-a724-492e21d6045f succeeded

    STEP: deleting the pod 01/30/23 11:36:18.955
    STEP: changing the externalName to bar.example.com 01/30/23 11:36:18.96
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-2616.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-2616.svc.cluster.local; sleep 1; done
     01/30/23 11:36:18.964
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-2616.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-2616.svc.cluster.local; sleep 1; done
     01/30/23 11:36:18.964
    STEP: creating a second pod to probe DNS 01/30/23 11:36:18.964
    STEP: submitting the pod to kubernetes 01/30/23 11:36:18.964
    Jan 30 11:36:18.967: INFO: Waiting up to 15m0s for pod "dns-test-bac3f8b1-3aaa-4fd0-8b4e-1b60a76758f9" in namespace "dns-2616" to be "running"
    Jan 30 11:36:18.969: INFO: Pod "dns-test-bac3f8b1-3aaa-4fd0-8b4e-1b60a76758f9": Phase="Pending", Reason="", readiness=false. Elapsed: 1.580358ms
    Jan 30 11:36:20.972: INFO: Pod "dns-test-bac3f8b1-3aaa-4fd0-8b4e-1b60a76758f9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004630836s
    Jan 30 11:36:22.972: INFO: Pod "dns-test-bac3f8b1-3aaa-4fd0-8b4e-1b60a76758f9": Phase="Running", Reason="", readiness=true. Elapsed: 4.004637313s
    Jan 30 11:36:22.972: INFO: Pod "dns-test-bac3f8b1-3aaa-4fd0-8b4e-1b60a76758f9" satisfied condition "running"
    STEP: retrieving the pod 01/30/23 11:36:22.972
    STEP: looking for the results for each expected name from probers 01/30/23 11:36:22.977
    Jan 30 11:36:22.982: INFO: DNS probes using dns-test-bac3f8b1-3aaa-4fd0-8b4e-1b60a76758f9 succeeded

    STEP: deleting the pod 01/30/23 11:36:22.982
    STEP: changing the service to type=ClusterIP 01/30/23 11:36:22.987
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-2616.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-2616.svc.cluster.local; sleep 1; done
     01/30/23 11:36:22.992
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-2616.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-2616.svc.cluster.local; sleep 1; done
     01/30/23 11:36:22.992
    STEP: creating a third pod to probe DNS 01/30/23 11:36:22.993
    STEP: submitting the pod to kubernetes 01/30/23 11:36:22.994
    Jan 30 11:36:22.997: INFO: Waiting up to 15m0s for pod "dns-test-8e9364a0-1ee6-4f9e-a00a-63c8a721831a" in namespace "dns-2616" to be "running"
    Jan 30 11:36:22.999: INFO: Pod "dns-test-8e9364a0-1ee6-4f9e-a00a-63c8a721831a": Phase="Pending", Reason="", readiness=false. Elapsed: 1.8062ms
    Jan 30 11:36:25.002: INFO: Pod "dns-test-8e9364a0-1ee6-4f9e-a00a-63c8a721831a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005599431s
    Jan 30 11:36:27.002: INFO: Pod "dns-test-8e9364a0-1ee6-4f9e-a00a-63c8a721831a": Phase="Running", Reason="", readiness=true. Elapsed: 4.005721535s
    Jan 30 11:36:27.002: INFO: Pod "dns-test-8e9364a0-1ee6-4f9e-a00a-63c8a721831a" satisfied condition "running"
    STEP: retrieving the pod 01/30/23 11:36:27.002
    STEP: looking for the results for each expected name from probers 01/30/23 11:36:27.004
    Jan 30 11:36:27.009: INFO: DNS probes using dns-test-8e9364a0-1ee6-4f9e-a00a-63c8a721831a succeeded

    STEP: deleting the pod 01/30/23 11:36:27.009
    STEP: deleting the test externalName service 01/30/23 11:36:27.014
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Jan 30 11:36:27.019: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-2616" for this suite. 01/30/23 11:36:27.021
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:291
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 11:36:27.024
Jan 30 11:36:27.024: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename webhook 01/30/23 11:36:27.025
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:36:27.031
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:36:27.033
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 01/30/23 11:36:27.041
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/30/23 11:36:27.635
STEP: Deploying the webhook pod 01/30/23 11:36:27.638
STEP: Wait for the deployment to be ready 01/30/23 11:36:27.644
Jan 30 11:36:27.647: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 01/30/23 11:36:29.657
STEP: Verifying the service has paired with the endpoint 01/30/23 11:36:29.66
Jan 30 11:36:30.660: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:291
Jan 30 11:36:30.662: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-5231-crds.webhook.example.com via the AdmissionRegistration API 01/30/23 11:36:31.168
STEP: Creating a custom resource that should be mutated by the webhook 01/30/23 11:36:31.178
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 30 11:36:33.740: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-5814" for this suite. 01/30/23 11:36:33.754
STEP: Destroying namespace "webhook-5814-markers" for this suite. 01/30/23 11:36:33.756
------------------------------
• [SLOW TEST] [6.734 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:291

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 11:36:27.024
    Jan 30 11:36:27.024: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename webhook 01/30/23 11:36:27.025
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:36:27.031
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:36:27.033
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 01/30/23 11:36:27.041
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/30/23 11:36:27.635
    STEP: Deploying the webhook pod 01/30/23 11:36:27.638
    STEP: Wait for the deployment to be ready 01/30/23 11:36:27.644
    Jan 30 11:36:27.647: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 01/30/23 11:36:29.657
    STEP: Verifying the service has paired with the endpoint 01/30/23 11:36:29.66
    Jan 30 11:36:30.660: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource [Conformance]
      test/e2e/apimachinery/webhook.go:291
    Jan 30 11:36:30.662: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-5231-crds.webhook.example.com via the AdmissionRegistration API 01/30/23 11:36:31.168
    STEP: Creating a custom resource that should be mutated by the webhook 01/30/23 11:36:31.178
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 30 11:36:33.740: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-5814" for this suite. 01/30/23 11:36:33.754
    STEP: Destroying namespace "webhook-5814-markers" for this suite. 01/30/23 11:36:33.756
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:323
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 11:36:33.759
Jan 30 11:36:33.759: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename webhook 01/30/23 11:36:33.76
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:36:33.765
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:36:33.767
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 01/30/23 11:36:33.775
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/30/23 11:36:34.358
STEP: Deploying the webhook pod 01/30/23 11:36:34.362
STEP: Wait for the deployment to be ready 01/30/23 11:36:34.367
Jan 30 11:36:34.370: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Jan 30 11:36:36.378: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 30, 11, 36, 34, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 30, 11, 36, 34, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 30, 11, 36, 34, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 30, 11, 36, 34, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5dbb7bc5bf\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 01/30/23 11:36:38.38
STEP: Verifying the service has paired with the endpoint 01/30/23 11:36:38.384
Jan 30 11:36:39.385: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:323
Jan 30 11:36:39.387: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-6078-crds.webhook.example.com via the AdmissionRegistration API 01/30/23 11:36:39.893
STEP: Creating a custom resource while v1 is storage version 01/30/23 11:36:39.902
STEP: Patching Custom Resource Definition to set v2 as storage 01/30/23 11:36:41.982
STEP: Patching the custom resource while v2 is storage version 01/30/23 11:36:42
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 30 11:36:42.580: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-9752" for this suite. 01/30/23 11:36:42.595
STEP: Destroying namespace "webhook-9752-markers" for this suite. 01/30/23 11:36:42.597
------------------------------
• [SLOW TEST] [8.840 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:323

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 11:36:33.759
    Jan 30 11:36:33.759: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename webhook 01/30/23 11:36:33.76
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:36:33.765
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:36:33.767
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 01/30/23 11:36:33.775
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/30/23 11:36:34.358
    STEP: Deploying the webhook pod 01/30/23 11:36:34.362
    STEP: Wait for the deployment to be ready 01/30/23 11:36:34.367
    Jan 30 11:36:34.370: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    Jan 30 11:36:36.378: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 30, 11, 36, 34, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 30, 11, 36, 34, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 30, 11, 36, 34, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 30, 11, 36, 34, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5dbb7bc5bf\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 01/30/23 11:36:38.38
    STEP: Verifying the service has paired with the endpoint 01/30/23 11:36:38.384
    Jan 30 11:36:39.385: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource with different stored version [Conformance]
      test/e2e/apimachinery/webhook.go:323
    Jan 30 11:36:39.387: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-6078-crds.webhook.example.com via the AdmissionRegistration API 01/30/23 11:36:39.893
    STEP: Creating a custom resource while v1 is storage version 01/30/23 11:36:39.902
    STEP: Patching Custom Resource Definition to set v2 as storage 01/30/23 11:36:41.982
    STEP: Patching the custom resource while v2 is storage version 01/30/23 11:36:42
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 30 11:36:42.580: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-9752" for this suite. 01/30/23 11:36:42.595
    STEP: Destroying namespace "webhook-9752-markers" for this suite. 01/30/23 11:36:42.597
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] Secrets
  should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:140
[BeforeEach] [sig-node] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 11:36:42.6
Jan 30 11:36:42.600: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename secrets 01/30/23 11:36:42.6
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:36:42.607
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:36:42.609
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:140
STEP: Creating projection with secret that has name secret-emptykey-test-5172e085-933f-4bc1-a7fe-166088958aae 01/30/23 11:36:42.611
[AfterEach] [sig-node] Secrets
  test/e2e/framework/node/init/init.go:32
Jan 30 11:36:42.612: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-1590" for this suite. 01/30/23 11:36:42.614
------------------------------
• [0.017 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:140

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 11:36:42.6
    Jan 30 11:36:42.600: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename secrets 01/30/23 11:36:42.6
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:36:42.607
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:36:42.609
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should fail to create secret due to empty secret key [Conformance]
      test/e2e/common/node/secrets.go:140
    STEP: Creating projection with secret that has name secret-emptykey-test-5172e085-933f-4bc1-a7fe-166088958aae 01/30/23 11:36:42.611
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/node/init/init.go:32
    Jan 30 11:36:42.612: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-1590" for this suite. 01/30/23 11:36:42.614
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:89
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 11:36:42.617
Jan 30 11:36:42.617: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename secrets 01/30/23 11:36:42.617
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:36:42.623
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:36:42.625
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:89
STEP: Creating secret with name secret-test-map-4bb18b42-1c81-48bb-b771-4f207cb354d8 01/30/23 11:36:42.627
STEP: Creating a pod to test consume secrets 01/30/23 11:36:42.629
Jan 30 11:36:42.633: INFO: Waiting up to 5m0s for pod "pod-secrets-850a8eaa-2cf0-42a5-a51b-dd359a3a76b2" in namespace "secrets-2298" to be "Succeeded or Failed"
Jan 30 11:36:42.634: INFO: Pod "pod-secrets-850a8eaa-2cf0-42a5-a51b-dd359a3a76b2": Phase="Pending", Reason="", readiness=false. Elapsed: 1.571312ms
Jan 30 11:36:44.637: INFO: Pod "pod-secrets-850a8eaa-2cf0-42a5-a51b-dd359a3a76b2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003939845s
Jan 30 11:36:46.638: INFO: Pod "pod-secrets-850a8eaa-2cf0-42a5-a51b-dd359a3a76b2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005134498s
STEP: Saw pod success 01/30/23 11:36:46.638
Jan 30 11:36:46.638: INFO: Pod "pod-secrets-850a8eaa-2cf0-42a5-a51b-dd359a3a76b2" satisfied condition "Succeeded or Failed"
Jan 30 11:36:46.639: INFO: Trying to get logs from node pubt2-nks-for-dev1.dg.163.org pod pod-secrets-850a8eaa-2cf0-42a5-a51b-dd359a3a76b2 container secret-volume-test: <nil>
STEP: delete the pod 01/30/23 11:36:46.652
Jan 30 11:36:46.656: INFO: Waiting for pod pod-secrets-850a8eaa-2cf0-42a5-a51b-dd359a3a76b2 to disappear
Jan 30 11:36:46.657: INFO: Pod pod-secrets-850a8eaa-2cf0-42a5-a51b-dd359a3a76b2 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Jan 30 11:36:46.657: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-2298" for this suite. 01/30/23 11:36:46.659
------------------------------
• [4.045 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:89

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 11:36:42.617
    Jan 30 11:36:42.617: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename secrets 01/30/23 11:36:42.617
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:36:42.623
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:36:42.625
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:89
    STEP: Creating secret with name secret-test-map-4bb18b42-1c81-48bb-b771-4f207cb354d8 01/30/23 11:36:42.627
    STEP: Creating a pod to test consume secrets 01/30/23 11:36:42.629
    Jan 30 11:36:42.633: INFO: Waiting up to 5m0s for pod "pod-secrets-850a8eaa-2cf0-42a5-a51b-dd359a3a76b2" in namespace "secrets-2298" to be "Succeeded or Failed"
    Jan 30 11:36:42.634: INFO: Pod "pod-secrets-850a8eaa-2cf0-42a5-a51b-dd359a3a76b2": Phase="Pending", Reason="", readiness=false. Elapsed: 1.571312ms
    Jan 30 11:36:44.637: INFO: Pod "pod-secrets-850a8eaa-2cf0-42a5-a51b-dd359a3a76b2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003939845s
    Jan 30 11:36:46.638: INFO: Pod "pod-secrets-850a8eaa-2cf0-42a5-a51b-dd359a3a76b2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005134498s
    STEP: Saw pod success 01/30/23 11:36:46.638
    Jan 30 11:36:46.638: INFO: Pod "pod-secrets-850a8eaa-2cf0-42a5-a51b-dd359a3a76b2" satisfied condition "Succeeded or Failed"
    Jan 30 11:36:46.639: INFO: Trying to get logs from node pubt2-nks-for-dev1.dg.163.org pod pod-secrets-850a8eaa-2cf0-42a5-a51b-dd359a3a76b2 container secret-volume-test: <nil>
    STEP: delete the pod 01/30/23 11:36:46.652
    Jan 30 11:36:46.656: INFO: Waiting for pod pod-secrets-850a8eaa-2cf0-42a5-a51b-dd359a3a76b2 to disappear
    Jan 30 11:36:46.657: INFO: Pod pod-secrets-850a8eaa-2cf0-42a5-a51b-dd359a3a76b2 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Jan 30 11:36:46.657: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-2298" for this suite. 01/30/23 11:36:46.659
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:277
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 11:36:46.662
Jan 30 11:36:46.662: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename webhook 01/30/23 11:36:46.662
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:36:46.668
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:36:46.67
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 01/30/23 11:36:46.677
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/30/23 11:36:46.995
STEP: Deploying the webhook pod 01/30/23 11:36:46.997
STEP: Wait for the deployment to be ready 01/30/23 11:36:47.002
Jan 30 11:36:47.005: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Jan 30 11:36:49.011: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 30, 11, 36, 47, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 30, 11, 36, 47, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 30, 11, 36, 47, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 30, 11, 36, 47, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5dbb7bc5bf\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 01/30/23 11:36:51.014
STEP: Verifying the service has paired with the endpoint 01/30/23 11:36:51.018
Jan 30 11:36:52.018: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:277
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 01/30/23 11:36:52.02
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 01/30/23 11:36:52.029
STEP: Creating a dummy validating-webhook-configuration object 01/30/23 11:36:52.037
STEP: Deleting the validating-webhook-configuration, which should be possible to remove 01/30/23 11:36:52.041
STEP: Creating a dummy mutating-webhook-configuration object 01/30/23 11:36:52.044
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove 01/30/23 11:36:52.048
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 30 11:36:52.054: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-9619" for this suite. 01/30/23 11:36:52.067
STEP: Destroying namespace "webhook-9619-markers" for this suite. 01/30/23 11:36:52.069
------------------------------
• [SLOW TEST] [5.410 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:277

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 11:36:46.662
    Jan 30 11:36:46.662: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename webhook 01/30/23 11:36:46.662
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:36:46.668
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:36:46.67
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 01/30/23 11:36:46.677
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/30/23 11:36:46.995
    STEP: Deploying the webhook pod 01/30/23 11:36:46.997
    STEP: Wait for the deployment to be ready 01/30/23 11:36:47.002
    Jan 30 11:36:47.005: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    Jan 30 11:36:49.011: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 30, 11, 36, 47, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 30, 11, 36, 47, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 30, 11, 36, 47, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 30, 11, 36, 47, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5dbb7bc5bf\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 01/30/23 11:36:51.014
    STEP: Verifying the service has paired with the endpoint 01/30/23 11:36:51.018
    Jan 30 11:36:52.018: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
      test/e2e/apimachinery/webhook.go:277
    STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 01/30/23 11:36:52.02
    STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 01/30/23 11:36:52.029
    STEP: Creating a dummy validating-webhook-configuration object 01/30/23 11:36:52.037
    STEP: Deleting the validating-webhook-configuration, which should be possible to remove 01/30/23 11:36:52.041
    STEP: Creating a dummy mutating-webhook-configuration object 01/30/23 11:36:52.044
    STEP: Deleting the mutating-webhook-configuration, which should be possible to remove 01/30/23 11:36:52.048
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 30 11:36:52.054: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-9619" for this suite. 01/30/23 11:36:52.067
    STEP: Destroying namespace "webhook-9619-markers" for this suite. 01/30/23 11:36:52.069
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-apps] Daemon set [Serial]
  should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:823
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 11:36:52.072
Jan 30 11:36:52.072: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename daemonsets 01/30/23 11:36:52.073
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:36:52.078
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:36:52.08
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:823
STEP: Creating simple DaemonSet "daemon-set" 01/30/23 11:36:52.09
STEP: Check that daemon pods launch on every node of the cluster. 01/30/23 11:36:52.093
Jan 30 11:36:52.096: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 30 11:36:52.096: INFO: Node pubt2-nks-for-dev1.dg.163.org is running 0 daemon pod, expected 1
Jan 30 11:36:53.101: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 30 11:36:53.101: INFO: Node pubt2-nks-for-dev1.dg.163.org is running 0 daemon pod, expected 1
Jan 30 11:36:54.101: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan 30 11:36:54.101: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
STEP: listing all DeamonSets 01/30/23 11:36:54.103
STEP: DeleteCollection of the DaemonSets 01/30/23 11:36:54.105
STEP: Verify that ReplicaSets have been deleted 01/30/23 11:36:54.108
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
Jan 30 11:36:54.113: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"14138"},"items":null}

Jan 30 11:36:54.116: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"14138"},"items":[{"metadata":{"name":"daemon-set-jzjbl","generateName":"daemon-set-","namespace":"daemonsets-6096","uid":"321badcb-6b4a-4b3c-aee7-5c7759082e7d","resourceVersion":"14138","creationTimestamp":"2023-01-30T11:36:52Z","deletionTimestamp":"2023-01-30T11:37:24Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"5f99d558c","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/podIP":"10.178.151.62/32","cni.projectcalico.org/podIPs":"10.178.151.62/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"ebf26cbd-393a-4331-abf7-62b8db42a93c","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-01-30T11:36:52Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-01-30T11:36:52Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ebf26cbd-393a-4331-abf7-62b8db42a93c\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-01-30T11:36:53Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.178.151.62\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-gjxmj","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"harbor.cloud.netease.com/qzprod-k8s/e2e/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-gjxmj","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"pubt2-nks-for-dev1.dg.163.org","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["pubt2-nks-for-dev1.dg.163.org"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-30T11:36:52Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-30T11:36:53Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-30T11:36:53Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-30T11:36:52Z"}],"hostIP":"10.182.0.82","podIP":"10.178.151.62","podIPs":[{"ip":"10.178.151.62"}],"startTime":"2023-01-30T11:36:52Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-01-30T11:36:53Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"harbor.cloud.netease.com/qzprod-k8s/e2e/httpd:2.4.38-4","imageID":"docker-pullable://harbor.cloud.netease.com/qzprod-k8s/e2e/httpd@sha256:8b745b6e9cb6a6184353110aabe28d8d6f6145d3fe7c0e1899aa70c8228c0fd3","containerID":"docker://45daf2d0364bcd4c8006882646197aa9dac2b9eef446c12f9728aafdd18a0c3e","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-vmxlr","generateName":"daemon-set-","namespace":"daemonsets-6096","uid":"65f3f01a-5c04-4ef4-9ff8-552c8b1ca312","resourceVersion":"14137","creationTimestamp":"2023-01-30T11:36:52Z","deletionTimestamp":"2023-01-30T11:37:24Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"5f99d558c","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/podIP":"10.178.197.11/32","cni.projectcalico.org/podIPs":"10.178.197.11/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"ebf26cbd-393a-4331-abf7-62b8db42a93c","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-01-30T11:36:52Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-01-30T11:36:52Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ebf26cbd-393a-4331-abf7-62b8db42a93c\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-01-30T11:36:53Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.178.197.11\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-9xqrb","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"harbor.cloud.netease.com/qzprod-k8s/e2e/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-9xqrb","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"pubt2-nks-for-dev3.dg.163.org","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["pubt2-nks-for-dev3.dg.163.org"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-30T11:36:52Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-30T11:36:53Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-30T11:36:53Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-30T11:36:52Z"}],"hostIP":"10.182.0.84","podIP":"10.178.197.11","podIPs":[{"ip":"10.178.197.11"}],"startTime":"2023-01-30T11:36:52Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-01-30T11:36:53Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"harbor.cloud.netease.com/qzprod-k8s/e2e/httpd:2.4.38-4","imageID":"docker-pullable://harbor.cloud.netease.com/qzprod-k8s/e2e/httpd@sha256:8b745b6e9cb6a6184353110aabe28d8d6f6145d3fe7c0e1899aa70c8228c0fd3","containerID":"docker://46ca32f8ea66cdb1022be5fe198d77b4c8de1831b29e02c2c654fa3596df3b38","started":true}],"qosClass":"BestEffort"}}]}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Jan 30 11:36:54.121: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-6096" for this suite. 01/30/23 11:36:54.123
------------------------------
• [2.053 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:823

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 11:36:52.072
    Jan 30 11:36:52.072: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename daemonsets 01/30/23 11:36:52.073
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:36:52.078
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:36:52.08
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should list and delete a collection of DaemonSets [Conformance]
      test/e2e/apps/daemon_set.go:823
    STEP: Creating simple DaemonSet "daemon-set" 01/30/23 11:36:52.09
    STEP: Check that daemon pods launch on every node of the cluster. 01/30/23 11:36:52.093
    Jan 30 11:36:52.096: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 30 11:36:52.096: INFO: Node pubt2-nks-for-dev1.dg.163.org is running 0 daemon pod, expected 1
    Jan 30 11:36:53.101: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 30 11:36:53.101: INFO: Node pubt2-nks-for-dev1.dg.163.org is running 0 daemon pod, expected 1
    Jan 30 11:36:54.101: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jan 30 11:36:54.101: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    STEP: listing all DeamonSets 01/30/23 11:36:54.103
    STEP: DeleteCollection of the DaemonSets 01/30/23 11:36:54.105
    STEP: Verify that ReplicaSets have been deleted 01/30/23 11:36:54.108
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    Jan 30 11:36:54.113: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"14138"},"items":null}

    Jan 30 11:36:54.116: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"14138"},"items":[{"metadata":{"name":"daemon-set-jzjbl","generateName":"daemon-set-","namespace":"daemonsets-6096","uid":"321badcb-6b4a-4b3c-aee7-5c7759082e7d","resourceVersion":"14138","creationTimestamp":"2023-01-30T11:36:52Z","deletionTimestamp":"2023-01-30T11:37:24Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"5f99d558c","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/podIP":"10.178.151.62/32","cni.projectcalico.org/podIPs":"10.178.151.62/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"ebf26cbd-393a-4331-abf7-62b8db42a93c","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-01-30T11:36:52Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-01-30T11:36:52Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ebf26cbd-393a-4331-abf7-62b8db42a93c\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-01-30T11:36:53Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.178.151.62\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-gjxmj","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"harbor.cloud.netease.com/qzprod-k8s/e2e/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-gjxmj","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"pubt2-nks-for-dev1.dg.163.org","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["pubt2-nks-for-dev1.dg.163.org"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-30T11:36:52Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-30T11:36:53Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-30T11:36:53Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-30T11:36:52Z"}],"hostIP":"10.182.0.82","podIP":"10.178.151.62","podIPs":[{"ip":"10.178.151.62"}],"startTime":"2023-01-30T11:36:52Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-01-30T11:36:53Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"harbor.cloud.netease.com/qzprod-k8s/e2e/httpd:2.4.38-4","imageID":"docker-pullable://harbor.cloud.netease.com/qzprod-k8s/e2e/httpd@sha256:8b745b6e9cb6a6184353110aabe28d8d6f6145d3fe7c0e1899aa70c8228c0fd3","containerID":"docker://45daf2d0364bcd4c8006882646197aa9dac2b9eef446c12f9728aafdd18a0c3e","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-vmxlr","generateName":"daemon-set-","namespace":"daemonsets-6096","uid":"65f3f01a-5c04-4ef4-9ff8-552c8b1ca312","resourceVersion":"14137","creationTimestamp":"2023-01-30T11:36:52Z","deletionTimestamp":"2023-01-30T11:37:24Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"5f99d558c","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/podIP":"10.178.197.11/32","cni.projectcalico.org/podIPs":"10.178.197.11/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"ebf26cbd-393a-4331-abf7-62b8db42a93c","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-01-30T11:36:52Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-01-30T11:36:52Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ebf26cbd-393a-4331-abf7-62b8db42a93c\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-01-30T11:36:53Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.178.197.11\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-9xqrb","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"harbor.cloud.netease.com/qzprod-k8s/e2e/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-9xqrb","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"pubt2-nks-for-dev3.dg.163.org","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["pubt2-nks-for-dev3.dg.163.org"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-30T11:36:52Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-30T11:36:53Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-30T11:36:53Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-30T11:36:52Z"}],"hostIP":"10.182.0.84","podIP":"10.178.197.11","podIPs":[{"ip":"10.178.197.11"}],"startTime":"2023-01-30T11:36:52Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-01-30T11:36:53Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"harbor.cloud.netease.com/qzprod-k8s/e2e/httpd:2.4.38-4","imageID":"docker-pullable://harbor.cloud.netease.com/qzprod-k8s/e2e/httpd@sha256:8b745b6e9cb6a6184353110aabe28d8d6f6145d3fe7c0e1899aa70c8228c0fd3","containerID":"docker://46ca32f8ea66cdb1022be5fe198d77b4c8de1831b29e02c2c654fa3596df3b38","started":true}],"qosClass":"BestEffort"}}]}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan 30 11:36:54.121: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-6096" for this suite. 01/30/23 11:36:54.123
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:93
[BeforeEach] [sig-network] Networking
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 11:36:54.126
Jan 30 11:36:54.126: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename pod-network-test 01/30/23 11:36:54.126
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:36:54.133
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:36:54.135
[BeforeEach] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:31
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:93
STEP: Performing setup for networking test in namespace pod-network-test-9161 01/30/23 11:36:54.137
STEP: creating a selector 01/30/23 11:36:54.137
STEP: Creating the service pods in kubernetes 01/30/23 11:36:54.137
Jan 30 11:36:54.137: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jan 30 11:36:54.147: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-9161" to be "running and ready"
Jan 30 11:36:54.149: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 1.575535ms
Jan 30 11:36:54.149: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jan 30 11:36:56.151: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.003978947s
Jan 30 11:36:56.151: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 30 11:36:58.151: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.004442147s
Jan 30 11:36:58.151: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 30 11:37:00.152: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.005067813s
Jan 30 11:37:00.152: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 30 11:37:02.151: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.004011053s
Jan 30 11:37:02.151: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 30 11:37:04.152: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.004529341s
Jan 30 11:37:04.152: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 30 11:37:06.152: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.005213887s
Jan 30 11:37:06.152: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 30 11:37:08.151: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.0042471s
Jan 30 11:37:08.151: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 30 11:37:10.152: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.005244795s
Jan 30 11:37:10.152: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 30 11:37:12.152: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.005102795s
Jan 30 11:37:12.152: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 30 11:37:14.152: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.00500815s
Jan 30 11:37:14.152: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 30 11:37:16.152: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.004858036s
Jan 30 11:37:16.152: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Jan 30 11:37:16.152: INFO: Pod "netserver-0" satisfied condition "running and ready"
Jan 30 11:37:16.154: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-9161" to be "running and ready"
Jan 30 11:37:16.155: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 1.581184ms
Jan 30 11:37:16.155: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Jan 30 11:37:16.155: INFO: Pod "netserver-1" satisfied condition "running and ready"
STEP: Creating test pods 01/30/23 11:37:16.157
Jan 30 11:37:16.160: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-9161" to be "running"
Jan 30 11:37:16.161: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 1.580233ms
Jan 30 11:37:18.164: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.004520342s
Jan 30 11:37:18.164: INFO: Pod "test-container-pod" satisfied condition "running"
Jan 30 11:37:18.166: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
Jan 30 11:37:18.166: INFO: Breadth first check of 10.178.151.63 on host 10.182.0.82...
Jan 30 11:37:18.167: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.178.151.64:9080/dial?request=hostname&protocol=udp&host=10.178.151.63&port=8081&tries=1'] Namespace:pod-network-test-9161 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 30 11:37:18.167: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
Jan 30 11:37:18.168: INFO: ExecWithOptions: Clientset creation
Jan 30 11:37:18.168: INFO: ExecWithOptions: execute(POST https://10.178.64.1:443/api/v1/namespaces/pod-network-test-9161/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.178.151.64%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.178.151.63%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Jan 30 11:37:18.241: INFO: Waiting for responses: map[]
Jan 30 11:37:18.241: INFO: reached 10.178.151.63 after 0/1 tries
Jan 30 11:37:18.241: INFO: Breadth first check of 10.178.197.12 on host 10.182.0.84...
Jan 30 11:37:18.243: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.178.151.64:9080/dial?request=hostname&protocol=udp&host=10.178.197.12&port=8081&tries=1'] Namespace:pod-network-test-9161 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 30 11:37:18.243: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
Jan 30 11:37:18.243: INFO: ExecWithOptions: Clientset creation
Jan 30 11:37:18.244: INFO: ExecWithOptions: execute(POST https://10.178.64.1:443/api/v1/namespaces/pod-network-test-9161/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.178.151.64%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.178.197.12%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Jan 30 11:37:18.337: INFO: Waiting for responses: map[]
Jan 30 11:37:18.337: INFO: reached 10.178.197.12 after 0/1 tries
Jan 30 11:37:18.337: INFO: Going to retry 0 out of 2 pods....
[AfterEach] [sig-network] Networking
  test/e2e/framework/node/init/init.go:32
Jan 30 11:37:18.337: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Networking
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Networking
  tear down framework | framework.go:193
STEP: Destroying namespace "pod-network-test-9161" for this suite. 01/30/23 11:37:18.34
------------------------------
• [SLOW TEST] [24.217 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:93

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 11:36:54.126
    Jan 30 11:36:54.126: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename pod-network-test 01/30/23 11:36:54.126
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:36:54.133
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:36:54.135
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:31
    [It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:93
    STEP: Performing setup for networking test in namespace pod-network-test-9161 01/30/23 11:36:54.137
    STEP: creating a selector 01/30/23 11:36:54.137
    STEP: Creating the service pods in kubernetes 01/30/23 11:36:54.137
    Jan 30 11:36:54.137: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Jan 30 11:36:54.147: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-9161" to be "running and ready"
    Jan 30 11:36:54.149: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 1.575535ms
    Jan 30 11:36:54.149: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Jan 30 11:36:56.151: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.003978947s
    Jan 30 11:36:56.151: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 30 11:36:58.151: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.004442147s
    Jan 30 11:36:58.151: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 30 11:37:00.152: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.005067813s
    Jan 30 11:37:00.152: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 30 11:37:02.151: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.004011053s
    Jan 30 11:37:02.151: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 30 11:37:04.152: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.004529341s
    Jan 30 11:37:04.152: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 30 11:37:06.152: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.005213887s
    Jan 30 11:37:06.152: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 30 11:37:08.151: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.0042471s
    Jan 30 11:37:08.151: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 30 11:37:10.152: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.005244795s
    Jan 30 11:37:10.152: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 30 11:37:12.152: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.005102795s
    Jan 30 11:37:12.152: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 30 11:37:14.152: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.00500815s
    Jan 30 11:37:14.152: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 30 11:37:16.152: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.004858036s
    Jan 30 11:37:16.152: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Jan 30 11:37:16.152: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Jan 30 11:37:16.154: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-9161" to be "running and ready"
    Jan 30 11:37:16.155: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 1.581184ms
    Jan 30 11:37:16.155: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Jan 30 11:37:16.155: INFO: Pod "netserver-1" satisfied condition "running and ready"
    STEP: Creating test pods 01/30/23 11:37:16.157
    Jan 30 11:37:16.160: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-9161" to be "running"
    Jan 30 11:37:16.161: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 1.580233ms
    Jan 30 11:37:18.164: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.004520342s
    Jan 30 11:37:18.164: INFO: Pod "test-container-pod" satisfied condition "running"
    Jan 30 11:37:18.166: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
    Jan 30 11:37:18.166: INFO: Breadth first check of 10.178.151.63 on host 10.182.0.82...
    Jan 30 11:37:18.167: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.178.151.64:9080/dial?request=hostname&protocol=udp&host=10.178.151.63&port=8081&tries=1'] Namespace:pod-network-test-9161 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 30 11:37:18.167: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    Jan 30 11:37:18.168: INFO: ExecWithOptions: Clientset creation
    Jan 30 11:37:18.168: INFO: ExecWithOptions: execute(POST https://10.178.64.1:443/api/v1/namespaces/pod-network-test-9161/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.178.151.64%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.178.151.63%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Jan 30 11:37:18.241: INFO: Waiting for responses: map[]
    Jan 30 11:37:18.241: INFO: reached 10.178.151.63 after 0/1 tries
    Jan 30 11:37:18.241: INFO: Breadth first check of 10.178.197.12 on host 10.182.0.84...
    Jan 30 11:37:18.243: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.178.151.64:9080/dial?request=hostname&protocol=udp&host=10.178.197.12&port=8081&tries=1'] Namespace:pod-network-test-9161 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 30 11:37:18.243: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    Jan 30 11:37:18.243: INFO: ExecWithOptions: Clientset creation
    Jan 30 11:37:18.244: INFO: ExecWithOptions: execute(POST https://10.178.64.1:443/api/v1/namespaces/pod-network-test-9161/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.178.151.64%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.178.197.12%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Jan 30 11:37:18.337: INFO: Waiting for responses: map[]
    Jan 30 11:37:18.337: INFO: reached 10.178.197.12 after 0/1 tries
    Jan 30 11:37:18.337: INFO: Going to retry 0 out of 2 pods....
    [AfterEach] [sig-network] Networking
      test/e2e/framework/node/init/init.go:32
    Jan 30 11:37:18.337: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Networking
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Networking
      tear down framework | framework.go:193
    STEP: Destroying namespace "pod-network-test-9161" for this suite. 01/30/23 11:37:18.34
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:122
[BeforeEach] [sig-network] Networking
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 11:37:18.343
Jan 30 11:37:18.343: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename pod-network-test 01/30/23 11:37:18.344
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:37:18.351
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:37:18.353
[BeforeEach] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:31
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:122
STEP: Performing setup for networking test in namespace pod-network-test-5130 01/30/23 11:37:18.355
STEP: creating a selector 01/30/23 11:37:18.355
STEP: Creating the service pods in kubernetes 01/30/23 11:37:18.355
Jan 30 11:37:18.355: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jan 30 11:37:18.366: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-5130" to be "running and ready"
Jan 30 11:37:18.367: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 1.668287ms
Jan 30 11:37:18.367: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jan 30 11:37:20.372: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.006037602s
Jan 30 11:37:20.372: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 30 11:37:22.372: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.005914979s
Jan 30 11:37:22.372: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 30 11:37:24.370: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.004395254s
Jan 30 11:37:24.370: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 30 11:37:26.372: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.005804456s
Jan 30 11:37:26.372: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 30 11:37:28.371: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.005447321s
Jan 30 11:37:28.371: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 30 11:37:30.371: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.005213274s
Jan 30 11:37:30.371: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 30 11:37:32.371: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.00487367s
Jan 30 11:37:32.371: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 30 11:37:34.371: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.005180997s
Jan 30 11:37:34.371: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 30 11:37:36.371: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.005204693s
Jan 30 11:37:36.371: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 30 11:37:38.370: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.004415509s
Jan 30 11:37:38.370: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 30 11:37:40.372: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.006064331s
Jan 30 11:37:40.372: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Jan 30 11:37:40.372: INFO: Pod "netserver-0" satisfied condition "running and ready"
Jan 30 11:37:40.374: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-5130" to be "running and ready"
Jan 30 11:37:40.376: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 1.844911ms
Jan 30 11:37:40.376: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Jan 30 11:37:40.376: INFO: Pod "netserver-1" satisfied condition "running and ready"
STEP: Creating test pods 01/30/23 11:37:40.377
Jan 30 11:37:40.382: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-5130" to be "running"
Jan 30 11:37:40.384: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 1.751854ms
Jan 30 11:37:42.387: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.004806199s
Jan 30 11:37:42.387: INFO: Pod "test-container-pod" satisfied condition "running"
Jan 30 11:37:42.389: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-5130" to be "running"
Jan 30 11:37:42.390: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 1.532268ms
Jan 30 11:37:42.390: INFO: Pod "host-test-container-pod" satisfied condition "running"
Jan 30 11:37:42.392: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
Jan 30 11:37:42.392: INFO: Going to poll 10.178.151.65 on port 8081 at least 0 times, with a maximum of 34 tries before failing
Jan 30 11:37:42.394: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.178.151.65 8081 | grep -v '^\s*$'] Namespace:pod-network-test-5130 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 30 11:37:42.394: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
Jan 30 11:37:42.394: INFO: ExecWithOptions: Clientset creation
Jan 30 11:37:42.394: INFO: ExecWithOptions: execute(POST https://10.178.64.1:443/api/v1/namespaces/pod-network-test-5130/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.178.151.65+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jan 30 11:37:43.484: INFO: Found all 1 expected endpoints: [netserver-0]
Jan 30 11:37:43.484: INFO: Going to poll 10.178.197.13 on port 8081 at least 0 times, with a maximum of 34 tries before failing
Jan 30 11:37:43.485: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.178.197.13 8081 | grep -v '^\s*$'] Namespace:pod-network-test-5130 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 30 11:37:43.485: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
Jan 30 11:37:43.486: INFO: ExecWithOptions: Clientset creation
Jan 30 11:37:43.486: INFO: ExecWithOptions: execute(POST https://10.178.64.1:443/api/v1/namespaces/pod-network-test-5130/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.178.197.13+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jan 30 11:37:44.583: INFO: Found all 1 expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  test/e2e/framework/node/init/init.go:32
Jan 30 11:37:44.583: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Networking
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Networking
  tear down framework | framework.go:193
STEP: Destroying namespace "pod-network-test-5130" for this suite. 01/30/23 11:37:44.586
------------------------------
• [SLOW TEST] [26.246 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 11:37:18.343
    Jan 30 11:37:18.343: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename pod-network-test 01/30/23 11:37:18.344
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:37:18.351
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:37:18.353
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:31
    [It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:122
    STEP: Performing setup for networking test in namespace pod-network-test-5130 01/30/23 11:37:18.355
    STEP: creating a selector 01/30/23 11:37:18.355
    STEP: Creating the service pods in kubernetes 01/30/23 11:37:18.355
    Jan 30 11:37:18.355: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Jan 30 11:37:18.366: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-5130" to be "running and ready"
    Jan 30 11:37:18.367: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 1.668287ms
    Jan 30 11:37:18.367: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Jan 30 11:37:20.372: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.006037602s
    Jan 30 11:37:20.372: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 30 11:37:22.372: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.005914979s
    Jan 30 11:37:22.372: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 30 11:37:24.370: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.004395254s
    Jan 30 11:37:24.370: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 30 11:37:26.372: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.005804456s
    Jan 30 11:37:26.372: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 30 11:37:28.371: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.005447321s
    Jan 30 11:37:28.371: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 30 11:37:30.371: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.005213274s
    Jan 30 11:37:30.371: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 30 11:37:32.371: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.00487367s
    Jan 30 11:37:32.371: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 30 11:37:34.371: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.005180997s
    Jan 30 11:37:34.371: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 30 11:37:36.371: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.005204693s
    Jan 30 11:37:36.371: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 30 11:37:38.370: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.004415509s
    Jan 30 11:37:38.370: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 30 11:37:40.372: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.006064331s
    Jan 30 11:37:40.372: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Jan 30 11:37:40.372: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Jan 30 11:37:40.374: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-5130" to be "running and ready"
    Jan 30 11:37:40.376: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 1.844911ms
    Jan 30 11:37:40.376: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Jan 30 11:37:40.376: INFO: Pod "netserver-1" satisfied condition "running and ready"
    STEP: Creating test pods 01/30/23 11:37:40.377
    Jan 30 11:37:40.382: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-5130" to be "running"
    Jan 30 11:37:40.384: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 1.751854ms
    Jan 30 11:37:42.387: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.004806199s
    Jan 30 11:37:42.387: INFO: Pod "test-container-pod" satisfied condition "running"
    Jan 30 11:37:42.389: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-5130" to be "running"
    Jan 30 11:37:42.390: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 1.532268ms
    Jan 30 11:37:42.390: INFO: Pod "host-test-container-pod" satisfied condition "running"
    Jan 30 11:37:42.392: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
    Jan 30 11:37:42.392: INFO: Going to poll 10.178.151.65 on port 8081 at least 0 times, with a maximum of 34 tries before failing
    Jan 30 11:37:42.394: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.178.151.65 8081 | grep -v '^\s*$'] Namespace:pod-network-test-5130 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 30 11:37:42.394: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    Jan 30 11:37:42.394: INFO: ExecWithOptions: Clientset creation
    Jan 30 11:37:42.394: INFO: ExecWithOptions: execute(POST https://10.178.64.1:443/api/v1/namespaces/pod-network-test-5130/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.178.151.65+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Jan 30 11:37:43.484: INFO: Found all 1 expected endpoints: [netserver-0]
    Jan 30 11:37:43.484: INFO: Going to poll 10.178.197.13 on port 8081 at least 0 times, with a maximum of 34 tries before failing
    Jan 30 11:37:43.485: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.178.197.13 8081 | grep -v '^\s*$'] Namespace:pod-network-test-5130 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 30 11:37:43.485: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    Jan 30 11:37:43.486: INFO: ExecWithOptions: Clientset creation
    Jan 30 11:37:43.486: INFO: ExecWithOptions: execute(POST https://10.178.64.1:443/api/v1/namespaces/pod-network-test-5130/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.178.197.13+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Jan 30 11:37:44.583: INFO: Found all 1 expected endpoints: [netserver-1]
    [AfterEach] [sig-network] Networking
      test/e2e/framework/node/init/init.go:32
    Jan 30 11:37:44.583: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Networking
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Networking
      tear down framework | framework.go:193
    STEP: Destroying namespace "pod-network-test-5130" for this suite. 01/30/23 11:37:44.586
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:230
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 11:37:44.591
Jan 30 11:37:44.591: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename resourcequota 01/30/23 11:37:44.592
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:37:44.599
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:37:44.601
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:230
STEP: Counting existing ResourceQuota 01/30/23 11:37:44.603
STEP: Creating a ResourceQuota 01/30/23 11:37:49.606
STEP: Ensuring resource quota status is calculated 01/30/23 11:37:49.609
STEP: Creating a Pod that fits quota 01/30/23 11:37:51.611
STEP: Ensuring ResourceQuota status captures the pod usage 01/30/23 11:37:51.62
STEP: Not allowing a pod to be created that exceeds remaining quota 01/30/23 11:37:53.623
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources) 01/30/23 11:37:53.625
STEP: Ensuring a pod cannot update its resource requirements 01/30/23 11:37:53.627
STEP: Ensuring attempts to update pod resource requirements did not change quota usage 01/30/23 11:37:53.63
STEP: Deleting the pod 01/30/23 11:37:55.633
STEP: Ensuring resource quota status released the pod usage 01/30/23 11:37:55.638
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Jan 30 11:37:57.641: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-4898" for this suite. 01/30/23 11:37:57.643
------------------------------
• [SLOW TEST] [13.055 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:230

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 11:37:44.591
    Jan 30 11:37:44.591: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename resourcequota 01/30/23 11:37:44.592
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:37:44.599
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:37:44.601
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a pod. [Conformance]
      test/e2e/apimachinery/resource_quota.go:230
    STEP: Counting existing ResourceQuota 01/30/23 11:37:44.603
    STEP: Creating a ResourceQuota 01/30/23 11:37:49.606
    STEP: Ensuring resource quota status is calculated 01/30/23 11:37:49.609
    STEP: Creating a Pod that fits quota 01/30/23 11:37:51.611
    STEP: Ensuring ResourceQuota status captures the pod usage 01/30/23 11:37:51.62
    STEP: Not allowing a pod to be created that exceeds remaining quota 01/30/23 11:37:53.623
    STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources) 01/30/23 11:37:53.625
    STEP: Ensuring a pod cannot update its resource requirements 01/30/23 11:37:53.627
    STEP: Ensuring attempts to update pod resource requirements did not change quota usage 01/30/23 11:37:53.63
    STEP: Deleting the pod 01/30/23 11:37:55.633
    STEP: Ensuring resource quota status released the pod usage 01/30/23 11:37:55.638
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Jan 30 11:37:57.641: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-4898" for this suite. 01/30/23 11:37:57.643
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-network] Services
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1515
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 11:37:57.647
Jan 30 11:37:57.647: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename services 01/30/23 11:37:57.648
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:37:57.654
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:37:57.656
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1515
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-3911 01/30/23 11:37:57.658
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 01/30/23 11:37:57.661
STEP: creating service externalsvc in namespace services-3911 01/30/23 11:37:57.662
STEP: creating replication controller externalsvc in namespace services-3911 01/30/23 11:37:57.665
I0130 11:37:57.668111      23 runners.go:193] Created replication controller with name: externalsvc, namespace: services-3911, replica count: 2
I0130 11:38:00.719349      23 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName 01/30/23 11:38:00.721
Jan 30 11:38:00.726: INFO: Creating new exec pod
Jan 30 11:38:00.728: INFO: Waiting up to 5m0s for pod "execpodx72p6" in namespace "services-3911" to be "running"
Jan 30 11:38:00.730: INFO: Pod "execpodx72p6": Phase="Pending", Reason="", readiness=false. Elapsed: 1.86184ms
Jan 30 11:38:02.733: INFO: Pod "execpodx72p6": Phase="Running", Reason="", readiness=true. Elapsed: 2.004922675s
Jan 30 11:38:02.733: INFO: Pod "execpodx72p6" satisfied condition "running"
Jan 30 11:38:02.733: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=services-3911 exec execpodx72p6 -- /bin/sh -x -c nslookup clusterip-service.services-3911.svc.cluster.local'
Jan 30 11:38:02.907: INFO: stderr: "+ nslookup clusterip-service.services-3911.svc.cluster.local\n"
Jan 30 11:38:02.907: INFO: stdout: "Server:\t\t10.178.64.64\nAddress:\t10.178.64.64#53\n\nclusterip-service.services-3911.svc.cluster.local\tcanonical name = externalsvc.services-3911.svc.cluster.local.\nName:\texternalsvc.services-3911.svc.cluster.local\nAddress: 10.178.73.225\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-3911, will wait for the garbage collector to delete the pods 01/30/23 11:38:02.907
Jan 30 11:38:02.962: INFO: Deleting ReplicationController externalsvc took: 2.465386ms
Jan 30 11:38:03.062: INFO: Terminating ReplicationController externalsvc pods took: 100.073876ms
Jan 30 11:38:05.667: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jan 30 11:38:05.671: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-3911" for this suite. 01/30/23 11:38:05.674
------------------------------
• [SLOW TEST] [8.030 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1515

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 11:37:57.647
    Jan 30 11:37:57.647: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename services 01/30/23 11:37:57.648
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:37:57.654
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:37:57.656
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to change the type from ClusterIP to ExternalName [Conformance]
      test/e2e/network/service.go:1515
    STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-3911 01/30/23 11:37:57.658
    STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 01/30/23 11:37:57.661
    STEP: creating service externalsvc in namespace services-3911 01/30/23 11:37:57.662
    STEP: creating replication controller externalsvc in namespace services-3911 01/30/23 11:37:57.665
    I0130 11:37:57.668111      23 runners.go:193] Created replication controller with name: externalsvc, namespace: services-3911, replica count: 2
    I0130 11:38:00.719349      23 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    STEP: changing the ClusterIP service to type=ExternalName 01/30/23 11:38:00.721
    Jan 30 11:38:00.726: INFO: Creating new exec pod
    Jan 30 11:38:00.728: INFO: Waiting up to 5m0s for pod "execpodx72p6" in namespace "services-3911" to be "running"
    Jan 30 11:38:00.730: INFO: Pod "execpodx72p6": Phase="Pending", Reason="", readiness=false. Elapsed: 1.86184ms
    Jan 30 11:38:02.733: INFO: Pod "execpodx72p6": Phase="Running", Reason="", readiness=true. Elapsed: 2.004922675s
    Jan 30 11:38:02.733: INFO: Pod "execpodx72p6" satisfied condition "running"
    Jan 30 11:38:02.733: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=services-3911 exec execpodx72p6 -- /bin/sh -x -c nslookup clusterip-service.services-3911.svc.cluster.local'
    Jan 30 11:38:02.907: INFO: stderr: "+ nslookup clusterip-service.services-3911.svc.cluster.local\n"
    Jan 30 11:38:02.907: INFO: stdout: "Server:\t\t10.178.64.64\nAddress:\t10.178.64.64#53\n\nclusterip-service.services-3911.svc.cluster.local\tcanonical name = externalsvc.services-3911.svc.cluster.local.\nName:\texternalsvc.services-3911.svc.cluster.local\nAddress: 10.178.73.225\n\n"
    STEP: deleting ReplicationController externalsvc in namespace services-3911, will wait for the garbage collector to delete the pods 01/30/23 11:38:02.907
    Jan 30 11:38:02.962: INFO: Deleting ReplicationController externalsvc took: 2.465386ms
    Jan 30 11:38:03.062: INFO: Terminating ReplicationController externalsvc pods took: 100.073876ms
    Jan 30 11:38:05.667: INFO: Cleaning up the ClusterIP to ExternalName test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jan 30 11:38:05.671: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-3911" for this suite. 01/30/23 11:38:05.674
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should apply changes to a resourcequota status [Conformance]
  test/e2e/apimachinery/resource_quota.go:1010
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 11:38:05.679
Jan 30 11:38:05.679: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename resourcequota 01/30/23 11:38:05.68
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:38:05.686
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:38:05.688
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should apply changes to a resourcequota status [Conformance]
  test/e2e/apimachinery/resource_quota.go:1010
STEP: Creating resourceQuota "e2e-rq-status-mqpd2" 01/30/23 11:38:05.691
Jan 30 11:38:05.694: INFO: Resource quota "e2e-rq-status-mqpd2" reports spec: hard cpu limit of 500m
Jan 30 11:38:05.694: INFO: Resource quota "e2e-rq-status-mqpd2" reports spec: hard memory limit of 500Mi
STEP: Updating resourceQuota "e2e-rq-status-mqpd2" /status 01/30/23 11:38:05.694
STEP: Confirm /status for "e2e-rq-status-mqpd2" resourceQuota via watch 01/30/23 11:38:05.698
Jan 30 11:38:05.699: INFO: observed resourceQuota "e2e-rq-status-mqpd2" in namespace "resourcequota-5342" with hard status: v1.ResourceList(nil)
Jan 30 11:38:05.699: INFO: Found resourceQuota "e2e-rq-status-mqpd2" in namespace "resourcequota-5342" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
Jan 30 11:38:05.699: INFO: ResourceQuota "e2e-rq-status-mqpd2" /status was updated
STEP: Patching hard spec values for cpu & memory 01/30/23 11:38:05.701
Jan 30 11:38:05.705: INFO: Resource quota "e2e-rq-status-mqpd2" reports spec: hard cpu limit of 1
Jan 30 11:38:05.705: INFO: Resource quota "e2e-rq-status-mqpd2" reports spec: hard memory limit of 1Gi
STEP: Patching "e2e-rq-status-mqpd2" /status 01/30/23 11:38:05.705
STEP: Confirm /status for "e2e-rq-status-mqpd2" resourceQuota via watch 01/30/23 11:38:05.709
Jan 30 11:38:05.710: INFO: observed resourceQuota "e2e-rq-status-mqpd2" in namespace "resourcequota-5342" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
Jan 30 11:38:05.710: INFO: Found resourceQuota "e2e-rq-status-mqpd2" in namespace "resourcequota-5342" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:1, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:1073741824, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1Gi", Format:"BinarySI"}}
Jan 30 11:38:05.710: INFO: ResourceQuota "e2e-rq-status-mqpd2" /status was patched
STEP: Get "e2e-rq-status-mqpd2" /status 01/30/23 11:38:05.71
Jan 30 11:38:05.712: INFO: Resourcequota "e2e-rq-status-mqpd2" reports status: hard cpu of 1
Jan 30 11:38:05.712: INFO: Resourcequota "e2e-rq-status-mqpd2" reports status: hard memory of 1Gi
STEP: Repatching "e2e-rq-status-mqpd2" /status before checking Spec is unchanged 01/30/23 11:38:05.714
Jan 30 11:38:05.717: INFO: Resourcequota "e2e-rq-status-mqpd2" reports status: hard cpu of 2
Jan 30 11:38:05.717: INFO: Resourcequota "e2e-rq-status-mqpd2" reports status: hard memory of 2Gi
Jan 30 11:38:05.718: INFO: Found resourceQuota "e2e-rq-status-mqpd2" in namespace "resourcequota-5342" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:2, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:2147483648, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2Gi", Format:"BinarySI"}}
Jan 30 11:41:45.722: INFO: ResourceQuota "e2e-rq-status-mqpd2" Spec was unchanged and /status reset
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Jan 30 11:41:45.722: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-5342" for this suite. 01/30/23 11:41:45.724
------------------------------
• [SLOW TEST] [220.048 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should apply changes to a resourcequota status [Conformance]
  test/e2e/apimachinery/resource_quota.go:1010

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 11:38:05.679
    Jan 30 11:38:05.679: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename resourcequota 01/30/23 11:38:05.68
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:38:05.686
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:38:05.688
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply changes to a resourcequota status [Conformance]
      test/e2e/apimachinery/resource_quota.go:1010
    STEP: Creating resourceQuota "e2e-rq-status-mqpd2" 01/30/23 11:38:05.691
    Jan 30 11:38:05.694: INFO: Resource quota "e2e-rq-status-mqpd2" reports spec: hard cpu limit of 500m
    Jan 30 11:38:05.694: INFO: Resource quota "e2e-rq-status-mqpd2" reports spec: hard memory limit of 500Mi
    STEP: Updating resourceQuota "e2e-rq-status-mqpd2" /status 01/30/23 11:38:05.694
    STEP: Confirm /status for "e2e-rq-status-mqpd2" resourceQuota via watch 01/30/23 11:38:05.698
    Jan 30 11:38:05.699: INFO: observed resourceQuota "e2e-rq-status-mqpd2" in namespace "resourcequota-5342" with hard status: v1.ResourceList(nil)
    Jan 30 11:38:05.699: INFO: Found resourceQuota "e2e-rq-status-mqpd2" in namespace "resourcequota-5342" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
    Jan 30 11:38:05.699: INFO: ResourceQuota "e2e-rq-status-mqpd2" /status was updated
    STEP: Patching hard spec values for cpu & memory 01/30/23 11:38:05.701
    Jan 30 11:38:05.705: INFO: Resource quota "e2e-rq-status-mqpd2" reports spec: hard cpu limit of 1
    Jan 30 11:38:05.705: INFO: Resource quota "e2e-rq-status-mqpd2" reports spec: hard memory limit of 1Gi
    STEP: Patching "e2e-rq-status-mqpd2" /status 01/30/23 11:38:05.705
    STEP: Confirm /status for "e2e-rq-status-mqpd2" resourceQuota via watch 01/30/23 11:38:05.709
    Jan 30 11:38:05.710: INFO: observed resourceQuota "e2e-rq-status-mqpd2" in namespace "resourcequota-5342" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
    Jan 30 11:38:05.710: INFO: Found resourceQuota "e2e-rq-status-mqpd2" in namespace "resourcequota-5342" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:1, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:1073741824, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1Gi", Format:"BinarySI"}}
    Jan 30 11:38:05.710: INFO: ResourceQuota "e2e-rq-status-mqpd2" /status was patched
    STEP: Get "e2e-rq-status-mqpd2" /status 01/30/23 11:38:05.71
    Jan 30 11:38:05.712: INFO: Resourcequota "e2e-rq-status-mqpd2" reports status: hard cpu of 1
    Jan 30 11:38:05.712: INFO: Resourcequota "e2e-rq-status-mqpd2" reports status: hard memory of 1Gi
    STEP: Repatching "e2e-rq-status-mqpd2" /status before checking Spec is unchanged 01/30/23 11:38:05.714
    Jan 30 11:38:05.717: INFO: Resourcequota "e2e-rq-status-mqpd2" reports status: hard cpu of 2
    Jan 30 11:38:05.717: INFO: Resourcequota "e2e-rq-status-mqpd2" reports status: hard memory of 2Gi
    Jan 30 11:38:05.718: INFO: Found resourceQuota "e2e-rq-status-mqpd2" in namespace "resourcequota-5342" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:2, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:2147483648, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2Gi", Format:"BinarySI"}}
    Jan 30 11:41:45.722: INFO: ResourceQuota "e2e-rq-status-mqpd2" Spec was unchanged and /status reset
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Jan 30 11:41:45.722: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-5342" for this suite. 01/30/23 11:41:45.724
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 11:41:45.727
Jan 30 11:41:45.728: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename gc 01/30/23 11:41:45.728
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:41:45.735
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:41:45.736
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735
STEP: create the rc1 01/30/23 11:41:45.74
STEP: create the rc2 01/30/23 11:41:45.743
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well 01/30/23 11:41:50.747
STEP: delete the rc simpletest-rc-to-be-deleted 01/30/23 11:41:50.972
STEP: wait for the rc to be deleted 01/30/23 11:41:50.975
STEP: Gathering metrics 01/30/23 11:41:55.985
Jan 30 11:41:56.561: INFO: Waiting up to 5m0s for pod "kube-controller-manager-pubt2-nks-for-dev3.dg.163.org" in namespace "kube-system" to be "running and ready"
Jan 30 11:41:56.564: INFO: Pod "kube-controller-manager-pubt2-nks-for-dev3.dg.163.org": Phase="Running", Reason="", readiness=true. Elapsed: 2.237475ms
Jan 30 11:41:56.564: INFO: The phase of Pod kube-controller-manager-pubt2-nks-for-dev3.dg.163.org is Running (Ready = true)
Jan 30 11:41:56.564: INFO: Pod "kube-controller-manager-pubt2-nks-for-dev3.dg.163.org" satisfied condition "running and ready"
Jan 30 11:41:57.112: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Jan 30 11:41:57.112: INFO: Deleting pod "simpletest-rc-to-be-deleted-2c6d4" in namespace "gc-3856"
Jan 30 11:41:57.116: INFO: Deleting pod "simpletest-rc-to-be-deleted-45v9w" in namespace "gc-3856"
Jan 30 11:41:57.121: INFO: Deleting pod "simpletest-rc-to-be-deleted-47lpd" in namespace "gc-3856"
Jan 30 11:41:57.124: INFO: Deleting pod "simpletest-rc-to-be-deleted-4h6kw" in namespace "gc-3856"
Jan 30 11:41:57.128: INFO: Deleting pod "simpletest-rc-to-be-deleted-4rzwk" in namespace "gc-3856"
Jan 30 11:41:57.132: INFO: Deleting pod "simpletest-rc-to-be-deleted-4v5qq" in namespace "gc-3856"
Jan 30 11:41:57.136: INFO: Deleting pod "simpletest-rc-to-be-deleted-5m4xv" in namespace "gc-3856"
Jan 30 11:41:57.140: INFO: Deleting pod "simpletest-rc-to-be-deleted-5vwwc" in namespace "gc-3856"
Jan 30 11:41:57.145: INFO: Deleting pod "simpletest-rc-to-be-deleted-5wzh9" in namespace "gc-3856"
Jan 30 11:41:57.149: INFO: Deleting pod "simpletest-rc-to-be-deleted-6pbkp" in namespace "gc-3856"
Jan 30 11:41:57.156: INFO: Deleting pod "simpletest-rc-to-be-deleted-6tpww" in namespace "gc-3856"
Jan 30 11:41:57.160: INFO: Deleting pod "simpletest-rc-to-be-deleted-6vxdn" in namespace "gc-3856"
Jan 30 11:41:57.164: INFO: Deleting pod "simpletest-rc-to-be-deleted-75g2p" in namespace "gc-3856"
Jan 30 11:41:57.168: INFO: Deleting pod "simpletest-rc-to-be-deleted-7src5" in namespace "gc-3856"
Jan 30 11:41:57.172: INFO: Deleting pod "simpletest-rc-to-be-deleted-7tt52" in namespace "gc-3856"
Jan 30 11:41:57.176: INFO: Deleting pod "simpletest-rc-to-be-deleted-8pnqz" in namespace "gc-3856"
Jan 30 11:41:57.180: INFO: Deleting pod "simpletest-rc-to-be-deleted-8q96v" in namespace "gc-3856"
Jan 30 11:41:57.184: INFO: Deleting pod "simpletest-rc-to-be-deleted-8vxvt" in namespace "gc-3856"
Jan 30 11:41:57.188: INFO: Deleting pod "simpletest-rc-to-be-deleted-9bq6s" in namespace "gc-3856"
Jan 30 11:41:57.192: INFO: Deleting pod "simpletest-rc-to-be-deleted-9k87w" in namespace "gc-3856"
Jan 30 11:41:57.196: INFO: Deleting pod "simpletest-rc-to-be-deleted-9qcn5" in namespace "gc-3856"
Jan 30 11:41:57.200: INFO: Deleting pod "simpletest-rc-to-be-deleted-9tjx6" in namespace "gc-3856"
Jan 30 11:41:57.204: INFO: Deleting pod "simpletest-rc-to-be-deleted-b8p5h" in namespace "gc-3856"
Jan 30 11:41:57.208: INFO: Deleting pod "simpletest-rc-to-be-deleted-b9f7g" in namespace "gc-3856"
Jan 30 11:41:57.212: INFO: Deleting pod "simpletest-rc-to-be-deleted-br8zp" in namespace "gc-3856"
Jan 30 11:41:57.215: INFO: Deleting pod "simpletest-rc-to-be-deleted-c2xkt" in namespace "gc-3856"
Jan 30 11:41:57.219: INFO: Deleting pod "simpletest-rc-to-be-deleted-c9lb9" in namespace "gc-3856"
Jan 30 11:41:57.223: INFO: Deleting pod "simpletest-rc-to-be-deleted-cb4k5" in namespace "gc-3856"
Jan 30 11:41:57.226: INFO: Deleting pod "simpletest-rc-to-be-deleted-cbqn2" in namespace "gc-3856"
Jan 30 11:41:57.230: INFO: Deleting pod "simpletest-rc-to-be-deleted-ckss4" in namespace "gc-3856"
Jan 30 11:41:57.234: INFO: Deleting pod "simpletest-rc-to-be-deleted-cm4gm" in namespace "gc-3856"
Jan 30 11:41:57.237: INFO: Deleting pod "simpletest-rc-to-be-deleted-cth6v" in namespace "gc-3856"
Jan 30 11:41:57.242: INFO: Deleting pod "simpletest-rc-to-be-deleted-ctkgr" in namespace "gc-3856"
Jan 30 11:41:57.245: INFO: Deleting pod "simpletest-rc-to-be-deleted-cx6tl" in namespace "gc-3856"
Jan 30 11:41:57.249: INFO: Deleting pod "simpletest-rc-to-be-deleted-dm8x4" in namespace "gc-3856"
Jan 30 11:41:57.254: INFO: Deleting pod "simpletest-rc-to-be-deleted-dts55" in namespace "gc-3856"
Jan 30 11:41:57.259: INFO: Deleting pod "simpletest-rc-to-be-deleted-f5g94" in namespace "gc-3856"
Jan 30 11:41:57.263: INFO: Deleting pod "simpletest-rc-to-be-deleted-f6mwv" in namespace "gc-3856"
Jan 30 11:41:57.267: INFO: Deleting pod "simpletest-rc-to-be-deleted-ff4wn" in namespace "gc-3856"
Jan 30 11:41:57.271: INFO: Deleting pod "simpletest-rc-to-be-deleted-fs2wf" in namespace "gc-3856"
Jan 30 11:41:57.274: INFO: Deleting pod "simpletest-rc-to-be-deleted-fvlvd" in namespace "gc-3856"
Jan 30 11:41:57.278: INFO: Deleting pod "simpletest-rc-to-be-deleted-gr2t9" in namespace "gc-3856"
Jan 30 11:41:57.282: INFO: Deleting pod "simpletest-rc-to-be-deleted-gvt7x" in namespace "gc-3856"
Jan 30 11:41:57.286: INFO: Deleting pod "simpletest-rc-to-be-deleted-gzgtj" in namespace "gc-3856"
Jan 30 11:41:57.289: INFO: Deleting pod "simpletest-rc-to-be-deleted-h95hk" in namespace "gc-3856"
Jan 30 11:41:57.293: INFO: Deleting pod "simpletest-rc-to-be-deleted-hbvcx" in namespace "gc-3856"
Jan 30 11:41:57.298: INFO: Deleting pod "simpletest-rc-to-be-deleted-hp6q9" in namespace "gc-3856"
Jan 30 11:41:57.303: INFO: Deleting pod "simpletest-rc-to-be-deleted-hpk6k" in namespace "gc-3856"
Jan 30 11:41:57.308: INFO: Deleting pod "simpletest-rc-to-be-deleted-hps97" in namespace "gc-3856"
Jan 30 11:41:57.313: INFO: Deleting pod "simpletest-rc-to-be-deleted-hvz95" in namespace "gc-3856"
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Jan 30 11:41:57.318: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-3856" for this suite. 01/30/23 11:41:57.32
------------------------------
• [SLOW TEST] [11.596 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 11:41:45.727
    Jan 30 11:41:45.728: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename gc 01/30/23 11:41:45.728
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:41:45.735
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:41:45.736
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
      test/e2e/apimachinery/garbage_collector.go:735
    STEP: create the rc1 01/30/23 11:41:45.74
    STEP: create the rc2 01/30/23 11:41:45.743
    STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well 01/30/23 11:41:50.747
    STEP: delete the rc simpletest-rc-to-be-deleted 01/30/23 11:41:50.972
    STEP: wait for the rc to be deleted 01/30/23 11:41:50.975
    STEP: Gathering metrics 01/30/23 11:41:55.985
    Jan 30 11:41:56.561: INFO: Waiting up to 5m0s for pod "kube-controller-manager-pubt2-nks-for-dev3.dg.163.org" in namespace "kube-system" to be "running and ready"
    Jan 30 11:41:56.564: INFO: Pod "kube-controller-manager-pubt2-nks-for-dev3.dg.163.org": Phase="Running", Reason="", readiness=true. Elapsed: 2.237475ms
    Jan 30 11:41:56.564: INFO: The phase of Pod kube-controller-manager-pubt2-nks-for-dev3.dg.163.org is Running (Ready = true)
    Jan 30 11:41:56.564: INFO: Pod "kube-controller-manager-pubt2-nks-for-dev3.dg.163.org" satisfied condition "running and ready"
    Jan 30 11:41:57.112: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    Jan 30 11:41:57.112: INFO: Deleting pod "simpletest-rc-to-be-deleted-2c6d4" in namespace "gc-3856"
    Jan 30 11:41:57.116: INFO: Deleting pod "simpletest-rc-to-be-deleted-45v9w" in namespace "gc-3856"
    Jan 30 11:41:57.121: INFO: Deleting pod "simpletest-rc-to-be-deleted-47lpd" in namespace "gc-3856"
    Jan 30 11:41:57.124: INFO: Deleting pod "simpletest-rc-to-be-deleted-4h6kw" in namespace "gc-3856"
    Jan 30 11:41:57.128: INFO: Deleting pod "simpletest-rc-to-be-deleted-4rzwk" in namespace "gc-3856"
    Jan 30 11:41:57.132: INFO: Deleting pod "simpletest-rc-to-be-deleted-4v5qq" in namespace "gc-3856"
    Jan 30 11:41:57.136: INFO: Deleting pod "simpletest-rc-to-be-deleted-5m4xv" in namespace "gc-3856"
    Jan 30 11:41:57.140: INFO: Deleting pod "simpletest-rc-to-be-deleted-5vwwc" in namespace "gc-3856"
    Jan 30 11:41:57.145: INFO: Deleting pod "simpletest-rc-to-be-deleted-5wzh9" in namespace "gc-3856"
    Jan 30 11:41:57.149: INFO: Deleting pod "simpletest-rc-to-be-deleted-6pbkp" in namespace "gc-3856"
    Jan 30 11:41:57.156: INFO: Deleting pod "simpletest-rc-to-be-deleted-6tpww" in namespace "gc-3856"
    Jan 30 11:41:57.160: INFO: Deleting pod "simpletest-rc-to-be-deleted-6vxdn" in namespace "gc-3856"
    Jan 30 11:41:57.164: INFO: Deleting pod "simpletest-rc-to-be-deleted-75g2p" in namespace "gc-3856"
    Jan 30 11:41:57.168: INFO: Deleting pod "simpletest-rc-to-be-deleted-7src5" in namespace "gc-3856"
    Jan 30 11:41:57.172: INFO: Deleting pod "simpletest-rc-to-be-deleted-7tt52" in namespace "gc-3856"
    Jan 30 11:41:57.176: INFO: Deleting pod "simpletest-rc-to-be-deleted-8pnqz" in namespace "gc-3856"
    Jan 30 11:41:57.180: INFO: Deleting pod "simpletest-rc-to-be-deleted-8q96v" in namespace "gc-3856"
    Jan 30 11:41:57.184: INFO: Deleting pod "simpletest-rc-to-be-deleted-8vxvt" in namespace "gc-3856"
    Jan 30 11:41:57.188: INFO: Deleting pod "simpletest-rc-to-be-deleted-9bq6s" in namespace "gc-3856"
    Jan 30 11:41:57.192: INFO: Deleting pod "simpletest-rc-to-be-deleted-9k87w" in namespace "gc-3856"
    Jan 30 11:41:57.196: INFO: Deleting pod "simpletest-rc-to-be-deleted-9qcn5" in namespace "gc-3856"
    Jan 30 11:41:57.200: INFO: Deleting pod "simpletest-rc-to-be-deleted-9tjx6" in namespace "gc-3856"
    Jan 30 11:41:57.204: INFO: Deleting pod "simpletest-rc-to-be-deleted-b8p5h" in namespace "gc-3856"
    Jan 30 11:41:57.208: INFO: Deleting pod "simpletest-rc-to-be-deleted-b9f7g" in namespace "gc-3856"
    Jan 30 11:41:57.212: INFO: Deleting pod "simpletest-rc-to-be-deleted-br8zp" in namespace "gc-3856"
    Jan 30 11:41:57.215: INFO: Deleting pod "simpletest-rc-to-be-deleted-c2xkt" in namespace "gc-3856"
    Jan 30 11:41:57.219: INFO: Deleting pod "simpletest-rc-to-be-deleted-c9lb9" in namespace "gc-3856"
    Jan 30 11:41:57.223: INFO: Deleting pod "simpletest-rc-to-be-deleted-cb4k5" in namespace "gc-3856"
    Jan 30 11:41:57.226: INFO: Deleting pod "simpletest-rc-to-be-deleted-cbqn2" in namespace "gc-3856"
    Jan 30 11:41:57.230: INFO: Deleting pod "simpletest-rc-to-be-deleted-ckss4" in namespace "gc-3856"
    Jan 30 11:41:57.234: INFO: Deleting pod "simpletest-rc-to-be-deleted-cm4gm" in namespace "gc-3856"
    Jan 30 11:41:57.237: INFO: Deleting pod "simpletest-rc-to-be-deleted-cth6v" in namespace "gc-3856"
    Jan 30 11:41:57.242: INFO: Deleting pod "simpletest-rc-to-be-deleted-ctkgr" in namespace "gc-3856"
    Jan 30 11:41:57.245: INFO: Deleting pod "simpletest-rc-to-be-deleted-cx6tl" in namespace "gc-3856"
    Jan 30 11:41:57.249: INFO: Deleting pod "simpletest-rc-to-be-deleted-dm8x4" in namespace "gc-3856"
    Jan 30 11:41:57.254: INFO: Deleting pod "simpletest-rc-to-be-deleted-dts55" in namespace "gc-3856"
    Jan 30 11:41:57.259: INFO: Deleting pod "simpletest-rc-to-be-deleted-f5g94" in namespace "gc-3856"
    Jan 30 11:41:57.263: INFO: Deleting pod "simpletest-rc-to-be-deleted-f6mwv" in namespace "gc-3856"
    Jan 30 11:41:57.267: INFO: Deleting pod "simpletest-rc-to-be-deleted-ff4wn" in namespace "gc-3856"
    Jan 30 11:41:57.271: INFO: Deleting pod "simpletest-rc-to-be-deleted-fs2wf" in namespace "gc-3856"
    Jan 30 11:41:57.274: INFO: Deleting pod "simpletest-rc-to-be-deleted-fvlvd" in namespace "gc-3856"
    Jan 30 11:41:57.278: INFO: Deleting pod "simpletest-rc-to-be-deleted-gr2t9" in namespace "gc-3856"
    Jan 30 11:41:57.282: INFO: Deleting pod "simpletest-rc-to-be-deleted-gvt7x" in namespace "gc-3856"
    Jan 30 11:41:57.286: INFO: Deleting pod "simpletest-rc-to-be-deleted-gzgtj" in namespace "gc-3856"
    Jan 30 11:41:57.289: INFO: Deleting pod "simpletest-rc-to-be-deleted-h95hk" in namespace "gc-3856"
    Jan 30 11:41:57.293: INFO: Deleting pod "simpletest-rc-to-be-deleted-hbvcx" in namespace "gc-3856"
    Jan 30 11:41:57.298: INFO: Deleting pod "simpletest-rc-to-be-deleted-hp6q9" in namespace "gc-3856"
    Jan 30 11:41:57.303: INFO: Deleting pod "simpletest-rc-to-be-deleted-hpk6k" in namespace "gc-3856"
    Jan 30 11:41:57.308: INFO: Deleting pod "simpletest-rc-to-be-deleted-hps97" in namespace "gc-3856"
    Jan 30 11:41:57.313: INFO: Deleting pod "simpletest-rc-to-be-deleted-hvz95" in namespace "gc-3856"
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Jan 30 11:41:57.318: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-3856" for this suite. 01/30/23 11:41:57.32
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:45
[BeforeEach] [sig-node] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 11:41:57.324
Jan 30 11:41:57.324: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename configmap 01/30/23 11:41:57.325
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:41:57.332
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:41:57.334
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:45
STEP: Creating configMap configmap-2410/configmap-test-353c85ac-8f50-4df5-b7d3-7bbc5b2ed226 01/30/23 11:41:57.336
STEP: Creating a pod to test consume configMaps 01/30/23 11:41:57.338
Jan 30 11:41:57.343: INFO: Waiting up to 5m0s for pod "pod-configmaps-c9bdbdd5-0707-48ca-a1fb-b9e2f417b93e" in namespace "configmap-2410" to be "Succeeded or Failed"
Jan 30 11:41:57.345: INFO: Pod "pod-configmaps-c9bdbdd5-0707-48ca-a1fb-b9e2f417b93e": Phase="Pending", Reason="", readiness=false. Elapsed: 1.827809ms
Jan 30 11:41:59.348: INFO: Pod "pod-configmaps-c9bdbdd5-0707-48ca-a1fb-b9e2f417b93e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004898807s
Jan 30 11:42:01.347: INFO: Pod "pod-configmaps-c9bdbdd5-0707-48ca-a1fb-b9e2f417b93e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.004580906s
Jan 30 11:42:03.347: INFO: Pod "pod-configmaps-c9bdbdd5-0707-48ca-a1fb-b9e2f417b93e": Phase="Pending", Reason="", readiness=false. Elapsed: 6.004101779s
Jan 30 11:42:05.347: INFO: Pod "pod-configmaps-c9bdbdd5-0707-48ca-a1fb-b9e2f417b93e": Phase="Pending", Reason="", readiness=false. Elapsed: 8.004073264s
Jan 30 11:42:07.347: INFO: Pod "pod-configmaps-c9bdbdd5-0707-48ca-a1fb-b9e2f417b93e": Phase="Pending", Reason="", readiness=false. Elapsed: 10.004507062s
Jan 30 11:42:09.347: INFO: Pod "pod-configmaps-c9bdbdd5-0707-48ca-a1fb-b9e2f417b93e": Phase="Pending", Reason="", readiness=false. Elapsed: 12.004419437s
Jan 30 11:42:11.348: INFO: Pod "pod-configmaps-c9bdbdd5-0707-48ca-a1fb-b9e2f417b93e": Phase="Pending", Reason="", readiness=false. Elapsed: 14.005260871s
Jan 30 11:42:13.347: INFO: Pod "pod-configmaps-c9bdbdd5-0707-48ca-a1fb-b9e2f417b93e": Phase="Pending", Reason="", readiness=false. Elapsed: 16.004308821s
Jan 30 11:42:15.348: INFO: Pod "pod-configmaps-c9bdbdd5-0707-48ca-a1fb-b9e2f417b93e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 18.005071252s
STEP: Saw pod success 01/30/23 11:42:15.348
Jan 30 11:42:15.348: INFO: Pod "pod-configmaps-c9bdbdd5-0707-48ca-a1fb-b9e2f417b93e" satisfied condition "Succeeded or Failed"
Jan 30 11:42:15.350: INFO: Trying to get logs from node pubt2-nks-for-dev1.dg.163.org pod pod-configmaps-c9bdbdd5-0707-48ca-a1fb-b9e2f417b93e container env-test: <nil>
STEP: delete the pod 01/30/23 11:42:15.363
Jan 30 11:42:15.367: INFO: Waiting for pod pod-configmaps-c9bdbdd5-0707-48ca-a1fb-b9e2f417b93e to disappear
Jan 30 11:42:15.369: INFO: Pod pod-configmaps-c9bdbdd5-0707-48ca-a1fb-b9e2f417b93e no longer exists
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jan 30 11:42:15.369: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-2410" for this suite. 01/30/23 11:42:15.372
------------------------------
• [SLOW TEST] [18.050 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:45

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 11:41:57.324
    Jan 30 11:41:57.324: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename configmap 01/30/23 11:41:57.325
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:41:57.332
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:41:57.334
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable via environment variable [NodeConformance] [Conformance]
      test/e2e/common/node/configmap.go:45
    STEP: Creating configMap configmap-2410/configmap-test-353c85ac-8f50-4df5-b7d3-7bbc5b2ed226 01/30/23 11:41:57.336
    STEP: Creating a pod to test consume configMaps 01/30/23 11:41:57.338
    Jan 30 11:41:57.343: INFO: Waiting up to 5m0s for pod "pod-configmaps-c9bdbdd5-0707-48ca-a1fb-b9e2f417b93e" in namespace "configmap-2410" to be "Succeeded or Failed"
    Jan 30 11:41:57.345: INFO: Pod "pod-configmaps-c9bdbdd5-0707-48ca-a1fb-b9e2f417b93e": Phase="Pending", Reason="", readiness=false. Elapsed: 1.827809ms
    Jan 30 11:41:59.348: INFO: Pod "pod-configmaps-c9bdbdd5-0707-48ca-a1fb-b9e2f417b93e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004898807s
    Jan 30 11:42:01.347: INFO: Pod "pod-configmaps-c9bdbdd5-0707-48ca-a1fb-b9e2f417b93e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.004580906s
    Jan 30 11:42:03.347: INFO: Pod "pod-configmaps-c9bdbdd5-0707-48ca-a1fb-b9e2f417b93e": Phase="Pending", Reason="", readiness=false. Elapsed: 6.004101779s
    Jan 30 11:42:05.347: INFO: Pod "pod-configmaps-c9bdbdd5-0707-48ca-a1fb-b9e2f417b93e": Phase="Pending", Reason="", readiness=false. Elapsed: 8.004073264s
    Jan 30 11:42:07.347: INFO: Pod "pod-configmaps-c9bdbdd5-0707-48ca-a1fb-b9e2f417b93e": Phase="Pending", Reason="", readiness=false. Elapsed: 10.004507062s
    Jan 30 11:42:09.347: INFO: Pod "pod-configmaps-c9bdbdd5-0707-48ca-a1fb-b9e2f417b93e": Phase="Pending", Reason="", readiness=false. Elapsed: 12.004419437s
    Jan 30 11:42:11.348: INFO: Pod "pod-configmaps-c9bdbdd5-0707-48ca-a1fb-b9e2f417b93e": Phase="Pending", Reason="", readiness=false. Elapsed: 14.005260871s
    Jan 30 11:42:13.347: INFO: Pod "pod-configmaps-c9bdbdd5-0707-48ca-a1fb-b9e2f417b93e": Phase="Pending", Reason="", readiness=false. Elapsed: 16.004308821s
    Jan 30 11:42:15.348: INFO: Pod "pod-configmaps-c9bdbdd5-0707-48ca-a1fb-b9e2f417b93e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 18.005071252s
    STEP: Saw pod success 01/30/23 11:42:15.348
    Jan 30 11:42:15.348: INFO: Pod "pod-configmaps-c9bdbdd5-0707-48ca-a1fb-b9e2f417b93e" satisfied condition "Succeeded or Failed"
    Jan 30 11:42:15.350: INFO: Trying to get logs from node pubt2-nks-for-dev1.dg.163.org pod pod-configmaps-c9bdbdd5-0707-48ca-a1fb-b9e2f417b93e container env-test: <nil>
    STEP: delete the pod 01/30/23 11:42:15.363
    Jan 30 11:42:15.367: INFO: Waiting for pod pod-configmaps-c9bdbdd5-0707-48ca-a1fb-b9e2f417b93e to disappear
    Jan 30 11:42:15.369: INFO: Pod pod-configmaps-c9bdbdd5-0707-48ca-a1fb-b9e2f417b93e no longer exists
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jan 30 11:42:15.369: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-2410" for this suite. 01/30/23 11:42:15.372
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-network] Services
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2228
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 11:42:15.375
Jan 30 11:42:15.375: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename services 01/30/23 11:42:15.375
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:42:15.381
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:42:15.383
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2228
STEP: creating service in namespace services-987 01/30/23 11:42:15.385
STEP: creating service affinity-nodeport in namespace services-987 01/30/23 11:42:15.385
STEP: creating replication controller affinity-nodeport in namespace services-987 01/30/23 11:42:15.39
I0130 11:42:15.393272      23 runners.go:193] Created replication controller with name: affinity-nodeport, namespace: services-987, replica count: 3
I0130 11:42:18.443625      23 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 1 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0130 11:42:21.444054      23 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 30 11:42:21.449: INFO: Creating new exec pod
Jan 30 11:42:21.451: INFO: Waiting up to 5m0s for pod "execpod-affinityf5hlw" in namespace "services-987" to be "running"
Jan 30 11:42:21.453: INFO: Pod "execpod-affinityf5hlw": Phase="Pending", Reason="", readiness=false. Elapsed: 1.527466ms
Jan 30 11:42:23.455: INFO: Pod "execpod-affinityf5hlw": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004034839s
Jan 30 11:42:25.455: INFO: Pod "execpod-affinityf5hlw": Phase="Running", Reason="", readiness=true. Elapsed: 4.003942217s
Jan 30 11:42:25.455: INFO: Pod "execpod-affinityf5hlw" satisfied condition "running"
Jan 30 11:42:26.458: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=services-987 exec execpod-affinityf5hlw -- /bin/sh -x -c nc -v -z -w 2 affinity-nodeport 80'
Jan 30 11:42:26.616: INFO: stderr: "+ nc -v -z -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
Jan 30 11:42:26.616: INFO: stdout: ""
Jan 30 11:42:26.616: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=services-987 exec execpod-affinityf5hlw -- /bin/sh -x -c nc -v -z -w 2 10.178.78.191 80'
Jan 30 11:42:26.747: INFO: stderr: "+ nc -v -z -w 2 10.178.78.191 80\nConnection to 10.178.78.191 80 port [tcp/http] succeeded!\n"
Jan 30 11:42:26.747: INFO: stdout: ""
Jan 30 11:42:26.747: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=services-987 exec execpod-affinityf5hlw -- /bin/sh -x -c nc -v -z -w 2 10.182.0.82 30600'
Jan 30 11:42:26.871: INFO: stderr: "+ nc -v -z -w 2 10.182.0.82 30600\nConnection to 10.182.0.82 30600 port [tcp/*] succeeded!\n"
Jan 30 11:42:26.871: INFO: stdout: ""
Jan 30 11:42:26.871: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=services-987 exec execpod-affinityf5hlw -- /bin/sh -x -c nc -v -z -w 2 10.182.0.84 30600'
Jan 30 11:42:26.999: INFO: stderr: "+ nc -v -z -w 2 10.182.0.84 30600\nConnection to 10.182.0.84 30600 port [tcp/*] succeeded!\n"
Jan 30 11:42:26.999: INFO: stdout: ""
Jan 30 11:42:26.999: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=services-987 exec execpod-affinityf5hlw -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.182.0.82:30600/ ; done'
Jan 30 11:42:27.174: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.182.0.82:30600/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.182.0.82:30600/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.182.0.82:30600/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.182.0.82:30600/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.182.0.82:30600/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.182.0.82:30600/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.182.0.82:30600/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.182.0.82:30600/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.182.0.82:30600/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.182.0.82:30600/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.182.0.82:30600/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.182.0.82:30600/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.182.0.82:30600/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.182.0.82:30600/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.182.0.82:30600/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.182.0.82:30600/\n"
Jan 30 11:42:27.175: INFO: stdout: "\naffinity-nodeport-qwl98\naffinity-nodeport-qwl98\naffinity-nodeport-qwl98\naffinity-nodeport-qwl98\naffinity-nodeport-qwl98\naffinity-nodeport-qwl98\naffinity-nodeport-qwl98\naffinity-nodeport-qwl98\naffinity-nodeport-qwl98\naffinity-nodeport-qwl98\naffinity-nodeport-qwl98\naffinity-nodeport-qwl98\naffinity-nodeport-qwl98\naffinity-nodeport-qwl98\naffinity-nodeport-qwl98\naffinity-nodeport-qwl98"
Jan 30 11:42:27.175: INFO: Received response from host: affinity-nodeport-qwl98
Jan 30 11:42:27.175: INFO: Received response from host: affinity-nodeport-qwl98
Jan 30 11:42:27.175: INFO: Received response from host: affinity-nodeport-qwl98
Jan 30 11:42:27.175: INFO: Received response from host: affinity-nodeport-qwl98
Jan 30 11:42:27.175: INFO: Received response from host: affinity-nodeport-qwl98
Jan 30 11:42:27.175: INFO: Received response from host: affinity-nodeport-qwl98
Jan 30 11:42:27.175: INFO: Received response from host: affinity-nodeport-qwl98
Jan 30 11:42:27.175: INFO: Received response from host: affinity-nodeport-qwl98
Jan 30 11:42:27.175: INFO: Received response from host: affinity-nodeport-qwl98
Jan 30 11:42:27.175: INFO: Received response from host: affinity-nodeport-qwl98
Jan 30 11:42:27.175: INFO: Received response from host: affinity-nodeport-qwl98
Jan 30 11:42:27.175: INFO: Received response from host: affinity-nodeport-qwl98
Jan 30 11:42:27.175: INFO: Received response from host: affinity-nodeport-qwl98
Jan 30 11:42:27.175: INFO: Received response from host: affinity-nodeport-qwl98
Jan 30 11:42:27.175: INFO: Received response from host: affinity-nodeport-qwl98
Jan 30 11:42:27.175: INFO: Received response from host: affinity-nodeport-qwl98
Jan 30 11:42:27.175: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport in namespace services-987, will wait for the garbage collector to delete the pods 01/30/23 11:42:27.179
Jan 30 11:42:27.236: INFO: Deleting ReplicationController affinity-nodeport took: 2.769616ms
Jan 30 11:42:27.337: INFO: Terminating ReplicationController affinity-nodeport pods took: 100.74025ms
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jan 30 11:42:29.745: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-987" for this suite. 01/30/23 11:42:29.747
------------------------------
• [SLOW TEST] [14.374 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2228

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 11:42:15.375
    Jan 30 11:42:15.375: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename services 01/30/23 11:42:15.375
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:42:15.381
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:42:15.383
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2228
    STEP: creating service in namespace services-987 01/30/23 11:42:15.385
    STEP: creating service affinity-nodeport in namespace services-987 01/30/23 11:42:15.385
    STEP: creating replication controller affinity-nodeport in namespace services-987 01/30/23 11:42:15.39
    I0130 11:42:15.393272      23 runners.go:193] Created replication controller with name: affinity-nodeport, namespace: services-987, replica count: 3
    I0130 11:42:18.443625      23 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 1 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0130 11:42:21.444054      23 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan 30 11:42:21.449: INFO: Creating new exec pod
    Jan 30 11:42:21.451: INFO: Waiting up to 5m0s for pod "execpod-affinityf5hlw" in namespace "services-987" to be "running"
    Jan 30 11:42:21.453: INFO: Pod "execpod-affinityf5hlw": Phase="Pending", Reason="", readiness=false. Elapsed: 1.527466ms
    Jan 30 11:42:23.455: INFO: Pod "execpod-affinityf5hlw": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004034839s
    Jan 30 11:42:25.455: INFO: Pod "execpod-affinityf5hlw": Phase="Running", Reason="", readiness=true. Elapsed: 4.003942217s
    Jan 30 11:42:25.455: INFO: Pod "execpod-affinityf5hlw" satisfied condition "running"
    Jan 30 11:42:26.458: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=services-987 exec execpod-affinityf5hlw -- /bin/sh -x -c nc -v -z -w 2 affinity-nodeport 80'
    Jan 30 11:42:26.616: INFO: stderr: "+ nc -v -z -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
    Jan 30 11:42:26.616: INFO: stdout: ""
    Jan 30 11:42:26.616: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=services-987 exec execpod-affinityf5hlw -- /bin/sh -x -c nc -v -z -w 2 10.178.78.191 80'
    Jan 30 11:42:26.747: INFO: stderr: "+ nc -v -z -w 2 10.178.78.191 80\nConnection to 10.178.78.191 80 port [tcp/http] succeeded!\n"
    Jan 30 11:42:26.747: INFO: stdout: ""
    Jan 30 11:42:26.747: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=services-987 exec execpod-affinityf5hlw -- /bin/sh -x -c nc -v -z -w 2 10.182.0.82 30600'
    Jan 30 11:42:26.871: INFO: stderr: "+ nc -v -z -w 2 10.182.0.82 30600\nConnection to 10.182.0.82 30600 port [tcp/*] succeeded!\n"
    Jan 30 11:42:26.871: INFO: stdout: ""
    Jan 30 11:42:26.871: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=services-987 exec execpod-affinityf5hlw -- /bin/sh -x -c nc -v -z -w 2 10.182.0.84 30600'
    Jan 30 11:42:26.999: INFO: stderr: "+ nc -v -z -w 2 10.182.0.84 30600\nConnection to 10.182.0.84 30600 port [tcp/*] succeeded!\n"
    Jan 30 11:42:26.999: INFO: stdout: ""
    Jan 30 11:42:26.999: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=services-987 exec execpod-affinityf5hlw -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.182.0.82:30600/ ; done'
    Jan 30 11:42:27.174: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.182.0.82:30600/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.182.0.82:30600/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.182.0.82:30600/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.182.0.82:30600/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.182.0.82:30600/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.182.0.82:30600/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.182.0.82:30600/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.182.0.82:30600/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.182.0.82:30600/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.182.0.82:30600/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.182.0.82:30600/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.182.0.82:30600/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.182.0.82:30600/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.182.0.82:30600/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.182.0.82:30600/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.182.0.82:30600/\n"
    Jan 30 11:42:27.175: INFO: stdout: "\naffinity-nodeport-qwl98\naffinity-nodeport-qwl98\naffinity-nodeport-qwl98\naffinity-nodeport-qwl98\naffinity-nodeport-qwl98\naffinity-nodeport-qwl98\naffinity-nodeport-qwl98\naffinity-nodeport-qwl98\naffinity-nodeport-qwl98\naffinity-nodeport-qwl98\naffinity-nodeport-qwl98\naffinity-nodeport-qwl98\naffinity-nodeport-qwl98\naffinity-nodeport-qwl98\naffinity-nodeport-qwl98\naffinity-nodeport-qwl98"
    Jan 30 11:42:27.175: INFO: Received response from host: affinity-nodeport-qwl98
    Jan 30 11:42:27.175: INFO: Received response from host: affinity-nodeport-qwl98
    Jan 30 11:42:27.175: INFO: Received response from host: affinity-nodeport-qwl98
    Jan 30 11:42:27.175: INFO: Received response from host: affinity-nodeport-qwl98
    Jan 30 11:42:27.175: INFO: Received response from host: affinity-nodeport-qwl98
    Jan 30 11:42:27.175: INFO: Received response from host: affinity-nodeport-qwl98
    Jan 30 11:42:27.175: INFO: Received response from host: affinity-nodeport-qwl98
    Jan 30 11:42:27.175: INFO: Received response from host: affinity-nodeport-qwl98
    Jan 30 11:42:27.175: INFO: Received response from host: affinity-nodeport-qwl98
    Jan 30 11:42:27.175: INFO: Received response from host: affinity-nodeport-qwl98
    Jan 30 11:42:27.175: INFO: Received response from host: affinity-nodeport-qwl98
    Jan 30 11:42:27.175: INFO: Received response from host: affinity-nodeport-qwl98
    Jan 30 11:42:27.175: INFO: Received response from host: affinity-nodeport-qwl98
    Jan 30 11:42:27.175: INFO: Received response from host: affinity-nodeport-qwl98
    Jan 30 11:42:27.175: INFO: Received response from host: affinity-nodeport-qwl98
    Jan 30 11:42:27.175: INFO: Received response from host: affinity-nodeport-qwl98
    Jan 30 11:42:27.175: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-nodeport in namespace services-987, will wait for the garbage collector to delete the pods 01/30/23 11:42:27.179
    Jan 30 11:42:27.236: INFO: Deleting ReplicationController affinity-nodeport took: 2.769616ms
    Jan 30 11:42:27.337: INFO: Terminating ReplicationController affinity-nodeport pods took: 100.74025ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jan 30 11:42:29.745: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-987" for this suite. 01/30/23 11:42:29.747
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods Extended Pods Set QOS Class
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  test/e2e/node/pods.go:161
[BeforeEach] [sig-node] Pods Extended
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 11:42:29.75
Jan 30 11:42:29.750: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename pods 01/30/23 11:42:29.751
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:42:29.756
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:42:29.758
[BeforeEach] [sig-node] Pods Extended
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Pods Set QOS Class
  test/e2e/node/pods.go:152
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  test/e2e/node/pods.go:161
STEP: creating the pod 01/30/23 11:42:29.76
STEP: submitting the pod to kubernetes 01/30/23 11:42:29.761
STEP: verifying QOS class is set on the pod 01/30/23 11:42:29.765
[AfterEach] [sig-node] Pods Extended
  test/e2e/framework/node/init/init.go:32
Jan 30 11:42:29.766: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods Extended
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods Extended
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods Extended
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-891" for this suite. 01/30/23 11:42:29.769
------------------------------
• [0.021 seconds]
[sig-node] Pods Extended
test/e2e/node/framework.go:23
  Pods Set QOS Class
  test/e2e/node/pods.go:150
    should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
    test/e2e/node/pods.go:161

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods Extended
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 11:42:29.75
    Jan 30 11:42:29.750: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename pods 01/30/23 11:42:29.751
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:42:29.756
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:42:29.758
    [BeforeEach] [sig-node] Pods Extended
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Pods Set QOS Class
      test/e2e/node/pods.go:152
    [It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
      test/e2e/node/pods.go:161
    STEP: creating the pod 01/30/23 11:42:29.76
    STEP: submitting the pod to kubernetes 01/30/23 11:42:29.761
    STEP: verifying QOS class is set on the pod 01/30/23 11:42:29.765
    [AfterEach] [sig-node] Pods Extended
      test/e2e/framework/node/init/init.go:32
    Jan 30 11:42:29.766: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods Extended
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods Extended
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods Extended
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-891" for this suite. 01/30/23 11:42:29.769
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:217
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 11:42:29.772
Jan 30 11:42:29.772: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename downward-api 01/30/23 11:42:29.772
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:42:29.778
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:42:29.78
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:217
STEP: Creating a pod to test downward api env vars 01/30/23 11:42:29.782
Jan 30 11:42:29.786: INFO: Waiting up to 5m0s for pod "downward-api-a5cfa4ac-cfe2-423d-b327-7f2be8f542a6" in namespace "downward-api-5942" to be "Succeeded or Failed"
Jan 30 11:42:29.787: INFO: Pod "downward-api-a5cfa4ac-cfe2-423d-b327-7f2be8f542a6": Phase="Pending", Reason="", readiness=false. Elapsed: 1.555255ms
Jan 30 11:42:31.790: INFO: Pod "downward-api-a5cfa4ac-cfe2-423d-b327-7f2be8f542a6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004444417s
Jan 30 11:42:33.789: INFO: Pod "downward-api-a5cfa4ac-cfe2-423d-b327-7f2be8f542a6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003812799s
STEP: Saw pod success 01/30/23 11:42:33.789
Jan 30 11:42:33.790: INFO: Pod "downward-api-a5cfa4ac-cfe2-423d-b327-7f2be8f542a6" satisfied condition "Succeeded or Failed"
Jan 30 11:42:33.791: INFO: Trying to get logs from node pubt2-nks-for-dev1.dg.163.org pod downward-api-a5cfa4ac-cfe2-423d-b327-7f2be8f542a6 container dapi-container: <nil>
STEP: delete the pod 01/30/23 11:42:33.796
Jan 30 11:42:33.800: INFO: Waiting for pod downward-api-a5cfa4ac-cfe2-423d-b327-7f2be8f542a6 to disappear
Jan 30 11:42:33.801: INFO: Pod downward-api-a5cfa4ac-cfe2-423d-b327-7f2be8f542a6 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Jan 30 11:42:33.802: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-5942" for this suite. 01/30/23 11:42:33.804
------------------------------
• [4.034 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:217

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 11:42:29.772
    Jan 30 11:42:29.772: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename downward-api 01/30/23 11:42:29.772
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:42:29.778
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:42:29.78
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:217
    STEP: Creating a pod to test downward api env vars 01/30/23 11:42:29.782
    Jan 30 11:42:29.786: INFO: Waiting up to 5m0s for pod "downward-api-a5cfa4ac-cfe2-423d-b327-7f2be8f542a6" in namespace "downward-api-5942" to be "Succeeded or Failed"
    Jan 30 11:42:29.787: INFO: Pod "downward-api-a5cfa4ac-cfe2-423d-b327-7f2be8f542a6": Phase="Pending", Reason="", readiness=false. Elapsed: 1.555255ms
    Jan 30 11:42:31.790: INFO: Pod "downward-api-a5cfa4ac-cfe2-423d-b327-7f2be8f542a6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004444417s
    Jan 30 11:42:33.789: INFO: Pod "downward-api-a5cfa4ac-cfe2-423d-b327-7f2be8f542a6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003812799s
    STEP: Saw pod success 01/30/23 11:42:33.789
    Jan 30 11:42:33.790: INFO: Pod "downward-api-a5cfa4ac-cfe2-423d-b327-7f2be8f542a6" satisfied condition "Succeeded or Failed"
    Jan 30 11:42:33.791: INFO: Trying to get logs from node pubt2-nks-for-dev1.dg.163.org pod downward-api-a5cfa4ac-cfe2-423d-b327-7f2be8f542a6 container dapi-container: <nil>
    STEP: delete the pod 01/30/23 11:42:33.796
    Jan 30 11:42:33.800: INFO: Waiting for pod downward-api-a5cfa4ac-cfe2-423d-b327-7f2be8f542a6 to disappear
    Jan 30 11:42:33.801: INFO: Pod downward-api-a5cfa4ac-cfe2-423d-b327-7f2be8f542a6 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Jan 30 11:42:33.802: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-5942" for this suite. 01/30/23 11:42:33.804
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] Secrets
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:99
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 11:42:33.806
Jan 30 11:42:33.806: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename secrets 01/30/23 11:42:33.807
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:42:33.813
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:42:33.815
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:99
STEP: Creating secret with name secret-test-78826cf4-ec8c-445d-8f14-225067fc8c46 01/30/23 11:42:33.828
STEP: Creating a pod to test consume secrets 01/30/23 11:42:33.831
Jan 30 11:42:33.834: INFO: Waiting up to 5m0s for pod "pod-secrets-1f8baae8-a416-42a7-9b76-9b13a06d9316" in namespace "secrets-415" to be "Succeeded or Failed"
Jan 30 11:42:33.836: INFO: Pod "pod-secrets-1f8baae8-a416-42a7-9b76-9b13a06d9316": Phase="Pending", Reason="", readiness=false. Elapsed: 1.595465ms
Jan 30 11:42:35.839: INFO: Pod "pod-secrets-1f8baae8-a416-42a7-9b76-9b13a06d9316": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004440416s
Jan 30 11:42:37.838: INFO: Pod "pod-secrets-1f8baae8-a416-42a7-9b76-9b13a06d9316": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003858003s
STEP: Saw pod success 01/30/23 11:42:37.838
Jan 30 11:42:37.838: INFO: Pod "pod-secrets-1f8baae8-a416-42a7-9b76-9b13a06d9316" satisfied condition "Succeeded or Failed"
Jan 30 11:42:37.840: INFO: Trying to get logs from node pubt2-nks-for-dev1.dg.163.org pod pod-secrets-1f8baae8-a416-42a7-9b76-9b13a06d9316 container secret-volume-test: <nil>
STEP: delete the pod 01/30/23 11:42:37.845
Jan 30 11:42:37.849: INFO: Waiting for pod pod-secrets-1f8baae8-a416-42a7-9b76-9b13a06d9316 to disappear
Jan 30 11:42:37.851: INFO: Pod pod-secrets-1f8baae8-a416-42a7-9b76-9b13a06d9316 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Jan 30 11:42:37.851: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-415" for this suite. 01/30/23 11:42:37.853
STEP: Destroying namespace "secret-namespace-2602" for this suite. 01/30/23 11:42:37.855
------------------------------
• [4.051 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:99

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 11:42:33.806
    Jan 30 11:42:33.806: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename secrets 01/30/23 11:42:33.807
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:42:33.813
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:42:33.815
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:99
    STEP: Creating secret with name secret-test-78826cf4-ec8c-445d-8f14-225067fc8c46 01/30/23 11:42:33.828
    STEP: Creating a pod to test consume secrets 01/30/23 11:42:33.831
    Jan 30 11:42:33.834: INFO: Waiting up to 5m0s for pod "pod-secrets-1f8baae8-a416-42a7-9b76-9b13a06d9316" in namespace "secrets-415" to be "Succeeded or Failed"
    Jan 30 11:42:33.836: INFO: Pod "pod-secrets-1f8baae8-a416-42a7-9b76-9b13a06d9316": Phase="Pending", Reason="", readiness=false. Elapsed: 1.595465ms
    Jan 30 11:42:35.839: INFO: Pod "pod-secrets-1f8baae8-a416-42a7-9b76-9b13a06d9316": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004440416s
    Jan 30 11:42:37.838: INFO: Pod "pod-secrets-1f8baae8-a416-42a7-9b76-9b13a06d9316": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003858003s
    STEP: Saw pod success 01/30/23 11:42:37.838
    Jan 30 11:42:37.838: INFO: Pod "pod-secrets-1f8baae8-a416-42a7-9b76-9b13a06d9316" satisfied condition "Succeeded or Failed"
    Jan 30 11:42:37.840: INFO: Trying to get logs from node pubt2-nks-for-dev1.dg.163.org pod pod-secrets-1f8baae8-a416-42a7-9b76-9b13a06d9316 container secret-volume-test: <nil>
    STEP: delete the pod 01/30/23 11:42:37.845
    Jan 30 11:42:37.849: INFO: Waiting for pod pod-secrets-1f8baae8-a416-42a7-9b76-9b13a06d9316 to disappear
    Jan 30 11:42:37.851: INFO: Pod pod-secrets-1f8baae8-a416-42a7-9b76-9b13a06d9316 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Jan 30 11:42:37.851: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-415" for this suite. 01/30/23 11:42:37.853
    STEP: Destroying namespace "secret-namespace-2602" for this suite. 01/30/23 11:42:37.855
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:197
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 11:42:37.858
Jan 30 11:42:37.858: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename emptydir 01/30/23 11:42:37.859
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:42:37.865
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:42:37.866
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:197
STEP: Creating a pod to test emptydir 0644 on node default medium 01/30/23 11:42:37.868
Jan 30 11:42:37.872: INFO: Waiting up to 5m0s for pod "pod-899c6d5e-7d50-4ec6-bcf1-d004e2faa885" in namespace "emptydir-3466" to be "Succeeded or Failed"
Jan 30 11:42:37.874: INFO: Pod "pod-899c6d5e-7d50-4ec6-bcf1-d004e2faa885": Phase="Pending", Reason="", readiness=false. Elapsed: 1.530521ms
Jan 30 11:42:39.877: INFO: Pod "pod-899c6d5e-7d50-4ec6-bcf1-d004e2faa885": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004209732s
Jan 30 11:42:41.878: INFO: Pod "pod-899c6d5e-7d50-4ec6-bcf1-d004e2faa885": Phase="Pending", Reason="", readiness=false. Elapsed: 4.005103695s
Jan 30 11:42:43.877: INFO: Pod "pod-899c6d5e-7d50-4ec6-bcf1-d004e2faa885": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.004578246s
STEP: Saw pod success 01/30/23 11:42:43.877
Jan 30 11:42:43.877: INFO: Pod "pod-899c6d5e-7d50-4ec6-bcf1-d004e2faa885" satisfied condition "Succeeded or Failed"
Jan 30 11:42:43.879: INFO: Trying to get logs from node pubt2-nks-for-dev1.dg.163.org pod pod-899c6d5e-7d50-4ec6-bcf1-d004e2faa885 container test-container: <nil>
STEP: delete the pod 01/30/23 11:42:43.884
Jan 30 11:42:43.888: INFO: Waiting for pod pod-899c6d5e-7d50-4ec6-bcf1-d004e2faa885 to disappear
Jan 30 11:42:43.890: INFO: Pod pod-899c6d5e-7d50-4ec6-bcf1-d004e2faa885 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jan 30 11:42:43.890: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-3466" for this suite. 01/30/23 11:42:43.892
------------------------------
• [SLOW TEST] [6.036 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:197

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 11:42:37.858
    Jan 30 11:42:37.858: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename emptydir 01/30/23 11:42:37.859
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:42:37.865
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:42:37.866
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:197
    STEP: Creating a pod to test emptydir 0644 on node default medium 01/30/23 11:42:37.868
    Jan 30 11:42:37.872: INFO: Waiting up to 5m0s for pod "pod-899c6d5e-7d50-4ec6-bcf1-d004e2faa885" in namespace "emptydir-3466" to be "Succeeded or Failed"
    Jan 30 11:42:37.874: INFO: Pod "pod-899c6d5e-7d50-4ec6-bcf1-d004e2faa885": Phase="Pending", Reason="", readiness=false. Elapsed: 1.530521ms
    Jan 30 11:42:39.877: INFO: Pod "pod-899c6d5e-7d50-4ec6-bcf1-d004e2faa885": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004209732s
    Jan 30 11:42:41.878: INFO: Pod "pod-899c6d5e-7d50-4ec6-bcf1-d004e2faa885": Phase="Pending", Reason="", readiness=false. Elapsed: 4.005103695s
    Jan 30 11:42:43.877: INFO: Pod "pod-899c6d5e-7d50-4ec6-bcf1-d004e2faa885": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.004578246s
    STEP: Saw pod success 01/30/23 11:42:43.877
    Jan 30 11:42:43.877: INFO: Pod "pod-899c6d5e-7d50-4ec6-bcf1-d004e2faa885" satisfied condition "Succeeded or Failed"
    Jan 30 11:42:43.879: INFO: Trying to get logs from node pubt2-nks-for-dev1.dg.163.org pod pod-899c6d5e-7d50-4ec6-bcf1-d004e2faa885 container test-container: <nil>
    STEP: delete the pod 01/30/23 11:42:43.884
    Jan 30 11:42:43.888: INFO: Waiting for pod pod-899c6d5e-7d50-4ec6-bcf1-d004e2faa885 to disappear
    Jan 30 11:42:43.890: INFO: Pod pod-899c6d5e-7d50-4ec6-bcf1-d004e2faa885 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jan 30 11:42:43.890: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-3466" for this suite. 01/30/23 11:42:43.892
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:44
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 11:42:43.895
Jan 30 11:42:43.895: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename var-expansion 01/30/23 11:42:43.896
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:42:43.902
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:42:43.904
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:44
STEP: Creating a pod to test env composition 01/30/23 11:42:43.906
Jan 30 11:42:43.909: INFO: Waiting up to 5m0s for pod "var-expansion-94379d9c-e996-4d0c-9222-826170289468" in namespace "var-expansion-4964" to be "Succeeded or Failed"
Jan 30 11:42:43.911: INFO: Pod "var-expansion-94379d9c-e996-4d0c-9222-826170289468": Phase="Pending", Reason="", readiness=false. Elapsed: 1.696093ms
Jan 30 11:42:45.913: INFO: Pod "var-expansion-94379d9c-e996-4d0c-9222-826170289468": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003846893s
Jan 30 11:42:47.914: INFO: Pod "var-expansion-94379d9c-e996-4d0c-9222-826170289468": Phase="Pending", Reason="", readiness=false. Elapsed: 4.004300471s
Jan 30 11:42:49.915: INFO: Pod "var-expansion-94379d9c-e996-4d0c-9222-826170289468": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.005621506s
STEP: Saw pod success 01/30/23 11:42:49.915
Jan 30 11:42:49.915: INFO: Pod "var-expansion-94379d9c-e996-4d0c-9222-826170289468" satisfied condition "Succeeded or Failed"
Jan 30 11:42:49.917: INFO: Trying to get logs from node pubt2-nks-for-dev1.dg.163.org pod var-expansion-94379d9c-e996-4d0c-9222-826170289468 container dapi-container: <nil>
STEP: delete the pod 01/30/23 11:42:49.922
Jan 30 11:42:49.926: INFO: Waiting for pod var-expansion-94379d9c-e996-4d0c-9222-826170289468 to disappear
Jan 30 11:42:49.928: INFO: Pod var-expansion-94379d9c-e996-4d0c-9222-826170289468 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Jan 30 11:42:49.928: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-4964" for this suite. 01/30/23 11:42:49.931
------------------------------
• [SLOW TEST] [6.038 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:44

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 11:42:43.895
    Jan 30 11:42:43.895: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename var-expansion 01/30/23 11:42:43.896
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:42:43.902
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:42:43.904
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:44
    STEP: Creating a pod to test env composition 01/30/23 11:42:43.906
    Jan 30 11:42:43.909: INFO: Waiting up to 5m0s for pod "var-expansion-94379d9c-e996-4d0c-9222-826170289468" in namespace "var-expansion-4964" to be "Succeeded or Failed"
    Jan 30 11:42:43.911: INFO: Pod "var-expansion-94379d9c-e996-4d0c-9222-826170289468": Phase="Pending", Reason="", readiness=false. Elapsed: 1.696093ms
    Jan 30 11:42:45.913: INFO: Pod "var-expansion-94379d9c-e996-4d0c-9222-826170289468": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003846893s
    Jan 30 11:42:47.914: INFO: Pod "var-expansion-94379d9c-e996-4d0c-9222-826170289468": Phase="Pending", Reason="", readiness=false. Elapsed: 4.004300471s
    Jan 30 11:42:49.915: INFO: Pod "var-expansion-94379d9c-e996-4d0c-9222-826170289468": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.005621506s
    STEP: Saw pod success 01/30/23 11:42:49.915
    Jan 30 11:42:49.915: INFO: Pod "var-expansion-94379d9c-e996-4d0c-9222-826170289468" satisfied condition "Succeeded or Failed"
    Jan 30 11:42:49.917: INFO: Trying to get logs from node pubt2-nks-for-dev1.dg.163.org pod var-expansion-94379d9c-e996-4d0c-9222-826170289468 container dapi-container: <nil>
    STEP: delete the pod 01/30/23 11:42:49.922
    Jan 30 11:42:49.926: INFO: Waiting for pod var-expansion-94379d9c-e996-4d0c-9222-826170289468 to disappear
    Jan 30 11:42:49.928: INFO: Pod var-expansion-94379d9c-e996-4d0c-9222-826170289468 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Jan 30 11:42:49.928: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-4964" for this suite. 01/30/23 11:42:49.931
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with secret pod [Conformance]
  test/e2e/storage/subpath.go:60
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 11:42:49.934
Jan 30 11:42:49.934: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename subpath 01/30/23 11:42:49.934
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:42:49.94
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:42:49.942
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 01/30/23 11:42:49.944
[It] should support subpaths with secret pod [Conformance]
  test/e2e/storage/subpath.go:60
STEP: Creating pod pod-subpath-test-secret-8hs5 01/30/23 11:42:49.948
STEP: Creating a pod to test atomic-volume-subpath 01/30/23 11:42:49.948
Jan 30 11:42:49.952: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-8hs5" in namespace "subpath-2867" to be "Succeeded or Failed"
Jan 30 11:42:49.954: INFO: Pod "pod-subpath-test-secret-8hs5": Phase="Pending", Reason="", readiness=false. Elapsed: 1.585245ms
Jan 30 11:42:51.957: INFO: Pod "pod-subpath-test-secret-8hs5": Phase="Running", Reason="", readiness=true. Elapsed: 2.004715828s
Jan 30 11:42:53.956: INFO: Pod "pod-subpath-test-secret-8hs5": Phase="Running", Reason="", readiness=true. Elapsed: 4.004170612s
Jan 30 11:42:55.957: INFO: Pod "pod-subpath-test-secret-8hs5": Phase="Running", Reason="", readiness=true. Elapsed: 6.005004601s
Jan 30 11:42:57.956: INFO: Pod "pod-subpath-test-secret-8hs5": Phase="Running", Reason="", readiness=true. Elapsed: 8.004225923s
Jan 30 11:42:59.956: INFO: Pod "pod-subpath-test-secret-8hs5": Phase="Running", Reason="", readiness=true. Elapsed: 10.0045504s
Jan 30 11:43:01.958: INFO: Pod "pod-subpath-test-secret-8hs5": Phase="Running", Reason="", readiness=true. Elapsed: 12.005900145s
Jan 30 11:43:03.958: INFO: Pod "pod-subpath-test-secret-8hs5": Phase="Running", Reason="", readiness=true. Elapsed: 14.005601652s
Jan 30 11:43:05.957: INFO: Pod "pod-subpath-test-secret-8hs5": Phase="Running", Reason="", readiness=true. Elapsed: 16.005335137s
Jan 30 11:43:07.956: INFO: Pod "pod-subpath-test-secret-8hs5": Phase="Running", Reason="", readiness=true. Elapsed: 18.00390514s
Jan 30 11:43:09.957: INFO: Pod "pod-subpath-test-secret-8hs5": Phase="Running", Reason="", readiness=true. Elapsed: 20.004640614s
Jan 30 11:43:11.958: INFO: Pod "pod-subpath-test-secret-8hs5": Phase="Running", Reason="", readiness=false. Elapsed: 22.005881911s
Jan 30 11:43:13.957: INFO: Pod "pod-subpath-test-secret-8hs5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.005079992s
STEP: Saw pod success 01/30/23 11:43:13.957
Jan 30 11:43:13.957: INFO: Pod "pod-subpath-test-secret-8hs5" satisfied condition "Succeeded or Failed"
Jan 30 11:43:13.959: INFO: Trying to get logs from node pubt2-nks-for-dev1.dg.163.org pod pod-subpath-test-secret-8hs5 container test-container-subpath-secret-8hs5: <nil>
STEP: delete the pod 01/30/23 11:43:13.966
Jan 30 11:43:13.971: INFO: Waiting for pod pod-subpath-test-secret-8hs5 to disappear
Jan 30 11:43:13.973: INFO: Pod pod-subpath-test-secret-8hs5 no longer exists
STEP: Deleting pod pod-subpath-test-secret-8hs5 01/30/23 11:43:13.973
Jan 30 11:43:13.973: INFO: Deleting pod "pod-subpath-test-secret-8hs5" in namespace "subpath-2867"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Jan 30 11:43:13.974: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-2867" for this suite. 01/30/23 11:43:13.977
------------------------------
• [SLOW TEST] [24.045 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with secret pod [Conformance]
    test/e2e/storage/subpath.go:60

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 11:42:49.934
    Jan 30 11:42:49.934: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename subpath 01/30/23 11:42:49.934
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:42:49.94
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:42:49.942
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 01/30/23 11:42:49.944
    [It] should support subpaths with secret pod [Conformance]
      test/e2e/storage/subpath.go:60
    STEP: Creating pod pod-subpath-test-secret-8hs5 01/30/23 11:42:49.948
    STEP: Creating a pod to test atomic-volume-subpath 01/30/23 11:42:49.948
    Jan 30 11:42:49.952: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-8hs5" in namespace "subpath-2867" to be "Succeeded or Failed"
    Jan 30 11:42:49.954: INFO: Pod "pod-subpath-test-secret-8hs5": Phase="Pending", Reason="", readiness=false. Elapsed: 1.585245ms
    Jan 30 11:42:51.957: INFO: Pod "pod-subpath-test-secret-8hs5": Phase="Running", Reason="", readiness=true. Elapsed: 2.004715828s
    Jan 30 11:42:53.956: INFO: Pod "pod-subpath-test-secret-8hs5": Phase="Running", Reason="", readiness=true. Elapsed: 4.004170612s
    Jan 30 11:42:55.957: INFO: Pod "pod-subpath-test-secret-8hs5": Phase="Running", Reason="", readiness=true. Elapsed: 6.005004601s
    Jan 30 11:42:57.956: INFO: Pod "pod-subpath-test-secret-8hs5": Phase="Running", Reason="", readiness=true. Elapsed: 8.004225923s
    Jan 30 11:42:59.956: INFO: Pod "pod-subpath-test-secret-8hs5": Phase="Running", Reason="", readiness=true. Elapsed: 10.0045504s
    Jan 30 11:43:01.958: INFO: Pod "pod-subpath-test-secret-8hs5": Phase="Running", Reason="", readiness=true. Elapsed: 12.005900145s
    Jan 30 11:43:03.958: INFO: Pod "pod-subpath-test-secret-8hs5": Phase="Running", Reason="", readiness=true. Elapsed: 14.005601652s
    Jan 30 11:43:05.957: INFO: Pod "pod-subpath-test-secret-8hs5": Phase="Running", Reason="", readiness=true. Elapsed: 16.005335137s
    Jan 30 11:43:07.956: INFO: Pod "pod-subpath-test-secret-8hs5": Phase="Running", Reason="", readiness=true. Elapsed: 18.00390514s
    Jan 30 11:43:09.957: INFO: Pod "pod-subpath-test-secret-8hs5": Phase="Running", Reason="", readiness=true. Elapsed: 20.004640614s
    Jan 30 11:43:11.958: INFO: Pod "pod-subpath-test-secret-8hs5": Phase="Running", Reason="", readiness=false. Elapsed: 22.005881911s
    Jan 30 11:43:13.957: INFO: Pod "pod-subpath-test-secret-8hs5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.005079992s
    STEP: Saw pod success 01/30/23 11:43:13.957
    Jan 30 11:43:13.957: INFO: Pod "pod-subpath-test-secret-8hs5" satisfied condition "Succeeded or Failed"
    Jan 30 11:43:13.959: INFO: Trying to get logs from node pubt2-nks-for-dev1.dg.163.org pod pod-subpath-test-secret-8hs5 container test-container-subpath-secret-8hs5: <nil>
    STEP: delete the pod 01/30/23 11:43:13.966
    Jan 30 11:43:13.971: INFO: Waiting for pod pod-subpath-test-secret-8hs5 to disappear
    Jan 30 11:43:13.973: INFO: Pod pod-subpath-test-secret-8hs5 no longer exists
    STEP: Deleting pod pod-subpath-test-secret-8hs5 01/30/23 11:43:13.973
    Jan 30 11:43:13.973: INFO: Deleting pod "pod-subpath-test-secret-8hs5" in namespace "subpath-2867"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Jan 30 11:43:13.974: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-2867" for this suite. 01/30/23 11:43:13.977
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] ConfigMap
  should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:138
[BeforeEach] [sig-node] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 11:43:13.981
Jan 30 11:43:13.981: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename configmap 01/30/23 11:43:13.981
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:43:13.989
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:43:13.991
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:138
STEP: Creating configMap that has name configmap-test-emptyKey-f47206fe-06bf-4008-90a7-a2fbfee37ffa 01/30/23 11:43:13.993
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jan 30 11:43:13.994: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-4558" for this suite. 01/30/23 11:43:13.997
------------------------------
• [0.018 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:138

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 11:43:13.981
    Jan 30 11:43:13.981: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename configmap 01/30/23 11:43:13.981
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:43:13.989
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:43:13.991
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should fail to create ConfigMap with empty key [Conformance]
      test/e2e/common/node/configmap.go:138
    STEP: Creating configMap that has name configmap-test-emptyKey-f47206fe-06bf-4008-90a7-a2fbfee37ffa 01/30/23 11:43:13.993
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jan 30 11:43:13.994: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-4558" for this suite. 01/30/23 11:43:13.997
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:75
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 11:43:13.999
Jan 30 11:43:14.000: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename resourcequota 01/30/23 11:43:14
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:43:14.006
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:43:14.008
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:75
STEP: Counting existing ResourceQuota 01/30/23 11:43:14.01
STEP: Creating a ResourceQuota 01/30/23 11:43:19.012
STEP: Ensuring resource quota status is calculated 01/30/23 11:43:19.015
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Jan 30 11:43:21.017: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-4123" for this suite. 01/30/23 11:43:21.02
------------------------------
• [SLOW TEST] [7.023 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:75

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 11:43:13.999
    Jan 30 11:43:14.000: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename resourcequota 01/30/23 11:43:14
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:43:14.006
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:43:14.008
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
      test/e2e/apimachinery/resource_quota.go:75
    STEP: Counting existing ResourceQuota 01/30/23 11:43:14.01
    STEP: Creating a ResourceQuota 01/30/23 11:43:19.012
    STEP: Ensuring resource quota status is calculated 01/30/23 11:43:19.015
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Jan 30 11:43:21.017: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-4123" for this suite. 01/30/23 11:43:21.02
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command in a pod
  should print the output to logs [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:52
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 11:43:21.023
Jan 30 11:43:21.023: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename kubelet-test 01/30/23 11:43:21.024
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:43:21.03
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:43:21.032
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should print the output to logs [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:52
Jan 30 11:43:21.039: INFO: Waiting up to 5m0s for pod "busybox-scheduling-f735792b-deb9-4bec-87e6-a95f9a976231" in namespace "kubelet-test-9141" to be "running and ready"
Jan 30 11:43:21.042: INFO: Pod "busybox-scheduling-f735792b-deb9-4bec-87e6-a95f9a976231": Phase="Pending", Reason="", readiness=false. Elapsed: 3.226322ms
Jan 30 11:43:21.042: INFO: The phase of Pod busybox-scheduling-f735792b-deb9-4bec-87e6-a95f9a976231 is Pending, waiting for it to be Running (with Ready = true)
Jan 30 11:43:23.045: INFO: Pod "busybox-scheduling-f735792b-deb9-4bec-87e6-a95f9a976231": Phase="Running", Reason="", readiness=true. Elapsed: 2.006775198s
Jan 30 11:43:23.045: INFO: The phase of Pod busybox-scheduling-f735792b-deb9-4bec-87e6-a95f9a976231 is Running (Ready = true)
Jan 30 11:43:23.045: INFO: Pod "busybox-scheduling-f735792b-deb9-4bec-87e6-a95f9a976231" satisfied condition "running and ready"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Jan 30 11:43:23.052: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-9141" for this suite. 01/30/23 11:43:23.054
------------------------------
• [2.033 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command in a pod
  test/e2e/common/node/kubelet.go:44
    should print the output to logs [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:52

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 11:43:21.023
    Jan 30 11:43:21.023: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename kubelet-test 01/30/23 11:43:21.024
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:43:21.03
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:43:21.032
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should print the output to logs [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:52
    Jan 30 11:43:21.039: INFO: Waiting up to 5m0s for pod "busybox-scheduling-f735792b-deb9-4bec-87e6-a95f9a976231" in namespace "kubelet-test-9141" to be "running and ready"
    Jan 30 11:43:21.042: INFO: Pod "busybox-scheduling-f735792b-deb9-4bec-87e6-a95f9a976231": Phase="Pending", Reason="", readiness=false. Elapsed: 3.226322ms
    Jan 30 11:43:21.042: INFO: The phase of Pod busybox-scheduling-f735792b-deb9-4bec-87e6-a95f9a976231 is Pending, waiting for it to be Running (with Ready = true)
    Jan 30 11:43:23.045: INFO: Pod "busybox-scheduling-f735792b-deb9-4bec-87e6-a95f9a976231": Phase="Running", Reason="", readiness=true. Elapsed: 2.006775198s
    Jan 30 11:43:23.045: INFO: The phase of Pod busybox-scheduling-f735792b-deb9-4bec-87e6-a95f9a976231 is Running (Ready = true)
    Jan 30 11:43:23.045: INFO: Pod "busybox-scheduling-f735792b-deb9-4bec-87e6-a95f9a976231" satisfied condition "running and ready"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Jan 30 11:43:23.052: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-9141" for this suite. 01/30/23 11:43:23.054
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:194
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 11:43:23.057
Jan 30 11:43:23.058: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename crd-publish-openapi 01/30/23 11:43:23.058
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:43:23.064
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:43:23.066
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:194
Jan 30 11:43:23.068: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 01/30/23 11:43:24.894
Jan 30 11:43:24.894: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=crd-publish-openapi-3254 --namespace=crd-publish-openapi-3254 create -f -'
Jan 30 11:43:26.037: INFO: stderr: ""
Jan 30 11:43:26.037: INFO: stdout: "e2e-test-crd-publish-openapi-2871-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Jan 30 11:43:26.037: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=crd-publish-openapi-3254 --namespace=crd-publish-openapi-3254 delete e2e-test-crd-publish-openapi-2871-crds test-cr'
Jan 30 11:43:26.098: INFO: stderr: ""
Jan 30 11:43:26.098: INFO: stdout: "e2e-test-crd-publish-openapi-2871-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Jan 30 11:43:26.098: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=crd-publish-openapi-3254 --namespace=crd-publish-openapi-3254 apply -f -'
Jan 30 11:43:26.268: INFO: stderr: ""
Jan 30 11:43:26.268: INFO: stdout: "e2e-test-crd-publish-openapi-2871-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Jan 30 11:43:26.268: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=crd-publish-openapi-3254 --namespace=crd-publish-openapi-3254 delete e2e-test-crd-publish-openapi-2871-crds test-cr'
Jan 30 11:43:26.329: INFO: stderr: ""
Jan 30 11:43:26.329: INFO: stdout: "e2e-test-crd-publish-openapi-2871-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR 01/30/23 11:43:26.329
Jan 30 11:43:26.329: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=crd-publish-openapi-3254 explain e2e-test-crd-publish-openapi-2871-crds'
Jan 30 11:43:26.494: INFO: stderr: ""
Jan 30 11:43:26.494: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-2871-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 30 11:43:28.303: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-3254" for this suite. 01/30/23 11:43:28.31
------------------------------
• [SLOW TEST] [5.256 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:194

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 11:43:23.057
    Jan 30 11:43:23.058: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename crd-publish-openapi 01/30/23 11:43:23.058
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:43:23.064
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:43:23.066
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for CRD preserving unknown fields at the schema root [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:194
    Jan 30 11:43:23.068: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 01/30/23 11:43:24.894
    Jan 30 11:43:24.894: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=crd-publish-openapi-3254 --namespace=crd-publish-openapi-3254 create -f -'
    Jan 30 11:43:26.037: INFO: stderr: ""
    Jan 30 11:43:26.037: INFO: stdout: "e2e-test-crd-publish-openapi-2871-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
    Jan 30 11:43:26.037: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=crd-publish-openapi-3254 --namespace=crd-publish-openapi-3254 delete e2e-test-crd-publish-openapi-2871-crds test-cr'
    Jan 30 11:43:26.098: INFO: stderr: ""
    Jan 30 11:43:26.098: INFO: stdout: "e2e-test-crd-publish-openapi-2871-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
    Jan 30 11:43:26.098: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=crd-publish-openapi-3254 --namespace=crd-publish-openapi-3254 apply -f -'
    Jan 30 11:43:26.268: INFO: stderr: ""
    Jan 30 11:43:26.268: INFO: stdout: "e2e-test-crd-publish-openapi-2871-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
    Jan 30 11:43:26.268: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=crd-publish-openapi-3254 --namespace=crd-publish-openapi-3254 delete e2e-test-crd-publish-openapi-2871-crds test-cr'
    Jan 30 11:43:26.329: INFO: stderr: ""
    Jan 30 11:43:26.329: INFO: stdout: "e2e-test-crd-publish-openapi-2871-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR 01/30/23 11:43:26.329
    Jan 30 11:43:26.329: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=crd-publish-openapi-3254 explain e2e-test-crd-publish-openapi-2871-crds'
    Jan 30 11:43:26.494: INFO: stderr: ""
    Jan 30 11:43:26.494: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-2871-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 30 11:43:28.303: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-3254" for this suite. 01/30/23 11:43:28.31
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 11:43:28.314
Jan 30 11:43:28.314: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename replicaset 01/30/23 11:43:28.314
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:43:28.322
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:43:28.324
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131
STEP: Given a Pod with a 'name' label pod-adoption-release is created 01/30/23 11:43:28.326
Jan 30 11:43:28.330: INFO: Waiting up to 5m0s for pod "pod-adoption-release" in namespace "replicaset-2032" to be "running and ready"
Jan 30 11:43:28.332: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 1.565122ms
Jan 30 11:43:28.332: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
Jan 30 11:43:30.334: INFO: Pod "pod-adoption-release": Phase="Running", Reason="", readiness=true. Elapsed: 2.00358654s
Jan 30 11:43:30.334: INFO: The phase of Pod pod-adoption-release is Running (Ready = true)
Jan 30 11:43:30.334: INFO: Pod "pod-adoption-release" satisfied condition "running and ready"
STEP: When a replicaset with a matching selector is created 01/30/23 11:43:30.335
STEP: Then the orphan pod is adopted 01/30/23 11:43:30.338
STEP: When the matched label of one of its pods change 01/30/23 11:43:31.343
Jan 30 11:43:31.345: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released 01/30/23 11:43:31.353
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Jan 30 11:43:32.357: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-2032" for this suite. 01/30/23 11:43:32.359
------------------------------
• [4.048 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 11:43:28.314
    Jan 30 11:43:28.314: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename replicaset 01/30/23 11:43:28.314
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:43:28.322
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:43:28.324
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] should adopt matching pods on creation and release no longer matching pods [Conformance]
      test/e2e/apps/replica_set.go:131
    STEP: Given a Pod with a 'name' label pod-adoption-release is created 01/30/23 11:43:28.326
    Jan 30 11:43:28.330: INFO: Waiting up to 5m0s for pod "pod-adoption-release" in namespace "replicaset-2032" to be "running and ready"
    Jan 30 11:43:28.332: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 1.565122ms
    Jan 30 11:43:28.332: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
    Jan 30 11:43:30.334: INFO: Pod "pod-adoption-release": Phase="Running", Reason="", readiness=true. Elapsed: 2.00358654s
    Jan 30 11:43:30.334: INFO: The phase of Pod pod-adoption-release is Running (Ready = true)
    Jan 30 11:43:30.334: INFO: Pod "pod-adoption-release" satisfied condition "running and ready"
    STEP: When a replicaset with a matching selector is created 01/30/23 11:43:30.335
    STEP: Then the orphan pod is adopted 01/30/23 11:43:30.338
    STEP: When the matched label of one of its pods change 01/30/23 11:43:31.343
    Jan 30 11:43:31.345: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
    STEP: Then the pod is released 01/30/23 11:43:31.353
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Jan 30 11:43:32.357: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-2032" for this suite. 01/30/23 11:43:32.359
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] CSIInlineVolumes
  should support CSIVolumeSource in Pod API [Conformance]
  test/e2e/storage/csi_inline.go:131
[BeforeEach] [sig-storage] CSIInlineVolumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 11:43:32.362
Jan 30 11:43:32.362: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename csiinlinevolumes 01/30/23 11:43:32.363
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:43:32.369
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:43:32.371
[BeforeEach] [sig-storage] CSIInlineVolumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support CSIVolumeSource in Pod API [Conformance]
  test/e2e/storage/csi_inline.go:131
STEP: creating 01/30/23 11:43:32.373
STEP: getting 01/30/23 11:43:32.38
STEP: listing in namespace 01/30/23 11:43:32.382
STEP: patching 01/30/23 11:43:32.383
STEP: deleting 01/30/23 11:43:32.387
[AfterEach] [sig-storage] CSIInlineVolumes
  test/e2e/framework/node/init/init.go:32
Jan 30 11:43:32.392: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  tear down framework | framework.go:193
STEP: Destroying namespace "csiinlinevolumes-2259" for this suite. 01/30/23 11:43:32.394
------------------------------
• [0.034 seconds]
[sig-storage] CSIInlineVolumes
test/e2e/storage/utils/framework.go:23
  should support CSIVolumeSource in Pod API [Conformance]
  test/e2e/storage/csi_inline.go:131

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] CSIInlineVolumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 11:43:32.362
    Jan 30 11:43:32.362: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename csiinlinevolumes 01/30/23 11:43:32.363
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:43:32.369
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:43:32.371
    [BeforeEach] [sig-storage] CSIInlineVolumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support CSIVolumeSource in Pod API [Conformance]
      test/e2e/storage/csi_inline.go:131
    STEP: creating 01/30/23 11:43:32.373
    STEP: getting 01/30/23 11:43:32.38
    STEP: listing in namespace 01/30/23 11:43:32.382
    STEP: patching 01/30/23 11:43:32.383
    STEP: deleting 01/30/23 11:43:32.387
    [AfterEach] [sig-storage] CSIInlineVolumes
      test/e2e/framework/node/init/init.go:32
    Jan 30 11:43:32.392: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "csiinlinevolumes-2259" for this suite. 01/30/23 11:43:32.394
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] RuntimeClass
  should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 11:43:32.396
Jan 30 11:43:32.396: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename runtimeclass 01/30/23 11:43:32.397
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:43:32.402
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:43:32.404
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It] should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Jan 30 11:43:32.410: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-2401" for this suite. 01/30/23 11:43:32.412
------------------------------
• [0.018 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 11:43:32.396
    Jan 30 11:43:32.396: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename runtimeclass 01/30/23 11:43:32.397
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:43:32.402
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:43:32.404
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It] should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:55
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Jan 30 11:43:32.410: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-2401" for this suite. 01/30/23 11:43:32.412
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation
  should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 11:43:32.415
Jan 30 11:43:32.415: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename tables 01/30/23 11:43:32.416
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:43:32.421
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:43:32.423
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/apimachinery/table_conversion.go:49
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/node/init/init.go:32
Jan 30 11:43:32.427: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
  tear down framework | framework.go:193
STEP: Destroying namespace "tables-9341" for this suite. 01/30/23 11:43:32.429
------------------------------
• [0.016 seconds]
[sig-api-machinery] Servers with support for Table transformation
test/e2e/apimachinery/framework.go:23
  should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 11:43:32.415
    Jan 30 11:43:32.415: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename tables 01/30/23 11:43:32.416
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:43:32.421
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:43:32.423
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/apimachinery/table_conversion.go:49
    [It] should return a 406 for a backend which does not implement metadata [Conformance]
      test/e2e/apimachinery/table_conversion.go:154
    [AfterEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/node/init/init.go:32
    Jan 30 11:43:32.427: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
      tear down framework | framework.go:193
    STEP: Destroying namespace "tables-9341" for this suite. 01/30/23 11:43:32.429
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-architecture] Conformance Tests
  should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38
[BeforeEach] [sig-architecture] Conformance Tests
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 11:43:32.432
Jan 30 11:43:32.432: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename conformance-tests 01/30/23 11:43:32.432
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:43:32.438
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:43:32.439
[BeforeEach] [sig-architecture] Conformance Tests
  test/e2e/framework/metrics/init/init.go:31
[It] should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38
STEP: Getting node addresses 01/30/23 11:43:32.441
Jan 30 11:43:32.441: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
[AfterEach] [sig-architecture] Conformance Tests
  test/e2e/framework/node/init/init.go:32
Jan 30 11:43:32.445: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-architecture] Conformance Tests
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-architecture] Conformance Tests
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-architecture] Conformance Tests
  tear down framework | framework.go:193
STEP: Destroying namespace "conformance-tests-9757" for this suite. 01/30/23 11:43:32.446
------------------------------
• [0.017 seconds]
[sig-architecture] Conformance Tests
test/e2e/architecture/framework.go:23
  should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-architecture] Conformance Tests
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 11:43:32.432
    Jan 30 11:43:32.432: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename conformance-tests 01/30/23 11:43:32.432
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:43:32.438
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:43:32.439
    [BeforeEach] [sig-architecture] Conformance Tests
      test/e2e/framework/metrics/init/init.go:31
    [It] should have at least two untainted nodes [Conformance]
      test/e2e/architecture/conformance.go:38
    STEP: Getting node addresses 01/30/23 11:43:32.441
    Jan 30 11:43:32.441: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    [AfterEach] [sig-architecture] Conformance Tests
      test/e2e/framework/node/init/init.go:32
    Jan 30 11:43:32.445: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-architecture] Conformance Tests
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-architecture] Conformance Tests
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-architecture] Conformance Tests
      tear down framework | framework.go:193
    STEP: Destroying namespace "conformance-tests-9757" for this suite. 01/30/23 11:43:32.446
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] Watchers
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 11:43:32.449
Jan 30 11:43:32.449: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename watch 01/30/23 11:43:32.449
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:43:32.455
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:43:32.457
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191
STEP: creating a watch on configmaps 01/30/23 11:43:32.459
STEP: creating a new configmap 01/30/23 11:43:32.46
STEP: modifying the configmap once 01/30/23 11:43:32.462
STEP: closing the watch once it receives two notifications 01/30/23 11:43:32.465
Jan 30 11:43:32.465: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-6836  d5c93e22-7232-4dd2-8e73-8e8c4917d706 17148 0 2023-01-30 11:43:32 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-01-30 11:43:32 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 30 11:43:32.465: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-6836  d5c93e22-7232-4dd2-8e73-8e8c4917d706 17149 0 2023-01-30 11:43:32 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-01-30 11:43:32 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time, while the watch is closed 01/30/23 11:43:32.465
STEP: creating a new watch on configmaps from the last resource version observed by the first watch 01/30/23 11:43:32.469
STEP: deleting the configmap 01/30/23 11:43:32.47
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed 01/30/23 11:43:32.472
Jan 30 11:43:32.472: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-6836  d5c93e22-7232-4dd2-8e73-8e8c4917d706 17150 0 2023-01-30 11:43:32 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-01-30 11:43:32 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 30 11:43:32.472: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-6836  d5c93e22-7232-4dd2-8e73-8e8c4917d706 17151 0 2023-01-30 11:43:32 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-01-30 11:43:32 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Jan 30 11:43:32.472: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-6836" for this suite. 01/30/23 11:43:32.474
------------------------------
• [0.027 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 11:43:32.449
    Jan 30 11:43:32.449: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename watch 01/30/23 11:43:32.449
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:43:32.455
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:43:32.457
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
      test/e2e/apimachinery/watch.go:191
    STEP: creating a watch on configmaps 01/30/23 11:43:32.459
    STEP: creating a new configmap 01/30/23 11:43:32.46
    STEP: modifying the configmap once 01/30/23 11:43:32.462
    STEP: closing the watch once it receives two notifications 01/30/23 11:43:32.465
    Jan 30 11:43:32.465: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-6836  d5c93e22-7232-4dd2-8e73-8e8c4917d706 17148 0 2023-01-30 11:43:32 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-01-30 11:43:32 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan 30 11:43:32.465: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-6836  d5c93e22-7232-4dd2-8e73-8e8c4917d706 17149 0 2023-01-30 11:43:32 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-01-30 11:43:32 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying the configmap a second time, while the watch is closed 01/30/23 11:43:32.465
    STEP: creating a new watch on configmaps from the last resource version observed by the first watch 01/30/23 11:43:32.469
    STEP: deleting the configmap 01/30/23 11:43:32.47
    STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed 01/30/23 11:43:32.472
    Jan 30 11:43:32.472: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-6836  d5c93e22-7232-4dd2-8e73-8e8c4917d706 17150 0 2023-01-30 11:43:32 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-01-30 11:43:32 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan 30 11:43:32.472: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-6836  d5c93e22-7232-4dd2-8e73-8e8c4917d706 17151 0 2023-01-30 11:43:32 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-01-30 11:43:32 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Jan 30 11:43:32.472: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-6836" for this suite. 01/30/23 11:43:32.474
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch
  watch on custom resource definition objects [Conformance]
  test/e2e/apimachinery/crd_watch.go:51
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 11:43:32.477
Jan 30 11:43:32.477: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename crd-watch 01/30/23 11:43:32.477
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:43:32.483
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:43:32.484
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] watch on custom resource definition objects [Conformance]
  test/e2e/apimachinery/crd_watch.go:51
Jan 30 11:43:32.486: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Creating first CR  01/30/23 11:43:35.041
Jan 30 11:43:35.044: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-30T11:43:35Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-30T11:43:35Z]] name:name1 resourceVersion:17184 uid:9b10cf82-24f4-4335-9b96-779d2c6ddea0] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR 01/30/23 11:43:45.045
Jan 30 11:43:45.049: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-30T11:43:45Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-30T11:43:45Z]] name:name2 resourceVersion:17249 uid:fce61aa7-18d2-4bd8-9f35-f6dc56c9f167] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR 01/30/23 11:43:55.05
Jan 30 11:43:55.054: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-30T11:43:35Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-30T11:43:55Z]] name:name1 resourceVersion:17266 uid:9b10cf82-24f4-4335-9b96-779d2c6ddea0] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR 01/30/23 11:44:05.056
Jan 30 11:44:05.061: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-30T11:43:45Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-30T11:44:05Z]] name:name2 resourceVersion:17290 uid:fce61aa7-18d2-4bd8-9f35-f6dc56c9f167] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR 01/30/23 11:44:15.061
Jan 30 11:44:15.066: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-30T11:43:35Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-30T11:43:55Z]] name:name1 resourceVersion:17309 uid:9b10cf82-24f4-4335-9b96-779d2c6ddea0] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR 01/30/23 11:44:25.066
Jan 30 11:44:25.070: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-30T11:43:45Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-30T11:44:05Z]] name:name2 resourceVersion:17328 uid:fce61aa7-18d2-4bd8-9f35-f6dc56c9f167] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 30 11:44:35.579: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-watch-8169" for this suite. 01/30/23 11:44:35.581
------------------------------
• [SLOW TEST] [63.106 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  test/e2e/apimachinery/crd_watch.go:44
    watch on custom resource definition objects [Conformance]
    test/e2e/apimachinery/crd_watch.go:51

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 11:43:32.477
    Jan 30 11:43:32.477: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename crd-watch 01/30/23 11:43:32.477
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:43:32.483
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:43:32.484
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] watch on custom resource definition objects [Conformance]
      test/e2e/apimachinery/crd_watch.go:51
    Jan 30 11:43:32.486: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Creating first CR  01/30/23 11:43:35.041
    Jan 30 11:43:35.044: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-30T11:43:35Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-30T11:43:35Z]] name:name1 resourceVersion:17184 uid:9b10cf82-24f4-4335-9b96-779d2c6ddea0] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Creating second CR 01/30/23 11:43:45.045
    Jan 30 11:43:45.049: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-30T11:43:45Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-30T11:43:45Z]] name:name2 resourceVersion:17249 uid:fce61aa7-18d2-4bd8-9f35-f6dc56c9f167] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Modifying first CR 01/30/23 11:43:55.05
    Jan 30 11:43:55.054: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-30T11:43:35Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-30T11:43:55Z]] name:name1 resourceVersion:17266 uid:9b10cf82-24f4-4335-9b96-779d2c6ddea0] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Modifying second CR 01/30/23 11:44:05.056
    Jan 30 11:44:05.061: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-30T11:43:45Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-30T11:44:05Z]] name:name2 resourceVersion:17290 uid:fce61aa7-18d2-4bd8-9f35-f6dc56c9f167] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Deleting first CR 01/30/23 11:44:15.061
    Jan 30 11:44:15.066: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-30T11:43:35Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-30T11:43:55Z]] name:name1 resourceVersion:17309 uid:9b10cf82-24f4-4335-9b96-779d2c6ddea0] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Deleting second CR 01/30/23 11:44:25.066
    Jan 30 11:44:25.070: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-30T11:43:45Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-30T11:44:05Z]] name:name2 resourceVersion:17328 uid:fce61aa7-18d2-4bd8-9f35-f6dc56c9f167] num:map[num1:9223372036854775807 num2:1000000]]}
    [AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 30 11:44:35.579: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-watch-8169" for this suite. 01/30/23 11:44:35.581
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath
  runs ReplicaSets to verify preemption running path [Conformance]
  test/e2e/scheduling/preemption.go:624
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 11:44:35.584
Jan 30 11:44:35.584: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename sched-preemption 01/30/23 11:44:35.584
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:44:35.59
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:44:35.592
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:97
Jan 30 11:44:35.599: INFO: Waiting up to 1m0s for all nodes to be ready
Jan 30 11:45:35.616: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PreemptionExecutionPath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 11:45:35.618
Jan 30 11:45:35.618: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename sched-preemption-path 01/30/23 11:45:35.618
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:45:35.625
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:45:35.627
[BeforeEach] PreemptionExecutionPath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:576
STEP: Finding an available node 01/30/23 11:45:35.629
STEP: Trying to launch a pod without a label to get a node which can launch it. 01/30/23 11:45:35.629
Jan 30 11:45:35.633: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-preemption-path-7097" to be "running"
Jan 30 11:45:35.635: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 1.59069ms
Jan 30 11:45:37.638: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.005070379s
Jan 30 11:45:37.638: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 01/30/23 11:45:37.64
Jan 30 11:45:37.644: INFO: found a healthy node: pubt2-nks-for-dev1.dg.163.org
[It] runs ReplicaSets to verify preemption running path [Conformance]
  test/e2e/scheduling/preemption.go:624
Jan 30 11:45:43.680: INFO: pods created so far: [1 1 1]
Jan 30 11:45:43.680: INFO: length of pods created so far: 3
Jan 30 11:45:45.685: INFO: pods created so far: [2 2 1]
[AfterEach] PreemptionExecutionPath
  test/e2e/framework/node/init/init.go:32
Jan 30 11:45:52.689: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:549
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/node/init/init.go:32
Jan 30 11:45:52.708: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:84
[DeferCleanup (Each)] PreemptionExecutionPath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] PreemptionExecutionPath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] PreemptionExecutionPath
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-path-7097" for this suite. 01/30/23 11:45:52.732
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-1394" for this suite. 01/30/23 11:45:52.734
------------------------------
• [SLOW TEST] [77.153 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:537
    runs ReplicaSets to verify preemption running path [Conformance]
    test/e2e/scheduling/preemption.go:624

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 11:44:35.584
    Jan 30 11:44:35.584: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename sched-preemption 01/30/23 11:44:35.584
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:44:35.59
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:44:35.592
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:97
    Jan 30 11:44:35.599: INFO: Waiting up to 1m0s for all nodes to be ready
    Jan 30 11:45:35.616: INFO: Waiting for terminating namespaces to be deleted...
    [BeforeEach] PreemptionExecutionPath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 11:45:35.618
    Jan 30 11:45:35.618: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename sched-preemption-path 01/30/23 11:45:35.618
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:45:35.625
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:45:35.627
    [BeforeEach] PreemptionExecutionPath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] PreemptionExecutionPath
      test/e2e/scheduling/preemption.go:576
    STEP: Finding an available node 01/30/23 11:45:35.629
    STEP: Trying to launch a pod without a label to get a node which can launch it. 01/30/23 11:45:35.629
    Jan 30 11:45:35.633: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-preemption-path-7097" to be "running"
    Jan 30 11:45:35.635: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 1.59069ms
    Jan 30 11:45:37.638: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.005070379s
    Jan 30 11:45:37.638: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 01/30/23 11:45:37.64
    Jan 30 11:45:37.644: INFO: found a healthy node: pubt2-nks-for-dev1.dg.163.org
    [It] runs ReplicaSets to verify preemption running path [Conformance]
      test/e2e/scheduling/preemption.go:624
    Jan 30 11:45:43.680: INFO: pods created so far: [1 1 1]
    Jan 30 11:45:43.680: INFO: length of pods created so far: 3
    Jan 30 11:45:45.685: INFO: pods created so far: [2 2 1]
    [AfterEach] PreemptionExecutionPath
      test/e2e/framework/node/init/init.go:32
    Jan 30 11:45:52.689: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] PreemptionExecutionPath
      test/e2e/scheduling/preemption.go:549
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan 30 11:45:52.708: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:84
    [DeferCleanup (Each)] PreemptionExecutionPath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] PreemptionExecutionPath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] PreemptionExecutionPath
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-path-7097" for this suite. 01/30/23 11:45:52.732
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-1394" for this suite. 01/30/23 11:45:52.734
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:326
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 11:45:52.737
Jan 30 11:45:52.737: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename resourcequota 01/30/23 11:45:52.738
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:45:52.745
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:45:52.747
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:326
STEP: Counting existing ResourceQuota 01/30/23 11:46:09.751
STEP: Creating a ResourceQuota 01/30/23 11:46:14.754
STEP: Ensuring resource quota status is calculated 01/30/23 11:46:14.757
STEP: Creating a ConfigMap 01/30/23 11:46:16.76
STEP: Ensuring resource quota status captures configMap creation 01/30/23 11:46:16.766
STEP: Deleting a ConfigMap 01/30/23 11:46:18.769
STEP: Ensuring resource quota status released usage 01/30/23 11:46:18.771
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Jan 30 11:46:20.774: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-3632" for this suite. 01/30/23 11:46:20.777
------------------------------
• [SLOW TEST] [28.043 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:326

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 11:45:52.737
    Jan 30 11:45:52.737: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename resourcequota 01/30/23 11:45:52.738
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:45:52.745
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:45:52.747
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
      test/e2e/apimachinery/resource_quota.go:326
    STEP: Counting existing ResourceQuota 01/30/23 11:46:09.751
    STEP: Creating a ResourceQuota 01/30/23 11:46:14.754
    STEP: Ensuring resource quota status is calculated 01/30/23 11:46:14.757
    STEP: Creating a ConfigMap 01/30/23 11:46:16.76
    STEP: Ensuring resource quota status captures configMap creation 01/30/23 11:46:16.766
    STEP: Deleting a ConfigMap 01/30/23 11:46:18.769
    STEP: Ensuring resource quota status released usage 01/30/23 11:46:18.771
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Jan 30 11:46:20.774: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-3632" for this suite. 01/30/23 11:46:20.777
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should delete a collection of services [Conformance]
  test/e2e/network/service.go:3654
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 11:46:20.78
Jan 30 11:46:20.780: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename services 01/30/23 11:46:20.781
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:46:20.787
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:46:20.789
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should delete a collection of services [Conformance]
  test/e2e/network/service.go:3654
STEP: creating a collection of services 01/30/23 11:46:20.791
Jan 30 11:46:20.791: INFO: Creating e2e-svc-a-4pl4z
Jan 30 11:46:20.795: INFO: Creating e2e-svc-b-46gqv
Jan 30 11:46:20.798: INFO: Creating e2e-svc-c-vrts5
STEP: deleting service collection 01/30/23 11:46:20.803
Jan 30 11:46:20.812: INFO: Collection of services has been deleted
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jan 30 11:46:20.812: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-3662" for this suite. 01/30/23 11:46:20.814
------------------------------
• [0.036 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should delete a collection of services [Conformance]
  test/e2e/network/service.go:3654

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 11:46:20.78
    Jan 30 11:46:20.780: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename services 01/30/23 11:46:20.781
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:46:20.787
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:46:20.789
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should delete a collection of services [Conformance]
      test/e2e/network/service.go:3654
    STEP: creating a collection of services 01/30/23 11:46:20.791
    Jan 30 11:46:20.791: INFO: Creating e2e-svc-a-4pl4z
    Jan 30 11:46:20.795: INFO: Creating e2e-svc-b-46gqv
    Jan 30 11:46:20.798: INFO: Creating e2e-svc-c-vrts5
    STEP: deleting service collection 01/30/23 11:46:20.803
    Jan 30 11:46:20.812: INFO: Collection of services has been deleted
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jan 30 11:46:20.812: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-3662" for this suite. 01/30/23 11:46:20.814
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 11:46:20.817
Jan 30 11:46:20.817: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename gc 01/30/23 11:46:20.817
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:46:20.823
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:46:20.824
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650
STEP: create the rc 01/30/23 11:46:20.828
STEP: delete the rc 01/30/23 11:46:25.833
STEP: wait for the rc to be deleted 01/30/23 11:46:25.836
Jan 30 11:46:26.847: INFO: 60 pods remaining
Jan 30 11:46:26.847: INFO: 60 pods has nil DeletionTimestamp
Jan 30 11:46:26.847: INFO: 
Jan 30 11:46:27.844: INFO: 40 pods remaining
Jan 30 11:46:27.844: INFO: 40 pods has nil DeletionTimestamp
Jan 30 11:46:27.844: INFO: 
Jan 30 11:46:28.841: INFO: 1 pods remaining
Jan 30 11:46:28.841: INFO: 1 pods has nil DeletionTimestamp
Jan 30 11:46:28.841: INFO: 
STEP: Gathering metrics 01/30/23 11:46:29.841
Jan 30 11:46:29.859: INFO: Waiting up to 5m0s for pod "kube-controller-manager-pubt2-nks-for-dev3.dg.163.org" in namespace "kube-system" to be "running and ready"
Jan 30 11:46:29.860: INFO: Pod "kube-controller-manager-pubt2-nks-for-dev3.dg.163.org": Phase="Running", Reason="", readiness=true. Elapsed: 1.665959ms
Jan 30 11:46:29.860: INFO: The phase of Pod kube-controller-manager-pubt2-nks-for-dev3.dg.163.org is Running (Ready = true)
Jan 30 11:46:29.860: INFO: Pod "kube-controller-manager-pubt2-nks-for-dev3.dg.163.org" satisfied condition "running and ready"
Jan 30 11:46:29.940: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Jan 30 11:46:29.940: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-1117" for this suite. 01/30/23 11:46:29.943
------------------------------
• [SLOW TEST] [9.130 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 11:46:20.817
    Jan 30 11:46:20.817: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename gc 01/30/23 11:46:20.817
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:46:20.823
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:46:20.824
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
      test/e2e/apimachinery/garbage_collector.go:650
    STEP: create the rc 01/30/23 11:46:20.828
    STEP: delete the rc 01/30/23 11:46:25.833
    STEP: wait for the rc to be deleted 01/30/23 11:46:25.836
    Jan 30 11:46:26.847: INFO: 60 pods remaining
    Jan 30 11:46:26.847: INFO: 60 pods has nil DeletionTimestamp
    Jan 30 11:46:26.847: INFO: 
    Jan 30 11:46:27.844: INFO: 40 pods remaining
    Jan 30 11:46:27.844: INFO: 40 pods has nil DeletionTimestamp
    Jan 30 11:46:27.844: INFO: 
    Jan 30 11:46:28.841: INFO: 1 pods remaining
    Jan 30 11:46:28.841: INFO: 1 pods has nil DeletionTimestamp
    Jan 30 11:46:28.841: INFO: 
    STEP: Gathering metrics 01/30/23 11:46:29.841
    Jan 30 11:46:29.859: INFO: Waiting up to 5m0s for pod "kube-controller-manager-pubt2-nks-for-dev3.dg.163.org" in namespace "kube-system" to be "running and ready"
    Jan 30 11:46:29.860: INFO: Pod "kube-controller-manager-pubt2-nks-for-dev3.dg.163.org": Phase="Running", Reason="", readiness=true. Elapsed: 1.665959ms
    Jan 30 11:46:29.860: INFO: The phase of Pod kube-controller-manager-pubt2-nks-for-dev3.dg.163.org is Running (Ready = true)
    Jan 30 11:46:29.860: INFO: Pod "kube-controller-manager-pubt2-nks-for-dev3.dg.163.org" satisfied condition "running and ready"
    Jan 30 11:46:29.940: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Jan 30 11:46:29.940: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-1117" for this suite. 01/30/23 11:46:29.943
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  test/e2e/apps/statefulset.go:317
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 11:46:29.947
Jan 30 11:46:29.947: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename statefulset 01/30/23 11:46:29.948
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:46:29.954
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:46:29.956
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-7212 01/30/23 11:46:29.958
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  test/e2e/apps/statefulset.go:317
STEP: Creating a new StatefulSet 01/30/23 11:46:29.96
Jan 30 11:46:29.965: INFO: Found 0 stateful pods, waiting for 3
Jan 30 11:46:39.968: INFO: Found 1 stateful pods, waiting for 3
Jan 30 11:46:49.967: INFO: Found 1 stateful pods, waiting for 3
Jan 30 11:46:59.968: INFO: Found 1 stateful pods, waiting for 3
Jan 30 11:47:09.969: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jan 30 11:47:09.969: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jan 30 11:47:09.969: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from harbor.cloud.netease.com/qzprod-k8s/e2e/httpd:2.4.38-4 to harbor.cloud.netease.com/qzprod-k8s/e2e/httpd:2.4.39-4 01/30/23 11:47:09.975
Jan 30 11:47:09.990: INFO: Updating stateful set ss2
STEP: Creating a new revision 01/30/23 11:47:09.99
STEP: Not applying an update when the partition is greater than the number of replicas 01/30/23 11:47:19.998
STEP: Performing a canary update 01/30/23 11:47:19.998
Jan 30 11:47:20.013: INFO: Updating stateful set ss2
Jan 30 11:47:20.016: INFO: Waiting for Pod statefulset-7212/ss2-2 to have revision ss2-78fd8f74b4 update revision ss2-774575cb9c
STEP: Restoring Pods to the correct revision when they are deleted 01/30/23 11:47:30.021
Jan 30 11:47:30.032: INFO: Found 1 stateful pods, waiting for 3
Jan 30 11:47:40.035: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jan 30 11:47:40.035: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jan 30 11:47:40.035: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update 01/30/23 11:47:40.039
Jan 30 11:47:40.055: INFO: Updating stateful set ss2
Jan 30 11:47:40.058: INFO: Waiting for Pod statefulset-7212/ss2-1 to have revision ss2-78fd8f74b4 update revision ss2-774575cb9c
Jan 30 11:47:50.078: INFO: Updating stateful set ss2
Jan 30 11:47:50.082: INFO: Waiting for StatefulSet statefulset-7212/ss2 to complete update
Jan 30 11:47:50.082: INFO: Waiting for Pod statefulset-7212/ss2-0 to have revision ss2-78fd8f74b4 update revision ss2-774575cb9c
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Jan 30 11:48:00.087: INFO: Deleting all statefulset in ns statefulset-7212
Jan 30 11:48:00.089: INFO: Scaling statefulset ss2 to 0
Jan 30 11:48:10.099: INFO: Waiting for statefulset status.replicas updated to 0
Jan 30 11:48:10.101: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Jan 30 11:48:10.107: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-7212" for this suite. 01/30/23 11:48:10.109
------------------------------
• [SLOW TEST] [100.165 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    test/e2e/apps/statefulset.go:317

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 11:46:29.947
    Jan 30 11:46:29.947: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename statefulset 01/30/23 11:46:29.948
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:46:29.954
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:46:29.956
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-7212 01/30/23 11:46:29.958
    [It] should perform canary updates and phased rolling updates of template modifications [Conformance]
      test/e2e/apps/statefulset.go:317
    STEP: Creating a new StatefulSet 01/30/23 11:46:29.96
    Jan 30 11:46:29.965: INFO: Found 0 stateful pods, waiting for 3
    Jan 30 11:46:39.968: INFO: Found 1 stateful pods, waiting for 3
    Jan 30 11:46:49.967: INFO: Found 1 stateful pods, waiting for 3
    Jan 30 11:46:59.968: INFO: Found 1 stateful pods, waiting for 3
    Jan 30 11:47:09.969: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Jan 30 11:47:09.969: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Jan 30 11:47:09.969: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Updating stateful set template: update image from harbor.cloud.netease.com/qzprod-k8s/e2e/httpd:2.4.38-4 to harbor.cloud.netease.com/qzprod-k8s/e2e/httpd:2.4.39-4 01/30/23 11:47:09.975
    Jan 30 11:47:09.990: INFO: Updating stateful set ss2
    STEP: Creating a new revision 01/30/23 11:47:09.99
    STEP: Not applying an update when the partition is greater than the number of replicas 01/30/23 11:47:19.998
    STEP: Performing a canary update 01/30/23 11:47:19.998
    Jan 30 11:47:20.013: INFO: Updating stateful set ss2
    Jan 30 11:47:20.016: INFO: Waiting for Pod statefulset-7212/ss2-2 to have revision ss2-78fd8f74b4 update revision ss2-774575cb9c
    STEP: Restoring Pods to the correct revision when they are deleted 01/30/23 11:47:30.021
    Jan 30 11:47:30.032: INFO: Found 1 stateful pods, waiting for 3
    Jan 30 11:47:40.035: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Jan 30 11:47:40.035: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Jan 30 11:47:40.035: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Performing a phased rolling update 01/30/23 11:47:40.039
    Jan 30 11:47:40.055: INFO: Updating stateful set ss2
    Jan 30 11:47:40.058: INFO: Waiting for Pod statefulset-7212/ss2-1 to have revision ss2-78fd8f74b4 update revision ss2-774575cb9c
    Jan 30 11:47:50.078: INFO: Updating stateful set ss2
    Jan 30 11:47:50.082: INFO: Waiting for StatefulSet statefulset-7212/ss2 to complete update
    Jan 30 11:47:50.082: INFO: Waiting for Pod statefulset-7212/ss2-0 to have revision ss2-78fd8f74b4 update revision ss2-774575cb9c
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Jan 30 11:48:00.087: INFO: Deleting all statefulset in ns statefulset-7212
    Jan 30 11:48:00.089: INFO: Scaling statefulset ss2 to 0
    Jan 30 11:48:10.099: INFO: Waiting for statefulset status.replicas updated to 0
    Jan 30 11:48:10.101: INFO: Deleting statefulset ss2
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Jan 30 11:48:10.107: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-7212" for this suite. 01/30/23 11:48:10.109
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 11:48:10.112
Jan 30 11:48:10.112: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename gc 01/30/23 11:48:10.113
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:48:10.119
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:48:10.121
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312
STEP: create the rc 01/30/23 11:48:10.123
STEP: delete the rc 01/30/23 11:48:15.13
STEP: wait for all pods to be garbage collected 01/30/23 11:48:15.132
STEP: Gathering metrics 01/30/23 11:48:20.136
Jan 30 11:48:20.155: INFO: Waiting up to 5m0s for pod "kube-controller-manager-pubt2-nks-for-dev3.dg.163.org" in namespace "kube-system" to be "running and ready"
Jan 30 11:48:20.156: INFO: Pod "kube-controller-manager-pubt2-nks-for-dev3.dg.163.org": Phase="Running", Reason="", readiness=true. Elapsed: 1.660936ms
Jan 30 11:48:20.156: INFO: The phase of Pod kube-controller-manager-pubt2-nks-for-dev3.dg.163.org is Running (Ready = true)
Jan 30 11:48:20.156: INFO: Pod "kube-controller-manager-pubt2-nks-for-dev3.dg.163.org" satisfied condition "running and ready"
Jan 30 11:48:20.229: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Jan 30 11:48:20.229: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-5335" for this suite. 01/30/23 11:48:20.231
------------------------------
• [SLOW TEST] [10.121 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 11:48:10.112
    Jan 30 11:48:10.112: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename gc 01/30/23 11:48:10.113
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:48:10.119
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:48:10.121
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete pods created by rc when not orphaning [Conformance]
      test/e2e/apimachinery/garbage_collector.go:312
    STEP: create the rc 01/30/23 11:48:10.123
    STEP: delete the rc 01/30/23 11:48:15.13
    STEP: wait for all pods to be garbage collected 01/30/23 11:48:15.132
    STEP: Gathering metrics 01/30/23 11:48:20.136
    Jan 30 11:48:20.155: INFO: Waiting up to 5m0s for pod "kube-controller-manager-pubt2-nks-for-dev3.dg.163.org" in namespace "kube-system" to be "running and ready"
    Jan 30 11:48:20.156: INFO: Pod "kube-controller-manager-pubt2-nks-for-dev3.dg.163.org": Phase="Running", Reason="", readiness=true. Elapsed: 1.660936ms
    Jan 30 11:48:20.156: INFO: The phase of Pod kube-controller-manager-pubt2-nks-for-dev3.dg.163.org is Running (Ready = true)
    Jan 30 11:48:20.156: INFO: Pod "kube-controller-manager-pubt2-nks-for-dev3.dg.163.org" satisfied condition "running and ready"
    Jan 30 11:48:20.229: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Jan 30 11:48:20.229: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-5335" for this suite. 01/30/23 11:48:20.231
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-network] HostPort
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63
[BeforeEach] [sig-network] HostPort
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 11:48:20.234
Jan 30 11:48:20.234: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename hostport 01/30/23 11:48:20.235
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:48:20.241
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:48:20.243
[BeforeEach] [sig-network] HostPort
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] HostPort
  test/e2e/network/hostport.go:49
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63
STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled 01/30/23 11:48:20.247
Jan 30 11:48:20.251: INFO: Waiting up to 5m0s for pod "pod1" in namespace "hostport-8363" to be "running and ready"
Jan 30 11:48:20.253: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 1.472617ms
Jan 30 11:48:20.253: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Jan 30 11:48:22.255: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.004312694s
Jan 30 11:48:22.255: INFO: The phase of Pod pod1 is Running (Ready = true)
Jan 30 11:48:22.255: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 10.182.0.82 on the node which pod1 resides and expect scheduled 01/30/23 11:48:22.255
Jan 30 11:48:22.258: INFO: Waiting up to 5m0s for pod "pod2" in namespace "hostport-8363" to be "running and ready"
Jan 30 11:48:22.260: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 1.51106ms
Jan 30 11:48:22.260: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Jan 30 11:48:24.263: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.004653236s
Jan 30 11:48:24.263: INFO: The phase of Pod pod2 is Running (Ready = true)
Jan 30 11:48:24.263: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 10.182.0.82 but use UDP protocol on the node which pod2 resides 01/30/23 11:48:24.263
Jan 30 11:48:24.266: INFO: Waiting up to 5m0s for pod "pod3" in namespace "hostport-8363" to be "running and ready"
Jan 30 11:48:24.267: INFO: Pod "pod3": Phase="Pending", Reason="", readiness=false. Elapsed: 1.583794ms
Jan 30 11:48:24.267: INFO: The phase of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
Jan 30 11:48:26.270: INFO: Pod "pod3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004305555s
Jan 30 11:48:26.270: INFO: The phase of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
Jan 30 11:48:28.270: INFO: Pod "pod3": Phase="Running", Reason="", readiness=true. Elapsed: 4.004546646s
Jan 30 11:48:28.270: INFO: The phase of Pod pod3 is Running (Ready = true)
Jan 30 11:48:28.270: INFO: Pod "pod3" satisfied condition "running and ready"
Jan 30 11:48:28.273: INFO: Waiting up to 5m0s for pod "e2e-host-exec" in namespace "hostport-8363" to be "running and ready"
Jan 30 11:48:28.275: INFO: Pod "e2e-host-exec": Phase="Pending", Reason="", readiness=false. Elapsed: 1.912159ms
Jan 30 11:48:28.275: INFO: The phase of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
Jan 30 11:48:30.278: INFO: Pod "e2e-host-exec": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005173745s
Jan 30 11:48:30.278: INFO: The phase of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
Jan 30 11:48:32.277: INFO: Pod "e2e-host-exec": Phase="Pending", Reason="", readiness=false. Elapsed: 4.004144613s
Jan 30 11:48:32.277: INFO: The phase of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
Jan 30 11:48:34.278: INFO: Pod "e2e-host-exec": Phase="Running", Reason="", readiness=true. Elapsed: 6.005367596s
Jan 30 11:48:34.278: INFO: The phase of Pod e2e-host-exec is Running (Ready = true)
Jan 30 11:48:34.278: INFO: Pod "e2e-host-exec" satisfied condition "running and ready"
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323 01/30/23 11:48:34.28
Jan 30 11:48:34.280: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.182.0.82 http://127.0.0.1:54323/hostname] Namespace:hostport-8363 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 30 11:48:34.280: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
Jan 30 11:48:34.280: INFO: ExecWithOptions: Clientset creation
Jan 30 11:48:34.280: INFO: ExecWithOptions: execute(POST https://10.178.64.1:443/api/v1/namespaces/hostport-8363/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+10.182.0.82+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.182.0.82, port: 54323 01/30/23 11:48:34.36
Jan 30 11:48:34.360: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.182.0.82:54323/hostname] Namespace:hostport-8363 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 30 11:48:34.360: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
Jan 30 11:48:34.361: INFO: ExecWithOptions: Clientset creation
Jan 30 11:48:34.361: INFO: ExecWithOptions: execute(POST https://10.178.64.1:443/api/v1/namespaces/hostport-8363/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F10.182.0.82%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.182.0.82, port: 54323 UDP 01/30/23 11:48:34.453
Jan 30 11:48:34.453: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostname | nc -u -w 5 10.182.0.82 54323] Namespace:hostport-8363 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 30 11:48:34.453: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
Jan 30 11:48:34.453: INFO: ExecWithOptions: Clientset creation
Jan 30 11:48:34.453: INFO: ExecWithOptions: execute(POST https://10.178.64.1:443/api/v1/namespaces/hostport-8363/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostname+%7C+nc+-u+-w+5+10.182.0.82+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
[AfterEach] [sig-network] HostPort
  test/e2e/framework/node/init/init.go:32
Jan 30 11:48:39.526: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] HostPort
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] HostPort
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] HostPort
  tear down framework | framework.go:193
STEP: Destroying namespace "hostport-8363" for this suite. 01/30/23 11:48:39.529
------------------------------
• [SLOW TEST] [19.297 seconds]
[sig-network] HostPort
test/e2e/network/common/framework.go:23
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] HostPort
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 11:48:20.234
    Jan 30 11:48:20.234: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename hostport 01/30/23 11:48:20.235
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:48:20.241
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:48:20.243
    [BeforeEach] [sig-network] HostPort
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] HostPort
      test/e2e/network/hostport.go:49
    [It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
      test/e2e/network/hostport.go:63
    STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled 01/30/23 11:48:20.247
    Jan 30 11:48:20.251: INFO: Waiting up to 5m0s for pod "pod1" in namespace "hostport-8363" to be "running and ready"
    Jan 30 11:48:20.253: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 1.472617ms
    Jan 30 11:48:20.253: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Jan 30 11:48:22.255: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.004312694s
    Jan 30 11:48:22.255: INFO: The phase of Pod pod1 is Running (Ready = true)
    Jan 30 11:48:22.255: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 10.182.0.82 on the node which pod1 resides and expect scheduled 01/30/23 11:48:22.255
    Jan 30 11:48:22.258: INFO: Waiting up to 5m0s for pod "pod2" in namespace "hostport-8363" to be "running and ready"
    Jan 30 11:48:22.260: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 1.51106ms
    Jan 30 11:48:22.260: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Jan 30 11:48:24.263: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.004653236s
    Jan 30 11:48:24.263: INFO: The phase of Pod pod2 is Running (Ready = true)
    Jan 30 11:48:24.263: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 10.182.0.82 but use UDP protocol on the node which pod2 resides 01/30/23 11:48:24.263
    Jan 30 11:48:24.266: INFO: Waiting up to 5m0s for pod "pod3" in namespace "hostport-8363" to be "running and ready"
    Jan 30 11:48:24.267: INFO: Pod "pod3": Phase="Pending", Reason="", readiness=false. Elapsed: 1.583794ms
    Jan 30 11:48:24.267: INFO: The phase of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
    Jan 30 11:48:26.270: INFO: Pod "pod3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004305555s
    Jan 30 11:48:26.270: INFO: The phase of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
    Jan 30 11:48:28.270: INFO: Pod "pod3": Phase="Running", Reason="", readiness=true. Elapsed: 4.004546646s
    Jan 30 11:48:28.270: INFO: The phase of Pod pod3 is Running (Ready = true)
    Jan 30 11:48:28.270: INFO: Pod "pod3" satisfied condition "running and ready"
    Jan 30 11:48:28.273: INFO: Waiting up to 5m0s for pod "e2e-host-exec" in namespace "hostport-8363" to be "running and ready"
    Jan 30 11:48:28.275: INFO: Pod "e2e-host-exec": Phase="Pending", Reason="", readiness=false. Elapsed: 1.912159ms
    Jan 30 11:48:28.275: INFO: The phase of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
    Jan 30 11:48:30.278: INFO: Pod "e2e-host-exec": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005173745s
    Jan 30 11:48:30.278: INFO: The phase of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
    Jan 30 11:48:32.277: INFO: Pod "e2e-host-exec": Phase="Pending", Reason="", readiness=false. Elapsed: 4.004144613s
    Jan 30 11:48:32.277: INFO: The phase of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
    Jan 30 11:48:34.278: INFO: Pod "e2e-host-exec": Phase="Running", Reason="", readiness=true. Elapsed: 6.005367596s
    Jan 30 11:48:34.278: INFO: The phase of Pod e2e-host-exec is Running (Ready = true)
    Jan 30 11:48:34.278: INFO: Pod "e2e-host-exec" satisfied condition "running and ready"
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323 01/30/23 11:48:34.28
    Jan 30 11:48:34.280: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.182.0.82 http://127.0.0.1:54323/hostname] Namespace:hostport-8363 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 30 11:48:34.280: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    Jan 30 11:48:34.280: INFO: ExecWithOptions: Clientset creation
    Jan 30 11:48:34.280: INFO: ExecWithOptions: execute(POST https://10.178.64.1:443/api/v1/namespaces/hostport-8363/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+10.182.0.82+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.182.0.82, port: 54323 01/30/23 11:48:34.36
    Jan 30 11:48:34.360: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.182.0.82:54323/hostname] Namespace:hostport-8363 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 30 11:48:34.360: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    Jan 30 11:48:34.361: INFO: ExecWithOptions: Clientset creation
    Jan 30 11:48:34.361: INFO: ExecWithOptions: execute(POST https://10.178.64.1:443/api/v1/namespaces/hostport-8363/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F10.182.0.82%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.182.0.82, port: 54323 UDP 01/30/23 11:48:34.453
    Jan 30 11:48:34.453: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostname | nc -u -w 5 10.182.0.82 54323] Namespace:hostport-8363 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 30 11:48:34.453: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    Jan 30 11:48:34.453: INFO: ExecWithOptions: Clientset creation
    Jan 30 11:48:34.453: INFO: ExecWithOptions: execute(POST https://10.178.64.1:443/api/v1/namespaces/hostport-8363/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostname+%7C+nc+-u+-w+5+10.182.0.82+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    [AfterEach] [sig-network] HostPort
      test/e2e/framework/node/init/init.go:32
    Jan 30 11:48:39.526: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] HostPort
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] HostPort
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] HostPort
      tear down framework | framework.go:193
    STEP: Destroying namespace "hostport-8363" for this suite. 01/30/23 11:48:39.529
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] ConfigMap
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:504
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 11:48:39.532
Jan 30 11:48:39.532: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename configmap 01/30/23 11:48:39.532
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:48:39.54
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:48:39.542
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:504
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jan 30 11:48:39.561: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-7189" for this suite. 01/30/23 11:48:39.563
------------------------------
• [0.034 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:504

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 11:48:39.532
    Jan 30 11:48:39.532: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename configmap 01/30/23 11:48:39.532
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:48:39.54
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:48:39.542
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be immutable if `immutable` field is set [Conformance]
      test/e2e/common/storage/configmap_volume.go:504
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jan 30 11:48:39.561: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-7189" for this suite. 01/30/23 11:48:39.563
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:174
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 11:48:39.566
Jan 30 11:48:39.566: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename projected 01/30/23 11:48:39.566
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:48:39.572
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:48:39.574
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:174
STEP: Creating configMap with name cm-test-opt-del-b03e8492-05c7-4c5b-9403-d11401a0e7e4 01/30/23 11:48:39.578
STEP: Creating configMap with name cm-test-opt-upd-fededd25-64d4-441f-8fc3-30f8c03599fe 01/30/23 11:48:39.58
STEP: Creating the pod 01/30/23 11:48:39.581
Jan 30 11:48:39.586: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-f943a2e0-8db3-4122-8474-8461ed23088f" in namespace "projected-9047" to be "running and ready"
Jan 30 11:48:39.588: INFO: Pod "pod-projected-configmaps-f943a2e0-8db3-4122-8474-8461ed23088f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.079415ms
Jan 30 11:48:39.588: INFO: The phase of Pod pod-projected-configmaps-f943a2e0-8db3-4122-8474-8461ed23088f is Pending, waiting for it to be Running (with Ready = true)
Jan 30 11:48:41.592: INFO: Pod "pod-projected-configmaps-f943a2e0-8db3-4122-8474-8461ed23088f": Phase="Running", Reason="", readiness=true. Elapsed: 2.00575701s
Jan 30 11:48:41.592: INFO: The phase of Pod pod-projected-configmaps-f943a2e0-8db3-4122-8474-8461ed23088f is Running (Ready = true)
Jan 30 11:48:41.592: INFO: Pod "pod-projected-configmaps-f943a2e0-8db3-4122-8474-8461ed23088f" satisfied condition "running and ready"
STEP: Deleting configmap cm-test-opt-del-b03e8492-05c7-4c5b-9403-d11401a0e7e4 01/30/23 11:48:41.618
STEP: Updating configmap cm-test-opt-upd-fededd25-64d4-441f-8fc3-30f8c03599fe 01/30/23 11:48:41.62
STEP: Creating configMap with name cm-test-opt-create-a9ae2e97-8579-4c34-aae5-333538b231dc 01/30/23 11:48:41.622
STEP: waiting to observe update in volume 01/30/23 11:48:41.624
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Jan 30 11:48:43.656: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-9047" for this suite. 01/30/23 11:48:43.658
------------------------------
• [4.095 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:174

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 11:48:39.566
    Jan 30 11:48:39.566: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename projected 01/30/23 11:48:39.566
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:48:39.572
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:48:39.574
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:174
    STEP: Creating configMap with name cm-test-opt-del-b03e8492-05c7-4c5b-9403-d11401a0e7e4 01/30/23 11:48:39.578
    STEP: Creating configMap with name cm-test-opt-upd-fededd25-64d4-441f-8fc3-30f8c03599fe 01/30/23 11:48:39.58
    STEP: Creating the pod 01/30/23 11:48:39.581
    Jan 30 11:48:39.586: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-f943a2e0-8db3-4122-8474-8461ed23088f" in namespace "projected-9047" to be "running and ready"
    Jan 30 11:48:39.588: INFO: Pod "pod-projected-configmaps-f943a2e0-8db3-4122-8474-8461ed23088f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.079415ms
    Jan 30 11:48:39.588: INFO: The phase of Pod pod-projected-configmaps-f943a2e0-8db3-4122-8474-8461ed23088f is Pending, waiting for it to be Running (with Ready = true)
    Jan 30 11:48:41.592: INFO: Pod "pod-projected-configmaps-f943a2e0-8db3-4122-8474-8461ed23088f": Phase="Running", Reason="", readiness=true. Elapsed: 2.00575701s
    Jan 30 11:48:41.592: INFO: The phase of Pod pod-projected-configmaps-f943a2e0-8db3-4122-8474-8461ed23088f is Running (Ready = true)
    Jan 30 11:48:41.592: INFO: Pod "pod-projected-configmaps-f943a2e0-8db3-4122-8474-8461ed23088f" satisfied condition "running and ready"
    STEP: Deleting configmap cm-test-opt-del-b03e8492-05c7-4c5b-9403-d11401a0e7e4 01/30/23 11:48:41.618
    STEP: Updating configmap cm-test-opt-upd-fededd25-64d4-441f-8fc3-30f8c03599fe 01/30/23 11:48:41.62
    STEP: Creating configMap with name cm-test-opt-create-a9ae2e97-8579-4c34-aae5-333538b231dc 01/30/23 11:48:41.622
    STEP: waiting to observe update in volume 01/30/23 11:48:41.624
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Jan 30 11:48:43.656: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-9047" for this suite. 01/30/23 11:48:43.658
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:101
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 11:48:43.661
Jan 30 11:48:43.661: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename replication-controller 01/30/23 11:48:43.662
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:48:43.667
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:48:43.669
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:101
STEP: Given a ReplicationController is created 01/30/23 11:48:43.671
STEP: When the matched label of one of its pods change 01/30/23 11:48:43.674
Jan 30 11:48:43.676: INFO: Pod name pod-release: Found 0 pods out of 1
Jan 30 11:48:48.678: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released 01/30/23 11:48:48.685
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Jan 30 11:48:49.689: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-6889" for this suite. 01/30/23 11:48:49.691
------------------------------
• [SLOW TEST] [6.032 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:101

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 11:48:43.661
    Jan 30 11:48:43.661: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename replication-controller 01/30/23 11:48:43.662
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:48:43.667
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:48:43.669
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should release no longer matching pods [Conformance]
      test/e2e/apps/rc.go:101
    STEP: Given a ReplicationController is created 01/30/23 11:48:43.671
    STEP: When the matched label of one of its pods change 01/30/23 11:48:43.674
    Jan 30 11:48:43.676: INFO: Pod name pod-release: Found 0 pods out of 1
    Jan 30 11:48:48.678: INFO: Pod name pod-release: Found 1 pods out of 1
    STEP: Then the pod is released 01/30/23 11:48:48.685
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Jan 30 11:48:49.689: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-6889" for this suite. 01/30/23 11:48:49.691
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PreStop
  should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168
[BeforeEach] [sig-node] PreStop
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 11:48:49.694
Jan 30 11:48:49.694: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename prestop 01/30/23 11:48:49.695
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:48:49.701
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:48:49.703
[BeforeEach] [sig-node] PreStop
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] PreStop
  test/e2e/node/pre_stop.go:159
[It] should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168
STEP: Creating server pod server in namespace prestop-8029 01/30/23 11:48:49.705
STEP: Waiting for pods to come up. 01/30/23 11:48:49.709
Jan 30 11:48:49.709: INFO: Waiting up to 5m0s for pod "server" in namespace "prestop-8029" to be "running"
Jan 30 11:48:49.711: INFO: Pod "server": Phase="Pending", Reason="", readiness=false. Elapsed: 1.58645ms
Jan 30 11:48:51.714: INFO: Pod "server": Phase="Running", Reason="", readiness=true. Elapsed: 2.00523493s
Jan 30 11:48:51.714: INFO: Pod "server" satisfied condition "running"
STEP: Creating tester pod tester in namespace prestop-8029 01/30/23 11:48:51.716
Jan 30 11:48:51.719: INFO: Waiting up to 5m0s for pod "tester" in namespace "prestop-8029" to be "running"
Jan 30 11:48:51.721: INFO: Pod "tester": Phase="Pending", Reason="", readiness=false. Elapsed: 1.638751ms
Jan 30 11:48:53.723: INFO: Pod "tester": Phase="Running", Reason="", readiness=true. Elapsed: 2.003712656s
Jan 30 11:48:53.723: INFO: Pod "tester" satisfied condition "running"
STEP: Deleting pre-stop pod 01/30/23 11:48:53.723
Jan 30 11:48:58.728: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod 01/30/23 11:48:58.728
[AfterEach] [sig-node] PreStop
  test/e2e/framework/node/init/init.go:32
Jan 30 11:48:58.733: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] PreStop
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] PreStop
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] PreStop
  tear down framework | framework.go:193
STEP: Destroying namespace "prestop-8029" for this suite. 01/30/23 11:48:58.735
------------------------------
• [SLOW TEST] [9.043 seconds]
[sig-node] PreStop
test/e2e/node/framework.go:23
  should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PreStop
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 11:48:49.694
    Jan 30 11:48:49.694: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename prestop 01/30/23 11:48:49.695
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:48:49.701
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:48:49.703
    [BeforeEach] [sig-node] PreStop
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] PreStop
      test/e2e/node/pre_stop.go:159
    [It] should call prestop when killing a pod  [Conformance]
      test/e2e/node/pre_stop.go:168
    STEP: Creating server pod server in namespace prestop-8029 01/30/23 11:48:49.705
    STEP: Waiting for pods to come up. 01/30/23 11:48:49.709
    Jan 30 11:48:49.709: INFO: Waiting up to 5m0s for pod "server" in namespace "prestop-8029" to be "running"
    Jan 30 11:48:49.711: INFO: Pod "server": Phase="Pending", Reason="", readiness=false. Elapsed: 1.58645ms
    Jan 30 11:48:51.714: INFO: Pod "server": Phase="Running", Reason="", readiness=true. Elapsed: 2.00523493s
    Jan 30 11:48:51.714: INFO: Pod "server" satisfied condition "running"
    STEP: Creating tester pod tester in namespace prestop-8029 01/30/23 11:48:51.716
    Jan 30 11:48:51.719: INFO: Waiting up to 5m0s for pod "tester" in namespace "prestop-8029" to be "running"
    Jan 30 11:48:51.721: INFO: Pod "tester": Phase="Pending", Reason="", readiness=false. Elapsed: 1.638751ms
    Jan 30 11:48:53.723: INFO: Pod "tester": Phase="Running", Reason="", readiness=true. Elapsed: 2.003712656s
    Jan 30 11:48:53.723: INFO: Pod "tester" satisfied condition "running"
    STEP: Deleting pre-stop pod 01/30/23 11:48:53.723
    Jan 30 11:48:58.728: INFO: Saw: {
    	"Hostname": "server",
    	"Sent": null,
    	"Received": {
    		"prestop": 1
    	},
    	"Errors": null,
    	"Log": [
    		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
    		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
    	],
    	"StillContactingPeers": true
    }
    STEP: Deleting the server pod 01/30/23 11:48:58.728
    [AfterEach] [sig-node] PreStop
      test/e2e/framework/node/init/init.go:32
    Jan 30 11:48:58.733: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] PreStop
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] PreStop
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] PreStop
      tear down framework | framework.go:193
    STEP: Destroying namespace "prestop-8029" for this suite. 01/30/23 11:48:58.735
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] Projected downwardAPI
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:130
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 11:48:58.738
Jan 30 11:48:58.738: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename projected 01/30/23 11:48:58.738
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:48:58.745
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:48:58.747
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:130
STEP: Creating the pod 01/30/23 11:48:58.749
Jan 30 11:48:58.753: INFO: Waiting up to 5m0s for pod "labelsupdatef6ccd309-69c3-4e3d-ae42-b88611bdb0b3" in namespace "projected-2632" to be "running and ready"
Jan 30 11:48:58.755: INFO: Pod "labelsupdatef6ccd309-69c3-4e3d-ae42-b88611bdb0b3": Phase="Pending", Reason="", readiness=false. Elapsed: 1.564863ms
Jan 30 11:48:58.755: INFO: The phase of Pod labelsupdatef6ccd309-69c3-4e3d-ae42-b88611bdb0b3 is Pending, waiting for it to be Running (with Ready = true)
Jan 30 11:49:00.758: INFO: Pod "labelsupdatef6ccd309-69c3-4e3d-ae42-b88611bdb0b3": Phase="Running", Reason="", readiness=true. Elapsed: 2.004245583s
Jan 30 11:49:00.758: INFO: The phase of Pod labelsupdatef6ccd309-69c3-4e3d-ae42-b88611bdb0b3 is Running (Ready = true)
Jan 30 11:49:00.758: INFO: Pod "labelsupdatef6ccd309-69c3-4e3d-ae42-b88611bdb0b3" satisfied condition "running and ready"
Jan 30 11:49:01.271: INFO: Successfully updated pod "labelsupdatef6ccd309-69c3-4e3d-ae42-b88611bdb0b3"
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Jan 30 11:49:03.284: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-2632" for this suite. 01/30/23 11:49:03.286
------------------------------
• [4.551 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:130

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 11:48:58.738
    Jan 30 11:48:58.738: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename projected 01/30/23 11:48:58.738
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:48:58.745
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:48:58.747
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should update labels on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:130
    STEP: Creating the pod 01/30/23 11:48:58.749
    Jan 30 11:48:58.753: INFO: Waiting up to 5m0s for pod "labelsupdatef6ccd309-69c3-4e3d-ae42-b88611bdb0b3" in namespace "projected-2632" to be "running and ready"
    Jan 30 11:48:58.755: INFO: Pod "labelsupdatef6ccd309-69c3-4e3d-ae42-b88611bdb0b3": Phase="Pending", Reason="", readiness=false. Elapsed: 1.564863ms
    Jan 30 11:48:58.755: INFO: The phase of Pod labelsupdatef6ccd309-69c3-4e3d-ae42-b88611bdb0b3 is Pending, waiting for it to be Running (with Ready = true)
    Jan 30 11:49:00.758: INFO: Pod "labelsupdatef6ccd309-69c3-4e3d-ae42-b88611bdb0b3": Phase="Running", Reason="", readiness=true. Elapsed: 2.004245583s
    Jan 30 11:49:00.758: INFO: The phase of Pod labelsupdatef6ccd309-69c3-4e3d-ae42-b88611bdb0b3 is Running (Ready = true)
    Jan 30 11:49:00.758: INFO: Pod "labelsupdatef6ccd309-69c3-4e3d-ae42-b88611bdb0b3" satisfied condition "running and ready"
    Jan 30 11:49:01.271: INFO: Successfully updated pod "labelsupdatef6ccd309-69c3-4e3d-ae42-b88611bdb0b3"
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Jan 30 11:49:03.284: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-2632" for this suite. 01/30/23 11:49:03.286
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:99
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 11:49:03.289
Jan 30 11:49:03.289: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename configmap 01/30/23 11:49:03.29
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:49:03.296
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:49:03.298
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:99
STEP: Creating configMap with name configmap-test-volume-map-bbfc2096-a612-43bf-b357-ecf59eb0e031 01/30/23 11:49:03.3
STEP: Creating a pod to test consume configMaps 01/30/23 11:49:03.302
Jan 30 11:49:03.306: INFO: Waiting up to 5m0s for pod "pod-configmaps-ac98d731-3e97-4546-a4b0-5b3ac6d07a10" in namespace "configmap-6122" to be "Succeeded or Failed"
Jan 30 11:49:03.308: INFO: Pod "pod-configmaps-ac98d731-3e97-4546-a4b0-5b3ac6d07a10": Phase="Pending", Reason="", readiness=false. Elapsed: 1.482998ms
Jan 30 11:49:05.310: INFO: Pod "pod-configmaps-ac98d731-3e97-4546-a4b0-5b3ac6d07a10": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003535766s
Jan 30 11:49:07.312: INFO: Pod "pod-configmaps-ac98d731-3e97-4546-a4b0-5b3ac6d07a10": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005326137s
STEP: Saw pod success 01/30/23 11:49:07.312
Jan 30 11:49:07.312: INFO: Pod "pod-configmaps-ac98d731-3e97-4546-a4b0-5b3ac6d07a10" satisfied condition "Succeeded or Failed"
Jan 30 11:49:07.314: INFO: Trying to get logs from node pubt2-nks-for-dev1.dg.163.org pod pod-configmaps-ac98d731-3e97-4546-a4b0-5b3ac6d07a10 container agnhost-container: <nil>
STEP: delete the pod 01/30/23 11:49:07.318
Jan 30 11:49:07.322: INFO: Waiting for pod pod-configmaps-ac98d731-3e97-4546-a4b0-5b3ac6d07a10 to disappear
Jan 30 11:49:07.324: INFO: Pod pod-configmaps-ac98d731-3e97-4546-a4b0-5b3ac6d07a10 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jan 30 11:49:07.324: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-6122" for this suite. 01/30/23 11:49:07.326
------------------------------
• [4.039 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:99

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 11:49:03.289
    Jan 30 11:49:03.289: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename configmap 01/30/23 11:49:03.29
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:49:03.296
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:49:03.298
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:99
    STEP: Creating configMap with name configmap-test-volume-map-bbfc2096-a612-43bf-b357-ecf59eb0e031 01/30/23 11:49:03.3
    STEP: Creating a pod to test consume configMaps 01/30/23 11:49:03.302
    Jan 30 11:49:03.306: INFO: Waiting up to 5m0s for pod "pod-configmaps-ac98d731-3e97-4546-a4b0-5b3ac6d07a10" in namespace "configmap-6122" to be "Succeeded or Failed"
    Jan 30 11:49:03.308: INFO: Pod "pod-configmaps-ac98d731-3e97-4546-a4b0-5b3ac6d07a10": Phase="Pending", Reason="", readiness=false. Elapsed: 1.482998ms
    Jan 30 11:49:05.310: INFO: Pod "pod-configmaps-ac98d731-3e97-4546-a4b0-5b3ac6d07a10": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003535766s
    Jan 30 11:49:07.312: INFO: Pod "pod-configmaps-ac98d731-3e97-4546-a4b0-5b3ac6d07a10": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005326137s
    STEP: Saw pod success 01/30/23 11:49:07.312
    Jan 30 11:49:07.312: INFO: Pod "pod-configmaps-ac98d731-3e97-4546-a4b0-5b3ac6d07a10" satisfied condition "Succeeded or Failed"
    Jan 30 11:49:07.314: INFO: Trying to get logs from node pubt2-nks-for-dev1.dg.163.org pod pod-configmaps-ac98d731-3e97-4546-a4b0-5b3ac6d07a10 container agnhost-container: <nil>
    STEP: delete the pod 01/30/23 11:49:07.318
    Jan 30 11:49:07.322: INFO: Waiting for pod pod-configmaps-ac98d731-3e97-4546-a4b0-5b3ac6d07a10 to disappear
    Jan 30 11:49:07.324: INFO: Pod pod-configmaps-ac98d731-3e97-4546-a4b0-5b3ac6d07a10 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jan 30 11:49:07.324: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-6122" for this suite. 01/30/23 11:49:07.326
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:205
[BeforeEach] [sig-network] EndpointSlice
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 11:49:07.329
Jan 30 11:49:07.329: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename endpointslice 01/30/23 11:49:07.33
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:49:07.336
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:49:07.338
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:52
[It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:205
STEP: referencing a single matching pod 01/30/23 11:49:12.362
STEP: referencing matching pods with named port 01/30/23 11:49:17.367
STEP: creating empty Endpoints and EndpointSlices for no matching Pods 01/30/23 11:49:22.373
STEP: recreating EndpointSlices after they've been deleted 01/30/23 11:49:27.377
Jan 30 11:49:27.385: INFO: EndpointSlice for Service endpointslice-2139/example-named-port not found
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/node/init/init.go:32
Jan 30 11:49:37.391: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSlice
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSlice
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslice-2139" for this suite. 01/30/23 11:49:37.394
------------------------------
• [SLOW TEST] [30.067 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:205

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 11:49:07.329
    Jan 30 11:49:07.329: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename endpointslice 01/30/23 11:49:07.33
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:49:07.336
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:49:07.338
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:52
    [It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
      test/e2e/network/endpointslice.go:205
    STEP: referencing a single matching pod 01/30/23 11:49:12.362
    STEP: referencing matching pods with named port 01/30/23 11:49:17.367
    STEP: creating empty Endpoints and EndpointSlices for no matching Pods 01/30/23 11:49:22.373
    STEP: recreating EndpointSlices after they've been deleted 01/30/23 11:49:27.377
    Jan 30 11:49:27.385: INFO: EndpointSlice for Service endpointslice-2139/example-named-port not found
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/node/init/init.go:32
    Jan 30 11:49:37.391: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslice-2139" for this suite. 01/30/23 11:49:37.394
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 11:49:37.396
Jan 30 11:49:37.396: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename gc 01/30/23 11:49:37.397
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:49:37.404
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:49:37.406
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370
STEP: create the rc 01/30/23 11:49:37.41
STEP: delete the rc 01/30/23 11:49:42.415
STEP: wait for the rc to be deleted 01/30/23 11:49:42.418
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods 01/30/23 11:49:47.422
STEP: Gathering metrics 01/30/23 11:50:17.44
Jan 30 11:50:17.458: INFO: Waiting up to 5m0s for pod "kube-controller-manager-pubt2-nks-for-dev3.dg.163.org" in namespace "kube-system" to be "running and ready"
Jan 30 11:50:17.459: INFO: Pod "kube-controller-manager-pubt2-nks-for-dev3.dg.163.org": Phase="Running", Reason="", readiness=true. Elapsed: 1.61354ms
Jan 30 11:50:17.459: INFO: The phase of Pod kube-controller-manager-pubt2-nks-for-dev3.dg.163.org is Running (Ready = true)
Jan 30 11:50:17.459: INFO: Pod "kube-controller-manager-pubt2-nks-for-dev3.dg.163.org" satisfied condition "running and ready"
Jan 30 11:50:17.547: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Jan 30 11:50:17.547: INFO: Deleting pod "simpletest.rc-2699s" in namespace "gc-1567"
Jan 30 11:50:17.551: INFO: Deleting pod "simpletest.rc-2fd7j" in namespace "gc-1567"
Jan 30 11:50:17.555: INFO: Deleting pod "simpletest.rc-2gk2t" in namespace "gc-1567"
Jan 30 11:50:17.560: INFO: Deleting pod "simpletest.rc-2tcjx" in namespace "gc-1567"
Jan 30 11:50:17.564: INFO: Deleting pod "simpletest.rc-42xj7" in namespace "gc-1567"
Jan 30 11:50:17.568: INFO: Deleting pod "simpletest.rc-545ld" in namespace "gc-1567"
Jan 30 11:50:17.573: INFO: Deleting pod "simpletest.rc-5qjpp" in namespace "gc-1567"
Jan 30 11:50:17.577: INFO: Deleting pod "simpletest.rc-5rnnq" in namespace "gc-1567"
Jan 30 11:50:17.582: INFO: Deleting pod "simpletest.rc-5vfft" in namespace "gc-1567"
Jan 30 11:50:17.588: INFO: Deleting pod "simpletest.rc-67s6s" in namespace "gc-1567"
Jan 30 11:50:17.592: INFO: Deleting pod "simpletest.rc-6dcdr" in namespace "gc-1567"
Jan 30 11:50:17.596: INFO: Deleting pod "simpletest.rc-6ftjw" in namespace "gc-1567"
Jan 30 11:50:17.601: INFO: Deleting pod "simpletest.rc-6hc57" in namespace "gc-1567"
Jan 30 11:50:17.605: INFO: Deleting pod "simpletest.rc-6nzll" in namespace "gc-1567"
Jan 30 11:50:17.609: INFO: Deleting pod "simpletest.rc-6qxt9" in namespace "gc-1567"
Jan 30 11:50:17.616: INFO: Deleting pod "simpletest.rc-77n4r" in namespace "gc-1567"
Jan 30 11:50:17.622: INFO: Deleting pod "simpletest.rc-7k7tn" in namespace "gc-1567"
Jan 30 11:50:17.627: INFO: Deleting pod "simpletest.rc-85mlt" in namespace "gc-1567"
Jan 30 11:50:17.632: INFO: Deleting pod "simpletest.rc-87xp4" in namespace "gc-1567"
Jan 30 11:50:17.636: INFO: Deleting pod "simpletest.rc-8gp92" in namespace "gc-1567"
Jan 30 11:50:17.639: INFO: Deleting pod "simpletest.rc-94rl5" in namespace "gc-1567"
Jan 30 11:50:17.644: INFO: Deleting pod "simpletest.rc-96dz5" in namespace "gc-1567"
Jan 30 11:50:17.648: INFO: Deleting pod "simpletest.rc-9l77n" in namespace "gc-1567"
Jan 30 11:50:17.653: INFO: Deleting pod "simpletest.rc-9lrvx" in namespace "gc-1567"
Jan 30 11:50:17.657: INFO: Deleting pod "simpletest.rc-b728c" in namespace "gc-1567"
Jan 30 11:50:17.661: INFO: Deleting pod "simpletest.rc-bb7h6" in namespace "gc-1567"
Jan 30 11:50:17.665: INFO: Deleting pod "simpletest.rc-bdglx" in namespace "gc-1567"
Jan 30 11:50:17.669: INFO: Deleting pod "simpletest.rc-btnqf" in namespace "gc-1567"
Jan 30 11:50:17.673: INFO: Deleting pod "simpletest.rc-bvdp2" in namespace "gc-1567"
Jan 30 11:50:17.678: INFO: Deleting pod "simpletest.rc-ccfsx" in namespace "gc-1567"
Jan 30 11:50:17.682: INFO: Deleting pod "simpletest.rc-clqbw" in namespace "gc-1567"
Jan 30 11:50:17.686: INFO: Deleting pod "simpletest.rc-dn2pv" in namespace "gc-1567"
Jan 30 11:50:17.690: INFO: Deleting pod "simpletest.rc-f4pjz" in namespace "gc-1567"
Jan 30 11:50:17.694: INFO: Deleting pod "simpletest.rc-fgvck" in namespace "gc-1567"
Jan 30 11:50:17.698: INFO: Deleting pod "simpletest.rc-fjx8l" in namespace "gc-1567"
Jan 30 11:50:17.702: INFO: Deleting pod "simpletest.rc-fksg9" in namespace "gc-1567"
Jan 30 11:50:17.707: INFO: Deleting pod "simpletest.rc-gl2hj" in namespace "gc-1567"
Jan 30 11:50:17.711: INFO: Deleting pod "simpletest.rc-gnvhz" in namespace "gc-1567"
Jan 30 11:50:17.715: INFO: Deleting pod "simpletest.rc-gpgkv" in namespace "gc-1567"
Jan 30 11:50:17.720: INFO: Deleting pod "simpletest.rc-h5whc" in namespace "gc-1567"
Jan 30 11:50:17.725: INFO: Deleting pod "simpletest.rc-h8rpt" in namespace "gc-1567"
Jan 30 11:50:17.729: INFO: Deleting pod "simpletest.rc-hd2v8" in namespace "gc-1567"
Jan 30 11:50:17.733: INFO: Deleting pod "simpletest.rc-hk857" in namespace "gc-1567"
Jan 30 11:50:17.738: INFO: Deleting pod "simpletest.rc-hm984" in namespace "gc-1567"
Jan 30 11:50:17.741: INFO: Deleting pod "simpletest.rc-hp5s7" in namespace "gc-1567"
Jan 30 11:50:17.746: INFO: Deleting pod "simpletest.rc-hwbl5" in namespace "gc-1567"
Jan 30 11:50:17.749: INFO: Deleting pod "simpletest.rc-jhcmk" in namespace "gc-1567"
Jan 30 11:50:17.754: INFO: Deleting pod "simpletest.rc-jjk74" in namespace "gc-1567"
Jan 30 11:50:17.759: INFO: Deleting pod "simpletest.rc-jkprt" in namespace "gc-1567"
Jan 30 11:50:17.764: INFO: Deleting pod "simpletest.rc-jlkqc" in namespace "gc-1567"
Jan 30 11:50:17.770: INFO: Deleting pod "simpletest.rc-jpvr6" in namespace "gc-1567"
Jan 30 11:50:17.775: INFO: Deleting pod "simpletest.rc-lfwhb" in namespace "gc-1567"
Jan 30 11:50:17.781: INFO: Deleting pod "simpletest.rc-llw8h" in namespace "gc-1567"
Jan 30 11:50:17.786: INFO: Deleting pod "simpletest.rc-lpkth" in namespace "gc-1567"
Jan 30 11:50:17.832: INFO: Deleting pod "simpletest.rc-lsz6z" in namespace "gc-1567"
Jan 30 11:50:17.883: INFO: Deleting pod "simpletest.rc-mklrh" in namespace "gc-1567"
Jan 30 11:50:17.932: INFO: Deleting pod "simpletest.rc-mr4x5" in namespace "gc-1567"
Jan 30 11:50:17.982: INFO: Deleting pod "simpletest.rc-mw69m" in namespace "gc-1567"
Jan 30 11:50:18.032: INFO: Deleting pod "simpletest.rc-mwfmz" in namespace "gc-1567"
Jan 30 11:50:18.081: INFO: Deleting pod "simpletest.rc-mz526" in namespace "gc-1567"
Jan 30 11:50:18.132: INFO: Deleting pod "simpletest.rc-ns99w" in namespace "gc-1567"
Jan 30 11:50:18.180: INFO: Deleting pod "simpletest.rc-nwznh" in namespace "gc-1567"
Jan 30 11:50:18.231: INFO: Deleting pod "simpletest.rc-p2hwm" in namespace "gc-1567"
Jan 30 11:50:18.282: INFO: Deleting pod "simpletest.rc-p77ws" in namespace "gc-1567"
Jan 30 11:50:18.331: INFO: Deleting pod "simpletest.rc-q7xpp" in namespace "gc-1567"
Jan 30 11:50:18.381: INFO: Deleting pod "simpletest.rc-qbvsk" in namespace "gc-1567"
Jan 30 11:50:18.431: INFO: Deleting pod "simpletest.rc-qf787" in namespace "gc-1567"
Jan 30 11:50:18.481: INFO: Deleting pod "simpletest.rc-qfwsr" in namespace "gc-1567"
Jan 30 11:50:18.531: INFO: Deleting pod "simpletest.rc-qg7ng" in namespace "gc-1567"
Jan 30 11:50:18.580: INFO: Deleting pod "simpletest.rc-qh98w" in namespace "gc-1567"
Jan 30 11:50:18.631: INFO: Deleting pod "simpletest.rc-qk9zx" in namespace "gc-1567"
Jan 30 11:50:18.681: INFO: Deleting pod "simpletest.rc-qpqk8" in namespace "gc-1567"
Jan 30 11:50:18.731: INFO: Deleting pod "simpletest.rc-qvlpx" in namespace "gc-1567"
Jan 30 11:50:18.782: INFO: Deleting pod "simpletest.rc-r75fx" in namespace "gc-1567"
Jan 30 11:50:18.831: INFO: Deleting pod "simpletest.rc-rmss9" in namespace "gc-1567"
Jan 30 11:50:18.881: INFO: Deleting pod "simpletest.rc-rzp4z" in namespace "gc-1567"
Jan 30 11:50:18.932: INFO: Deleting pod "simpletest.rc-rzrck" in namespace "gc-1567"
Jan 30 11:50:18.981: INFO: Deleting pod "simpletest.rc-s4jhf" in namespace "gc-1567"
Jan 30 11:50:19.032: INFO: Deleting pod "simpletest.rc-svf7v" in namespace "gc-1567"
Jan 30 11:50:19.082: INFO: Deleting pod "simpletest.rc-swcrf" in namespace "gc-1567"
Jan 30 11:50:19.131: INFO: Deleting pod "simpletest.rc-szk78" in namespace "gc-1567"
Jan 30 11:50:19.181: INFO: Deleting pod "simpletest.rc-t88tr" in namespace "gc-1567"
Jan 30 11:50:19.231: INFO: Deleting pod "simpletest.rc-tsqwq" in namespace "gc-1567"
Jan 30 11:50:19.281: INFO: Deleting pod "simpletest.rc-vbsk6" in namespace "gc-1567"
Jan 30 11:50:19.335: INFO: Deleting pod "simpletest.rc-vftpd" in namespace "gc-1567"
Jan 30 11:50:19.381: INFO: Deleting pod "simpletest.rc-vltp9" in namespace "gc-1567"
Jan 30 11:50:19.431: INFO: Deleting pod "simpletest.rc-vlz89" in namespace "gc-1567"
Jan 30 11:50:19.480: INFO: Deleting pod "simpletest.rc-vnxml" in namespace "gc-1567"
Jan 30 11:50:19.531: INFO: Deleting pod "simpletest.rc-w42x7" in namespace "gc-1567"
Jan 30 11:50:19.585: INFO: Deleting pod "simpletest.rc-whhwf" in namespace "gc-1567"
Jan 30 11:50:19.631: INFO: Deleting pod "simpletest.rc-wx8pb" in namespace "gc-1567"
Jan 30 11:50:19.683: INFO: Deleting pod "simpletest.rc-x8xfl" in namespace "gc-1567"
Jan 30 11:50:19.732: INFO: Deleting pod "simpletest.rc-x9m44" in namespace "gc-1567"
Jan 30 11:50:19.780: INFO: Deleting pod "simpletest.rc-xnxdk" in namespace "gc-1567"
Jan 30 11:50:19.832: INFO: Deleting pod "simpletest.rc-xrh8d" in namespace "gc-1567"
Jan 30 11:50:19.882: INFO: Deleting pod "simpletest.rc-xs7pg" in namespace "gc-1567"
Jan 30 11:50:19.931: INFO: Deleting pod "simpletest.rc-xtxnm" in namespace "gc-1567"
Jan 30 11:50:19.981: INFO: Deleting pod "simpletest.rc-zkvrt" in namespace "gc-1567"
Jan 30 11:50:20.033: INFO: Deleting pod "simpletest.rc-zkwr8" in namespace "gc-1567"
Jan 30 11:50:20.083: INFO: Deleting pod "simpletest.rc-zwzbk" in namespace "gc-1567"
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Jan 30 11:50:20.131: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-1567" for this suite. 01/30/23 11:50:20.18
------------------------------
• [SLOW TEST] [42.843 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 11:49:37.396
    Jan 30 11:49:37.396: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename gc 01/30/23 11:49:37.397
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:49:37.404
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:49:37.406
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should orphan pods created by rc if delete options say so [Conformance]
      test/e2e/apimachinery/garbage_collector.go:370
    STEP: create the rc 01/30/23 11:49:37.41
    STEP: delete the rc 01/30/23 11:49:42.415
    STEP: wait for the rc to be deleted 01/30/23 11:49:42.418
    STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods 01/30/23 11:49:47.422
    STEP: Gathering metrics 01/30/23 11:50:17.44
    Jan 30 11:50:17.458: INFO: Waiting up to 5m0s for pod "kube-controller-manager-pubt2-nks-for-dev3.dg.163.org" in namespace "kube-system" to be "running and ready"
    Jan 30 11:50:17.459: INFO: Pod "kube-controller-manager-pubt2-nks-for-dev3.dg.163.org": Phase="Running", Reason="", readiness=true. Elapsed: 1.61354ms
    Jan 30 11:50:17.459: INFO: The phase of Pod kube-controller-manager-pubt2-nks-for-dev3.dg.163.org is Running (Ready = true)
    Jan 30 11:50:17.459: INFO: Pod "kube-controller-manager-pubt2-nks-for-dev3.dg.163.org" satisfied condition "running and ready"
    Jan 30 11:50:17.547: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    Jan 30 11:50:17.547: INFO: Deleting pod "simpletest.rc-2699s" in namespace "gc-1567"
    Jan 30 11:50:17.551: INFO: Deleting pod "simpletest.rc-2fd7j" in namespace "gc-1567"
    Jan 30 11:50:17.555: INFO: Deleting pod "simpletest.rc-2gk2t" in namespace "gc-1567"
    Jan 30 11:50:17.560: INFO: Deleting pod "simpletest.rc-2tcjx" in namespace "gc-1567"
    Jan 30 11:50:17.564: INFO: Deleting pod "simpletest.rc-42xj7" in namespace "gc-1567"
    Jan 30 11:50:17.568: INFO: Deleting pod "simpletest.rc-545ld" in namespace "gc-1567"
    Jan 30 11:50:17.573: INFO: Deleting pod "simpletest.rc-5qjpp" in namespace "gc-1567"
    Jan 30 11:50:17.577: INFO: Deleting pod "simpletest.rc-5rnnq" in namespace "gc-1567"
    Jan 30 11:50:17.582: INFO: Deleting pod "simpletest.rc-5vfft" in namespace "gc-1567"
    Jan 30 11:50:17.588: INFO: Deleting pod "simpletest.rc-67s6s" in namespace "gc-1567"
    Jan 30 11:50:17.592: INFO: Deleting pod "simpletest.rc-6dcdr" in namespace "gc-1567"
    Jan 30 11:50:17.596: INFO: Deleting pod "simpletest.rc-6ftjw" in namespace "gc-1567"
    Jan 30 11:50:17.601: INFO: Deleting pod "simpletest.rc-6hc57" in namespace "gc-1567"
    Jan 30 11:50:17.605: INFO: Deleting pod "simpletest.rc-6nzll" in namespace "gc-1567"
    Jan 30 11:50:17.609: INFO: Deleting pod "simpletest.rc-6qxt9" in namespace "gc-1567"
    Jan 30 11:50:17.616: INFO: Deleting pod "simpletest.rc-77n4r" in namespace "gc-1567"
    Jan 30 11:50:17.622: INFO: Deleting pod "simpletest.rc-7k7tn" in namespace "gc-1567"
    Jan 30 11:50:17.627: INFO: Deleting pod "simpletest.rc-85mlt" in namespace "gc-1567"
    Jan 30 11:50:17.632: INFO: Deleting pod "simpletest.rc-87xp4" in namespace "gc-1567"
    Jan 30 11:50:17.636: INFO: Deleting pod "simpletest.rc-8gp92" in namespace "gc-1567"
    Jan 30 11:50:17.639: INFO: Deleting pod "simpletest.rc-94rl5" in namespace "gc-1567"
    Jan 30 11:50:17.644: INFO: Deleting pod "simpletest.rc-96dz5" in namespace "gc-1567"
    Jan 30 11:50:17.648: INFO: Deleting pod "simpletest.rc-9l77n" in namespace "gc-1567"
    Jan 30 11:50:17.653: INFO: Deleting pod "simpletest.rc-9lrvx" in namespace "gc-1567"
    Jan 30 11:50:17.657: INFO: Deleting pod "simpletest.rc-b728c" in namespace "gc-1567"
    Jan 30 11:50:17.661: INFO: Deleting pod "simpletest.rc-bb7h6" in namespace "gc-1567"
    Jan 30 11:50:17.665: INFO: Deleting pod "simpletest.rc-bdglx" in namespace "gc-1567"
    Jan 30 11:50:17.669: INFO: Deleting pod "simpletest.rc-btnqf" in namespace "gc-1567"
    Jan 30 11:50:17.673: INFO: Deleting pod "simpletest.rc-bvdp2" in namespace "gc-1567"
    Jan 30 11:50:17.678: INFO: Deleting pod "simpletest.rc-ccfsx" in namespace "gc-1567"
    Jan 30 11:50:17.682: INFO: Deleting pod "simpletest.rc-clqbw" in namespace "gc-1567"
    Jan 30 11:50:17.686: INFO: Deleting pod "simpletest.rc-dn2pv" in namespace "gc-1567"
    Jan 30 11:50:17.690: INFO: Deleting pod "simpletest.rc-f4pjz" in namespace "gc-1567"
    Jan 30 11:50:17.694: INFO: Deleting pod "simpletest.rc-fgvck" in namespace "gc-1567"
    Jan 30 11:50:17.698: INFO: Deleting pod "simpletest.rc-fjx8l" in namespace "gc-1567"
    Jan 30 11:50:17.702: INFO: Deleting pod "simpletest.rc-fksg9" in namespace "gc-1567"
    Jan 30 11:50:17.707: INFO: Deleting pod "simpletest.rc-gl2hj" in namespace "gc-1567"
    Jan 30 11:50:17.711: INFO: Deleting pod "simpletest.rc-gnvhz" in namespace "gc-1567"
    Jan 30 11:50:17.715: INFO: Deleting pod "simpletest.rc-gpgkv" in namespace "gc-1567"
    Jan 30 11:50:17.720: INFO: Deleting pod "simpletest.rc-h5whc" in namespace "gc-1567"
    Jan 30 11:50:17.725: INFO: Deleting pod "simpletest.rc-h8rpt" in namespace "gc-1567"
    Jan 30 11:50:17.729: INFO: Deleting pod "simpletest.rc-hd2v8" in namespace "gc-1567"
    Jan 30 11:50:17.733: INFO: Deleting pod "simpletest.rc-hk857" in namespace "gc-1567"
    Jan 30 11:50:17.738: INFO: Deleting pod "simpletest.rc-hm984" in namespace "gc-1567"
    Jan 30 11:50:17.741: INFO: Deleting pod "simpletest.rc-hp5s7" in namespace "gc-1567"
    Jan 30 11:50:17.746: INFO: Deleting pod "simpletest.rc-hwbl5" in namespace "gc-1567"
    Jan 30 11:50:17.749: INFO: Deleting pod "simpletest.rc-jhcmk" in namespace "gc-1567"
    Jan 30 11:50:17.754: INFO: Deleting pod "simpletest.rc-jjk74" in namespace "gc-1567"
    Jan 30 11:50:17.759: INFO: Deleting pod "simpletest.rc-jkprt" in namespace "gc-1567"
    Jan 30 11:50:17.764: INFO: Deleting pod "simpletest.rc-jlkqc" in namespace "gc-1567"
    Jan 30 11:50:17.770: INFO: Deleting pod "simpletest.rc-jpvr6" in namespace "gc-1567"
    Jan 30 11:50:17.775: INFO: Deleting pod "simpletest.rc-lfwhb" in namespace "gc-1567"
    Jan 30 11:50:17.781: INFO: Deleting pod "simpletest.rc-llw8h" in namespace "gc-1567"
    Jan 30 11:50:17.786: INFO: Deleting pod "simpletest.rc-lpkth" in namespace "gc-1567"
    Jan 30 11:50:17.832: INFO: Deleting pod "simpletest.rc-lsz6z" in namespace "gc-1567"
    Jan 30 11:50:17.883: INFO: Deleting pod "simpletest.rc-mklrh" in namespace "gc-1567"
    Jan 30 11:50:17.932: INFO: Deleting pod "simpletest.rc-mr4x5" in namespace "gc-1567"
    Jan 30 11:50:17.982: INFO: Deleting pod "simpletest.rc-mw69m" in namespace "gc-1567"
    Jan 30 11:50:18.032: INFO: Deleting pod "simpletest.rc-mwfmz" in namespace "gc-1567"
    Jan 30 11:50:18.081: INFO: Deleting pod "simpletest.rc-mz526" in namespace "gc-1567"
    Jan 30 11:50:18.132: INFO: Deleting pod "simpletest.rc-ns99w" in namespace "gc-1567"
    Jan 30 11:50:18.180: INFO: Deleting pod "simpletest.rc-nwznh" in namespace "gc-1567"
    Jan 30 11:50:18.231: INFO: Deleting pod "simpletest.rc-p2hwm" in namespace "gc-1567"
    Jan 30 11:50:18.282: INFO: Deleting pod "simpletest.rc-p77ws" in namespace "gc-1567"
    Jan 30 11:50:18.331: INFO: Deleting pod "simpletest.rc-q7xpp" in namespace "gc-1567"
    Jan 30 11:50:18.381: INFO: Deleting pod "simpletest.rc-qbvsk" in namespace "gc-1567"
    Jan 30 11:50:18.431: INFO: Deleting pod "simpletest.rc-qf787" in namespace "gc-1567"
    Jan 30 11:50:18.481: INFO: Deleting pod "simpletest.rc-qfwsr" in namespace "gc-1567"
    Jan 30 11:50:18.531: INFO: Deleting pod "simpletest.rc-qg7ng" in namespace "gc-1567"
    Jan 30 11:50:18.580: INFO: Deleting pod "simpletest.rc-qh98w" in namespace "gc-1567"
    Jan 30 11:50:18.631: INFO: Deleting pod "simpletest.rc-qk9zx" in namespace "gc-1567"
    Jan 30 11:50:18.681: INFO: Deleting pod "simpletest.rc-qpqk8" in namespace "gc-1567"
    Jan 30 11:50:18.731: INFO: Deleting pod "simpletest.rc-qvlpx" in namespace "gc-1567"
    Jan 30 11:50:18.782: INFO: Deleting pod "simpletest.rc-r75fx" in namespace "gc-1567"
    Jan 30 11:50:18.831: INFO: Deleting pod "simpletest.rc-rmss9" in namespace "gc-1567"
    Jan 30 11:50:18.881: INFO: Deleting pod "simpletest.rc-rzp4z" in namespace "gc-1567"
    Jan 30 11:50:18.932: INFO: Deleting pod "simpletest.rc-rzrck" in namespace "gc-1567"
    Jan 30 11:50:18.981: INFO: Deleting pod "simpletest.rc-s4jhf" in namespace "gc-1567"
    Jan 30 11:50:19.032: INFO: Deleting pod "simpletest.rc-svf7v" in namespace "gc-1567"
    Jan 30 11:50:19.082: INFO: Deleting pod "simpletest.rc-swcrf" in namespace "gc-1567"
    Jan 30 11:50:19.131: INFO: Deleting pod "simpletest.rc-szk78" in namespace "gc-1567"
    Jan 30 11:50:19.181: INFO: Deleting pod "simpletest.rc-t88tr" in namespace "gc-1567"
    Jan 30 11:50:19.231: INFO: Deleting pod "simpletest.rc-tsqwq" in namespace "gc-1567"
    Jan 30 11:50:19.281: INFO: Deleting pod "simpletest.rc-vbsk6" in namespace "gc-1567"
    Jan 30 11:50:19.335: INFO: Deleting pod "simpletest.rc-vftpd" in namespace "gc-1567"
    Jan 30 11:50:19.381: INFO: Deleting pod "simpletest.rc-vltp9" in namespace "gc-1567"
    Jan 30 11:50:19.431: INFO: Deleting pod "simpletest.rc-vlz89" in namespace "gc-1567"
    Jan 30 11:50:19.480: INFO: Deleting pod "simpletest.rc-vnxml" in namespace "gc-1567"
    Jan 30 11:50:19.531: INFO: Deleting pod "simpletest.rc-w42x7" in namespace "gc-1567"
    Jan 30 11:50:19.585: INFO: Deleting pod "simpletest.rc-whhwf" in namespace "gc-1567"
    Jan 30 11:50:19.631: INFO: Deleting pod "simpletest.rc-wx8pb" in namespace "gc-1567"
    Jan 30 11:50:19.683: INFO: Deleting pod "simpletest.rc-x8xfl" in namespace "gc-1567"
    Jan 30 11:50:19.732: INFO: Deleting pod "simpletest.rc-x9m44" in namespace "gc-1567"
    Jan 30 11:50:19.780: INFO: Deleting pod "simpletest.rc-xnxdk" in namespace "gc-1567"
    Jan 30 11:50:19.832: INFO: Deleting pod "simpletest.rc-xrh8d" in namespace "gc-1567"
    Jan 30 11:50:19.882: INFO: Deleting pod "simpletest.rc-xs7pg" in namespace "gc-1567"
    Jan 30 11:50:19.931: INFO: Deleting pod "simpletest.rc-xtxnm" in namespace "gc-1567"
    Jan 30 11:50:19.981: INFO: Deleting pod "simpletest.rc-zkvrt" in namespace "gc-1567"
    Jan 30 11:50:20.033: INFO: Deleting pod "simpletest.rc-zkwr8" in namespace "gc-1567"
    Jan 30 11:50:20.083: INFO: Deleting pod "simpletest.rc-zwzbk" in namespace "gc-1567"
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Jan 30 11:50:20.131: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-1567" for this suite. 01/30/23 11:50:20.18
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a read only busybox container
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:184
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 11:50:20.241
Jan 30 11:50:20.241: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename kubelet-test 01/30/23 11:50:20.242
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:50:20.251
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:50:20.253
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:184
Jan 30 11:50:20.260: INFO: Waiting up to 5m0s for pod "busybox-readonly-fs2fb9c192-1a26-4b8e-87dd-c156a8f6a1d3" in namespace "kubelet-test-3690" to be "running and ready"
Jan 30 11:50:20.262: INFO: Pod "busybox-readonly-fs2fb9c192-1a26-4b8e-87dd-c156a8f6a1d3": Phase="Pending", Reason="", readiness=false. Elapsed: 1.685112ms
Jan 30 11:50:20.262: INFO: The phase of Pod busybox-readonly-fs2fb9c192-1a26-4b8e-87dd-c156a8f6a1d3 is Pending, waiting for it to be Running (with Ready = true)
Jan 30 11:50:22.264: INFO: Pod "busybox-readonly-fs2fb9c192-1a26-4b8e-87dd-c156a8f6a1d3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004302535s
Jan 30 11:50:22.264: INFO: The phase of Pod busybox-readonly-fs2fb9c192-1a26-4b8e-87dd-c156a8f6a1d3 is Pending, waiting for it to be Running (with Ready = true)
Jan 30 11:50:24.265: INFO: Pod "busybox-readonly-fs2fb9c192-1a26-4b8e-87dd-c156a8f6a1d3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.005193388s
Jan 30 11:50:24.265: INFO: The phase of Pod busybox-readonly-fs2fb9c192-1a26-4b8e-87dd-c156a8f6a1d3 is Pending, waiting for it to be Running (with Ready = true)
Jan 30 11:50:26.265: INFO: Pod "busybox-readonly-fs2fb9c192-1a26-4b8e-87dd-c156a8f6a1d3": Phase="Pending", Reason="", readiness=false. Elapsed: 6.004657254s
Jan 30 11:50:26.265: INFO: The phase of Pod busybox-readonly-fs2fb9c192-1a26-4b8e-87dd-c156a8f6a1d3 is Pending, waiting for it to be Running (with Ready = true)
Jan 30 11:50:28.264: INFO: Pod "busybox-readonly-fs2fb9c192-1a26-4b8e-87dd-c156a8f6a1d3": Phase="Running", Reason="", readiness=true. Elapsed: 8.004308436s
Jan 30 11:50:28.264: INFO: The phase of Pod busybox-readonly-fs2fb9c192-1a26-4b8e-87dd-c156a8f6a1d3 is Running (Ready = true)
Jan 30 11:50:28.264: INFO: Pod "busybox-readonly-fs2fb9c192-1a26-4b8e-87dd-c156a8f6a1d3" satisfied condition "running and ready"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Jan 30 11:50:28.271: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-3690" for this suite. 01/30/23 11:50:28.273
------------------------------
• [SLOW TEST] [8.035 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a read only busybox container
  test/e2e/common/node/kubelet.go:175
    should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 11:50:20.241
    Jan 30 11:50:20.241: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename kubelet-test 01/30/23 11:50:20.242
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:50:20.251
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:50:20.253
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:184
    Jan 30 11:50:20.260: INFO: Waiting up to 5m0s for pod "busybox-readonly-fs2fb9c192-1a26-4b8e-87dd-c156a8f6a1d3" in namespace "kubelet-test-3690" to be "running and ready"
    Jan 30 11:50:20.262: INFO: Pod "busybox-readonly-fs2fb9c192-1a26-4b8e-87dd-c156a8f6a1d3": Phase="Pending", Reason="", readiness=false. Elapsed: 1.685112ms
    Jan 30 11:50:20.262: INFO: The phase of Pod busybox-readonly-fs2fb9c192-1a26-4b8e-87dd-c156a8f6a1d3 is Pending, waiting for it to be Running (with Ready = true)
    Jan 30 11:50:22.264: INFO: Pod "busybox-readonly-fs2fb9c192-1a26-4b8e-87dd-c156a8f6a1d3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004302535s
    Jan 30 11:50:22.264: INFO: The phase of Pod busybox-readonly-fs2fb9c192-1a26-4b8e-87dd-c156a8f6a1d3 is Pending, waiting for it to be Running (with Ready = true)
    Jan 30 11:50:24.265: INFO: Pod "busybox-readonly-fs2fb9c192-1a26-4b8e-87dd-c156a8f6a1d3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.005193388s
    Jan 30 11:50:24.265: INFO: The phase of Pod busybox-readonly-fs2fb9c192-1a26-4b8e-87dd-c156a8f6a1d3 is Pending, waiting for it to be Running (with Ready = true)
    Jan 30 11:50:26.265: INFO: Pod "busybox-readonly-fs2fb9c192-1a26-4b8e-87dd-c156a8f6a1d3": Phase="Pending", Reason="", readiness=false. Elapsed: 6.004657254s
    Jan 30 11:50:26.265: INFO: The phase of Pod busybox-readonly-fs2fb9c192-1a26-4b8e-87dd-c156a8f6a1d3 is Pending, waiting for it to be Running (with Ready = true)
    Jan 30 11:50:28.264: INFO: Pod "busybox-readonly-fs2fb9c192-1a26-4b8e-87dd-c156a8f6a1d3": Phase="Running", Reason="", readiness=true. Elapsed: 8.004308436s
    Jan 30 11:50:28.264: INFO: The phase of Pod busybox-readonly-fs2fb9c192-1a26-4b8e-87dd-c156a8f6a1d3 is Running (Ready = true)
    Jan 30 11:50:28.264: INFO: Pod "busybox-readonly-fs2fb9c192-1a26-4b8e-87dd-c156a8f6a1d3" satisfied condition "running and ready"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Jan 30 11:50:28.271: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-3690" for this suite. 01/30/23 11:50:28.273
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:90
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 11:50:28.276
Jan 30 11:50:28.277: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename downward-api 01/30/23 11:50:28.277
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:50:28.284
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:50:28.286
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:90
STEP: Creating a pod to test downward api env vars 01/30/23 11:50:28.288
Jan 30 11:50:28.292: INFO: Waiting up to 5m0s for pod "downward-api-09b8ac00-725f-4cec-804a-a16f1bd95440" in namespace "downward-api-1194" to be "Succeeded or Failed"
Jan 30 11:50:28.294: INFO: Pod "downward-api-09b8ac00-725f-4cec-804a-a16f1bd95440": Phase="Pending", Reason="", readiness=false. Elapsed: 1.702221ms
Jan 30 11:50:30.296: INFO: Pod "downward-api-09b8ac00-725f-4cec-804a-a16f1bd95440": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004300652s
Jan 30 11:50:32.297: INFO: Pod "downward-api-09b8ac00-725f-4cec-804a-a16f1bd95440": Phase="Pending", Reason="", readiness=false. Elapsed: 4.005277359s
Jan 30 11:50:34.298: INFO: Pod "downward-api-09b8ac00-725f-4cec-804a-a16f1bd95440": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.005874204s
STEP: Saw pod success 01/30/23 11:50:34.298
Jan 30 11:50:34.298: INFO: Pod "downward-api-09b8ac00-725f-4cec-804a-a16f1bd95440" satisfied condition "Succeeded or Failed"
Jan 30 11:50:34.300: INFO: Trying to get logs from node pubt2-nks-for-dev1.dg.163.org pod downward-api-09b8ac00-725f-4cec-804a-a16f1bd95440 container dapi-container: <nil>
STEP: delete the pod 01/30/23 11:50:34.306
Jan 30 11:50:34.310: INFO: Waiting for pod downward-api-09b8ac00-725f-4cec-804a-a16f1bd95440 to disappear
Jan 30 11:50:34.312: INFO: Pod downward-api-09b8ac00-725f-4cec-804a-a16f1bd95440 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Jan 30 11:50:34.312: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-1194" for this suite. 01/30/23 11:50:34.314
------------------------------
• [SLOW TEST] [6.040 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:90

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 11:50:28.276
    Jan 30 11:50:28.277: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename downward-api 01/30/23 11:50:28.277
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:50:28.284
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:50:28.286
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide host IP as an env var [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:90
    STEP: Creating a pod to test downward api env vars 01/30/23 11:50:28.288
    Jan 30 11:50:28.292: INFO: Waiting up to 5m0s for pod "downward-api-09b8ac00-725f-4cec-804a-a16f1bd95440" in namespace "downward-api-1194" to be "Succeeded or Failed"
    Jan 30 11:50:28.294: INFO: Pod "downward-api-09b8ac00-725f-4cec-804a-a16f1bd95440": Phase="Pending", Reason="", readiness=false. Elapsed: 1.702221ms
    Jan 30 11:50:30.296: INFO: Pod "downward-api-09b8ac00-725f-4cec-804a-a16f1bd95440": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004300652s
    Jan 30 11:50:32.297: INFO: Pod "downward-api-09b8ac00-725f-4cec-804a-a16f1bd95440": Phase="Pending", Reason="", readiness=false. Elapsed: 4.005277359s
    Jan 30 11:50:34.298: INFO: Pod "downward-api-09b8ac00-725f-4cec-804a-a16f1bd95440": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.005874204s
    STEP: Saw pod success 01/30/23 11:50:34.298
    Jan 30 11:50:34.298: INFO: Pod "downward-api-09b8ac00-725f-4cec-804a-a16f1bd95440" satisfied condition "Succeeded or Failed"
    Jan 30 11:50:34.300: INFO: Trying to get logs from node pubt2-nks-for-dev1.dg.163.org pod downward-api-09b8ac00-725f-4cec-804a-a16f1bd95440 container dapi-container: <nil>
    STEP: delete the pod 01/30/23 11:50:34.306
    Jan 30 11:50:34.310: INFO: Waiting for pod downward-api-09b8ac00-725f-4cec-804a-a16f1bd95440 to disappear
    Jan 30 11:50:34.312: INFO: Pod downward-api-09b8ac00-725f-4cec-804a-a16f1bd95440 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Jan 30 11:50:34.312: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-1194" for this suite. 01/30/23 11:50:34.314
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:207
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 11:50:34.317
Jan 30 11:50:34.317: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename downward-api 01/30/23 11:50:34.318
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:50:34.324
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:50:34.326
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:207
STEP: Creating a pod to test downward API volume plugin 01/30/23 11:50:34.328
Jan 30 11:50:34.333: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b1026abe-ab97-40f6-893b-291f81e830fd" in namespace "downward-api-99" to be "Succeeded or Failed"
Jan 30 11:50:34.335: INFO: Pod "downwardapi-volume-b1026abe-ab97-40f6-893b-291f81e830fd": Phase="Pending", Reason="", readiness=false. Elapsed: 1.611458ms
Jan 30 11:50:36.338: INFO: Pod "downwardapi-volume-b1026abe-ab97-40f6-893b-291f81e830fd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004979665s
Jan 30 11:50:38.338: INFO: Pod "downwardapi-volume-b1026abe-ab97-40f6-893b-291f81e830fd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005077854s
STEP: Saw pod success 01/30/23 11:50:38.338
Jan 30 11:50:38.338: INFO: Pod "downwardapi-volume-b1026abe-ab97-40f6-893b-291f81e830fd" satisfied condition "Succeeded or Failed"
Jan 30 11:50:38.340: INFO: Trying to get logs from node pubt2-nks-for-dev1.dg.163.org pod downwardapi-volume-b1026abe-ab97-40f6-893b-291f81e830fd container client-container: <nil>
STEP: delete the pod 01/30/23 11:50:38.344
Jan 30 11:50:38.349: INFO: Waiting for pod downwardapi-volume-b1026abe-ab97-40f6-893b-291f81e830fd to disappear
Jan 30 11:50:38.350: INFO: Pod downwardapi-volume-b1026abe-ab97-40f6-893b-291f81e830fd no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Jan 30 11:50:38.351: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-99" for this suite. 01/30/23 11:50:38.353
------------------------------
• [4.038 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 11:50:34.317
    Jan 30 11:50:34.317: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename downward-api 01/30/23 11:50:34.318
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:50:34.324
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:50:34.326
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide container's memory limit [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:207
    STEP: Creating a pod to test downward API volume plugin 01/30/23 11:50:34.328
    Jan 30 11:50:34.333: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b1026abe-ab97-40f6-893b-291f81e830fd" in namespace "downward-api-99" to be "Succeeded or Failed"
    Jan 30 11:50:34.335: INFO: Pod "downwardapi-volume-b1026abe-ab97-40f6-893b-291f81e830fd": Phase="Pending", Reason="", readiness=false. Elapsed: 1.611458ms
    Jan 30 11:50:36.338: INFO: Pod "downwardapi-volume-b1026abe-ab97-40f6-893b-291f81e830fd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004979665s
    Jan 30 11:50:38.338: INFO: Pod "downwardapi-volume-b1026abe-ab97-40f6-893b-291f81e830fd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005077854s
    STEP: Saw pod success 01/30/23 11:50:38.338
    Jan 30 11:50:38.338: INFO: Pod "downwardapi-volume-b1026abe-ab97-40f6-893b-291f81e830fd" satisfied condition "Succeeded or Failed"
    Jan 30 11:50:38.340: INFO: Trying to get logs from node pubt2-nks-for-dev1.dg.163.org pod downwardapi-volume-b1026abe-ab97-40f6-893b-291f81e830fd container client-container: <nil>
    STEP: delete the pod 01/30/23 11:50:38.344
    Jan 30 11:50:38.349: INFO: Waiting for pod downwardapi-volume-b1026abe-ab97-40f6-893b-291f81e830fd to disappear
    Jan 30 11:50:38.350: INFO: Pod downwardapi-volume-b1026abe-ab97-40f6-893b-291f81e830fd no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Jan 30 11:50:38.351: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-99" for this suite. 01/30/23 11:50:38.353
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:248
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 11:50:38.356
Jan 30 11:50:38.356: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename container-runtime 01/30/23 11:50:38.357
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:50:38.363
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:50:38.365
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:248
STEP: create the container 01/30/23 11:50:38.367
STEP: wait for the container to reach Succeeded 01/30/23 11:50:38.372
STEP: get the container status 01/30/23 11:50:42.381
STEP: the container should be terminated 01/30/23 11:50:42.383
STEP: the termination message should be set 01/30/23 11:50:42.383
Jan 30 11:50:42.383: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container 01/30/23 11:50:42.383
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Jan 30 11:50:42.388: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-300" for this suite. 01/30/23 11:50:42.391
------------------------------
• [4.037 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    on terminated container
    test/e2e/common/node/runtime.go:137
      should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 11:50:38.356
    Jan 30 11:50:38.356: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename container-runtime 01/30/23 11:50:38.357
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:50:38.363
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:50:38.365
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:248
    STEP: create the container 01/30/23 11:50:38.367
    STEP: wait for the container to reach Succeeded 01/30/23 11:50:38.372
    STEP: get the container status 01/30/23 11:50:42.381
    STEP: the container should be terminated 01/30/23 11:50:42.383
    STEP: the termination message should be set 01/30/23 11:50:42.383
    Jan 30 11:50:42.383: INFO: Expected: &{OK} to match Container's Termination Message: OK --
    STEP: delete the container 01/30/23 11:50:42.383
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Jan 30 11:50:42.388: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-300" for this suite. 01/30/23 11:50:42.391
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-apps] Deployment
  RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 11:50:42.393
Jan 30 11:50:42.393: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename deployment 01/30/23 11:50:42.394
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:50:42.4
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:50:42.401
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113
Jan 30 11:50:42.403: INFO: Creating deployment "test-recreate-deployment"
Jan 30 11:50:42.406: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Jan 30 11:50:42.409: INFO: new replicaset for deployment "test-recreate-deployment" is yet to be created
Jan 30 11:50:44.415: INFO: Waiting deployment "test-recreate-deployment" to complete
Jan 30 11:50:44.417: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Jan 30 11:50:44.422: INFO: Updating deployment test-recreate-deployment
Jan 30 11:50:44.422: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan 30 11:50:44.459: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-8470  5d0cd533-5020-464e-b7b9-b7f39ac8ac27 22689 2 2023-01-30 11:50:42 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-01-30 11:50:44 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-30 11:50:44 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] [] []} {[] [] [{httpd harbor.cloud.netease.com/qzprod-k8s/e2e/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc006adbe08 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-01-30 11:50:44 +0000 UTC,LastTransitionTime:2023-01-30 11:50:44 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-58766bf649" is progressing.,LastUpdateTime:2023-01-30 11:50:44 +0000 UTC,LastTransitionTime:2023-01-30 11:50:42 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Jan 30 11:50:44.461: INFO: New ReplicaSet "test-recreate-deployment-58766bf649" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-58766bf649  deployment-8470  0f4ca709-1da7-469d-883b-801426ce91f5 22686 1 2023-01-30 11:50:44 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:58766bf649] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 5d0cd533-5020-464e-b7b9-b7f39ac8ac27 0xc009b7c2f7 0xc009b7c2f8}] [] [{kube-controller-manager Update apps/v1 2023-01-30 11:50:44 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5d0cd533-5020-464e-b7b9-b7f39ac8ac27\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-30 11:50:44 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 58766bf649,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:58766bf649] map[] [] [] []} {[] [] [{httpd harbor.cloud.netease.com/qzprod-k8s/e2e/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc009b7c398 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan 30 11:50:44.461: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Jan 30 11:50:44.461: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-79954b4b8d  deployment-8470  a057a192-0f92-4121-affc-3f0fc9727147 22677 2 2023-01-30 11:50:42 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:79954b4b8d] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 5d0cd533-5020-464e-b7b9-b7f39ac8ac27 0xc009b7c407 0xc009b7c408}] [] [{kube-controller-manager Update apps/v1 2023-01-30 11:50:44 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5d0cd533-5020-464e-b7b9-b7f39ac8ac27\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-30 11:50:44 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 79954b4b8d,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:79954b4b8d] map[] [] [] []} {[] [] [{agnhost harbor.cloud.netease.com/qzprod-k8s/e2e/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc009b7c4b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan 30 11:50:44.463: INFO: Pod "test-recreate-deployment-58766bf649-fq8cc" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-58766bf649-fq8cc test-recreate-deployment-58766bf649- deployment-8470  31947bfe-42f5-4019-a3c6-e9d65753f63e 22688 0 2023-01-30 11:50:44 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:58766bf649] map[] [{apps/v1 ReplicaSet test-recreate-deployment-58766bf649 0f4ca709-1da7-469d-883b-801426ce91f5 0xc004c780c7 0xc004c780c8}] [] [{kube-controller-manager Update v1 2023-01-30 11:50:44 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0f4ca709-1da7-469d-883b-801426ce91f5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-30 11:50:44 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9f692,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:harbor.cloud.netease.com/qzprod-k8s/e2e/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9f692,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pubt2-nks-for-dev1.dg.163.org,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 11:50:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 11:50:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 11:50:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 11:50:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.182.0.82,PodIP:,StartTime:2023-01-30 11:50:44 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:harbor.cloud.netease.com/qzprod-k8s/e2e/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Jan 30 11:50:44.463: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-8470" for this suite. 01/30/23 11:50:44.465
------------------------------
• [2.074 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 11:50:42.393
    Jan 30 11:50:42.393: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename deployment 01/30/23 11:50:42.394
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:50:42.4
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:50:42.401
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] RecreateDeployment should delete old pods and create new ones [Conformance]
      test/e2e/apps/deployment.go:113
    Jan 30 11:50:42.403: INFO: Creating deployment "test-recreate-deployment"
    Jan 30 11:50:42.406: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
    Jan 30 11:50:42.409: INFO: new replicaset for deployment "test-recreate-deployment" is yet to be created
    Jan 30 11:50:44.415: INFO: Waiting deployment "test-recreate-deployment" to complete
    Jan 30 11:50:44.417: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
    Jan 30 11:50:44.422: INFO: Updating deployment test-recreate-deployment
    Jan 30 11:50:44.422: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jan 30 11:50:44.459: INFO: Deployment "test-recreate-deployment":
    &Deployment{ObjectMeta:{test-recreate-deployment  deployment-8470  5d0cd533-5020-464e-b7b9-b7f39ac8ac27 22689 2 2023-01-30 11:50:42 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-01-30 11:50:44 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-30 11:50:44 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] [] []} {[] [] [{httpd harbor.cloud.netease.com/qzprod-k8s/e2e/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc006adbe08 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-01-30 11:50:44 +0000 UTC,LastTransitionTime:2023-01-30 11:50:44 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-58766bf649" is progressing.,LastUpdateTime:2023-01-30 11:50:44 +0000 UTC,LastTransitionTime:2023-01-30 11:50:42 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

    Jan 30 11:50:44.461: INFO: New ReplicaSet "test-recreate-deployment-58766bf649" of Deployment "test-recreate-deployment":
    &ReplicaSet{ObjectMeta:{test-recreate-deployment-58766bf649  deployment-8470  0f4ca709-1da7-469d-883b-801426ce91f5 22686 1 2023-01-30 11:50:44 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:58766bf649] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 5d0cd533-5020-464e-b7b9-b7f39ac8ac27 0xc009b7c2f7 0xc009b7c2f8}] [] [{kube-controller-manager Update apps/v1 2023-01-30 11:50:44 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5d0cd533-5020-464e-b7b9-b7f39ac8ac27\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-30 11:50:44 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 58766bf649,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:58766bf649] map[] [] [] []} {[] [] [{httpd harbor.cloud.netease.com/qzprod-k8s/e2e/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc009b7c398 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Jan 30 11:50:44.461: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
    Jan 30 11:50:44.461: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-79954b4b8d  deployment-8470  a057a192-0f92-4121-affc-3f0fc9727147 22677 2 2023-01-30 11:50:42 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:79954b4b8d] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 5d0cd533-5020-464e-b7b9-b7f39ac8ac27 0xc009b7c407 0xc009b7c408}] [] [{kube-controller-manager Update apps/v1 2023-01-30 11:50:44 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5d0cd533-5020-464e-b7b9-b7f39ac8ac27\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-30 11:50:44 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 79954b4b8d,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:79954b4b8d] map[] [] [] []} {[] [] [{agnhost harbor.cloud.netease.com/qzprod-k8s/e2e/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc009b7c4b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Jan 30 11:50:44.463: INFO: Pod "test-recreate-deployment-58766bf649-fq8cc" is not available:
    &Pod{ObjectMeta:{test-recreate-deployment-58766bf649-fq8cc test-recreate-deployment-58766bf649- deployment-8470  31947bfe-42f5-4019-a3c6-e9d65753f63e 22688 0 2023-01-30 11:50:44 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:58766bf649] map[] [{apps/v1 ReplicaSet test-recreate-deployment-58766bf649 0f4ca709-1da7-469d-883b-801426ce91f5 0xc004c780c7 0xc004c780c8}] [] [{kube-controller-manager Update v1 2023-01-30 11:50:44 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0f4ca709-1da7-469d-883b-801426ce91f5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-30 11:50:44 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9f692,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:harbor.cloud.netease.com/qzprod-k8s/e2e/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9f692,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pubt2-nks-for-dev1.dg.163.org,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 11:50:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 11:50:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 11:50:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 11:50:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.182.0.82,PodIP:,StartTime:2023-01-30 11:50:44 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:harbor.cloud.netease.com/qzprod-k8s/e2e/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Jan 30 11:50:44.463: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-8470" for this suite. 01/30/23 11:50:44.465
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod
  should be possible to delete [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:135
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 11:50:44.468
Jan 30 11:50:44.468: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename kubelet-test 01/30/23 11:50:44.469
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:50:44.475
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:50:44.477
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[BeforeEach] when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:85
[It] should be possible to delete [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:135
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Jan 30 11:50:44.486: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-5139" for this suite. 01/30/23 11:50:44.488
------------------------------
• [0.022 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:82
    should be possible to delete [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:135

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 11:50:44.468
    Jan 30 11:50:44.468: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename kubelet-test 01/30/23 11:50:44.469
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:50:44.475
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:50:44.477
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [BeforeEach] when scheduling a busybox command that always fails in a pod
      test/e2e/common/node/kubelet.go:85
    [It] should be possible to delete [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:135
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Jan 30 11:50:44.486: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-5139" for this suite. 01/30/23 11:50:44.488
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Containers
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:87
[BeforeEach] [sig-node] Containers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 11:50:44.491
Jan 30 11:50:44.491: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename containers 01/30/23 11:50:44.491
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:50:44.497
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:50:44.499
[BeforeEach] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:87
STEP: Creating a pod to test override all 01/30/23 11:50:44.5
Jan 30 11:50:44.504: INFO: Waiting up to 5m0s for pod "client-containers-b10ce6a6-a00c-4f0c-989d-90a7c3c216b5" in namespace "containers-4427" to be "Succeeded or Failed"
Jan 30 11:50:44.506: INFO: Pod "client-containers-b10ce6a6-a00c-4f0c-989d-90a7c3c216b5": Phase="Pending", Reason="", readiness=false. Elapsed: 1.481743ms
Jan 30 11:50:46.509: INFO: Pod "client-containers-b10ce6a6-a00c-4f0c-989d-90a7c3c216b5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004421423s
Jan 30 11:50:48.508: INFO: Pod "client-containers-b10ce6a6-a00c-4f0c-989d-90a7c3c216b5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004136157s
STEP: Saw pod success 01/30/23 11:50:48.508
Jan 30 11:50:48.508: INFO: Pod "client-containers-b10ce6a6-a00c-4f0c-989d-90a7c3c216b5" satisfied condition "Succeeded or Failed"
Jan 30 11:50:48.510: INFO: Trying to get logs from node pubt2-nks-for-dev1.dg.163.org pod client-containers-b10ce6a6-a00c-4f0c-989d-90a7c3c216b5 container agnhost-container: <nil>
STEP: delete the pod 01/30/23 11:50:48.515
Jan 30 11:50:48.519: INFO: Waiting for pod client-containers-b10ce6a6-a00c-4f0c-989d-90a7c3c216b5 to disappear
Jan 30 11:50:48.520: INFO: Pod client-containers-b10ce6a6-a00c-4f0c-989d-90a7c3c216b5 no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/node/init/init.go:32
Jan 30 11:50:48.520: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Containers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Containers
  tear down framework | framework.go:193
STEP: Destroying namespace "containers-4427" for this suite. 01/30/23 11:50:48.523
------------------------------
• [4.034 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:87

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 11:50:44.491
    Jan 30 11:50:44.491: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename containers 01/30/23 11:50:44.491
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:50:44.497
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:50:44.499
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:87
    STEP: Creating a pod to test override all 01/30/23 11:50:44.5
    Jan 30 11:50:44.504: INFO: Waiting up to 5m0s for pod "client-containers-b10ce6a6-a00c-4f0c-989d-90a7c3c216b5" in namespace "containers-4427" to be "Succeeded or Failed"
    Jan 30 11:50:44.506: INFO: Pod "client-containers-b10ce6a6-a00c-4f0c-989d-90a7c3c216b5": Phase="Pending", Reason="", readiness=false. Elapsed: 1.481743ms
    Jan 30 11:50:46.509: INFO: Pod "client-containers-b10ce6a6-a00c-4f0c-989d-90a7c3c216b5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004421423s
    Jan 30 11:50:48.508: INFO: Pod "client-containers-b10ce6a6-a00c-4f0c-989d-90a7c3c216b5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004136157s
    STEP: Saw pod success 01/30/23 11:50:48.508
    Jan 30 11:50:48.508: INFO: Pod "client-containers-b10ce6a6-a00c-4f0c-989d-90a7c3c216b5" satisfied condition "Succeeded or Failed"
    Jan 30 11:50:48.510: INFO: Trying to get logs from node pubt2-nks-for-dev1.dg.163.org pod client-containers-b10ce6a6-a00c-4f0c-989d-90a7c3c216b5 container agnhost-container: <nil>
    STEP: delete the pod 01/30/23 11:50:48.515
    Jan 30 11:50:48.519: INFO: Waiting for pod client-containers-b10ce6a6-a00c-4f0c-989d-90a7c3c216b5 to disappear
    Jan 30 11:50:48.520: INFO: Pod client-containers-b10ce6a6-a00c-4f0c-989d-90a7c3c216b5 no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/node/init/init.go:32
    Jan 30 11:50:48.520: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Containers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Containers
      tear down framework | framework.go:193
    STEP: Destroying namespace "containers-4427" for this suite. 01/30/23 11:50:48.523
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-network] Proxy version v1
  A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  test/e2e/network/proxy.go:286
[BeforeEach] version v1
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 11:50:48.525
Jan 30 11:50:48.525: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename proxy 01/30/23 11:50:48.526
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:50:48.531
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:50:48.533
[BeforeEach] version v1
  test/e2e/framework/metrics/init/init.go:31
[It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  test/e2e/network/proxy.go:286
Jan 30 11:50:48.535: INFO: Creating pod...
Jan 30 11:50:48.539: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-6807" to be "running"
Jan 30 11:50:48.541: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 1.506182ms
Jan 30 11:50:50.544: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.00433643s
Jan 30 11:50:50.544: INFO: Pod "agnhost" satisfied condition "running"
Jan 30 11:50:50.544: INFO: Creating service...
Jan 30 11:50:50.547: INFO: Starting http.Client for https://10.178.64.1:443/api/v1/namespaces/proxy-6807/pods/agnhost/proxy/some/path/with/DELETE
Jan 30 11:50:50.550: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Jan 30 11:50:50.550: INFO: Starting http.Client for https://10.178.64.1:443/api/v1/namespaces/proxy-6807/pods/agnhost/proxy/some/path/with/GET
Jan 30 11:50:50.552: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Jan 30 11:50:50.552: INFO: Starting http.Client for https://10.178.64.1:443/api/v1/namespaces/proxy-6807/pods/agnhost/proxy/some/path/with/HEAD
Jan 30 11:50:50.554: INFO: http.Client request:HEAD | StatusCode:200
Jan 30 11:50:50.554: INFO: Starting http.Client for https://10.178.64.1:443/api/v1/namespaces/proxy-6807/pods/agnhost/proxy/some/path/with/OPTIONS
Jan 30 11:50:50.556: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Jan 30 11:50:50.556: INFO: Starting http.Client for https://10.178.64.1:443/api/v1/namespaces/proxy-6807/pods/agnhost/proxy/some/path/with/PATCH
Jan 30 11:50:50.557: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Jan 30 11:50:50.557: INFO: Starting http.Client for https://10.178.64.1:443/api/v1/namespaces/proxy-6807/pods/agnhost/proxy/some/path/with/POST
Jan 30 11:50:50.559: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Jan 30 11:50:50.559: INFO: Starting http.Client for https://10.178.64.1:443/api/v1/namespaces/proxy-6807/pods/agnhost/proxy/some/path/with/PUT
Jan 30 11:50:50.561: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Jan 30 11:50:50.561: INFO: Starting http.Client for https://10.178.64.1:443/api/v1/namespaces/proxy-6807/services/test-service/proxy/some/path/with/DELETE
Jan 30 11:50:50.563: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Jan 30 11:50:50.563: INFO: Starting http.Client for https://10.178.64.1:443/api/v1/namespaces/proxy-6807/services/test-service/proxy/some/path/with/GET
Jan 30 11:50:50.565: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Jan 30 11:50:50.565: INFO: Starting http.Client for https://10.178.64.1:443/api/v1/namespaces/proxy-6807/services/test-service/proxy/some/path/with/HEAD
Jan 30 11:50:50.567: INFO: http.Client request:HEAD | StatusCode:200
Jan 30 11:50:50.567: INFO: Starting http.Client for https://10.178.64.1:443/api/v1/namespaces/proxy-6807/services/test-service/proxy/some/path/with/OPTIONS
Jan 30 11:50:50.569: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Jan 30 11:50:50.569: INFO: Starting http.Client for https://10.178.64.1:443/api/v1/namespaces/proxy-6807/services/test-service/proxy/some/path/with/PATCH
Jan 30 11:50:50.572: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Jan 30 11:50:50.572: INFO: Starting http.Client for https://10.178.64.1:443/api/v1/namespaces/proxy-6807/services/test-service/proxy/some/path/with/POST
Jan 30 11:50:50.574: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Jan 30 11:50:50.574: INFO: Starting http.Client for https://10.178.64.1:443/api/v1/namespaces/proxy-6807/services/test-service/proxy/some/path/with/PUT
Jan 30 11:50:50.576: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
[AfterEach] version v1
  test/e2e/framework/node/init/init.go:32
Jan 30 11:50:50.576: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] version v1
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] version v1
  dump namespaces | framework.go:196
[DeferCleanup (Each)] version v1
  tear down framework | framework.go:193
STEP: Destroying namespace "proxy-6807" for this suite. 01/30/23 11:50:50.578
------------------------------
• [2.055 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
    test/e2e/network/proxy.go:286

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 11:50:48.525
    Jan 30 11:50:48.525: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename proxy 01/30/23 11:50:48.526
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:50:48.531
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:50:48.533
    [BeforeEach] version v1
      test/e2e/framework/metrics/init/init.go:31
    [It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
      test/e2e/network/proxy.go:286
    Jan 30 11:50:48.535: INFO: Creating pod...
    Jan 30 11:50:48.539: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-6807" to be "running"
    Jan 30 11:50:48.541: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 1.506182ms
    Jan 30 11:50:50.544: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.00433643s
    Jan 30 11:50:50.544: INFO: Pod "agnhost" satisfied condition "running"
    Jan 30 11:50:50.544: INFO: Creating service...
    Jan 30 11:50:50.547: INFO: Starting http.Client for https://10.178.64.1:443/api/v1/namespaces/proxy-6807/pods/agnhost/proxy/some/path/with/DELETE
    Jan 30 11:50:50.550: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Jan 30 11:50:50.550: INFO: Starting http.Client for https://10.178.64.1:443/api/v1/namespaces/proxy-6807/pods/agnhost/proxy/some/path/with/GET
    Jan 30 11:50:50.552: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
    Jan 30 11:50:50.552: INFO: Starting http.Client for https://10.178.64.1:443/api/v1/namespaces/proxy-6807/pods/agnhost/proxy/some/path/with/HEAD
    Jan 30 11:50:50.554: INFO: http.Client request:HEAD | StatusCode:200
    Jan 30 11:50:50.554: INFO: Starting http.Client for https://10.178.64.1:443/api/v1/namespaces/proxy-6807/pods/agnhost/proxy/some/path/with/OPTIONS
    Jan 30 11:50:50.556: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Jan 30 11:50:50.556: INFO: Starting http.Client for https://10.178.64.1:443/api/v1/namespaces/proxy-6807/pods/agnhost/proxy/some/path/with/PATCH
    Jan 30 11:50:50.557: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Jan 30 11:50:50.557: INFO: Starting http.Client for https://10.178.64.1:443/api/v1/namespaces/proxy-6807/pods/agnhost/proxy/some/path/with/POST
    Jan 30 11:50:50.559: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Jan 30 11:50:50.559: INFO: Starting http.Client for https://10.178.64.1:443/api/v1/namespaces/proxy-6807/pods/agnhost/proxy/some/path/with/PUT
    Jan 30 11:50:50.561: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Jan 30 11:50:50.561: INFO: Starting http.Client for https://10.178.64.1:443/api/v1/namespaces/proxy-6807/services/test-service/proxy/some/path/with/DELETE
    Jan 30 11:50:50.563: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Jan 30 11:50:50.563: INFO: Starting http.Client for https://10.178.64.1:443/api/v1/namespaces/proxy-6807/services/test-service/proxy/some/path/with/GET
    Jan 30 11:50:50.565: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
    Jan 30 11:50:50.565: INFO: Starting http.Client for https://10.178.64.1:443/api/v1/namespaces/proxy-6807/services/test-service/proxy/some/path/with/HEAD
    Jan 30 11:50:50.567: INFO: http.Client request:HEAD | StatusCode:200
    Jan 30 11:50:50.567: INFO: Starting http.Client for https://10.178.64.1:443/api/v1/namespaces/proxy-6807/services/test-service/proxy/some/path/with/OPTIONS
    Jan 30 11:50:50.569: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Jan 30 11:50:50.569: INFO: Starting http.Client for https://10.178.64.1:443/api/v1/namespaces/proxy-6807/services/test-service/proxy/some/path/with/PATCH
    Jan 30 11:50:50.572: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Jan 30 11:50:50.572: INFO: Starting http.Client for https://10.178.64.1:443/api/v1/namespaces/proxy-6807/services/test-service/proxy/some/path/with/POST
    Jan 30 11:50:50.574: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Jan 30 11:50:50.574: INFO: Starting http.Client for https://10.178.64.1:443/api/v1/namespaces/proxy-6807/services/test-service/proxy/some/path/with/PUT
    Jan 30 11:50:50.576: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    [AfterEach] version v1
      test/e2e/framework/node/init/init.go:32
    Jan 30 11:50:50.576: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] version v1
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] version v1
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] version v1
      tear down framework | framework.go:193
    STEP: Destroying namespace "proxy-6807" for this suite. 01/30/23 11:50:50.578
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:649
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 11:50:50.583
Jan 30 11:50:50.583: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename svcaccounts 01/30/23 11:50:50.584
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:50:50.591
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:50:50.593
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:649
STEP: creating a ServiceAccount 01/30/23 11:50:50.595
STEP: watching for the ServiceAccount to be added 01/30/23 11:50:50.598
STEP: patching the ServiceAccount 01/30/23 11:50:50.599
STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector) 01/30/23 11:50:50.603
STEP: deleting the ServiceAccount 01/30/23 11:50:50.605
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Jan 30 11:50:50.609: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-9791" for this suite. 01/30/23 11:50:50.611
------------------------------
• [0.030 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:649

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 11:50:50.583
    Jan 30 11:50:50.583: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename svcaccounts 01/30/23 11:50:50.584
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:50:50.591
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:50:50.593
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should run through the lifecycle of a ServiceAccount [Conformance]
      test/e2e/auth/service_accounts.go:649
    STEP: creating a ServiceAccount 01/30/23 11:50:50.595
    STEP: watching for the ServiceAccount to be added 01/30/23 11:50:50.598
    STEP: patching the ServiceAccount 01/30/23 11:50:50.599
    STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector) 01/30/23 11:50:50.603
    STEP: deleting the ServiceAccount 01/30/23 11:50:50.605
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Jan 30 11:50:50.609: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-9791" for this suite. 01/30/23 11:50:50.611
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:896
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 11:50:50.614
Jan 30 11:50:50.614: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename pods 01/30/23 11:50:50.614
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:50:50.62
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:50:50.622
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:896
STEP: creating a Pod with a static label 01/30/23 11:50:50.629
STEP: watching for Pod to be ready 01/30/23 11:50:50.633
Jan 30 11:50:50.634: INFO: observed Pod pod-test in namespace pods-9493 in phase Pending with labels: map[test-pod-static:true] & conditions []
Jan 30 11:50:50.636: INFO: observed Pod pod-test in namespace pods-9493 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-30 11:50:50 +0000 UTC  }]
Jan 30 11:50:50.644: INFO: observed Pod pod-test in namespace pods-9493 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-30 11:50:50 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-30 11:50:50 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-30 11:50:50 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-30 11:50:50 +0000 UTC  }]
Jan 30 11:50:51.461: INFO: observed Pod pod-test in namespace pods-9493 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-30 11:50:50 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-30 11:50:50 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-30 11:50:50 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-30 11:50:50 +0000 UTC  }]
Jan 30 11:50:51.891: INFO: Found Pod pod-test in namespace pods-9493 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-30 11:50:50 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2023-01-30 11:50:51 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2023-01-30 11:50:51 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-30 11:50:50 +0000 UTC  }]
STEP: patching the Pod with a new Label and updated data 01/30/23 11:50:51.893
STEP: getting the Pod and ensuring that it's patched 01/30/23 11:50:51.899
STEP: replacing the Pod's status Ready condition to False 01/30/23 11:50:51.901
STEP: check the Pod again to ensure its Ready conditions are False 01/30/23 11:50:51.908
STEP: deleting the Pod via a Collection with a LabelSelector 01/30/23 11:50:51.908
STEP: watching for the Pod to be deleted 01/30/23 11:50:51.911
Jan 30 11:50:51.913: INFO: observed event type MODIFIED
Jan 30 11:50:53.933: INFO: observed event type MODIFIED
Jan 30 11:50:54.220: INFO: observed event type MODIFIED
Jan 30 11:50:54.952: INFO: observed event type MODIFIED
Jan 30 11:50:54.955: INFO: observed event type MODIFIED
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Jan 30 11:50:54.959: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-9493" for this suite. 01/30/23 11:50:54.961
------------------------------
• [4.349 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:896

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 11:50:50.614
    Jan 30 11:50:50.614: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename pods 01/30/23 11:50:50.614
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:50:50.62
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:50:50.622
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should run through the lifecycle of Pods and PodStatus [Conformance]
      test/e2e/common/node/pods.go:896
    STEP: creating a Pod with a static label 01/30/23 11:50:50.629
    STEP: watching for Pod to be ready 01/30/23 11:50:50.633
    Jan 30 11:50:50.634: INFO: observed Pod pod-test in namespace pods-9493 in phase Pending with labels: map[test-pod-static:true] & conditions []
    Jan 30 11:50:50.636: INFO: observed Pod pod-test in namespace pods-9493 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-30 11:50:50 +0000 UTC  }]
    Jan 30 11:50:50.644: INFO: observed Pod pod-test in namespace pods-9493 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-30 11:50:50 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-30 11:50:50 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-30 11:50:50 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-30 11:50:50 +0000 UTC  }]
    Jan 30 11:50:51.461: INFO: observed Pod pod-test in namespace pods-9493 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-30 11:50:50 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-30 11:50:50 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-30 11:50:50 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-30 11:50:50 +0000 UTC  }]
    Jan 30 11:50:51.891: INFO: Found Pod pod-test in namespace pods-9493 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-30 11:50:50 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2023-01-30 11:50:51 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2023-01-30 11:50:51 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-30 11:50:50 +0000 UTC  }]
    STEP: patching the Pod with a new Label and updated data 01/30/23 11:50:51.893
    STEP: getting the Pod and ensuring that it's patched 01/30/23 11:50:51.899
    STEP: replacing the Pod's status Ready condition to False 01/30/23 11:50:51.901
    STEP: check the Pod again to ensure its Ready conditions are False 01/30/23 11:50:51.908
    STEP: deleting the Pod via a Collection with a LabelSelector 01/30/23 11:50:51.908
    STEP: watching for the Pod to be deleted 01/30/23 11:50:51.911
    Jan 30 11:50:51.913: INFO: observed event type MODIFIED
    Jan 30 11:50:53.933: INFO: observed event type MODIFIED
    Jan 30 11:50:54.220: INFO: observed event type MODIFIED
    Jan 30 11:50:54.952: INFO: observed event type MODIFIED
    Jan 30 11:50:54.955: INFO: observed event type MODIFIED
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Jan 30 11:50:54.959: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-9493" for this suite. 01/30/23 11:50:54.961
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] Deployment
  deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 11:50:54.963
Jan 30 11:50:54.964: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename deployment 01/30/23 11:50:54.964
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:50:54.971
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:50:54.973
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160
Jan 30 11:50:54.975: INFO: Creating deployment "webserver-deployment"
Jan 30 11:50:54.977: INFO: Waiting for observed generation 1
Jan 30 11:50:56.981: INFO: Waiting for all required pods to come up
Jan 30 11:50:56.984: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running 01/30/23 11:50:56.984
Jan 30 11:50:56.984: INFO: Waiting up to 5m0s for pod "webserver-deployment-5fd5d6fdb9-xv86w" in namespace "deployment-1250" to be "running"
Jan 30 11:50:56.984: INFO: Waiting up to 5m0s for pod "webserver-deployment-5fd5d6fdb9-9cn5k" in namespace "deployment-1250" to be "running"
Jan 30 11:50:56.984: INFO: Waiting up to 5m0s for pod "webserver-deployment-5fd5d6fdb9-dc8zv" in namespace "deployment-1250" to be "running"
Jan 30 11:50:56.984: INFO: Waiting up to 5m0s for pod "webserver-deployment-5fd5d6fdb9-rwqfr" in namespace "deployment-1250" to be "running"
Jan 30 11:50:56.984: INFO: Waiting up to 5m0s for pod "webserver-deployment-5fd5d6fdb9-ncl8k" in namespace "deployment-1250" to be "running"
Jan 30 11:50:56.984: INFO: Waiting up to 5m0s for pod "webserver-deployment-5fd5d6fdb9-4s5tq" in namespace "deployment-1250" to be "running"
Jan 30 11:50:56.984: INFO: Waiting up to 5m0s for pod "webserver-deployment-5fd5d6fdb9-r24vd" in namespace "deployment-1250" to be "running"
Jan 30 11:50:56.984: INFO: Waiting up to 5m0s for pod "webserver-deployment-5fd5d6fdb9-qcq74" in namespace "deployment-1250" to be "running"
Jan 30 11:50:56.984: INFO: Waiting up to 5m0s for pod "webserver-deployment-5fd5d6fdb9-vxdlh" in namespace "deployment-1250" to be "running"
Jan 30 11:50:56.984: INFO: Waiting up to 5m0s for pod "webserver-deployment-5fd5d6fdb9-87n4r" in namespace "deployment-1250" to be "running"
Jan 30 11:50:56.986: INFO: Pod "webserver-deployment-5fd5d6fdb9-9cn5k": Phase="Pending", Reason="", readiness=false. Elapsed: 2.071721ms
Jan 30 11:50:56.986: INFO: Pod "webserver-deployment-5fd5d6fdb9-xv86w": Phase="Pending", Reason="", readiness=false. Elapsed: 2.09671ms
Jan 30 11:50:56.987: INFO: Pod "webserver-deployment-5fd5d6fdb9-vxdlh": Phase="Pending", Reason="", readiness=false. Elapsed: 2.472751ms
Jan 30 11:50:56.987: INFO: Pod "webserver-deployment-5fd5d6fdb9-qcq74": Phase="Pending", Reason="", readiness=false. Elapsed: 2.646829ms
Jan 30 11:50:56.987: INFO: Pod "webserver-deployment-5fd5d6fdb9-87n4r": Phase="Pending", Reason="", readiness=false. Elapsed: 2.694099ms
Jan 30 11:50:56.987: INFO: Pod "webserver-deployment-5fd5d6fdb9-dc8zv": Phase="Pending", Reason="", readiness=false. Elapsed: 2.823086ms
Jan 30 11:50:56.987: INFO: Pod "webserver-deployment-5fd5d6fdb9-ncl8k": Phase="Pending", Reason="", readiness=false. Elapsed: 2.917926ms
Jan 30 11:50:56.987: INFO: Pod "webserver-deployment-5fd5d6fdb9-rwqfr": Phase="Pending", Reason="", readiness=false. Elapsed: 2.938382ms
Jan 30 11:50:56.987: INFO: Pod "webserver-deployment-5fd5d6fdb9-4s5tq": Phase="Pending", Reason="", readiness=false. Elapsed: 2.919579ms
Jan 30 11:50:56.987: INFO: Pod "webserver-deployment-5fd5d6fdb9-r24vd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.903083ms
Jan 30 11:50:58.989: INFO: Pod "webserver-deployment-5fd5d6fdb9-9cn5k": Phase="Running", Reason="", readiness=true. Elapsed: 2.004690551s
Jan 30 11:50:58.989: INFO: Pod "webserver-deployment-5fd5d6fdb9-9cn5k" satisfied condition "running"
Jan 30 11:50:58.989: INFO: Pod "webserver-deployment-5fd5d6fdb9-r24vd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005005045s
Jan 30 11:50:58.989: INFO: Pod "webserver-deployment-5fd5d6fdb9-xv86w": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005214303s
Jan 30 11:50:58.989: INFO: Pod "webserver-deployment-5fd5d6fdb9-rwqfr": Phase="Running", Reason="", readiness=true. Elapsed: 2.005395933s
Jan 30 11:50:58.989: INFO: Pod "webserver-deployment-5fd5d6fdb9-rwqfr" satisfied condition "running"
Jan 30 11:50:58.989: INFO: Pod "webserver-deployment-5fd5d6fdb9-ncl8k": Phase="Running", Reason="", readiness=true. Elapsed: 2.005390285s
Jan 30 11:50:58.989: INFO: Pod "webserver-deployment-5fd5d6fdb9-ncl8k" satisfied condition "running"
Jan 30 11:50:58.989: INFO: Pod "webserver-deployment-5fd5d6fdb9-87n4r": Phase="Running", Reason="", readiness=true. Elapsed: 2.005390061s
Jan 30 11:50:58.989: INFO: Pod "webserver-deployment-5fd5d6fdb9-87n4r" satisfied condition "running"
Jan 30 11:50:58.989: INFO: Pod "webserver-deployment-5fd5d6fdb9-4s5tq": Phase="Running", Reason="", readiness=true. Elapsed: 2.005480327s
Jan 30 11:50:58.989: INFO: Pod "webserver-deployment-5fd5d6fdb9-4s5tq" satisfied condition "running"
Jan 30 11:50:58.990: INFO: Pod "webserver-deployment-5fd5d6fdb9-dc8zv": Phase="Running", Reason="", readiness=true. Elapsed: 2.005549226s
Jan 30 11:50:58.990: INFO: Pod "webserver-deployment-5fd5d6fdb9-dc8zv" satisfied condition "running"
Jan 30 11:50:58.990: INFO: Pod "webserver-deployment-5fd5d6fdb9-vxdlh": Phase="Running", Reason="", readiness=true. Elapsed: 2.005517082s
Jan 30 11:50:58.990: INFO: Pod "webserver-deployment-5fd5d6fdb9-qcq74": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005532142s
Jan 30 11:50:58.990: INFO: Pod "webserver-deployment-5fd5d6fdb9-vxdlh" satisfied condition "running"
Jan 30 11:51:00.989: INFO: Pod "webserver-deployment-5fd5d6fdb9-xv86w": Phase="Running", Reason="", readiness=true. Elapsed: 4.005125751s
Jan 30 11:51:00.989: INFO: Pod "webserver-deployment-5fd5d6fdb9-xv86w" satisfied condition "running"
Jan 30 11:51:00.989: INFO: Pod "webserver-deployment-5fd5d6fdb9-qcq74": Phase="Running", Reason="", readiness=true. Elapsed: 4.005013242s
Jan 30 11:51:00.989: INFO: Pod "webserver-deployment-5fd5d6fdb9-qcq74" satisfied condition "running"
Jan 30 11:51:00.989: INFO: Pod "webserver-deployment-5fd5d6fdb9-r24vd": Phase="Running", Reason="", readiness=true. Elapsed: 4.005154184s
Jan 30 11:51:00.989: INFO: Pod "webserver-deployment-5fd5d6fdb9-r24vd" satisfied condition "running"
Jan 30 11:51:00.989: INFO: Waiting for deployment "webserver-deployment" to complete
Jan 30 11:51:00.993: INFO: Updating deployment "webserver-deployment" with a non-existent image
Jan 30 11:51:00.998: INFO: Updating deployment webserver-deployment
Jan 30 11:51:00.998: INFO: Waiting for observed generation 2
Jan 30 11:51:03.002: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Jan 30 11:51:03.004: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Jan 30 11:51:03.005: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Jan 30 11:51:03.010: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Jan 30 11:51:03.010: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Jan 30 11:51:03.011: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Jan 30 11:51:03.014: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Jan 30 11:51:03.014: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Jan 30 11:51:03.019: INFO: Updating deployment webserver-deployment
Jan 30 11:51:03.019: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Jan 30 11:51:03.022: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Jan 30 11:51:05.026: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan 30 11:51:05.030: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-1250  62284527-e53d-4ba3-90d8-13dfed8d38a9 23195 3 2023-01-30 11:50:54 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-01-30 11:51:03 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-30 11:51:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc006c7e748 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-01-30 11:51:03 +0000 UTC,LastTransitionTime:2023-01-30 11:51:03 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-d9f79cb5" is progressing.,LastUpdateTime:2023-01-30 11:51:03 +0000 UTC,LastTransitionTime:2023-01-30 11:50:54 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

Jan 30 11:51:05.032: INFO: New ReplicaSet "webserver-deployment-d9f79cb5" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-d9f79cb5  deployment-1250  fd35b1eb-85bf-4c28-9962-33d36cfed5bf 23190 3 2023-01-30 11:51:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 62284527-e53d-4ba3-90d8-13dfed8d38a9 0xc0031151a7 0xc0031151a8}] [] [{kube-controller-manager Update apps/v1 2023-01-30 11:51:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"62284527-e53d-4ba3-90d8-13dfed8d38a9\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-30 11:51:03 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: d9f79cb5,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003115248 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan 30 11:51:05.032: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Jan 30 11:51:05.032: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-5fd5d6fdb9  deployment-1250  2b8b6be1-993a-47ab-9af1-82630db5cf44 23185 3 2023-01-30 11:50:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5fd5d6fdb9] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 62284527-e53d-4ba3-90d8-13dfed8d38a9 0xc0031150a7 0xc0031150a8}] [] [{kube-controller-manager Update apps/v1 2023-01-30 11:51:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"62284527-e53d-4ba3-90d8-13dfed8d38a9\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-30 11:51:03 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 5fd5d6fdb9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5fd5d6fdb9] map[] [] [] []} {[] [] [{httpd harbor.cloud.netease.com/qzprod-k8s/e2e/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003115148 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
Jan 30 11:51:05.037: INFO: Pod "webserver-deployment-5fd5d6fdb9-2qxz4" is not available:
&Pod{ObjectMeta:{webserver-deployment-5fd5d6fdb9-2qxz4 webserver-deployment-5fd5d6fdb9- deployment-1250  323d07de-cd84-49a8-b4ef-9cf294d68acc 23159 0 2023-01-30 11:51:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5fd5d6fdb9] map[] [{apps/v1 ReplicaSet webserver-deployment-5fd5d6fdb9 2b8b6be1-993a-47ab-9af1-82630db5cf44 0xc006c7eb17 0xc006c7eb18}] [] [{kube-controller-manager Update v1 2023-01-30 11:51:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2b8b6be1-993a-47ab-9af1-82630db5cf44\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6gbnc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:harbor.cloud.netease.com/qzprod-k8s/e2e/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6gbnc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pubt2-nks-for-dev1.dg.163.org,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 11:51:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 30 11:51:05.038: INFO: Pod "webserver-deployment-5fd5d6fdb9-4s5tq" is available:
&Pod{ObjectMeta:{webserver-deployment-5fd5d6fdb9-4s5tq webserver-deployment-5fd5d6fdb9- deployment-1250  0124b092-f5f1-48aa-8aed-63b1be3d7394 22988 0 2023-01-30 11:50:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5fd5d6fdb9] map[cni.projectcalico.org/podIP:10.178.151.226/32 cni.projectcalico.org/podIPs:10.178.151.226/32] [{apps/v1 ReplicaSet webserver-deployment-5fd5d6fdb9 2b8b6be1-993a-47ab-9af1-82630db5cf44 0xc006c7ec70 0xc006c7ec71}] [] [{kube-controller-manager Update v1 2023-01-30 11:50:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2b8b6be1-993a-47ab-9af1-82630db5cf44\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-30 11:50:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-30 11:50:57 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.178.151.226\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7lg7c,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:harbor.cloud.netease.com/qzprod-k8s/e2e/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7lg7c,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pubt2-nks-for-dev1.dg.163.org,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 11:50:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 11:50:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 11:50:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 11:50:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.182.0.82,PodIP:10.178.151.226,StartTime:2023-01-30 11:50:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-30 11:50:56 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:harbor.cloud.netease.com/qzprod-k8s/e2e/httpd:2.4.38-4,ImageID:docker-pullable://harbor.cloud.netease.com/qzprod-k8s/e2e/httpd@sha256:8b745b6e9cb6a6184353110aabe28d8d6f6145d3fe7c0e1899aa70c8228c0fd3,ContainerID:docker://25a86fdb9b7111669608f4b3f93d2682b5d96b6e67e78629d1b0f98102cd9e27,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.178.151.226,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 30 11:51:05.038: INFO: Pod "webserver-deployment-5fd5d6fdb9-656k2" is not available:
&Pod{ObjectMeta:{webserver-deployment-5fd5d6fdb9-656k2 webserver-deployment-5fd5d6fdb9- deployment-1250  16d06aa3-1e59-4dcb-a210-0b2de44735ab 23220 0 2023-01-30 11:51:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5fd5d6fdb9] map[cni.projectcalico.org/podIP:10.178.197.139/32 cni.projectcalico.org/podIPs:10.178.197.139/32] [{apps/v1 ReplicaSet webserver-deployment-5fd5d6fdb9 2b8b6be1-993a-47ab-9af1-82630db5cf44 0xc006c7ee67 0xc006c7ee68}] [] [{kube-controller-manager Update v1 2023-01-30 11:51:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2b8b6be1-993a-47ab-9af1-82630db5cf44\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-30 11:51:03 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-01-30 11:51:04 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hlt8v,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:harbor.cloud.netease.com/qzprod-k8s/e2e/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hlt8v,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pubt2-nks-for-dev3.dg.163.org,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 11:51:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 11:51:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 11:51:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 11:51:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.182.0.84,PodIP:,StartTime:2023-01-30 11:51:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:harbor.cloud.netease.com/qzprod-k8s/e2e/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 30 11:51:05.038: INFO: Pod "webserver-deployment-5fd5d6fdb9-6bbqk" is not available:
&Pod{ObjectMeta:{webserver-deployment-5fd5d6fdb9-6bbqk webserver-deployment-5fd5d6fdb9- deployment-1250  e9a391f5-3c85-447a-9a84-0e2b0ee9d89d 23239 0 2023-01-30 11:51:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5fd5d6fdb9] map[cni.projectcalico.org/podIP:10.178.197.142/32 cni.projectcalico.org/podIPs:10.178.197.142/32] [{apps/v1 ReplicaSet webserver-deployment-5fd5d6fdb9 2b8b6be1-993a-47ab-9af1-82630db5cf44 0xc006c7f057 0xc006c7f058}] [] [{kube-controller-manager Update v1 2023-01-30 11:51:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2b8b6be1-993a-47ab-9af1-82630db5cf44\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-30 11:51:03 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-01-30 11:51:04 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-sl58t,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:harbor.cloud.netease.com/qzprod-k8s/e2e/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-sl58t,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pubt2-nks-for-dev3.dg.163.org,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 11:51:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 11:51:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 11:51:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 11:51:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.182.0.84,PodIP:,StartTime:2023-01-30 11:51:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:harbor.cloud.netease.com/qzprod-k8s/e2e/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 30 11:51:05.038: INFO: Pod "webserver-deployment-5fd5d6fdb9-87n4r" is available:
&Pod{ObjectMeta:{webserver-deployment-5fd5d6fdb9-87n4r webserver-deployment-5fd5d6fdb9- deployment-1250  21e1cb26-4a17-4fcb-a51c-ee993b1f08ed 22994 0 2023-01-30 11:50:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5fd5d6fdb9] map[cni.projectcalico.org/podIP:10.178.197.136/32 cni.projectcalico.org/podIPs:10.178.197.136/32] [{apps/v1 ReplicaSet webserver-deployment-5fd5d6fdb9 2b8b6be1-993a-47ab-9af1-82630db5cf44 0xc006c7f237 0xc006c7f238}] [] [{kube-controller-manager Update v1 2023-01-30 11:50:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2b8b6be1-993a-47ab-9af1-82630db5cf44\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-30 11:50:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-30 11:50:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.178.197.136\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qkg4h,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:harbor.cloud.netease.com/qzprod-k8s/e2e/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qkg4h,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pubt2-nks-for-dev3.dg.163.org,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 11:50:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 11:50:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 11:50:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 11:50:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.182.0.84,PodIP:10.178.197.136,StartTime:2023-01-30 11:50:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-30 11:50:57 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:harbor.cloud.netease.com/qzprod-k8s/e2e/httpd:2.4.38-4,ImageID:docker-pullable://harbor.cloud.netease.com/qzprod-k8s/e2e/httpd@sha256:8b745b6e9cb6a6184353110aabe28d8d6f6145d3fe7c0e1899aa70c8228c0fd3,ContainerID:docker://5550c8678f317a1c40c50f8360e5c824b821b6b4f67909d6ccc96de2d9a94adf,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.178.197.136,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 30 11:51:05.038: INFO: Pod "webserver-deployment-5fd5d6fdb9-9cn5k" is available:
&Pod{ObjectMeta:{webserver-deployment-5fd5d6fdb9-9cn5k webserver-deployment-5fd5d6fdb9- deployment-1250  f14df27e-7c38-4b9c-ac86-fcb9d4ce0170 22970 0 2023-01-30 11:50:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5fd5d6fdb9] map[cni.projectcalico.org/podIP:10.178.197.132/32 cni.projectcalico.org/podIPs:10.178.197.132/32] [{apps/v1 ReplicaSet webserver-deployment-5fd5d6fdb9 2b8b6be1-993a-47ab-9af1-82630db5cf44 0xc006c7f437 0xc006c7f438}] [] [{kube-controller-manager Update v1 2023-01-30 11:50:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2b8b6be1-993a-47ab-9af1-82630db5cf44\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-30 11:50:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-30 11:50:57 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.178.197.132\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-m8lnh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:harbor.cloud.netease.com/qzprod-k8s/e2e/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-m8lnh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pubt2-nks-for-dev3.dg.163.org,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 11:50:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 11:50:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 11:50:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 11:50:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.182.0.84,PodIP:10.178.197.132,StartTime:2023-01-30 11:50:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-30 11:50:56 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:harbor.cloud.netease.com/qzprod-k8s/e2e/httpd:2.4.38-4,ImageID:docker-pullable://harbor.cloud.netease.com/qzprod-k8s/e2e/httpd@sha256:8b745b6e9cb6a6184353110aabe28d8d6f6145d3fe7c0e1899aa70c8228c0fd3,ContainerID:docker://1c15e2d52c4640aecc19181dc8b9e652dc6491f090a1b6ed02e9fd2a668d98e4,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.178.197.132,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 30 11:51:05.039: INFO: Pod "webserver-deployment-5fd5d6fdb9-dc8zv" is available:
&Pod{ObjectMeta:{webserver-deployment-5fd5d6fdb9-dc8zv webserver-deployment-5fd5d6fdb9- deployment-1250  5ade750d-2f8e-4c22-ae2b-7fd8d8b2e04b 22992 0 2023-01-30 11:50:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5fd5d6fdb9] map[cni.projectcalico.org/podIP:10.178.197.135/32 cni.projectcalico.org/podIPs:10.178.197.135/32] [{apps/v1 ReplicaSet webserver-deployment-5fd5d6fdb9 2b8b6be1-993a-47ab-9af1-82630db5cf44 0xc006c7f637 0xc006c7f638}] [] [{kube-controller-manager Update v1 2023-01-30 11:50:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2b8b6be1-993a-47ab-9af1-82630db5cf44\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-30 11:50:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-30 11:50:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.178.197.135\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8d266,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:harbor.cloud.netease.com/qzprod-k8s/e2e/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8d266,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pubt2-nks-for-dev3.dg.163.org,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 11:50:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 11:50:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 11:50:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 11:50:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.182.0.84,PodIP:10.178.197.135,StartTime:2023-01-30 11:50:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-30 11:50:57 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:harbor.cloud.netease.com/qzprod-k8s/e2e/httpd:2.4.38-4,ImageID:docker-pullable://harbor.cloud.netease.com/qzprod-k8s/e2e/httpd@sha256:8b745b6e9cb6a6184353110aabe28d8d6f6145d3fe7c0e1899aa70c8228c0fd3,ContainerID:docker://b776c43385d88e3693c4c514ef57010ffcb8c0c24b403561df821184dfebb0c7,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.178.197.135,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 30 11:51:05.039: INFO: Pod "webserver-deployment-5fd5d6fdb9-glrwp" is not available:
&Pod{ObjectMeta:{webserver-deployment-5fd5d6fdb9-glrwp webserver-deployment-5fd5d6fdb9- deployment-1250  886233b7-6e39-4c47-9430-e5e9dacdec4c 23193 0 2023-01-30 11:51:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5fd5d6fdb9] map[] [{apps/v1 ReplicaSet webserver-deployment-5fd5d6fdb9 2b8b6be1-993a-47ab-9af1-82630db5cf44 0xc006c7f837 0xc006c7f838}] [] [{kube-controller-manager Update v1 2023-01-30 11:51:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2b8b6be1-993a-47ab-9af1-82630db5cf44\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-30 11:51:03 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-dr8dv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:harbor.cloud.netease.com/qzprod-k8s/e2e/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dr8dv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pubt2-nks-for-dev3.dg.163.org,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 11:51:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 11:51:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 11:51:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 11:51:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.182.0.84,PodIP:,StartTime:2023-01-30 11:51:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:harbor.cloud.netease.com/qzprod-k8s/e2e/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 30 11:51:05.039: INFO: Pod "webserver-deployment-5fd5d6fdb9-gt4h7" is not available:
&Pod{ObjectMeta:{webserver-deployment-5fd5d6fdb9-gt4h7 webserver-deployment-5fd5d6fdb9- deployment-1250  ccd1ac80-b41f-4141-993a-3338fca85d7f 23152 0 2023-01-30 11:51:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5fd5d6fdb9] map[] [{apps/v1 ReplicaSet webserver-deployment-5fd5d6fdb9 2b8b6be1-993a-47ab-9af1-82630db5cf44 0xc006c7f9f7 0xc006c7f9f8}] [] [{kube-controller-manager Update v1 2023-01-30 11:51:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2b8b6be1-993a-47ab-9af1-82630db5cf44\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8vb4r,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:harbor.cloud.netease.com/qzprod-k8s/e2e/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8vb4r,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pubt2-nks-for-dev1.dg.163.org,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 11:51:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 30 11:51:05.039: INFO: Pod "webserver-deployment-5fd5d6fdb9-h4km9" is not available:
&Pod{ObjectMeta:{webserver-deployment-5fd5d6fdb9-h4km9 webserver-deployment-5fd5d6fdb9- deployment-1250  bfda5d4d-0a27-46c8-a2a0-421f60d8121e 23169 0 2023-01-30 11:51:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5fd5d6fdb9] map[] [{apps/v1 ReplicaSet webserver-deployment-5fd5d6fdb9 2b8b6be1-993a-47ab-9af1-82630db5cf44 0xc006c7fb50 0xc006c7fb51}] [] [{kube-controller-manager Update v1 2023-01-30 11:51:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2b8b6be1-993a-47ab-9af1-82630db5cf44\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-99mbn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:harbor.cloud.netease.com/qzprod-k8s/e2e/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-99mbn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pubt2-nks-for-dev1.dg.163.org,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 11:51:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 30 11:51:05.039: INFO: Pod "webserver-deployment-5fd5d6fdb9-jt44h" is not available:
&Pod{ObjectMeta:{webserver-deployment-5fd5d6fdb9-jt44h webserver-deployment-5fd5d6fdb9- deployment-1250  38308d3b-4bd4-4546-a296-a82f8750911e 23211 0 2023-01-30 11:51:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5fd5d6fdb9] map[] [{apps/v1 ReplicaSet webserver-deployment-5fd5d6fdb9 2b8b6be1-993a-47ab-9af1-82630db5cf44 0xc006c7fcb0 0xc006c7fcb1}] [] [{kube-controller-manager Update v1 2023-01-30 11:51:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2b8b6be1-993a-47ab-9af1-82630db5cf44\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-30 11:51:03 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-j9mdk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:harbor.cloud.netease.com/qzprod-k8s/e2e/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-j9mdk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pubt2-nks-for-dev1.dg.163.org,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 11:51:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 11:51:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 11:51:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 11:51:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.182.0.82,PodIP:,StartTime:2023-01-30 11:51:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:harbor.cloud.netease.com/qzprod-k8s/e2e/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 30 11:51:05.039: INFO: Pod "webserver-deployment-5fd5d6fdb9-m96n9" is not available:
&Pod{ObjectMeta:{webserver-deployment-5fd5d6fdb9-m96n9 webserver-deployment-5fd5d6fdb9- deployment-1250  975e2291-5e07-4d38-8baa-207f74a7b2d2 23157 0 2023-01-30 11:51:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5fd5d6fdb9] map[] [{apps/v1 ReplicaSet webserver-deployment-5fd5d6fdb9 2b8b6be1-993a-47ab-9af1-82630db5cf44 0xc006c7fe67 0xc006c7fe68}] [] [{kube-controller-manager Update v1 2023-01-30 11:51:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2b8b6be1-993a-47ab-9af1-82630db5cf44\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-30 11:51:03 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vskgx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:harbor.cloud.netease.com/qzprod-k8s/e2e/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vskgx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pubt2-nks-for-dev1.dg.163.org,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 11:51:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 11:51:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 11:51:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 11:51:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.182.0.82,PodIP:,StartTime:2023-01-30 11:51:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:harbor.cloud.netease.com/qzprod-k8s/e2e/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 30 11:51:05.040: INFO: Pod "webserver-deployment-5fd5d6fdb9-qcq74" is available:
&Pod{ObjectMeta:{webserver-deployment-5fd5d6fdb9-qcq74 webserver-deployment-5fd5d6fdb9- deployment-1250  8d4c47b6-5238-4da8-9390-29e583143cfa 23012 0 2023-01-30 11:50:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5fd5d6fdb9] map[cni.projectcalico.org/podIP:10.178.151.230/32 cni.projectcalico.org/podIPs:10.178.151.230/32] [{apps/v1 ReplicaSet webserver-deployment-5fd5d6fdb9 2b8b6be1-993a-47ab-9af1-82630db5cf44 0xc005902027 0xc005902028}] [] [{kube-controller-manager Update v1 2023-01-30 11:50:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2b8b6be1-993a-47ab-9af1-82630db5cf44\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-30 11:50:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-30 11:50:59 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.178.151.230\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-dtpbm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:harbor.cloud.netease.com/qzprod-k8s/e2e/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dtpbm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pubt2-nks-for-dev1.dg.163.org,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 11:50:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 11:50:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 11:50:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 11:50:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.182.0.82,PodIP:10.178.151.230,StartTime:2023-01-30 11:50:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-30 11:50:57 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:harbor.cloud.netease.com/qzprod-k8s/e2e/httpd:2.4.38-4,ImageID:docker-pullable://harbor.cloud.netease.com/qzprod-k8s/e2e/httpd@sha256:8b745b6e9cb6a6184353110aabe28d8d6f6145d3fe7c0e1899aa70c8228c0fd3,ContainerID:docker://8fef8acf1ec66844fad4670cd6890597c109df93ab605f93992875c96590893e,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.178.151.230,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 30 11:51:05.040: INFO: Pod "webserver-deployment-5fd5d6fdb9-r24vd" is available:
&Pod{ObjectMeta:{webserver-deployment-5fd5d6fdb9-r24vd webserver-deployment-5fd5d6fdb9- deployment-1250  ad3384d2-10d5-46e0-b3f6-8b5aebd7a4ff 23009 0 2023-01-30 11:50:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5fd5d6fdb9] map[cni.projectcalico.org/podIP:10.178.151.228/32 cni.projectcalico.org/podIPs:10.178.151.228/32] [{apps/v1 ReplicaSet webserver-deployment-5fd5d6fdb9 2b8b6be1-993a-47ab-9af1-82630db5cf44 0xc005902227 0xc005902228}] [] [{kube-controller-manager Update v1 2023-01-30 11:50:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2b8b6be1-993a-47ab-9af1-82630db5cf44\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-30 11:50:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-30 11:50:59 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.178.151.228\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-sdd2l,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:harbor.cloud.netease.com/qzprod-k8s/e2e/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-sdd2l,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pubt2-nks-for-dev1.dg.163.org,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 11:50:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 11:50:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 11:50:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 11:50:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.182.0.82,PodIP:10.178.151.228,StartTime:2023-01-30 11:50:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-30 11:50:57 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:harbor.cloud.netease.com/qzprod-k8s/e2e/httpd:2.4.38-4,ImageID:docker-pullable://harbor.cloud.netease.com/qzprod-k8s/e2e/httpd@sha256:8b745b6e9cb6a6184353110aabe28d8d6f6145d3fe7c0e1899aa70c8228c0fd3,ContainerID:docker://761b554728bad0999b2604617dd727c6cfa73711859ff8a6467fcdf3f2a6b504,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.178.151.228,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 30 11:51:05.040: INFO: Pod "webserver-deployment-5fd5d6fdb9-rwqfr" is available:
&Pod{ObjectMeta:{webserver-deployment-5fd5d6fdb9-rwqfr webserver-deployment-5fd5d6fdb9- deployment-1250  9b173ff6-5263-4bcc-87cd-71f91904b88a 22967 0 2023-01-30 11:50:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5fd5d6fdb9] map[cni.projectcalico.org/podIP:10.178.197.133/32 cni.projectcalico.org/podIPs:10.178.197.133/32] [{apps/v1 ReplicaSet webserver-deployment-5fd5d6fdb9 2b8b6be1-993a-47ab-9af1-82630db5cf44 0xc005902427 0xc005902428}] [] [{kube-controller-manager Update v1 2023-01-30 11:50:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2b8b6be1-993a-47ab-9af1-82630db5cf44\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-30 11:50:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-30 11:50:57 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.178.197.133\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-584b8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:harbor.cloud.netease.com/qzprod-k8s/e2e/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-584b8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pubt2-nks-for-dev3.dg.163.org,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 11:50:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 11:50:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 11:50:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 11:50:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.182.0.84,PodIP:10.178.197.133,StartTime:2023-01-30 11:50:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-30 11:50:56 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:harbor.cloud.netease.com/qzprod-k8s/e2e/httpd:2.4.38-4,ImageID:docker-pullable://harbor.cloud.netease.com/qzprod-k8s/e2e/httpd@sha256:8b745b6e9cb6a6184353110aabe28d8d6f6145d3fe7c0e1899aa70c8228c0fd3,ContainerID:docker://b0118e397da0a4bdf894830866c6ec629b7ce88415b57e6d00d3351c5d823894,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.178.197.133,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 30 11:51:05.040: INFO: Pod "webserver-deployment-5fd5d6fdb9-tpgd4" is not available:
&Pod{ObjectMeta:{webserver-deployment-5fd5d6fdb9-tpgd4 webserver-deployment-5fd5d6fdb9- deployment-1250  f8b2f903-276c-4fc0-8834-a1c80977f40b 23250 0 2023-01-30 11:51:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5fd5d6fdb9] map[cni.projectcalico.org/podIP:10.178.151.234/32 cni.projectcalico.org/podIPs:10.178.151.234/32] [{apps/v1 ReplicaSet webserver-deployment-5fd5d6fdb9 2b8b6be1-993a-47ab-9af1-82630db5cf44 0xc005902627 0xc005902628}] [] [{kube-controller-manager Update v1 2023-01-30 11:51:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2b8b6be1-993a-47ab-9af1-82630db5cf44\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-30 11:51:04 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-cdvbd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:harbor.cloud.netease.com/qzprod-k8s/e2e/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-cdvbd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pubt2-nks-for-dev1.dg.163.org,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 11:51:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 30 11:51:05.040: INFO: Pod "webserver-deployment-5fd5d6fdb9-vxdlh" is available:
&Pod{ObjectMeta:{webserver-deployment-5fd5d6fdb9-vxdlh webserver-deployment-5fd5d6fdb9- deployment-1250  3d432818-6faa-411e-ab59-5da048b45e87 22999 0 2023-01-30 11:50:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5fd5d6fdb9] map[cni.projectcalico.org/podIP:10.178.197.134/32 cni.projectcalico.org/podIPs:10.178.197.134/32] [{apps/v1 ReplicaSet webserver-deployment-5fd5d6fdb9 2b8b6be1-993a-47ab-9af1-82630db5cf44 0xc0059027a0 0xc0059027a1}] [] [{kube-controller-manager Update v1 2023-01-30 11:50:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2b8b6be1-993a-47ab-9af1-82630db5cf44\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-30 11:50:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-30 11:50:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.178.197.134\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-z8qjt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:harbor.cloud.netease.com/qzprod-k8s/e2e/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-z8qjt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pubt2-nks-for-dev3.dg.163.org,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 11:50:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 11:50:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 11:50:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 11:50:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.182.0.84,PodIP:10.178.197.134,StartTime:2023-01-30 11:50:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-30 11:50:57 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:harbor.cloud.netease.com/qzprod-k8s/e2e/httpd:2.4.38-4,ImageID:docker-pullable://harbor.cloud.netease.com/qzprod-k8s/e2e/httpd@sha256:8b745b6e9cb6a6184353110aabe28d8d6f6145d3fe7c0e1899aa70c8228c0fd3,ContainerID:docker://f02f138f4c8ac2ec8e509c9ed5d6524480df1c8e19987037c3541b4e98a037fa,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.178.197.134,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 30 11:51:05.040: INFO: Pod "webserver-deployment-5fd5d6fdb9-x4kmt" is not available:
&Pod{ObjectMeta:{webserver-deployment-5fd5d6fdb9-x4kmt webserver-deployment-5fd5d6fdb9- deployment-1250  4df70344-0bd2-4bf1-a274-94c2e229bc5d 23165 0 2023-01-30 11:51:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5fd5d6fdb9] map[] [{apps/v1 ReplicaSet webserver-deployment-5fd5d6fdb9 2b8b6be1-993a-47ab-9af1-82630db5cf44 0xc005902997 0xc005902998}] [] [{kube-controller-manager Update v1 2023-01-30 11:51:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2b8b6be1-993a-47ab-9af1-82630db5cf44\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-48bjg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:harbor.cloud.netease.com/qzprod-k8s/e2e/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-48bjg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pubt2-nks-for-dev3.dg.163.org,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 11:51:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 30 11:51:05.041: INFO: Pod "webserver-deployment-5fd5d6fdb9-xfmmv" is not available:
&Pod{ObjectMeta:{webserver-deployment-5fd5d6fdb9-xfmmv webserver-deployment-5fd5d6fdb9- deployment-1250  5cd754ba-1dd4-4e3a-b8d1-42fc99cb6e9b 23139 0 2023-01-30 11:51:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5fd5d6fdb9] map[] [{apps/v1 ReplicaSet webserver-deployment-5fd5d6fdb9 2b8b6be1-993a-47ab-9af1-82630db5cf44 0xc005902af0 0xc005902af1}] [] [{kube-controller-manager Update v1 2023-01-30 11:51:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2b8b6be1-993a-47ab-9af1-82630db5cf44\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pskcs,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:harbor.cloud.netease.com/qzprod-k8s/e2e/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pskcs,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pubt2-nks-for-dev1.dg.163.org,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 11:51:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 30 11:51:05.041: INFO: Pod "webserver-deployment-5fd5d6fdb9-zcf9q" is not available:
&Pod{ObjectMeta:{webserver-deployment-5fd5d6fdb9-zcf9q webserver-deployment-5fd5d6fdb9- deployment-1250  715930e2-10eb-473e-93d3-e15619a6ccd3 23252 0 2023-01-30 11:51:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5fd5d6fdb9] map[cni.projectcalico.org/podIP:10.178.197.143/32 cni.projectcalico.org/podIPs:10.178.197.143/32] [{apps/v1 ReplicaSet webserver-deployment-5fd5d6fdb9 2b8b6be1-993a-47ab-9af1-82630db5cf44 0xc005902c40 0xc005902c41}] [] [{kube-controller-manager Update v1 2023-01-30 11:51:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2b8b6be1-993a-47ab-9af1-82630db5cf44\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-30 11:51:04 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-p8nft,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:harbor.cloud.netease.com/qzprod-k8s/e2e/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-p8nft,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pubt2-nks-for-dev3.dg.163.org,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 11:51:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 30 11:51:05.041: INFO: Pod "webserver-deployment-d9f79cb5-45zbq" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-45zbq webserver-deployment-d9f79cb5- deployment-1250  e7ead0e8-1502-47e5-b903-615f4f070da4 23110 0 2023-01-30 11:51:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/podIP:10.178.151.233/32 cni.projectcalico.org/podIPs:10.178.151.233/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 fd35b1eb-85bf-4c28-9962-33d36cfed5bf 0xc005902d9f 0xc005902db0}] [] [{kube-controller-manager Update v1 2023-01-30 11:51:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fd35b1eb-85bf-4c28-9962-33d36cfed5bf\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-30 11:51:02 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-30 11:51:02 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-s9nfp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-s9nfp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pubt2-nks-for-dev1.dg.163.org,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 11:51:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 11:51:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 11:51:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 11:51:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.182.0.82,PodIP:,StartTime:2023-01-30 11:51:01 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 30 11:51:05.041: INFO: Pod "webserver-deployment-d9f79cb5-7xjc2" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-7xjc2 webserver-deployment-d9f79cb5- deployment-1250  dc6ce146-84dd-4917-abd8-c13ea31c223a 23106 0 2023-01-30 11:51:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/podIP:10.178.151.232/32 cni.projectcalico.org/podIPs:10.178.151.232/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 fd35b1eb-85bf-4c28-9962-33d36cfed5bf 0xc005902f8f 0xc005902fa0}] [] [{kube-controller-manager Update v1 2023-01-30 11:51:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fd35b1eb-85bf-4c28-9962-33d36cfed5bf\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-30 11:51:01 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-01-30 11:51:02 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mbzmp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mbzmp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pubt2-nks-for-dev1.dg.163.org,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 11:51:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 11:51:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 11:51:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 11:51:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.182.0.82,PodIP:,StartTime:2023-01-30 11:51:01 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 30 11:51:05.041: INFO: Pod "webserver-deployment-d9f79cb5-bqknr" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-bqknr webserver-deployment-d9f79cb5- deployment-1250  00506c8d-fce9-4c34-83ee-843aa8b8911b 23094 0 2023-01-30 11:51:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/podIP:10.178.197.138/32 cni.projectcalico.org/podIPs:10.178.197.138/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 fd35b1eb-85bf-4c28-9962-33d36cfed5bf 0xc00590317f 0xc005903190}] [] [{calico Update v1 2023-01-30 11:51:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-01-30 11:51:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fd35b1eb-85bf-4c28-9962-33d36cfed5bf\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-30 11:51:01 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9tbvf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9tbvf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pubt2-nks-for-dev3.dg.163.org,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 11:51:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 11:51:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 11:51:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 11:51:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.182.0.84,PodIP:,StartTime:2023-01-30 11:51:01 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 30 11:51:05.041: INFO: Pod "webserver-deployment-d9f79cb5-g45b6" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-g45b6 webserver-deployment-d9f79cb5- deployment-1250  696c55e9-c0cb-4d05-88b0-094df3e4aef6 23187 0 2023-01-30 11:51:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 fd35b1eb-85bf-4c28-9962-33d36cfed5bf 0xc00590336f 0xc005903380}] [] [{kube-controller-manager Update v1 2023-01-30 11:51:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fd35b1eb-85bf-4c28-9962-33d36cfed5bf\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-30 11:51:03 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-s2wts,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-s2wts,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pubt2-nks-for-dev3.dg.163.org,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 11:51:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 11:51:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 11:51:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 11:51:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.182.0.84,PodIP:,StartTime:2023-01-30 11:51:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 30 11:51:05.042: INFO: Pod "webserver-deployment-d9f79cb5-g6qr5" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-g6qr5 webserver-deployment-d9f79cb5- deployment-1250  aa289320-32a2-433f-a3eb-ed59aea05f75 23162 0 2023-01-30 11:51:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 fd35b1eb-85bf-4c28-9962-33d36cfed5bf 0xc00590353f 0xc005903550}] [] [{kube-controller-manager Update v1 2023-01-30 11:51:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fd35b1eb-85bf-4c28-9962-33d36cfed5bf\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lbnzn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lbnzn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pubt2-nks-for-dev1.dg.163.org,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 11:51:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 30 11:51:05.042: INFO: Pod "webserver-deployment-d9f79cb5-jhrmj" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-jhrmj webserver-deployment-d9f79cb5- deployment-1250  07c6accd-fe84-4a0d-aaed-25a2d52bb2a3 23257 0 2023-01-30 11:51:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/podIP:10.178.151.235/32 cni.projectcalico.org/podIPs:10.178.151.235/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 fd35b1eb-85bf-4c28-9962-33d36cfed5bf 0xc00590369f 0xc0059036b0}] [] [{kube-controller-manager Update v1 2023-01-30 11:51:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fd35b1eb-85bf-4c28-9962-33d36cfed5bf\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-30 11:51:04 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zx7wx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zx7wx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pubt2-nks-for-dev1.dg.163.org,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 11:51:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 30 11:51:05.042: INFO: Pod "webserver-deployment-d9f79cb5-lhxkt" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-lhxkt webserver-deployment-d9f79cb5- deployment-1250  4c26f4c4-6642-4faf-ae15-19c6225f97c8 23098 0 2023-01-30 11:51:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/podIP:10.178.151.231/32 cni.projectcalico.org/podIPs:10.178.151.231/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 fd35b1eb-85bf-4c28-9962-33d36cfed5bf 0xc00590381f 0xc005903830}] [] [{kube-controller-manager Update v1 2023-01-30 11:51:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fd35b1eb-85bf-4c28-9962-33d36cfed5bf\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-30 11:51:01 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-01-30 11:51:02 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vcwfw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vcwfw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pubt2-nks-for-dev1.dg.163.org,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 11:51:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 11:51:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 11:51:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 11:51:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.182.0.82,PodIP:,StartTime:2023-01-30 11:51:01 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 30 11:51:05.042: INFO: Pod "webserver-deployment-d9f79cb5-nshn2" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-nshn2 webserver-deployment-d9f79cb5- deployment-1250  06b9ed82-824a-441f-a6aa-ca40388e280e 23173 0 2023-01-30 11:51:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 fd35b1eb-85bf-4c28-9962-33d36cfed5bf 0xc005903a0f 0xc005903a20}] [] [{kube-controller-manager Update v1 2023-01-30 11:51:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fd35b1eb-85bf-4c28-9962-33d36cfed5bf\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qgz92,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qgz92,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pubt2-nks-for-dev1.dg.163.org,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 11:51:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 30 11:51:05.042: INFO: Pod "webserver-deployment-d9f79cb5-pt5m8" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-pt5m8 webserver-deployment-d9f79cb5- deployment-1250  ac205318-e2ad-4cd7-9998-7e9f6cc46382 23232 0 2023-01-30 11:51:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/podIP:10.178.197.141/32 cni.projectcalico.org/podIPs:10.178.197.141/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 fd35b1eb-85bf-4c28-9962-33d36cfed5bf 0xc005903b6f 0xc005903b80}] [] [{kube-controller-manager Update v1 2023-01-30 11:51:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fd35b1eb-85bf-4c28-9962-33d36cfed5bf\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-30 11:51:04 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pdf2d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pdf2d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pubt2-nks-for-dev3.dg.163.org,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 11:51:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 30 11:51:05.042: INFO: Pod "webserver-deployment-d9f79cb5-sd5tj" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-sd5tj webserver-deployment-d9f79cb5- deployment-1250  2ae2ce0a-8c26-48bd-92e5-bb45f3240afe 23221 0 2023-01-30 11:51:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/podIP:10.178.197.140/32 cni.projectcalico.org/podIPs:10.178.197.140/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 fd35b1eb-85bf-4c28-9962-33d36cfed5bf 0xc005903cef 0xc005903d00}] [] [{kube-controller-manager Update v1 2023-01-30 11:51:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fd35b1eb-85bf-4c28-9962-33d36cfed5bf\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-30 11:51:04 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jj6tn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jj6tn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pubt2-nks-for-dev3.dg.163.org,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 11:51:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 30 11:51:05.042: INFO: Pod "webserver-deployment-d9f79cb5-sfbbd" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-sfbbd webserver-deployment-d9f79cb5- deployment-1250  e48d4768-2677-4594-8624-e6e09074e887 23092 0 2023-01-30 11:51:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/podIP:10.178.197.137/32 cni.projectcalico.org/podIPs:10.178.197.137/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 fd35b1eb-85bf-4c28-9962-33d36cfed5bf 0xc005903e6f 0xc005903e80}] [] [{calico Update v1 2023-01-30 11:51:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-01-30 11:51:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fd35b1eb-85bf-4c28-9962-33d36cfed5bf\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-30 11:51:01 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-d2trb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-d2trb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pubt2-nks-for-dev3.dg.163.org,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 11:51:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 11:51:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 11:51:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 11:51:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.182.0.84,PodIP:,StartTime:2023-01-30 11:51:01 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 30 11:51:05.043: INFO: Pod "webserver-deployment-d9f79cb5-t4598" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-t4598 webserver-deployment-d9f79cb5- deployment-1250  bdac5af9-5625-4738-9bb7-2b16c11c77e5 23149 0 2023-01-30 11:51:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 fd35b1eb-85bf-4c28-9962-33d36cfed5bf 0xc00592205f 0xc005922070}] [] [{kube-controller-manager Update v1 2023-01-30 11:51:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fd35b1eb-85bf-4c28-9962-33d36cfed5bf\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-s7btm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-s7btm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pubt2-nks-for-dev1.dg.163.org,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 11:51:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 30 11:51:05.043: INFO: Pod "webserver-deployment-d9f79cb5-zxnxg" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-zxnxg webserver-deployment-d9f79cb5- deployment-1250  07b211d8-3681-4c30-88b3-f410e4b40875 23200 0 2023-01-30 11:51:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 fd35b1eb-85bf-4c28-9962-33d36cfed5bf 0xc0059221bf 0xc0059221d0}] [] [{kube-controller-manager Update v1 2023-01-30 11:51:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fd35b1eb-85bf-4c28-9962-33d36cfed5bf\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-30 11:51:03 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ct6zz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ct6zz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pubt2-nks-for-dev3.dg.163.org,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 11:51:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 11:51:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 11:51:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 11:51:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.182.0.84,PodIP:,StartTime:2023-01-30 11:51:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Jan 30 11:51:05.043: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-1250" for this suite. 01/30/23 11:51:05.046
------------------------------
• [SLOW TEST] [10.088 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 11:50:54.963
    Jan 30 11:50:54.964: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename deployment 01/30/23 11:50:54.964
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:50:54.971
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:50:54.973
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should support proportional scaling [Conformance]
      test/e2e/apps/deployment.go:160
    Jan 30 11:50:54.975: INFO: Creating deployment "webserver-deployment"
    Jan 30 11:50:54.977: INFO: Waiting for observed generation 1
    Jan 30 11:50:56.981: INFO: Waiting for all required pods to come up
    Jan 30 11:50:56.984: INFO: Pod name httpd: Found 10 pods out of 10
    STEP: ensuring each pod is running 01/30/23 11:50:56.984
    Jan 30 11:50:56.984: INFO: Waiting up to 5m0s for pod "webserver-deployment-5fd5d6fdb9-xv86w" in namespace "deployment-1250" to be "running"
    Jan 30 11:50:56.984: INFO: Waiting up to 5m0s for pod "webserver-deployment-5fd5d6fdb9-9cn5k" in namespace "deployment-1250" to be "running"
    Jan 30 11:50:56.984: INFO: Waiting up to 5m0s for pod "webserver-deployment-5fd5d6fdb9-dc8zv" in namespace "deployment-1250" to be "running"
    Jan 30 11:50:56.984: INFO: Waiting up to 5m0s for pod "webserver-deployment-5fd5d6fdb9-rwqfr" in namespace "deployment-1250" to be "running"
    Jan 30 11:50:56.984: INFO: Waiting up to 5m0s for pod "webserver-deployment-5fd5d6fdb9-ncl8k" in namespace "deployment-1250" to be "running"
    Jan 30 11:50:56.984: INFO: Waiting up to 5m0s for pod "webserver-deployment-5fd5d6fdb9-4s5tq" in namespace "deployment-1250" to be "running"
    Jan 30 11:50:56.984: INFO: Waiting up to 5m0s for pod "webserver-deployment-5fd5d6fdb9-r24vd" in namespace "deployment-1250" to be "running"
    Jan 30 11:50:56.984: INFO: Waiting up to 5m0s for pod "webserver-deployment-5fd5d6fdb9-qcq74" in namespace "deployment-1250" to be "running"
    Jan 30 11:50:56.984: INFO: Waiting up to 5m0s for pod "webserver-deployment-5fd5d6fdb9-vxdlh" in namespace "deployment-1250" to be "running"
    Jan 30 11:50:56.984: INFO: Waiting up to 5m0s for pod "webserver-deployment-5fd5d6fdb9-87n4r" in namespace "deployment-1250" to be "running"
    Jan 30 11:50:56.986: INFO: Pod "webserver-deployment-5fd5d6fdb9-9cn5k": Phase="Pending", Reason="", readiness=false. Elapsed: 2.071721ms
    Jan 30 11:50:56.986: INFO: Pod "webserver-deployment-5fd5d6fdb9-xv86w": Phase="Pending", Reason="", readiness=false. Elapsed: 2.09671ms
    Jan 30 11:50:56.987: INFO: Pod "webserver-deployment-5fd5d6fdb9-vxdlh": Phase="Pending", Reason="", readiness=false. Elapsed: 2.472751ms
    Jan 30 11:50:56.987: INFO: Pod "webserver-deployment-5fd5d6fdb9-qcq74": Phase="Pending", Reason="", readiness=false. Elapsed: 2.646829ms
    Jan 30 11:50:56.987: INFO: Pod "webserver-deployment-5fd5d6fdb9-87n4r": Phase="Pending", Reason="", readiness=false. Elapsed: 2.694099ms
    Jan 30 11:50:56.987: INFO: Pod "webserver-deployment-5fd5d6fdb9-dc8zv": Phase="Pending", Reason="", readiness=false. Elapsed: 2.823086ms
    Jan 30 11:50:56.987: INFO: Pod "webserver-deployment-5fd5d6fdb9-ncl8k": Phase="Pending", Reason="", readiness=false. Elapsed: 2.917926ms
    Jan 30 11:50:56.987: INFO: Pod "webserver-deployment-5fd5d6fdb9-rwqfr": Phase="Pending", Reason="", readiness=false. Elapsed: 2.938382ms
    Jan 30 11:50:56.987: INFO: Pod "webserver-deployment-5fd5d6fdb9-4s5tq": Phase="Pending", Reason="", readiness=false. Elapsed: 2.919579ms
    Jan 30 11:50:56.987: INFO: Pod "webserver-deployment-5fd5d6fdb9-r24vd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.903083ms
    Jan 30 11:50:58.989: INFO: Pod "webserver-deployment-5fd5d6fdb9-9cn5k": Phase="Running", Reason="", readiness=true. Elapsed: 2.004690551s
    Jan 30 11:50:58.989: INFO: Pod "webserver-deployment-5fd5d6fdb9-9cn5k" satisfied condition "running"
    Jan 30 11:50:58.989: INFO: Pod "webserver-deployment-5fd5d6fdb9-r24vd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005005045s
    Jan 30 11:50:58.989: INFO: Pod "webserver-deployment-5fd5d6fdb9-xv86w": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005214303s
    Jan 30 11:50:58.989: INFO: Pod "webserver-deployment-5fd5d6fdb9-rwqfr": Phase="Running", Reason="", readiness=true. Elapsed: 2.005395933s
    Jan 30 11:50:58.989: INFO: Pod "webserver-deployment-5fd5d6fdb9-rwqfr" satisfied condition "running"
    Jan 30 11:50:58.989: INFO: Pod "webserver-deployment-5fd5d6fdb9-ncl8k": Phase="Running", Reason="", readiness=true. Elapsed: 2.005390285s
    Jan 30 11:50:58.989: INFO: Pod "webserver-deployment-5fd5d6fdb9-ncl8k" satisfied condition "running"
    Jan 30 11:50:58.989: INFO: Pod "webserver-deployment-5fd5d6fdb9-87n4r": Phase="Running", Reason="", readiness=true. Elapsed: 2.005390061s
    Jan 30 11:50:58.989: INFO: Pod "webserver-deployment-5fd5d6fdb9-87n4r" satisfied condition "running"
    Jan 30 11:50:58.989: INFO: Pod "webserver-deployment-5fd5d6fdb9-4s5tq": Phase="Running", Reason="", readiness=true. Elapsed: 2.005480327s
    Jan 30 11:50:58.989: INFO: Pod "webserver-deployment-5fd5d6fdb9-4s5tq" satisfied condition "running"
    Jan 30 11:50:58.990: INFO: Pod "webserver-deployment-5fd5d6fdb9-dc8zv": Phase="Running", Reason="", readiness=true. Elapsed: 2.005549226s
    Jan 30 11:50:58.990: INFO: Pod "webserver-deployment-5fd5d6fdb9-dc8zv" satisfied condition "running"
    Jan 30 11:50:58.990: INFO: Pod "webserver-deployment-5fd5d6fdb9-vxdlh": Phase="Running", Reason="", readiness=true. Elapsed: 2.005517082s
    Jan 30 11:50:58.990: INFO: Pod "webserver-deployment-5fd5d6fdb9-qcq74": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005532142s
    Jan 30 11:50:58.990: INFO: Pod "webserver-deployment-5fd5d6fdb9-vxdlh" satisfied condition "running"
    Jan 30 11:51:00.989: INFO: Pod "webserver-deployment-5fd5d6fdb9-xv86w": Phase="Running", Reason="", readiness=true. Elapsed: 4.005125751s
    Jan 30 11:51:00.989: INFO: Pod "webserver-deployment-5fd5d6fdb9-xv86w" satisfied condition "running"
    Jan 30 11:51:00.989: INFO: Pod "webserver-deployment-5fd5d6fdb9-qcq74": Phase="Running", Reason="", readiness=true. Elapsed: 4.005013242s
    Jan 30 11:51:00.989: INFO: Pod "webserver-deployment-5fd5d6fdb9-qcq74" satisfied condition "running"
    Jan 30 11:51:00.989: INFO: Pod "webserver-deployment-5fd5d6fdb9-r24vd": Phase="Running", Reason="", readiness=true. Elapsed: 4.005154184s
    Jan 30 11:51:00.989: INFO: Pod "webserver-deployment-5fd5d6fdb9-r24vd" satisfied condition "running"
    Jan 30 11:51:00.989: INFO: Waiting for deployment "webserver-deployment" to complete
    Jan 30 11:51:00.993: INFO: Updating deployment "webserver-deployment" with a non-existent image
    Jan 30 11:51:00.998: INFO: Updating deployment webserver-deployment
    Jan 30 11:51:00.998: INFO: Waiting for observed generation 2
    Jan 30 11:51:03.002: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
    Jan 30 11:51:03.004: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
    Jan 30 11:51:03.005: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
    Jan 30 11:51:03.010: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
    Jan 30 11:51:03.010: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
    Jan 30 11:51:03.011: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
    Jan 30 11:51:03.014: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
    Jan 30 11:51:03.014: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
    Jan 30 11:51:03.019: INFO: Updating deployment webserver-deployment
    Jan 30 11:51:03.019: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
    Jan 30 11:51:03.022: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
    Jan 30 11:51:05.026: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jan 30 11:51:05.030: INFO: Deployment "webserver-deployment":
    &Deployment{ObjectMeta:{webserver-deployment  deployment-1250  62284527-e53d-4ba3-90d8-13dfed8d38a9 23195 3 2023-01-30 11:50:54 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-01-30 11:51:03 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-30 11:51:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc006c7e748 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-01-30 11:51:03 +0000 UTC,LastTransitionTime:2023-01-30 11:51:03 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-d9f79cb5" is progressing.,LastUpdateTime:2023-01-30 11:51:03 +0000 UTC,LastTransitionTime:2023-01-30 11:50:54 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

    Jan 30 11:51:05.032: INFO: New ReplicaSet "webserver-deployment-d9f79cb5" of Deployment "webserver-deployment":
    &ReplicaSet{ObjectMeta:{webserver-deployment-d9f79cb5  deployment-1250  fd35b1eb-85bf-4c28-9962-33d36cfed5bf 23190 3 2023-01-30 11:51:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 62284527-e53d-4ba3-90d8-13dfed8d38a9 0xc0031151a7 0xc0031151a8}] [] [{kube-controller-manager Update apps/v1 2023-01-30 11:51:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"62284527-e53d-4ba3-90d8-13dfed8d38a9\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-30 11:51:03 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: d9f79cb5,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003115248 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Jan 30 11:51:05.032: INFO: All old ReplicaSets of Deployment "webserver-deployment":
    Jan 30 11:51:05.032: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-5fd5d6fdb9  deployment-1250  2b8b6be1-993a-47ab-9af1-82630db5cf44 23185 3 2023-01-30 11:50:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5fd5d6fdb9] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 62284527-e53d-4ba3-90d8-13dfed8d38a9 0xc0031150a7 0xc0031150a8}] [] [{kube-controller-manager Update apps/v1 2023-01-30 11:51:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"62284527-e53d-4ba3-90d8-13dfed8d38a9\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-30 11:51:03 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 5fd5d6fdb9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5fd5d6fdb9] map[] [] [] []} {[] [] [{httpd harbor.cloud.netease.com/qzprod-k8s/e2e/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003115148 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
    Jan 30 11:51:05.037: INFO: Pod "webserver-deployment-5fd5d6fdb9-2qxz4" is not available:
    &Pod{ObjectMeta:{webserver-deployment-5fd5d6fdb9-2qxz4 webserver-deployment-5fd5d6fdb9- deployment-1250  323d07de-cd84-49a8-b4ef-9cf294d68acc 23159 0 2023-01-30 11:51:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5fd5d6fdb9] map[] [{apps/v1 ReplicaSet webserver-deployment-5fd5d6fdb9 2b8b6be1-993a-47ab-9af1-82630db5cf44 0xc006c7eb17 0xc006c7eb18}] [] [{kube-controller-manager Update v1 2023-01-30 11:51:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2b8b6be1-993a-47ab-9af1-82630db5cf44\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6gbnc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:harbor.cloud.netease.com/qzprod-k8s/e2e/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6gbnc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pubt2-nks-for-dev1.dg.163.org,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 11:51:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 30 11:51:05.038: INFO: Pod "webserver-deployment-5fd5d6fdb9-4s5tq" is available:
    &Pod{ObjectMeta:{webserver-deployment-5fd5d6fdb9-4s5tq webserver-deployment-5fd5d6fdb9- deployment-1250  0124b092-f5f1-48aa-8aed-63b1be3d7394 22988 0 2023-01-30 11:50:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5fd5d6fdb9] map[cni.projectcalico.org/podIP:10.178.151.226/32 cni.projectcalico.org/podIPs:10.178.151.226/32] [{apps/v1 ReplicaSet webserver-deployment-5fd5d6fdb9 2b8b6be1-993a-47ab-9af1-82630db5cf44 0xc006c7ec70 0xc006c7ec71}] [] [{kube-controller-manager Update v1 2023-01-30 11:50:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2b8b6be1-993a-47ab-9af1-82630db5cf44\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-30 11:50:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-30 11:50:57 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.178.151.226\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7lg7c,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:harbor.cloud.netease.com/qzprod-k8s/e2e/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7lg7c,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pubt2-nks-for-dev1.dg.163.org,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 11:50:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 11:50:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 11:50:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 11:50:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.182.0.82,PodIP:10.178.151.226,StartTime:2023-01-30 11:50:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-30 11:50:56 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:harbor.cloud.netease.com/qzprod-k8s/e2e/httpd:2.4.38-4,ImageID:docker-pullable://harbor.cloud.netease.com/qzprod-k8s/e2e/httpd@sha256:8b745b6e9cb6a6184353110aabe28d8d6f6145d3fe7c0e1899aa70c8228c0fd3,ContainerID:docker://25a86fdb9b7111669608f4b3f93d2682b5d96b6e67e78629d1b0f98102cd9e27,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.178.151.226,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 30 11:51:05.038: INFO: Pod "webserver-deployment-5fd5d6fdb9-656k2" is not available:
    &Pod{ObjectMeta:{webserver-deployment-5fd5d6fdb9-656k2 webserver-deployment-5fd5d6fdb9- deployment-1250  16d06aa3-1e59-4dcb-a210-0b2de44735ab 23220 0 2023-01-30 11:51:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5fd5d6fdb9] map[cni.projectcalico.org/podIP:10.178.197.139/32 cni.projectcalico.org/podIPs:10.178.197.139/32] [{apps/v1 ReplicaSet webserver-deployment-5fd5d6fdb9 2b8b6be1-993a-47ab-9af1-82630db5cf44 0xc006c7ee67 0xc006c7ee68}] [] [{kube-controller-manager Update v1 2023-01-30 11:51:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2b8b6be1-993a-47ab-9af1-82630db5cf44\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-30 11:51:03 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-01-30 11:51:04 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hlt8v,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:harbor.cloud.netease.com/qzprod-k8s/e2e/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hlt8v,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pubt2-nks-for-dev3.dg.163.org,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 11:51:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 11:51:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 11:51:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 11:51:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.182.0.84,PodIP:,StartTime:2023-01-30 11:51:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:harbor.cloud.netease.com/qzprod-k8s/e2e/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 30 11:51:05.038: INFO: Pod "webserver-deployment-5fd5d6fdb9-6bbqk" is not available:
    &Pod{ObjectMeta:{webserver-deployment-5fd5d6fdb9-6bbqk webserver-deployment-5fd5d6fdb9- deployment-1250  e9a391f5-3c85-447a-9a84-0e2b0ee9d89d 23239 0 2023-01-30 11:51:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5fd5d6fdb9] map[cni.projectcalico.org/podIP:10.178.197.142/32 cni.projectcalico.org/podIPs:10.178.197.142/32] [{apps/v1 ReplicaSet webserver-deployment-5fd5d6fdb9 2b8b6be1-993a-47ab-9af1-82630db5cf44 0xc006c7f057 0xc006c7f058}] [] [{kube-controller-manager Update v1 2023-01-30 11:51:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2b8b6be1-993a-47ab-9af1-82630db5cf44\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-30 11:51:03 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-01-30 11:51:04 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-sl58t,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:harbor.cloud.netease.com/qzprod-k8s/e2e/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-sl58t,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pubt2-nks-for-dev3.dg.163.org,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 11:51:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 11:51:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 11:51:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 11:51:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.182.0.84,PodIP:,StartTime:2023-01-30 11:51:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:harbor.cloud.netease.com/qzprod-k8s/e2e/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 30 11:51:05.038: INFO: Pod "webserver-deployment-5fd5d6fdb9-87n4r" is available:
    &Pod{ObjectMeta:{webserver-deployment-5fd5d6fdb9-87n4r webserver-deployment-5fd5d6fdb9- deployment-1250  21e1cb26-4a17-4fcb-a51c-ee993b1f08ed 22994 0 2023-01-30 11:50:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5fd5d6fdb9] map[cni.projectcalico.org/podIP:10.178.197.136/32 cni.projectcalico.org/podIPs:10.178.197.136/32] [{apps/v1 ReplicaSet webserver-deployment-5fd5d6fdb9 2b8b6be1-993a-47ab-9af1-82630db5cf44 0xc006c7f237 0xc006c7f238}] [] [{kube-controller-manager Update v1 2023-01-30 11:50:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2b8b6be1-993a-47ab-9af1-82630db5cf44\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-30 11:50:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-30 11:50:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.178.197.136\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qkg4h,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:harbor.cloud.netease.com/qzprod-k8s/e2e/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qkg4h,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pubt2-nks-for-dev3.dg.163.org,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 11:50:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 11:50:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 11:50:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 11:50:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.182.0.84,PodIP:10.178.197.136,StartTime:2023-01-30 11:50:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-30 11:50:57 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:harbor.cloud.netease.com/qzprod-k8s/e2e/httpd:2.4.38-4,ImageID:docker-pullable://harbor.cloud.netease.com/qzprod-k8s/e2e/httpd@sha256:8b745b6e9cb6a6184353110aabe28d8d6f6145d3fe7c0e1899aa70c8228c0fd3,ContainerID:docker://5550c8678f317a1c40c50f8360e5c824b821b6b4f67909d6ccc96de2d9a94adf,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.178.197.136,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 30 11:51:05.038: INFO: Pod "webserver-deployment-5fd5d6fdb9-9cn5k" is available:
    &Pod{ObjectMeta:{webserver-deployment-5fd5d6fdb9-9cn5k webserver-deployment-5fd5d6fdb9- deployment-1250  f14df27e-7c38-4b9c-ac86-fcb9d4ce0170 22970 0 2023-01-30 11:50:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5fd5d6fdb9] map[cni.projectcalico.org/podIP:10.178.197.132/32 cni.projectcalico.org/podIPs:10.178.197.132/32] [{apps/v1 ReplicaSet webserver-deployment-5fd5d6fdb9 2b8b6be1-993a-47ab-9af1-82630db5cf44 0xc006c7f437 0xc006c7f438}] [] [{kube-controller-manager Update v1 2023-01-30 11:50:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2b8b6be1-993a-47ab-9af1-82630db5cf44\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-30 11:50:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-30 11:50:57 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.178.197.132\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-m8lnh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:harbor.cloud.netease.com/qzprod-k8s/e2e/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-m8lnh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pubt2-nks-for-dev3.dg.163.org,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 11:50:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 11:50:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 11:50:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 11:50:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.182.0.84,PodIP:10.178.197.132,StartTime:2023-01-30 11:50:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-30 11:50:56 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:harbor.cloud.netease.com/qzprod-k8s/e2e/httpd:2.4.38-4,ImageID:docker-pullable://harbor.cloud.netease.com/qzprod-k8s/e2e/httpd@sha256:8b745b6e9cb6a6184353110aabe28d8d6f6145d3fe7c0e1899aa70c8228c0fd3,ContainerID:docker://1c15e2d52c4640aecc19181dc8b9e652dc6491f090a1b6ed02e9fd2a668d98e4,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.178.197.132,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 30 11:51:05.039: INFO: Pod "webserver-deployment-5fd5d6fdb9-dc8zv" is available:
    &Pod{ObjectMeta:{webserver-deployment-5fd5d6fdb9-dc8zv webserver-deployment-5fd5d6fdb9- deployment-1250  5ade750d-2f8e-4c22-ae2b-7fd8d8b2e04b 22992 0 2023-01-30 11:50:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5fd5d6fdb9] map[cni.projectcalico.org/podIP:10.178.197.135/32 cni.projectcalico.org/podIPs:10.178.197.135/32] [{apps/v1 ReplicaSet webserver-deployment-5fd5d6fdb9 2b8b6be1-993a-47ab-9af1-82630db5cf44 0xc006c7f637 0xc006c7f638}] [] [{kube-controller-manager Update v1 2023-01-30 11:50:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2b8b6be1-993a-47ab-9af1-82630db5cf44\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-30 11:50:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-30 11:50:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.178.197.135\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8d266,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:harbor.cloud.netease.com/qzprod-k8s/e2e/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8d266,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pubt2-nks-for-dev3.dg.163.org,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 11:50:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 11:50:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 11:50:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 11:50:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.182.0.84,PodIP:10.178.197.135,StartTime:2023-01-30 11:50:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-30 11:50:57 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:harbor.cloud.netease.com/qzprod-k8s/e2e/httpd:2.4.38-4,ImageID:docker-pullable://harbor.cloud.netease.com/qzprod-k8s/e2e/httpd@sha256:8b745b6e9cb6a6184353110aabe28d8d6f6145d3fe7c0e1899aa70c8228c0fd3,ContainerID:docker://b776c43385d88e3693c4c514ef57010ffcb8c0c24b403561df821184dfebb0c7,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.178.197.135,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 30 11:51:05.039: INFO: Pod "webserver-deployment-5fd5d6fdb9-glrwp" is not available:
    &Pod{ObjectMeta:{webserver-deployment-5fd5d6fdb9-glrwp webserver-deployment-5fd5d6fdb9- deployment-1250  886233b7-6e39-4c47-9430-e5e9dacdec4c 23193 0 2023-01-30 11:51:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5fd5d6fdb9] map[] [{apps/v1 ReplicaSet webserver-deployment-5fd5d6fdb9 2b8b6be1-993a-47ab-9af1-82630db5cf44 0xc006c7f837 0xc006c7f838}] [] [{kube-controller-manager Update v1 2023-01-30 11:51:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2b8b6be1-993a-47ab-9af1-82630db5cf44\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-30 11:51:03 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-dr8dv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:harbor.cloud.netease.com/qzprod-k8s/e2e/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dr8dv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pubt2-nks-for-dev3.dg.163.org,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 11:51:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 11:51:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 11:51:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 11:51:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.182.0.84,PodIP:,StartTime:2023-01-30 11:51:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:harbor.cloud.netease.com/qzprod-k8s/e2e/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 30 11:51:05.039: INFO: Pod "webserver-deployment-5fd5d6fdb9-gt4h7" is not available:
    &Pod{ObjectMeta:{webserver-deployment-5fd5d6fdb9-gt4h7 webserver-deployment-5fd5d6fdb9- deployment-1250  ccd1ac80-b41f-4141-993a-3338fca85d7f 23152 0 2023-01-30 11:51:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5fd5d6fdb9] map[] [{apps/v1 ReplicaSet webserver-deployment-5fd5d6fdb9 2b8b6be1-993a-47ab-9af1-82630db5cf44 0xc006c7f9f7 0xc006c7f9f8}] [] [{kube-controller-manager Update v1 2023-01-30 11:51:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2b8b6be1-993a-47ab-9af1-82630db5cf44\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8vb4r,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:harbor.cloud.netease.com/qzprod-k8s/e2e/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8vb4r,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pubt2-nks-for-dev1.dg.163.org,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 11:51:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 30 11:51:05.039: INFO: Pod "webserver-deployment-5fd5d6fdb9-h4km9" is not available:
    &Pod{ObjectMeta:{webserver-deployment-5fd5d6fdb9-h4km9 webserver-deployment-5fd5d6fdb9- deployment-1250  bfda5d4d-0a27-46c8-a2a0-421f60d8121e 23169 0 2023-01-30 11:51:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5fd5d6fdb9] map[] [{apps/v1 ReplicaSet webserver-deployment-5fd5d6fdb9 2b8b6be1-993a-47ab-9af1-82630db5cf44 0xc006c7fb50 0xc006c7fb51}] [] [{kube-controller-manager Update v1 2023-01-30 11:51:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2b8b6be1-993a-47ab-9af1-82630db5cf44\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-99mbn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:harbor.cloud.netease.com/qzprod-k8s/e2e/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-99mbn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pubt2-nks-for-dev1.dg.163.org,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 11:51:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 30 11:51:05.039: INFO: Pod "webserver-deployment-5fd5d6fdb9-jt44h" is not available:
    &Pod{ObjectMeta:{webserver-deployment-5fd5d6fdb9-jt44h webserver-deployment-5fd5d6fdb9- deployment-1250  38308d3b-4bd4-4546-a296-a82f8750911e 23211 0 2023-01-30 11:51:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5fd5d6fdb9] map[] [{apps/v1 ReplicaSet webserver-deployment-5fd5d6fdb9 2b8b6be1-993a-47ab-9af1-82630db5cf44 0xc006c7fcb0 0xc006c7fcb1}] [] [{kube-controller-manager Update v1 2023-01-30 11:51:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2b8b6be1-993a-47ab-9af1-82630db5cf44\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-30 11:51:03 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-j9mdk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:harbor.cloud.netease.com/qzprod-k8s/e2e/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-j9mdk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pubt2-nks-for-dev1.dg.163.org,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 11:51:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 11:51:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 11:51:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 11:51:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.182.0.82,PodIP:,StartTime:2023-01-30 11:51:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:harbor.cloud.netease.com/qzprod-k8s/e2e/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 30 11:51:05.039: INFO: Pod "webserver-deployment-5fd5d6fdb9-m96n9" is not available:
    &Pod{ObjectMeta:{webserver-deployment-5fd5d6fdb9-m96n9 webserver-deployment-5fd5d6fdb9- deployment-1250  975e2291-5e07-4d38-8baa-207f74a7b2d2 23157 0 2023-01-30 11:51:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5fd5d6fdb9] map[] [{apps/v1 ReplicaSet webserver-deployment-5fd5d6fdb9 2b8b6be1-993a-47ab-9af1-82630db5cf44 0xc006c7fe67 0xc006c7fe68}] [] [{kube-controller-manager Update v1 2023-01-30 11:51:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2b8b6be1-993a-47ab-9af1-82630db5cf44\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-30 11:51:03 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vskgx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:harbor.cloud.netease.com/qzprod-k8s/e2e/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vskgx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pubt2-nks-for-dev1.dg.163.org,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 11:51:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 11:51:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 11:51:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 11:51:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.182.0.82,PodIP:,StartTime:2023-01-30 11:51:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:harbor.cloud.netease.com/qzprod-k8s/e2e/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 30 11:51:05.040: INFO: Pod "webserver-deployment-5fd5d6fdb9-qcq74" is available:
    &Pod{ObjectMeta:{webserver-deployment-5fd5d6fdb9-qcq74 webserver-deployment-5fd5d6fdb9- deployment-1250  8d4c47b6-5238-4da8-9390-29e583143cfa 23012 0 2023-01-30 11:50:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5fd5d6fdb9] map[cni.projectcalico.org/podIP:10.178.151.230/32 cni.projectcalico.org/podIPs:10.178.151.230/32] [{apps/v1 ReplicaSet webserver-deployment-5fd5d6fdb9 2b8b6be1-993a-47ab-9af1-82630db5cf44 0xc005902027 0xc005902028}] [] [{kube-controller-manager Update v1 2023-01-30 11:50:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2b8b6be1-993a-47ab-9af1-82630db5cf44\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-30 11:50:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-30 11:50:59 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.178.151.230\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-dtpbm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:harbor.cloud.netease.com/qzprod-k8s/e2e/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dtpbm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pubt2-nks-for-dev1.dg.163.org,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 11:50:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 11:50:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 11:50:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 11:50:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.182.0.82,PodIP:10.178.151.230,StartTime:2023-01-30 11:50:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-30 11:50:57 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:harbor.cloud.netease.com/qzprod-k8s/e2e/httpd:2.4.38-4,ImageID:docker-pullable://harbor.cloud.netease.com/qzprod-k8s/e2e/httpd@sha256:8b745b6e9cb6a6184353110aabe28d8d6f6145d3fe7c0e1899aa70c8228c0fd3,ContainerID:docker://8fef8acf1ec66844fad4670cd6890597c109df93ab605f93992875c96590893e,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.178.151.230,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 30 11:51:05.040: INFO: Pod "webserver-deployment-5fd5d6fdb9-r24vd" is available:
    &Pod{ObjectMeta:{webserver-deployment-5fd5d6fdb9-r24vd webserver-deployment-5fd5d6fdb9- deployment-1250  ad3384d2-10d5-46e0-b3f6-8b5aebd7a4ff 23009 0 2023-01-30 11:50:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5fd5d6fdb9] map[cni.projectcalico.org/podIP:10.178.151.228/32 cni.projectcalico.org/podIPs:10.178.151.228/32] [{apps/v1 ReplicaSet webserver-deployment-5fd5d6fdb9 2b8b6be1-993a-47ab-9af1-82630db5cf44 0xc005902227 0xc005902228}] [] [{kube-controller-manager Update v1 2023-01-30 11:50:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2b8b6be1-993a-47ab-9af1-82630db5cf44\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-30 11:50:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-30 11:50:59 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.178.151.228\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-sdd2l,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:harbor.cloud.netease.com/qzprod-k8s/e2e/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-sdd2l,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pubt2-nks-for-dev1.dg.163.org,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 11:50:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 11:50:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 11:50:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 11:50:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.182.0.82,PodIP:10.178.151.228,StartTime:2023-01-30 11:50:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-30 11:50:57 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:harbor.cloud.netease.com/qzprod-k8s/e2e/httpd:2.4.38-4,ImageID:docker-pullable://harbor.cloud.netease.com/qzprod-k8s/e2e/httpd@sha256:8b745b6e9cb6a6184353110aabe28d8d6f6145d3fe7c0e1899aa70c8228c0fd3,ContainerID:docker://761b554728bad0999b2604617dd727c6cfa73711859ff8a6467fcdf3f2a6b504,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.178.151.228,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 30 11:51:05.040: INFO: Pod "webserver-deployment-5fd5d6fdb9-rwqfr" is available:
    &Pod{ObjectMeta:{webserver-deployment-5fd5d6fdb9-rwqfr webserver-deployment-5fd5d6fdb9- deployment-1250  9b173ff6-5263-4bcc-87cd-71f91904b88a 22967 0 2023-01-30 11:50:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5fd5d6fdb9] map[cni.projectcalico.org/podIP:10.178.197.133/32 cni.projectcalico.org/podIPs:10.178.197.133/32] [{apps/v1 ReplicaSet webserver-deployment-5fd5d6fdb9 2b8b6be1-993a-47ab-9af1-82630db5cf44 0xc005902427 0xc005902428}] [] [{kube-controller-manager Update v1 2023-01-30 11:50:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2b8b6be1-993a-47ab-9af1-82630db5cf44\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-30 11:50:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-30 11:50:57 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.178.197.133\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-584b8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:harbor.cloud.netease.com/qzprod-k8s/e2e/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-584b8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pubt2-nks-for-dev3.dg.163.org,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 11:50:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 11:50:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 11:50:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 11:50:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.182.0.84,PodIP:10.178.197.133,StartTime:2023-01-30 11:50:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-30 11:50:56 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:harbor.cloud.netease.com/qzprod-k8s/e2e/httpd:2.4.38-4,ImageID:docker-pullable://harbor.cloud.netease.com/qzprod-k8s/e2e/httpd@sha256:8b745b6e9cb6a6184353110aabe28d8d6f6145d3fe7c0e1899aa70c8228c0fd3,ContainerID:docker://b0118e397da0a4bdf894830866c6ec629b7ce88415b57e6d00d3351c5d823894,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.178.197.133,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 30 11:51:05.040: INFO: Pod "webserver-deployment-5fd5d6fdb9-tpgd4" is not available:
    &Pod{ObjectMeta:{webserver-deployment-5fd5d6fdb9-tpgd4 webserver-deployment-5fd5d6fdb9- deployment-1250  f8b2f903-276c-4fc0-8834-a1c80977f40b 23250 0 2023-01-30 11:51:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5fd5d6fdb9] map[cni.projectcalico.org/podIP:10.178.151.234/32 cni.projectcalico.org/podIPs:10.178.151.234/32] [{apps/v1 ReplicaSet webserver-deployment-5fd5d6fdb9 2b8b6be1-993a-47ab-9af1-82630db5cf44 0xc005902627 0xc005902628}] [] [{kube-controller-manager Update v1 2023-01-30 11:51:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2b8b6be1-993a-47ab-9af1-82630db5cf44\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-30 11:51:04 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-cdvbd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:harbor.cloud.netease.com/qzprod-k8s/e2e/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-cdvbd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pubt2-nks-for-dev1.dg.163.org,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 11:51:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 30 11:51:05.040: INFO: Pod "webserver-deployment-5fd5d6fdb9-vxdlh" is available:
    &Pod{ObjectMeta:{webserver-deployment-5fd5d6fdb9-vxdlh webserver-deployment-5fd5d6fdb9- deployment-1250  3d432818-6faa-411e-ab59-5da048b45e87 22999 0 2023-01-30 11:50:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5fd5d6fdb9] map[cni.projectcalico.org/podIP:10.178.197.134/32 cni.projectcalico.org/podIPs:10.178.197.134/32] [{apps/v1 ReplicaSet webserver-deployment-5fd5d6fdb9 2b8b6be1-993a-47ab-9af1-82630db5cf44 0xc0059027a0 0xc0059027a1}] [] [{kube-controller-manager Update v1 2023-01-30 11:50:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2b8b6be1-993a-47ab-9af1-82630db5cf44\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-30 11:50:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-30 11:50:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.178.197.134\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-z8qjt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:harbor.cloud.netease.com/qzprod-k8s/e2e/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-z8qjt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pubt2-nks-for-dev3.dg.163.org,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 11:50:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 11:50:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 11:50:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 11:50:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.182.0.84,PodIP:10.178.197.134,StartTime:2023-01-30 11:50:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-30 11:50:57 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:harbor.cloud.netease.com/qzprod-k8s/e2e/httpd:2.4.38-4,ImageID:docker-pullable://harbor.cloud.netease.com/qzprod-k8s/e2e/httpd@sha256:8b745b6e9cb6a6184353110aabe28d8d6f6145d3fe7c0e1899aa70c8228c0fd3,ContainerID:docker://f02f138f4c8ac2ec8e509c9ed5d6524480df1c8e19987037c3541b4e98a037fa,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.178.197.134,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 30 11:51:05.040: INFO: Pod "webserver-deployment-5fd5d6fdb9-x4kmt" is not available:
    &Pod{ObjectMeta:{webserver-deployment-5fd5d6fdb9-x4kmt webserver-deployment-5fd5d6fdb9- deployment-1250  4df70344-0bd2-4bf1-a274-94c2e229bc5d 23165 0 2023-01-30 11:51:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5fd5d6fdb9] map[] [{apps/v1 ReplicaSet webserver-deployment-5fd5d6fdb9 2b8b6be1-993a-47ab-9af1-82630db5cf44 0xc005902997 0xc005902998}] [] [{kube-controller-manager Update v1 2023-01-30 11:51:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2b8b6be1-993a-47ab-9af1-82630db5cf44\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-48bjg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:harbor.cloud.netease.com/qzprod-k8s/e2e/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-48bjg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pubt2-nks-for-dev3.dg.163.org,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 11:51:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 30 11:51:05.041: INFO: Pod "webserver-deployment-5fd5d6fdb9-xfmmv" is not available:
    &Pod{ObjectMeta:{webserver-deployment-5fd5d6fdb9-xfmmv webserver-deployment-5fd5d6fdb9- deployment-1250  5cd754ba-1dd4-4e3a-b8d1-42fc99cb6e9b 23139 0 2023-01-30 11:51:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5fd5d6fdb9] map[] [{apps/v1 ReplicaSet webserver-deployment-5fd5d6fdb9 2b8b6be1-993a-47ab-9af1-82630db5cf44 0xc005902af0 0xc005902af1}] [] [{kube-controller-manager Update v1 2023-01-30 11:51:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2b8b6be1-993a-47ab-9af1-82630db5cf44\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pskcs,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:harbor.cloud.netease.com/qzprod-k8s/e2e/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pskcs,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pubt2-nks-for-dev1.dg.163.org,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 11:51:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 30 11:51:05.041: INFO: Pod "webserver-deployment-5fd5d6fdb9-zcf9q" is not available:
    &Pod{ObjectMeta:{webserver-deployment-5fd5d6fdb9-zcf9q webserver-deployment-5fd5d6fdb9- deployment-1250  715930e2-10eb-473e-93d3-e15619a6ccd3 23252 0 2023-01-30 11:51:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5fd5d6fdb9] map[cni.projectcalico.org/podIP:10.178.197.143/32 cni.projectcalico.org/podIPs:10.178.197.143/32] [{apps/v1 ReplicaSet webserver-deployment-5fd5d6fdb9 2b8b6be1-993a-47ab-9af1-82630db5cf44 0xc005902c40 0xc005902c41}] [] [{kube-controller-manager Update v1 2023-01-30 11:51:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2b8b6be1-993a-47ab-9af1-82630db5cf44\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-30 11:51:04 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-p8nft,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:harbor.cloud.netease.com/qzprod-k8s/e2e/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-p8nft,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pubt2-nks-for-dev3.dg.163.org,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 11:51:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 30 11:51:05.041: INFO: Pod "webserver-deployment-d9f79cb5-45zbq" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-45zbq webserver-deployment-d9f79cb5- deployment-1250  e7ead0e8-1502-47e5-b903-615f4f070da4 23110 0 2023-01-30 11:51:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/podIP:10.178.151.233/32 cni.projectcalico.org/podIPs:10.178.151.233/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 fd35b1eb-85bf-4c28-9962-33d36cfed5bf 0xc005902d9f 0xc005902db0}] [] [{kube-controller-manager Update v1 2023-01-30 11:51:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fd35b1eb-85bf-4c28-9962-33d36cfed5bf\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-30 11:51:02 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-30 11:51:02 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-s9nfp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-s9nfp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pubt2-nks-for-dev1.dg.163.org,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 11:51:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 11:51:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 11:51:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 11:51:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.182.0.82,PodIP:,StartTime:2023-01-30 11:51:01 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 30 11:51:05.041: INFO: Pod "webserver-deployment-d9f79cb5-7xjc2" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-7xjc2 webserver-deployment-d9f79cb5- deployment-1250  dc6ce146-84dd-4917-abd8-c13ea31c223a 23106 0 2023-01-30 11:51:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/podIP:10.178.151.232/32 cni.projectcalico.org/podIPs:10.178.151.232/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 fd35b1eb-85bf-4c28-9962-33d36cfed5bf 0xc005902f8f 0xc005902fa0}] [] [{kube-controller-manager Update v1 2023-01-30 11:51:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fd35b1eb-85bf-4c28-9962-33d36cfed5bf\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-30 11:51:01 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-01-30 11:51:02 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mbzmp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mbzmp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pubt2-nks-for-dev1.dg.163.org,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 11:51:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 11:51:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 11:51:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 11:51:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.182.0.82,PodIP:,StartTime:2023-01-30 11:51:01 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 30 11:51:05.041: INFO: Pod "webserver-deployment-d9f79cb5-bqknr" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-bqknr webserver-deployment-d9f79cb5- deployment-1250  00506c8d-fce9-4c34-83ee-843aa8b8911b 23094 0 2023-01-30 11:51:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/podIP:10.178.197.138/32 cni.projectcalico.org/podIPs:10.178.197.138/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 fd35b1eb-85bf-4c28-9962-33d36cfed5bf 0xc00590317f 0xc005903190}] [] [{calico Update v1 2023-01-30 11:51:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-01-30 11:51:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fd35b1eb-85bf-4c28-9962-33d36cfed5bf\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-30 11:51:01 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9tbvf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9tbvf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pubt2-nks-for-dev3.dg.163.org,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 11:51:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 11:51:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 11:51:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 11:51:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.182.0.84,PodIP:,StartTime:2023-01-30 11:51:01 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 30 11:51:05.041: INFO: Pod "webserver-deployment-d9f79cb5-g45b6" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-g45b6 webserver-deployment-d9f79cb5- deployment-1250  696c55e9-c0cb-4d05-88b0-094df3e4aef6 23187 0 2023-01-30 11:51:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 fd35b1eb-85bf-4c28-9962-33d36cfed5bf 0xc00590336f 0xc005903380}] [] [{kube-controller-manager Update v1 2023-01-30 11:51:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fd35b1eb-85bf-4c28-9962-33d36cfed5bf\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-30 11:51:03 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-s2wts,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-s2wts,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pubt2-nks-for-dev3.dg.163.org,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 11:51:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 11:51:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 11:51:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 11:51:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.182.0.84,PodIP:,StartTime:2023-01-30 11:51:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 30 11:51:05.042: INFO: Pod "webserver-deployment-d9f79cb5-g6qr5" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-g6qr5 webserver-deployment-d9f79cb5- deployment-1250  aa289320-32a2-433f-a3eb-ed59aea05f75 23162 0 2023-01-30 11:51:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 fd35b1eb-85bf-4c28-9962-33d36cfed5bf 0xc00590353f 0xc005903550}] [] [{kube-controller-manager Update v1 2023-01-30 11:51:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fd35b1eb-85bf-4c28-9962-33d36cfed5bf\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lbnzn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lbnzn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pubt2-nks-for-dev1.dg.163.org,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 11:51:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 30 11:51:05.042: INFO: Pod "webserver-deployment-d9f79cb5-jhrmj" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-jhrmj webserver-deployment-d9f79cb5- deployment-1250  07c6accd-fe84-4a0d-aaed-25a2d52bb2a3 23257 0 2023-01-30 11:51:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/podIP:10.178.151.235/32 cni.projectcalico.org/podIPs:10.178.151.235/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 fd35b1eb-85bf-4c28-9962-33d36cfed5bf 0xc00590369f 0xc0059036b0}] [] [{kube-controller-manager Update v1 2023-01-30 11:51:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fd35b1eb-85bf-4c28-9962-33d36cfed5bf\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-30 11:51:04 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zx7wx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zx7wx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pubt2-nks-for-dev1.dg.163.org,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 11:51:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 30 11:51:05.042: INFO: Pod "webserver-deployment-d9f79cb5-lhxkt" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-lhxkt webserver-deployment-d9f79cb5- deployment-1250  4c26f4c4-6642-4faf-ae15-19c6225f97c8 23098 0 2023-01-30 11:51:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/podIP:10.178.151.231/32 cni.projectcalico.org/podIPs:10.178.151.231/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 fd35b1eb-85bf-4c28-9962-33d36cfed5bf 0xc00590381f 0xc005903830}] [] [{kube-controller-manager Update v1 2023-01-30 11:51:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fd35b1eb-85bf-4c28-9962-33d36cfed5bf\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-30 11:51:01 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-01-30 11:51:02 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vcwfw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vcwfw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pubt2-nks-for-dev1.dg.163.org,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 11:51:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 11:51:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 11:51:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 11:51:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.182.0.82,PodIP:,StartTime:2023-01-30 11:51:01 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 30 11:51:05.042: INFO: Pod "webserver-deployment-d9f79cb5-nshn2" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-nshn2 webserver-deployment-d9f79cb5- deployment-1250  06b9ed82-824a-441f-a6aa-ca40388e280e 23173 0 2023-01-30 11:51:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 fd35b1eb-85bf-4c28-9962-33d36cfed5bf 0xc005903a0f 0xc005903a20}] [] [{kube-controller-manager Update v1 2023-01-30 11:51:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fd35b1eb-85bf-4c28-9962-33d36cfed5bf\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qgz92,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qgz92,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pubt2-nks-for-dev1.dg.163.org,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 11:51:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 30 11:51:05.042: INFO: Pod "webserver-deployment-d9f79cb5-pt5m8" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-pt5m8 webserver-deployment-d9f79cb5- deployment-1250  ac205318-e2ad-4cd7-9998-7e9f6cc46382 23232 0 2023-01-30 11:51:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/podIP:10.178.197.141/32 cni.projectcalico.org/podIPs:10.178.197.141/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 fd35b1eb-85bf-4c28-9962-33d36cfed5bf 0xc005903b6f 0xc005903b80}] [] [{kube-controller-manager Update v1 2023-01-30 11:51:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fd35b1eb-85bf-4c28-9962-33d36cfed5bf\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-30 11:51:04 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pdf2d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pdf2d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pubt2-nks-for-dev3.dg.163.org,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 11:51:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 30 11:51:05.042: INFO: Pod "webserver-deployment-d9f79cb5-sd5tj" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-sd5tj webserver-deployment-d9f79cb5- deployment-1250  2ae2ce0a-8c26-48bd-92e5-bb45f3240afe 23221 0 2023-01-30 11:51:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/podIP:10.178.197.140/32 cni.projectcalico.org/podIPs:10.178.197.140/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 fd35b1eb-85bf-4c28-9962-33d36cfed5bf 0xc005903cef 0xc005903d00}] [] [{kube-controller-manager Update v1 2023-01-30 11:51:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fd35b1eb-85bf-4c28-9962-33d36cfed5bf\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-30 11:51:04 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jj6tn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jj6tn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pubt2-nks-for-dev3.dg.163.org,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 11:51:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 30 11:51:05.042: INFO: Pod "webserver-deployment-d9f79cb5-sfbbd" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-sfbbd webserver-deployment-d9f79cb5- deployment-1250  e48d4768-2677-4594-8624-e6e09074e887 23092 0 2023-01-30 11:51:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/podIP:10.178.197.137/32 cni.projectcalico.org/podIPs:10.178.197.137/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 fd35b1eb-85bf-4c28-9962-33d36cfed5bf 0xc005903e6f 0xc005903e80}] [] [{calico Update v1 2023-01-30 11:51:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-01-30 11:51:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fd35b1eb-85bf-4c28-9962-33d36cfed5bf\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-30 11:51:01 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-d2trb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-d2trb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pubt2-nks-for-dev3.dg.163.org,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 11:51:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 11:51:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 11:51:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 11:51:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.182.0.84,PodIP:,StartTime:2023-01-30 11:51:01 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 30 11:51:05.043: INFO: Pod "webserver-deployment-d9f79cb5-t4598" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-t4598 webserver-deployment-d9f79cb5- deployment-1250  bdac5af9-5625-4738-9bb7-2b16c11c77e5 23149 0 2023-01-30 11:51:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 fd35b1eb-85bf-4c28-9962-33d36cfed5bf 0xc00592205f 0xc005922070}] [] [{kube-controller-manager Update v1 2023-01-30 11:51:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fd35b1eb-85bf-4c28-9962-33d36cfed5bf\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-s7btm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-s7btm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pubt2-nks-for-dev1.dg.163.org,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 11:51:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 30 11:51:05.043: INFO: Pod "webserver-deployment-d9f79cb5-zxnxg" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-zxnxg webserver-deployment-d9f79cb5- deployment-1250  07b211d8-3681-4c30-88b3-f410e4b40875 23200 0 2023-01-30 11:51:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 fd35b1eb-85bf-4c28-9962-33d36cfed5bf 0xc0059221bf 0xc0059221d0}] [] [{kube-controller-manager Update v1 2023-01-30 11:51:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fd35b1eb-85bf-4c28-9962-33d36cfed5bf\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-30 11:51:03 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ct6zz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ct6zz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pubt2-nks-for-dev3.dg.163.org,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 11:51:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 11:51:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 11:51:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 11:51:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.182.0.84,PodIP:,StartTime:2023-01-30 11:51:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Jan 30 11:51:05.043: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-1250" for this suite. 01/30/23 11:51:05.046
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:66
[BeforeEach] [sig-network] EndpointSlice
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 11:51:05.054
Jan 30 11:51:05.054: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename endpointslice 01/30/23 11:51:05.055
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:51:05.062
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:51:05.064
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:52
[It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:66
Jan 30 11:51:05.071: INFO: Endpoints addresses: [10.182.0.84] , ports: [6443]
Jan 30 11:51:05.071: INFO: EndpointSlices addresses: [10.182.0.84] , ports: [6443]
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/node/init/init.go:32
Jan 30 11:51:05.071: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSlice
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSlice
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslice-1228" for this suite. 01/30/23 11:51:05.073
------------------------------
• [0.022 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:66

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 11:51:05.054
    Jan 30 11:51:05.054: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename endpointslice 01/30/23 11:51:05.055
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:51:05.062
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:51:05.064
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:52
    [It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
      test/e2e/network/endpointslice.go:66
    Jan 30 11:51:05.071: INFO: Endpoints addresses: [10.182.0.84] , ports: [6443]
    Jan 30 11:51:05.071: INFO: EndpointSlices addresses: [10.182.0.84] , ports: [6443]
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/node/init/init.go:32
    Jan 30 11:51:05.071: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslice-1228" for this suite. 01/30/23 11:51:05.073
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] RuntimeClass
   should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 11:51:05.076
Jan 30 11:51:05.076: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename runtimeclass 01/30/23 11:51:05.077
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:51:05.086
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:51:05.088
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It]  should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189
STEP: getting /apis 01/30/23 11:51:05.09
STEP: getting /apis/node.k8s.io 01/30/23 11:51:05.092
STEP: getting /apis/node.k8s.io/v1 01/30/23 11:51:05.093
STEP: creating 01/30/23 11:51:05.094
STEP: watching 01/30/23 11:51:05.101
Jan 30 11:51:05.101: INFO: starting watch
STEP: getting 01/30/23 11:51:05.103
STEP: listing 01/30/23 11:51:05.105
STEP: patching 01/30/23 11:51:05.107
STEP: updating 01/30/23 11:51:05.109
Jan 30 11:51:05.112: INFO: waiting for watch events with expected annotations
STEP: deleting 01/30/23 11:51:05.112
STEP: deleting a collection 01/30/23 11:51:05.118
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Jan 30 11:51:05.124: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-8703" for this suite. 01/30/23 11:51:05.126
------------------------------
• [0.053 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
   should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 11:51:05.076
    Jan 30 11:51:05.076: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename runtimeclass 01/30/23 11:51:05.077
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:51:05.086
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:51:05.088
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It]  should support RuntimeClasses API operations [Conformance]
      test/e2e/common/node/runtimeclass.go:189
    STEP: getting /apis 01/30/23 11:51:05.09
    STEP: getting /apis/node.k8s.io 01/30/23 11:51:05.092
    STEP: getting /apis/node.k8s.io/v1 01/30/23 11:51:05.093
    STEP: creating 01/30/23 11:51:05.094
    STEP: watching 01/30/23 11:51:05.101
    Jan 30 11:51:05.101: INFO: starting watch
    STEP: getting 01/30/23 11:51:05.103
    STEP: listing 01/30/23 11:51:05.105
    STEP: patching 01/30/23 11:51:05.107
    STEP: updating 01/30/23 11:51:05.109
    Jan 30 11:51:05.112: INFO: waiting for watch events with expected annotations
    STEP: deleting 01/30/23 11:51:05.112
    STEP: deleting a collection 01/30/23 11:51:05.118
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Jan 30 11:51:05.124: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-8703" for this suite. 01/30/23 11:51:05.126
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:57
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 11:51:05.13
Jan 30 11:51:05.130: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename projected 01/30/23 11:51:05.131
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:51:05.14
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:51:05.143
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:57
STEP: Creating configMap with name projected-configmap-test-volume-776197de-b549-403f-b057-5af899fc2c75 01/30/23 11:51:05.145
STEP: Creating a pod to test consume configMaps 01/30/23 11:51:05.147
Jan 30 11:51:05.152: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-7c84bb00-00dd-4415-9252-6a9e60bba102" in namespace "projected-6097" to be "Succeeded or Failed"
Jan 30 11:51:05.153: INFO: Pod "pod-projected-configmaps-7c84bb00-00dd-4415-9252-6a9e60bba102": Phase="Pending", Reason="", readiness=false. Elapsed: 1.649082ms
Jan 30 11:51:07.156: INFO: Pod "pod-projected-configmaps-7c84bb00-00dd-4415-9252-6a9e60bba102": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004633434s
Jan 30 11:51:09.156: INFO: Pod "pod-projected-configmaps-7c84bb00-00dd-4415-9252-6a9e60bba102": Phase="Pending", Reason="", readiness=false. Elapsed: 4.004030457s
Jan 30 11:51:11.157: INFO: Pod "pod-projected-configmaps-7c84bb00-00dd-4415-9252-6a9e60bba102": Phase="Pending", Reason="", readiness=false. Elapsed: 6.004844653s
Jan 30 11:51:13.156: INFO: Pod "pod-projected-configmaps-7c84bb00-00dd-4415-9252-6a9e60bba102": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.004462118s
STEP: Saw pod success 01/30/23 11:51:13.156
Jan 30 11:51:13.156: INFO: Pod "pod-projected-configmaps-7c84bb00-00dd-4415-9252-6a9e60bba102" satisfied condition "Succeeded or Failed"
Jan 30 11:51:13.158: INFO: Trying to get logs from node pubt2-nks-for-dev1.dg.163.org pod pod-projected-configmaps-7c84bb00-00dd-4415-9252-6a9e60bba102 container agnhost-container: <nil>
STEP: delete the pod 01/30/23 11:51:13.162
Jan 30 11:51:13.167: INFO: Waiting for pod pod-projected-configmaps-7c84bb00-00dd-4415-9252-6a9e60bba102 to disappear
Jan 30 11:51:13.169: INFO: Pod pod-projected-configmaps-7c84bb00-00dd-4415-9252-6a9e60bba102 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Jan 30 11:51:13.169: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-6097" for this suite. 01/30/23 11:51:13.171
------------------------------
• [SLOW TEST] [8.043 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 11:51:05.13
    Jan 30 11:51:05.130: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename projected 01/30/23 11:51:05.131
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:51:05.14
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:51:05.143
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:57
    STEP: Creating configMap with name projected-configmap-test-volume-776197de-b549-403f-b057-5af899fc2c75 01/30/23 11:51:05.145
    STEP: Creating a pod to test consume configMaps 01/30/23 11:51:05.147
    Jan 30 11:51:05.152: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-7c84bb00-00dd-4415-9252-6a9e60bba102" in namespace "projected-6097" to be "Succeeded or Failed"
    Jan 30 11:51:05.153: INFO: Pod "pod-projected-configmaps-7c84bb00-00dd-4415-9252-6a9e60bba102": Phase="Pending", Reason="", readiness=false. Elapsed: 1.649082ms
    Jan 30 11:51:07.156: INFO: Pod "pod-projected-configmaps-7c84bb00-00dd-4415-9252-6a9e60bba102": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004633434s
    Jan 30 11:51:09.156: INFO: Pod "pod-projected-configmaps-7c84bb00-00dd-4415-9252-6a9e60bba102": Phase="Pending", Reason="", readiness=false. Elapsed: 4.004030457s
    Jan 30 11:51:11.157: INFO: Pod "pod-projected-configmaps-7c84bb00-00dd-4415-9252-6a9e60bba102": Phase="Pending", Reason="", readiness=false. Elapsed: 6.004844653s
    Jan 30 11:51:13.156: INFO: Pod "pod-projected-configmaps-7c84bb00-00dd-4415-9252-6a9e60bba102": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.004462118s
    STEP: Saw pod success 01/30/23 11:51:13.156
    Jan 30 11:51:13.156: INFO: Pod "pod-projected-configmaps-7c84bb00-00dd-4415-9252-6a9e60bba102" satisfied condition "Succeeded or Failed"
    Jan 30 11:51:13.158: INFO: Trying to get logs from node pubt2-nks-for-dev1.dg.163.org pod pod-projected-configmaps-7c84bb00-00dd-4415-9252-6a9e60bba102 container agnhost-container: <nil>
    STEP: delete the pod 01/30/23 11:51:13.162
    Jan 30 11:51:13.167: INFO: Waiting for pod pod-projected-configmaps-7c84bb00-00dd-4415-9252-6a9e60bba102 to disappear
    Jan 30 11:51:13.169: INFO: Pod pod-projected-configmaps-7c84bb00-00dd-4415-9252-6a9e60bba102 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Jan 30 11:51:13.169: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-6097" for this suite. 01/30/23 11:51:13.171
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:127
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 11:51:13.174
Jan 30 11:51:13.174: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename emptydir 01/30/23 11:51:13.174
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:51:13.18
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:51:13.182
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:127
STEP: Creating a pod to test emptydir 0644 on tmpfs 01/30/23 11:51:13.184
Jan 30 11:51:13.188: INFO: Waiting up to 5m0s for pod "pod-7fde0905-c91a-42b3-bfc5-71641afe6b16" in namespace "emptydir-2349" to be "Succeeded or Failed"
Jan 30 11:51:13.190: INFO: Pod "pod-7fde0905-c91a-42b3-bfc5-71641afe6b16": Phase="Pending", Reason="", readiness=false. Elapsed: 1.643774ms
Jan 30 11:51:15.192: INFO: Pod "pod-7fde0905-c91a-42b3-bfc5-71641afe6b16": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004274156s
Jan 30 11:51:17.194: INFO: Pod "pod-7fde0905-c91a-42b3-bfc5-71641afe6b16": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005745867s
STEP: Saw pod success 01/30/23 11:51:17.194
Jan 30 11:51:17.194: INFO: Pod "pod-7fde0905-c91a-42b3-bfc5-71641afe6b16" satisfied condition "Succeeded or Failed"
Jan 30 11:51:17.196: INFO: Trying to get logs from node pubt2-nks-for-dev1.dg.163.org pod pod-7fde0905-c91a-42b3-bfc5-71641afe6b16 container test-container: <nil>
STEP: delete the pod 01/30/23 11:51:17.201
Jan 30 11:51:17.205: INFO: Waiting for pod pod-7fde0905-c91a-42b3-bfc5-71641afe6b16 to disappear
Jan 30 11:51:17.206: INFO: Pod pod-7fde0905-c91a-42b3-bfc5-71641afe6b16 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jan 30 11:51:17.206: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-2349" for this suite. 01/30/23 11:51:17.209
------------------------------
• [4.037 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:127

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 11:51:13.174
    Jan 30 11:51:13.174: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename emptydir 01/30/23 11:51:13.174
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:51:13.18
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:51:13.182
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:127
    STEP: Creating a pod to test emptydir 0644 on tmpfs 01/30/23 11:51:13.184
    Jan 30 11:51:13.188: INFO: Waiting up to 5m0s for pod "pod-7fde0905-c91a-42b3-bfc5-71641afe6b16" in namespace "emptydir-2349" to be "Succeeded or Failed"
    Jan 30 11:51:13.190: INFO: Pod "pod-7fde0905-c91a-42b3-bfc5-71641afe6b16": Phase="Pending", Reason="", readiness=false. Elapsed: 1.643774ms
    Jan 30 11:51:15.192: INFO: Pod "pod-7fde0905-c91a-42b3-bfc5-71641afe6b16": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004274156s
    Jan 30 11:51:17.194: INFO: Pod "pod-7fde0905-c91a-42b3-bfc5-71641afe6b16": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005745867s
    STEP: Saw pod success 01/30/23 11:51:17.194
    Jan 30 11:51:17.194: INFO: Pod "pod-7fde0905-c91a-42b3-bfc5-71641afe6b16" satisfied condition "Succeeded or Failed"
    Jan 30 11:51:17.196: INFO: Trying to get logs from node pubt2-nks-for-dev1.dg.163.org pod pod-7fde0905-c91a-42b3-bfc5-71641afe6b16 container test-container: <nil>
    STEP: delete the pod 01/30/23 11:51:17.201
    Jan 30 11:51:17.205: INFO: Waiting for pod pod-7fde0905-c91a-42b3-bfc5-71641afe6b16 to disappear
    Jan 30 11:51:17.206: INFO: Pod pod-7fde0905-c91a-42b3-bfc5-71641afe6b16 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jan 30 11:51:17.206: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-2349" for this suite. 01/30/23 11:51:17.209
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:130
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 11:51:17.212
Jan 30 11:51:17.212: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename downward-api 01/30/23 11:51:17.213
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:51:17.219
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:51:17.221
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:130
STEP: Creating the pod 01/30/23 11:51:17.223
Jan 30 11:51:17.227: INFO: Waiting up to 5m0s for pod "labelsupdate2bb0f3f6-9a8a-412e-bc5b-874d3e5b1438" in namespace "downward-api-5229" to be "running and ready"
Jan 30 11:51:17.229: INFO: Pod "labelsupdate2bb0f3f6-9a8a-412e-bc5b-874d3e5b1438": Phase="Pending", Reason="", readiness=false. Elapsed: 1.547159ms
Jan 30 11:51:17.229: INFO: The phase of Pod labelsupdate2bb0f3f6-9a8a-412e-bc5b-874d3e5b1438 is Pending, waiting for it to be Running (with Ready = true)
Jan 30 11:51:19.232: INFO: Pod "labelsupdate2bb0f3f6-9a8a-412e-bc5b-874d3e5b1438": Phase="Running", Reason="", readiness=true. Elapsed: 2.004826607s
Jan 30 11:51:19.232: INFO: The phase of Pod labelsupdate2bb0f3f6-9a8a-412e-bc5b-874d3e5b1438 is Running (Ready = true)
Jan 30 11:51:19.232: INFO: Pod "labelsupdate2bb0f3f6-9a8a-412e-bc5b-874d3e5b1438" satisfied condition "running and ready"
Jan 30 11:51:19.746: INFO: Successfully updated pod "labelsupdate2bb0f3f6-9a8a-412e-bc5b-874d3e5b1438"
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Jan 30 11:51:21.757: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-5229" for this suite. 01/30/23 11:51:21.759
------------------------------
• [4.550 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:130

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 11:51:17.212
    Jan 30 11:51:17.212: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename downward-api 01/30/23 11:51:17.213
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:51:17.219
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:51:17.221
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should update labels on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:130
    STEP: Creating the pod 01/30/23 11:51:17.223
    Jan 30 11:51:17.227: INFO: Waiting up to 5m0s for pod "labelsupdate2bb0f3f6-9a8a-412e-bc5b-874d3e5b1438" in namespace "downward-api-5229" to be "running and ready"
    Jan 30 11:51:17.229: INFO: Pod "labelsupdate2bb0f3f6-9a8a-412e-bc5b-874d3e5b1438": Phase="Pending", Reason="", readiness=false. Elapsed: 1.547159ms
    Jan 30 11:51:17.229: INFO: The phase of Pod labelsupdate2bb0f3f6-9a8a-412e-bc5b-874d3e5b1438 is Pending, waiting for it to be Running (with Ready = true)
    Jan 30 11:51:19.232: INFO: Pod "labelsupdate2bb0f3f6-9a8a-412e-bc5b-874d3e5b1438": Phase="Running", Reason="", readiness=true. Elapsed: 2.004826607s
    Jan 30 11:51:19.232: INFO: The phase of Pod labelsupdate2bb0f3f6-9a8a-412e-bc5b-874d3e5b1438 is Running (Ready = true)
    Jan 30 11:51:19.232: INFO: Pod "labelsupdate2bb0f3f6-9a8a-412e-bc5b-874d3e5b1438" satisfied condition "running and ready"
    Jan 30 11:51:19.746: INFO: Successfully updated pod "labelsupdate2bb0f3f6-9a8a-412e-bc5b-874d3e5b1438"
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Jan 30 11:51:21.757: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-5229" for this suite. 01/30/23 11:51:21.759
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:386
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 11:51:21.762
Jan 30 11:51:21.762: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename secrets 01/30/23 11:51:21.763
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:51:21.769
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:51:21.771
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:386
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Jan 30 11:51:21.789: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-6327" for this suite. 01/30/23 11:51:21.791
------------------------------
• [0.031 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:386

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 11:51:21.762
    Jan 30 11:51:21.762: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename secrets 01/30/23 11:51:21.763
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:51:21.769
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:51:21.771
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be immutable if `immutable` field is set [Conformance]
      test/e2e/common/storage/secrets_volume.go:386
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Jan 30 11:51:21.789: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-6327" for this suite. 01/30/23 11:51:21.791
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-network] IngressClass API
   should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223
[BeforeEach] [sig-network] IngressClass API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 11:51:21.794
Jan 30 11:51:21.794: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename ingressclass 01/30/23 11:51:21.794
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:51:21.8
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:51:21.802
[BeforeEach] [sig-network] IngressClass API
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] IngressClass API
  test/e2e/network/ingressclass.go:211
[It]  should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223
STEP: getting /apis 01/30/23 11:51:21.803
STEP: getting /apis/networking.k8s.io 01/30/23 11:51:21.805
STEP: getting /apis/networking.k8s.iov1 01/30/23 11:51:21.806
STEP: creating 01/30/23 11:51:21.807
STEP: getting 01/30/23 11:51:21.812
STEP: listing 01/30/23 11:51:21.814
STEP: watching 01/30/23 11:51:21.815
Jan 30 11:51:21.815: INFO: starting watch
STEP: patching 01/30/23 11:51:21.816
STEP: updating 01/30/23 11:51:21.818
Jan 30 11:51:21.821: INFO: waiting for watch events with expected annotations
Jan 30 11:51:21.821: INFO: saw patched and updated annotations
STEP: deleting 01/30/23 11:51:21.821
STEP: deleting a collection 01/30/23 11:51:21.826
[AfterEach] [sig-network] IngressClass API
  test/e2e/framework/node/init/init.go:32
Jan 30 11:51:21.831: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] IngressClass API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] IngressClass API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] IngressClass API
  tear down framework | framework.go:193
STEP: Destroying namespace "ingressclass-2992" for this suite. 01/30/23 11:51:21.833
------------------------------
• [0.042 seconds]
[sig-network] IngressClass API
test/e2e/network/common/framework.go:23
   should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] IngressClass API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 11:51:21.794
    Jan 30 11:51:21.794: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename ingressclass 01/30/23 11:51:21.794
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:51:21.8
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:51:21.802
    [BeforeEach] [sig-network] IngressClass API
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] IngressClass API
      test/e2e/network/ingressclass.go:211
    [It]  should support creating IngressClass API operations [Conformance]
      test/e2e/network/ingressclass.go:223
    STEP: getting /apis 01/30/23 11:51:21.803
    STEP: getting /apis/networking.k8s.io 01/30/23 11:51:21.805
    STEP: getting /apis/networking.k8s.iov1 01/30/23 11:51:21.806
    STEP: creating 01/30/23 11:51:21.807
    STEP: getting 01/30/23 11:51:21.812
    STEP: listing 01/30/23 11:51:21.814
    STEP: watching 01/30/23 11:51:21.815
    Jan 30 11:51:21.815: INFO: starting watch
    STEP: patching 01/30/23 11:51:21.816
    STEP: updating 01/30/23 11:51:21.818
    Jan 30 11:51:21.821: INFO: waiting for watch events with expected annotations
    Jan 30 11:51:21.821: INFO: saw patched and updated annotations
    STEP: deleting 01/30/23 11:51:21.821
    STEP: deleting a collection 01/30/23 11:51:21.826
    [AfterEach] [sig-network] IngressClass API
      test/e2e/framework/node/init/init.go:32
    Jan 30 11:51:21.831: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] IngressClass API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] IngressClass API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] IngressClass API
      tear down framework | framework.go:193
    STEP: Destroying namespace "ingressclass-2992" for this suite. 01/30/23 11:51:21.833
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:57
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 11:51:21.835
Jan 30 11:51:21.835: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename secrets 01/30/23 11:51:21.836
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:51:21.841
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:51:21.843
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:57
STEP: Creating secret with name secret-test-9414d5f5-90d2-4574-8317-58dbf6babf23 01/30/23 11:51:21.844
STEP: Creating a pod to test consume secrets 01/30/23 11:51:21.846
Jan 30 11:51:21.850: INFO: Waiting up to 5m0s for pod "pod-secrets-2254de5d-64d9-4bd1-b597-e341731acd49" in namespace "secrets-9021" to be "Succeeded or Failed"
Jan 30 11:51:21.852: INFO: Pod "pod-secrets-2254de5d-64d9-4bd1-b597-e341731acd49": Phase="Pending", Reason="", readiness=false. Elapsed: 1.484307ms
Jan 30 11:51:23.855: INFO: Pod "pod-secrets-2254de5d-64d9-4bd1-b597-e341731acd49": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004426151s
Jan 30 11:51:25.856: INFO: Pod "pod-secrets-2254de5d-64d9-4bd1-b597-e341731acd49": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005624768s
STEP: Saw pod success 01/30/23 11:51:25.856
Jan 30 11:51:25.856: INFO: Pod "pod-secrets-2254de5d-64d9-4bd1-b597-e341731acd49" satisfied condition "Succeeded or Failed"
Jan 30 11:51:25.858: INFO: Trying to get logs from node pubt2-nks-for-dev1.dg.163.org pod pod-secrets-2254de5d-64d9-4bd1-b597-e341731acd49 container secret-volume-test: <nil>
STEP: delete the pod 01/30/23 11:51:25.862
Jan 30 11:51:25.867: INFO: Waiting for pod pod-secrets-2254de5d-64d9-4bd1-b597-e341731acd49 to disappear
Jan 30 11:51:25.868: INFO: Pod pod-secrets-2254de5d-64d9-4bd1-b597-e341731acd49 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Jan 30 11:51:25.868: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-9021" for this suite. 01/30/23 11:51:25.87
------------------------------
• [4.037 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 11:51:21.835
    Jan 30 11:51:21.835: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename secrets 01/30/23 11:51:21.836
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:51:21.841
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:51:21.843
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:57
    STEP: Creating secret with name secret-test-9414d5f5-90d2-4574-8317-58dbf6babf23 01/30/23 11:51:21.844
    STEP: Creating a pod to test consume secrets 01/30/23 11:51:21.846
    Jan 30 11:51:21.850: INFO: Waiting up to 5m0s for pod "pod-secrets-2254de5d-64d9-4bd1-b597-e341731acd49" in namespace "secrets-9021" to be "Succeeded or Failed"
    Jan 30 11:51:21.852: INFO: Pod "pod-secrets-2254de5d-64d9-4bd1-b597-e341731acd49": Phase="Pending", Reason="", readiness=false. Elapsed: 1.484307ms
    Jan 30 11:51:23.855: INFO: Pod "pod-secrets-2254de5d-64d9-4bd1-b597-e341731acd49": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004426151s
    Jan 30 11:51:25.856: INFO: Pod "pod-secrets-2254de5d-64d9-4bd1-b597-e341731acd49": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005624768s
    STEP: Saw pod success 01/30/23 11:51:25.856
    Jan 30 11:51:25.856: INFO: Pod "pod-secrets-2254de5d-64d9-4bd1-b597-e341731acd49" satisfied condition "Succeeded or Failed"
    Jan 30 11:51:25.858: INFO: Trying to get logs from node pubt2-nks-for-dev1.dg.163.org pod pod-secrets-2254de5d-64d9-4bd1-b597-e341731acd49 container secret-volume-test: <nil>
    STEP: delete the pod 01/30/23 11:51:25.862
    Jan 30 11:51:25.867: INFO: Waiting for pod pod-secrets-2254de5d-64d9-4bd1-b597-e341731acd49 to disappear
    Jan 30 11:51:25.868: INFO: Pod pod-secrets-2254de5d-64d9-4bd1-b597-e341731acd49 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Jan 30 11:51:25.868: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-9021" for this suite. 01/30/23 11:51:25.87
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 11:51:25.875
Jan 30 11:51:25.875: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename dns 01/30/23 11:51:25.876
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:51:25.881
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:51:25.884
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
 01/30/23 11:51:25.886
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
 01/30/23 11:51:25.886
STEP: creating a pod to probe DNS 01/30/23 11:51:25.886
STEP: submitting the pod to kubernetes 01/30/23 11:51:25.886
Jan 30 11:51:25.890: INFO: Waiting up to 15m0s for pod "dns-test-bfac6c33-5d5a-4c5e-ae9e-345381d466e5" in namespace "dns-4056" to be "running"
Jan 30 11:51:25.892: INFO: Pod "dns-test-bfac6c33-5d5a-4c5e-ae9e-345381d466e5": Phase="Pending", Reason="", readiness=false. Elapsed: 1.658049ms
Jan 30 11:51:27.894: INFO: Pod "dns-test-bfac6c33-5d5a-4c5e-ae9e-345381d466e5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003969245s
Jan 30 11:51:29.895: INFO: Pod "dns-test-bfac6c33-5d5a-4c5e-ae9e-345381d466e5": Phase="Running", Reason="", readiness=true. Elapsed: 4.005447043s
Jan 30 11:51:29.895: INFO: Pod "dns-test-bfac6c33-5d5a-4c5e-ae9e-345381d466e5" satisfied condition "running"
STEP: retrieving the pod 01/30/23 11:51:29.895
STEP: looking for the results for each expected name from probers 01/30/23 11:51:29.898
Jan 30 11:51:29.907: INFO: DNS probes using dns-4056/dns-test-bfac6c33-5d5a-4c5e-ae9e-345381d466e5 succeeded

STEP: deleting the pod 01/30/23 11:51:29.907
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Jan 30 11:51:29.911: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-4056" for this suite. 01/30/23 11:51:29.914
------------------------------
• [4.041 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 11:51:25.875
    Jan 30 11:51:25.875: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename dns 01/30/23 11:51:25.876
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:51:25.881
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:51:25.884
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for the cluster  [Conformance]
      test/e2e/network/dns.go:50
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
     01/30/23 11:51:25.886
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
     01/30/23 11:51:25.886
    STEP: creating a pod to probe DNS 01/30/23 11:51:25.886
    STEP: submitting the pod to kubernetes 01/30/23 11:51:25.886
    Jan 30 11:51:25.890: INFO: Waiting up to 15m0s for pod "dns-test-bfac6c33-5d5a-4c5e-ae9e-345381d466e5" in namespace "dns-4056" to be "running"
    Jan 30 11:51:25.892: INFO: Pod "dns-test-bfac6c33-5d5a-4c5e-ae9e-345381d466e5": Phase="Pending", Reason="", readiness=false. Elapsed: 1.658049ms
    Jan 30 11:51:27.894: INFO: Pod "dns-test-bfac6c33-5d5a-4c5e-ae9e-345381d466e5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003969245s
    Jan 30 11:51:29.895: INFO: Pod "dns-test-bfac6c33-5d5a-4c5e-ae9e-345381d466e5": Phase="Running", Reason="", readiness=true. Elapsed: 4.005447043s
    Jan 30 11:51:29.895: INFO: Pod "dns-test-bfac6c33-5d5a-4c5e-ae9e-345381d466e5" satisfied condition "running"
    STEP: retrieving the pod 01/30/23 11:51:29.895
    STEP: looking for the results for each expected name from probers 01/30/23 11:51:29.898
    Jan 30 11:51:29.907: INFO: DNS probes using dns-4056/dns-test-bfac6c33-5d5a-4c5e-ae9e-345381d466e5 succeeded

    STEP: deleting the pod 01/30/23 11:51:29.907
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Jan 30 11:51:29.911: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-4056" for this suite. 01/30/23 11:51:29.914
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] ConfigMap
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:240
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 11:51:29.916
Jan 30 11:51:29.917: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename configmap 01/30/23 11:51:29.917
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:51:29.924
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:51:29.925
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:240
STEP: Creating configMap with name cm-test-opt-del-9b50c8c0-e362-4b10-85b9-78946b5da9cb 01/30/23 11:51:29.929
STEP: Creating configMap with name cm-test-opt-upd-7b80d81b-0a31-4c06-95fd-75474be8c352 01/30/23 11:51:29.931
STEP: Creating the pod 01/30/23 11:51:29.933
Jan 30 11:51:29.937: INFO: Waiting up to 5m0s for pod "pod-configmaps-824c4c00-8d4a-443b-8410-f8e999048f0f" in namespace "configmap-401" to be "running and ready"
Jan 30 11:51:29.939: INFO: Pod "pod-configmaps-824c4c00-8d4a-443b-8410-f8e999048f0f": Phase="Pending", Reason="", readiness=false. Elapsed: 1.580892ms
Jan 30 11:51:29.939: INFO: The phase of Pod pod-configmaps-824c4c00-8d4a-443b-8410-f8e999048f0f is Pending, waiting for it to be Running (with Ready = true)
Jan 30 11:51:31.941: INFO: Pod "pod-configmaps-824c4c00-8d4a-443b-8410-f8e999048f0f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004012688s
Jan 30 11:51:31.941: INFO: The phase of Pod pod-configmaps-824c4c00-8d4a-443b-8410-f8e999048f0f is Pending, waiting for it to be Running (with Ready = true)
Jan 30 11:51:33.942: INFO: Pod "pod-configmaps-824c4c00-8d4a-443b-8410-f8e999048f0f": Phase="Running", Reason="", readiness=true. Elapsed: 4.004703233s
Jan 30 11:51:33.942: INFO: The phase of Pod pod-configmaps-824c4c00-8d4a-443b-8410-f8e999048f0f is Running (Ready = true)
Jan 30 11:51:33.942: INFO: Pod "pod-configmaps-824c4c00-8d4a-443b-8410-f8e999048f0f" satisfied condition "running and ready"
STEP: Deleting configmap cm-test-opt-del-9b50c8c0-e362-4b10-85b9-78946b5da9cb 01/30/23 11:51:33.958
STEP: Updating configmap cm-test-opt-upd-7b80d81b-0a31-4c06-95fd-75474be8c352 01/30/23 11:51:33.96
STEP: Creating configMap with name cm-test-opt-create-dab8d21a-6e70-4566-b54d-17579a8ece83 01/30/23 11:51:33.962
STEP: waiting to observe update in volume 01/30/23 11:51:33.964
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jan 30 11:52:40.201: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-401" for this suite. 01/30/23 11:52:40.203
------------------------------
• [SLOW TEST] [70.289 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:240

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 11:51:29.916
    Jan 30 11:51:29.917: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename configmap 01/30/23 11:51:29.917
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:51:29.924
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:51:29.925
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:240
    STEP: Creating configMap with name cm-test-opt-del-9b50c8c0-e362-4b10-85b9-78946b5da9cb 01/30/23 11:51:29.929
    STEP: Creating configMap with name cm-test-opt-upd-7b80d81b-0a31-4c06-95fd-75474be8c352 01/30/23 11:51:29.931
    STEP: Creating the pod 01/30/23 11:51:29.933
    Jan 30 11:51:29.937: INFO: Waiting up to 5m0s for pod "pod-configmaps-824c4c00-8d4a-443b-8410-f8e999048f0f" in namespace "configmap-401" to be "running and ready"
    Jan 30 11:51:29.939: INFO: Pod "pod-configmaps-824c4c00-8d4a-443b-8410-f8e999048f0f": Phase="Pending", Reason="", readiness=false. Elapsed: 1.580892ms
    Jan 30 11:51:29.939: INFO: The phase of Pod pod-configmaps-824c4c00-8d4a-443b-8410-f8e999048f0f is Pending, waiting for it to be Running (with Ready = true)
    Jan 30 11:51:31.941: INFO: Pod "pod-configmaps-824c4c00-8d4a-443b-8410-f8e999048f0f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004012688s
    Jan 30 11:51:31.941: INFO: The phase of Pod pod-configmaps-824c4c00-8d4a-443b-8410-f8e999048f0f is Pending, waiting for it to be Running (with Ready = true)
    Jan 30 11:51:33.942: INFO: Pod "pod-configmaps-824c4c00-8d4a-443b-8410-f8e999048f0f": Phase="Running", Reason="", readiness=true. Elapsed: 4.004703233s
    Jan 30 11:51:33.942: INFO: The phase of Pod pod-configmaps-824c4c00-8d4a-443b-8410-f8e999048f0f is Running (Ready = true)
    Jan 30 11:51:33.942: INFO: Pod "pod-configmaps-824c4c00-8d4a-443b-8410-f8e999048f0f" satisfied condition "running and ready"
    STEP: Deleting configmap cm-test-opt-del-9b50c8c0-e362-4b10-85b9-78946b5da9cb 01/30/23 11:51:33.958
    STEP: Updating configmap cm-test-opt-upd-7b80d81b-0a31-4c06-95fd-75474be8c352 01/30/23 11:51:33.96
    STEP: Creating configMap with name cm-test-opt-create-dab8d21a-6e70-4566-b54d-17579a8ece83 01/30/23 11:51:33.962
    STEP: waiting to observe update in volume 01/30/23 11:51:33.964
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jan 30 11:52:40.201: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-401" for this suite. 01/30/23 11:52:40.203
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-network] Services
  should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:848
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 11:52:40.206
Jan 30 11:52:40.206: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename services 01/30/23 11:52:40.206
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:52:40.213
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:52:40.214
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:848
STEP: creating service multi-endpoint-test in namespace services-8183 01/30/23 11:52:40.217
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8183 to expose endpoints map[] 01/30/23 11:52:40.22
Jan 30 11:52:40.221: INFO: Failed go get Endpoints object: endpoints "multi-endpoint-test" not found
Jan 30 11:52:41.225: INFO: successfully validated that service multi-endpoint-test in namespace services-8183 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-8183 01/30/23 11:52:41.225
Jan 30 11:52:41.230: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-8183" to be "running and ready"
Jan 30 11:52:41.231: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 1.578176ms
Jan 30 11:52:41.231: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Jan 30 11:52:43.234: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.00445927s
Jan 30 11:52:43.234: INFO: The phase of Pod pod1 is Running (Ready = true)
Jan 30 11:52:43.234: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8183 to expose endpoints map[pod1:[100]] 01/30/23 11:52:43.236
Jan 30 11:52:43.241: INFO: successfully validated that service multi-endpoint-test in namespace services-8183 exposes endpoints map[pod1:[100]]
STEP: Creating pod pod2 in namespace services-8183 01/30/23 11:52:43.241
Jan 30 11:52:43.244: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-8183" to be "running and ready"
Jan 30 11:52:43.245: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 1.629049ms
Jan 30 11:52:43.245: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Jan 30 11:52:45.247: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.003861263s
Jan 30 11:52:45.248: INFO: The phase of Pod pod2 is Running (Ready = true)
Jan 30 11:52:45.248: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8183 to expose endpoints map[pod1:[100] pod2:[101]] 01/30/23 11:52:45.249
Jan 30 11:52:45.256: INFO: successfully validated that service multi-endpoint-test in namespace services-8183 exposes endpoints map[pod1:[100] pod2:[101]]
STEP: Checking if the Service forwards traffic to pods 01/30/23 11:52:45.256
Jan 30 11:52:45.256: INFO: Creating new exec pod
Jan 30 11:52:45.259: INFO: Waiting up to 5m0s for pod "execpodl6rqh" in namespace "services-8183" to be "running"
Jan 30 11:52:45.260: INFO: Pod "execpodl6rqh": Phase="Pending", Reason="", readiness=false. Elapsed: 1.580666ms
Jan 30 11:52:47.263: INFO: Pod "execpodl6rqh": Phase="Running", Reason="", readiness=true. Elapsed: 2.004090867s
Jan 30 11:52:47.263: INFO: Pod "execpodl6rqh" satisfied condition "running"
Jan 30 11:52:48.263: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=services-8183 exec execpodl6rqh -- /bin/sh -x -c nc -v -z -w 2 multi-endpoint-test 80'
Jan 30 11:52:48.413: INFO: stderr: "+ nc -v -z -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
Jan 30 11:52:48.413: INFO: stdout: ""
Jan 30 11:52:48.413: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=services-8183 exec execpodl6rqh -- /bin/sh -x -c nc -v -z -w 2 10.178.75.151 80'
Jan 30 11:52:48.548: INFO: stderr: "+ nc -v -z -w 2 10.178.75.151 80\nConnection to 10.178.75.151 80 port [tcp/http] succeeded!\n"
Jan 30 11:52:48.548: INFO: stdout: ""
Jan 30 11:52:48.548: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=services-8183 exec execpodl6rqh -- /bin/sh -x -c nc -v -z -w 2 multi-endpoint-test 81'
Jan 30 11:52:49.331: INFO: stderr: "+ nc -v -z -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
Jan 30 11:52:49.332: INFO: stdout: ""
Jan 30 11:52:49.332: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=services-8183 exec execpodl6rqh -- /bin/sh -x -c nc -v -z -w 2 10.178.75.151 81'
Jan 30 11:52:49.465: INFO: stderr: "+ nc -v -z -w 2 10.178.75.151 81\nConnection to 10.178.75.151 81 port [tcp/*] succeeded!\n"
Jan 30 11:52:49.465: INFO: stdout: ""
STEP: Deleting pod pod1 in namespace services-8183 01/30/23 11:52:49.465
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8183 to expose endpoints map[pod2:[101]] 01/30/23 11:52:49.47
Jan 30 11:52:49.475: INFO: successfully validated that service multi-endpoint-test in namespace services-8183 exposes endpoints map[pod2:[101]]
STEP: Deleting pod pod2 in namespace services-8183 01/30/23 11:52:49.475
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8183 to expose endpoints map[] 01/30/23 11:52:49.48
Jan 30 11:52:50.489: INFO: successfully validated that service multi-endpoint-test in namespace services-8183 exposes endpoints map[]
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jan 30 11:52:50.494: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-8183" for this suite. 01/30/23 11:52:50.496
------------------------------
• [SLOW TEST] [10.293 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:848

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 11:52:40.206
    Jan 30 11:52:40.206: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename services 01/30/23 11:52:40.206
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:52:40.213
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:52:40.214
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should serve multiport endpoints from pods  [Conformance]
      test/e2e/network/service.go:848
    STEP: creating service multi-endpoint-test in namespace services-8183 01/30/23 11:52:40.217
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8183 to expose endpoints map[] 01/30/23 11:52:40.22
    Jan 30 11:52:40.221: INFO: Failed go get Endpoints object: endpoints "multi-endpoint-test" not found
    Jan 30 11:52:41.225: INFO: successfully validated that service multi-endpoint-test in namespace services-8183 exposes endpoints map[]
    STEP: Creating pod pod1 in namespace services-8183 01/30/23 11:52:41.225
    Jan 30 11:52:41.230: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-8183" to be "running and ready"
    Jan 30 11:52:41.231: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 1.578176ms
    Jan 30 11:52:41.231: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Jan 30 11:52:43.234: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.00445927s
    Jan 30 11:52:43.234: INFO: The phase of Pod pod1 is Running (Ready = true)
    Jan 30 11:52:43.234: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8183 to expose endpoints map[pod1:[100]] 01/30/23 11:52:43.236
    Jan 30 11:52:43.241: INFO: successfully validated that service multi-endpoint-test in namespace services-8183 exposes endpoints map[pod1:[100]]
    STEP: Creating pod pod2 in namespace services-8183 01/30/23 11:52:43.241
    Jan 30 11:52:43.244: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-8183" to be "running and ready"
    Jan 30 11:52:43.245: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 1.629049ms
    Jan 30 11:52:43.245: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Jan 30 11:52:45.247: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.003861263s
    Jan 30 11:52:45.248: INFO: The phase of Pod pod2 is Running (Ready = true)
    Jan 30 11:52:45.248: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8183 to expose endpoints map[pod1:[100] pod2:[101]] 01/30/23 11:52:45.249
    Jan 30 11:52:45.256: INFO: successfully validated that service multi-endpoint-test in namespace services-8183 exposes endpoints map[pod1:[100] pod2:[101]]
    STEP: Checking if the Service forwards traffic to pods 01/30/23 11:52:45.256
    Jan 30 11:52:45.256: INFO: Creating new exec pod
    Jan 30 11:52:45.259: INFO: Waiting up to 5m0s for pod "execpodl6rqh" in namespace "services-8183" to be "running"
    Jan 30 11:52:45.260: INFO: Pod "execpodl6rqh": Phase="Pending", Reason="", readiness=false. Elapsed: 1.580666ms
    Jan 30 11:52:47.263: INFO: Pod "execpodl6rqh": Phase="Running", Reason="", readiness=true. Elapsed: 2.004090867s
    Jan 30 11:52:47.263: INFO: Pod "execpodl6rqh" satisfied condition "running"
    Jan 30 11:52:48.263: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=services-8183 exec execpodl6rqh -- /bin/sh -x -c nc -v -z -w 2 multi-endpoint-test 80'
    Jan 30 11:52:48.413: INFO: stderr: "+ nc -v -z -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
    Jan 30 11:52:48.413: INFO: stdout: ""
    Jan 30 11:52:48.413: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=services-8183 exec execpodl6rqh -- /bin/sh -x -c nc -v -z -w 2 10.178.75.151 80'
    Jan 30 11:52:48.548: INFO: stderr: "+ nc -v -z -w 2 10.178.75.151 80\nConnection to 10.178.75.151 80 port [tcp/http] succeeded!\n"
    Jan 30 11:52:48.548: INFO: stdout: ""
    Jan 30 11:52:48.548: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=services-8183 exec execpodl6rqh -- /bin/sh -x -c nc -v -z -w 2 multi-endpoint-test 81'
    Jan 30 11:52:49.331: INFO: stderr: "+ nc -v -z -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
    Jan 30 11:52:49.332: INFO: stdout: ""
    Jan 30 11:52:49.332: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=services-8183 exec execpodl6rqh -- /bin/sh -x -c nc -v -z -w 2 10.178.75.151 81'
    Jan 30 11:52:49.465: INFO: stderr: "+ nc -v -z -w 2 10.178.75.151 81\nConnection to 10.178.75.151 81 port [tcp/*] succeeded!\n"
    Jan 30 11:52:49.465: INFO: stdout: ""
    STEP: Deleting pod pod1 in namespace services-8183 01/30/23 11:52:49.465
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8183 to expose endpoints map[pod2:[101]] 01/30/23 11:52:49.47
    Jan 30 11:52:49.475: INFO: successfully validated that service multi-endpoint-test in namespace services-8183 exposes endpoints map[pod2:[101]]
    STEP: Deleting pod pod2 in namespace services-8183 01/30/23 11:52:49.475
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8183 to expose endpoints map[] 01/30/23 11:52:49.48
    Jan 30 11:52:50.489: INFO: successfully validated that service multi-endpoint-test in namespace services-8183 exposes endpoints map[]
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jan 30 11:52:50.494: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-8183" for this suite. 01/30/23 11:52:50.496
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:166
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 11:52:50.501
Jan 30 11:52:50.501: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename daemonsets 01/30/23 11:52:50.501
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:52:50.508
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:52:50.51
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:166
STEP: Creating simple DaemonSet "daemon-set" 01/30/23 11:52:50.52
STEP: Check that daemon pods launch on every node of the cluster. 01/30/23 11:52:50.523
Jan 30 11:52:50.527: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 30 11:52:50.527: INFO: Node pubt2-nks-for-dev1.dg.163.org is running 0 daemon pod, expected 1
Jan 30 11:52:51.532: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 30 11:52:51.532: INFO: Node pubt2-nks-for-dev1.dg.163.org is running 0 daemon pod, expected 1
Jan 30 11:52:52.531: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan 30 11:52:52.531: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
STEP: Stop a daemon pod, check that the daemon pod is revived. 01/30/23 11:52:52.532
Jan 30 11:52:52.541: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 30 11:52:52.541: INFO: Node pubt2-nks-for-dev3.dg.163.org is running 0 daemon pod, expected 1
Jan 30 11:52:53.545: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 30 11:52:53.545: INFO: Node pubt2-nks-for-dev3.dg.163.org is running 0 daemon pod, expected 1
Jan 30 11:52:54.545: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 30 11:52:54.545: INFO: Node pubt2-nks-for-dev3.dg.163.org is running 0 daemon pod, expected 1
Jan 30 11:52:55.545: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 30 11:52:55.545: INFO: Node pubt2-nks-for-dev3.dg.163.org is running 0 daemon pod, expected 1
Jan 30 11:52:56.546: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 30 11:52:56.546: INFO: Node pubt2-nks-for-dev3.dg.163.org is running 0 daemon pod, expected 1
Jan 30 11:52:57.546: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan 30 11:52:57.546: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 01/30/23 11:52:57.547
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5664, will wait for the garbage collector to delete the pods 01/30/23 11:52:57.547
Jan 30 11:52:57.601: INFO: Deleting DaemonSet.extensions daemon-set took: 2.293354ms
Jan 30 11:52:57.702: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.168992ms
Jan 30 11:53:00.504: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 30 11:53:00.504: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jan 30 11:53:00.506: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"24269"},"items":null}

Jan 30 11:53:00.507: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"24269"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Jan 30 11:53:00.513: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-5664" for this suite. 01/30/23 11:53:00.515
------------------------------
• [SLOW TEST] [10.016 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:166

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 11:52:50.501
    Jan 30 11:52:50.501: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename daemonsets 01/30/23 11:52:50.501
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:52:50.508
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:52:50.51
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should run and stop simple daemon [Conformance]
      test/e2e/apps/daemon_set.go:166
    STEP: Creating simple DaemonSet "daemon-set" 01/30/23 11:52:50.52
    STEP: Check that daemon pods launch on every node of the cluster. 01/30/23 11:52:50.523
    Jan 30 11:52:50.527: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 30 11:52:50.527: INFO: Node pubt2-nks-for-dev1.dg.163.org is running 0 daemon pod, expected 1
    Jan 30 11:52:51.532: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 30 11:52:51.532: INFO: Node pubt2-nks-for-dev1.dg.163.org is running 0 daemon pod, expected 1
    Jan 30 11:52:52.531: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jan 30 11:52:52.531: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    STEP: Stop a daemon pod, check that the daemon pod is revived. 01/30/23 11:52:52.532
    Jan 30 11:52:52.541: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan 30 11:52:52.541: INFO: Node pubt2-nks-for-dev3.dg.163.org is running 0 daemon pod, expected 1
    Jan 30 11:52:53.545: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan 30 11:52:53.545: INFO: Node pubt2-nks-for-dev3.dg.163.org is running 0 daemon pod, expected 1
    Jan 30 11:52:54.545: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan 30 11:52:54.545: INFO: Node pubt2-nks-for-dev3.dg.163.org is running 0 daemon pod, expected 1
    Jan 30 11:52:55.545: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan 30 11:52:55.545: INFO: Node pubt2-nks-for-dev3.dg.163.org is running 0 daemon pod, expected 1
    Jan 30 11:52:56.546: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan 30 11:52:56.546: INFO: Node pubt2-nks-for-dev3.dg.163.org is running 0 daemon pod, expected 1
    Jan 30 11:52:57.546: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jan 30 11:52:57.546: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 01/30/23 11:52:57.547
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5664, will wait for the garbage collector to delete the pods 01/30/23 11:52:57.547
    Jan 30 11:52:57.601: INFO: Deleting DaemonSet.extensions daemon-set took: 2.293354ms
    Jan 30 11:52:57.702: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.168992ms
    Jan 30 11:53:00.504: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 30 11:53:00.504: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Jan 30 11:53:00.506: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"24269"},"items":null}

    Jan 30 11:53:00.507: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"24269"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan 30 11:53:00.513: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-5664" for this suite. 01/30/23 11:53:00.515
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] Watchers
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 11:53:00.517
Jan 30 11:53:00.517: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename watch 01/30/23 11:53:00.518
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:53:00.524
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:53:00.526
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60
STEP: creating a watch on configmaps with label A 01/30/23 11:53:00.528
STEP: creating a watch on configmaps with label B 01/30/23 11:53:00.529
STEP: creating a watch on configmaps with label A or B 01/30/23 11:53:00.53
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification 01/30/23 11:53:00.531
Jan 30 11:53:00.533: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8855  6d8fff89-1cfc-44b5-93f8-bbd4897f443c 24274 0 2023-01-30 11:53:00 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-30 11:53:00 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 30 11:53:00.533: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8855  6d8fff89-1cfc-44b5-93f8-bbd4897f443c 24274 0 2023-01-30 11:53:00 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-30 11:53:00 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A and ensuring the correct watchers observe the notification 01/30/23 11:53:00.533
Jan 30 11:53:00.536: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8855  6d8fff89-1cfc-44b5-93f8-bbd4897f443c 24275 0 2023-01-30 11:53:00 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-30 11:53:00 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 30 11:53:00.536: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8855  6d8fff89-1cfc-44b5-93f8-bbd4897f443c 24275 0 2023-01-30 11:53:00 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-30 11:53:00 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification 01/30/23 11:53:00.536
Jan 30 11:53:00.540: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8855  6d8fff89-1cfc-44b5-93f8-bbd4897f443c 24276 0 2023-01-30 11:53:00 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-30 11:53:00 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 30 11:53:00.540: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8855  6d8fff89-1cfc-44b5-93f8-bbd4897f443c 24276 0 2023-01-30 11:53:00 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-30 11:53:00 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap A and ensuring the correct watchers observe the notification 01/30/23 11:53:00.54
Jan 30 11:53:00.542: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8855  6d8fff89-1cfc-44b5-93f8-bbd4897f443c 24277 0 2023-01-30 11:53:00 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-30 11:53:00 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 30 11:53:00.542: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8855  6d8fff89-1cfc-44b5-93f8-bbd4897f443c 24277 0 2023-01-30 11:53:00 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-30 11:53:00 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification 01/30/23 11:53:00.542
Jan 30 11:53:00.544: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-8855  54640c9a-55b0-432e-986c-dc4e13ccc6cf 24278 0 2023-01-30 11:53:00 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-01-30 11:53:00 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 30 11:53:00.544: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-8855  54640c9a-55b0-432e-986c-dc4e13ccc6cf 24278 0 2023-01-30 11:53:00 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-01-30 11:53:00 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap B and ensuring the correct watchers observe the notification 01/30/23 11:53:10.544
Jan 30 11:53:10.547: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-8855  54640c9a-55b0-432e-986c-dc4e13ccc6cf 24317 0 2023-01-30 11:53:00 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-01-30 11:53:00 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 30 11:53:10.547: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-8855  54640c9a-55b0-432e-986c-dc4e13ccc6cf 24317 0 2023-01-30 11:53:00 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-01-30 11:53:00 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Jan 30 11:53:20.549: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-8855" for this suite. 01/30/23 11:53:20.553
------------------------------
• [SLOW TEST] [20.038 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 11:53:00.517
    Jan 30 11:53:00.517: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename watch 01/30/23 11:53:00.518
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:53:00.524
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:53:00.526
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should observe add, update, and delete watch notifications on configmaps [Conformance]
      test/e2e/apimachinery/watch.go:60
    STEP: creating a watch on configmaps with label A 01/30/23 11:53:00.528
    STEP: creating a watch on configmaps with label B 01/30/23 11:53:00.529
    STEP: creating a watch on configmaps with label A or B 01/30/23 11:53:00.53
    STEP: creating a configmap with label A and ensuring the correct watchers observe the notification 01/30/23 11:53:00.531
    Jan 30 11:53:00.533: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8855  6d8fff89-1cfc-44b5-93f8-bbd4897f443c 24274 0 2023-01-30 11:53:00 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-30 11:53:00 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan 30 11:53:00.533: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8855  6d8fff89-1cfc-44b5-93f8-bbd4897f443c 24274 0 2023-01-30 11:53:00 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-30 11:53:00 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying configmap A and ensuring the correct watchers observe the notification 01/30/23 11:53:00.533
    Jan 30 11:53:00.536: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8855  6d8fff89-1cfc-44b5-93f8-bbd4897f443c 24275 0 2023-01-30 11:53:00 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-30 11:53:00 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan 30 11:53:00.536: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8855  6d8fff89-1cfc-44b5-93f8-bbd4897f443c 24275 0 2023-01-30 11:53:00 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-30 11:53:00 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying configmap A again and ensuring the correct watchers observe the notification 01/30/23 11:53:00.536
    Jan 30 11:53:00.540: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8855  6d8fff89-1cfc-44b5-93f8-bbd4897f443c 24276 0 2023-01-30 11:53:00 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-30 11:53:00 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan 30 11:53:00.540: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8855  6d8fff89-1cfc-44b5-93f8-bbd4897f443c 24276 0 2023-01-30 11:53:00 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-30 11:53:00 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: deleting configmap A and ensuring the correct watchers observe the notification 01/30/23 11:53:00.54
    Jan 30 11:53:00.542: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8855  6d8fff89-1cfc-44b5-93f8-bbd4897f443c 24277 0 2023-01-30 11:53:00 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-30 11:53:00 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan 30 11:53:00.542: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8855  6d8fff89-1cfc-44b5-93f8-bbd4897f443c 24277 0 2023-01-30 11:53:00 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-30 11:53:00 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: creating a configmap with label B and ensuring the correct watchers observe the notification 01/30/23 11:53:00.542
    Jan 30 11:53:00.544: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-8855  54640c9a-55b0-432e-986c-dc4e13ccc6cf 24278 0 2023-01-30 11:53:00 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-01-30 11:53:00 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan 30 11:53:00.544: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-8855  54640c9a-55b0-432e-986c-dc4e13ccc6cf 24278 0 2023-01-30 11:53:00 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-01-30 11:53:00 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: deleting configmap B and ensuring the correct watchers observe the notification 01/30/23 11:53:10.544
    Jan 30 11:53:10.547: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-8855  54640c9a-55b0-432e-986c-dc4e13ccc6cf 24317 0 2023-01-30 11:53:00 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-01-30 11:53:00 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan 30 11:53:10.547: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-8855  54640c9a-55b0-432e-986c-dc4e13ccc6cf 24317 0 2023-01-30 11:53:00 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-01-30 11:53:00 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Jan 30 11:53:20.549: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-8855" for this suite. 01/30/23 11:53:20.553
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:193
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 11:53:20.556
Jan 30 11:53:20.556: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename projected 01/30/23 11:53:20.557
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:53:20.563
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:53:20.565
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:193
STEP: Creating a pod to test downward API volume plugin 01/30/23 11:53:20.567
Jan 30 11:53:20.571: INFO: Waiting up to 5m0s for pod "downwardapi-volume-bdced82a-364a-4376-be9d-557a4b194a2e" in namespace "projected-7802" to be "Succeeded or Failed"
Jan 30 11:53:20.573: INFO: Pod "downwardapi-volume-bdced82a-364a-4376-be9d-557a4b194a2e": Phase="Pending", Reason="", readiness=false. Elapsed: 1.584309ms
Jan 30 11:53:22.576: INFO: Pod "downwardapi-volume-bdced82a-364a-4376-be9d-557a4b194a2e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004517221s
Jan 30 11:53:24.577: INFO: Pod "downwardapi-volume-bdced82a-364a-4376-be9d-557a4b194a2e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.00546045s
Jan 30 11:53:26.577: INFO: Pod "downwardapi-volume-bdced82a-364a-4376-be9d-557a4b194a2e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.005345322s
STEP: Saw pod success 01/30/23 11:53:26.577
Jan 30 11:53:26.577: INFO: Pod "downwardapi-volume-bdced82a-364a-4376-be9d-557a4b194a2e" satisfied condition "Succeeded or Failed"
Jan 30 11:53:26.578: INFO: Trying to get logs from node pubt2-nks-for-dev1.dg.163.org pod downwardapi-volume-bdced82a-364a-4376-be9d-557a4b194a2e container client-container: <nil>
STEP: delete the pod 01/30/23 11:53:26.583
Jan 30 11:53:26.587: INFO: Waiting for pod downwardapi-volume-bdced82a-364a-4376-be9d-557a4b194a2e to disappear
Jan 30 11:53:26.588: INFO: Pod downwardapi-volume-bdced82a-364a-4376-be9d-557a4b194a2e no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Jan 30 11:53:26.589: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-7802" for this suite. 01/30/23 11:53:26.591
------------------------------
• [SLOW TEST] [6.037 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 11:53:20.556
    Jan 30 11:53:20.556: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename projected 01/30/23 11:53:20.557
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:53:20.563
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:53:20.565
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide container's cpu limit [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:193
    STEP: Creating a pod to test downward API volume plugin 01/30/23 11:53:20.567
    Jan 30 11:53:20.571: INFO: Waiting up to 5m0s for pod "downwardapi-volume-bdced82a-364a-4376-be9d-557a4b194a2e" in namespace "projected-7802" to be "Succeeded or Failed"
    Jan 30 11:53:20.573: INFO: Pod "downwardapi-volume-bdced82a-364a-4376-be9d-557a4b194a2e": Phase="Pending", Reason="", readiness=false. Elapsed: 1.584309ms
    Jan 30 11:53:22.576: INFO: Pod "downwardapi-volume-bdced82a-364a-4376-be9d-557a4b194a2e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004517221s
    Jan 30 11:53:24.577: INFO: Pod "downwardapi-volume-bdced82a-364a-4376-be9d-557a4b194a2e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.00546045s
    Jan 30 11:53:26.577: INFO: Pod "downwardapi-volume-bdced82a-364a-4376-be9d-557a4b194a2e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.005345322s
    STEP: Saw pod success 01/30/23 11:53:26.577
    Jan 30 11:53:26.577: INFO: Pod "downwardapi-volume-bdced82a-364a-4376-be9d-557a4b194a2e" satisfied condition "Succeeded or Failed"
    Jan 30 11:53:26.578: INFO: Trying to get logs from node pubt2-nks-for-dev1.dg.163.org pod downwardapi-volume-bdced82a-364a-4376-be9d-557a4b194a2e container client-container: <nil>
    STEP: delete the pod 01/30/23 11:53:26.583
    Jan 30 11:53:26.587: INFO: Waiting for pod downwardapi-volume-bdced82a-364a-4376-be9d-557a4b194a2e to disappear
    Jan 30 11:53:26.588: INFO: Pod downwardapi-volume-bdced82a-364a-4376-be9d-557a4b194a2e no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Jan 30 11:53:26.589: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-7802" for this suite. 01/30/23 11:53:26.591
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:57
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 11:53:26.593
Jan 30 11:53:26.593: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename configmap 01/30/23 11:53:26.594
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:53:26.599
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:53:26.601
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:57
STEP: Creating configMap with name configmap-test-volume-bfbf05e3-daa6-474e-980d-a799af54ded4 01/30/23 11:53:26.603
STEP: Creating a pod to test consume configMaps 01/30/23 11:53:26.605
Jan 30 11:53:26.609: INFO: Waiting up to 5m0s for pod "pod-configmaps-9db859c9-2bea-45b8-89de-b2bd3d4c142a" in namespace "configmap-5499" to be "Succeeded or Failed"
Jan 30 11:53:26.610: INFO: Pod "pod-configmaps-9db859c9-2bea-45b8-89de-b2bd3d4c142a": Phase="Pending", Reason="", readiness=false. Elapsed: 1.552133ms
Jan 30 11:53:28.614: INFO: Pod "pod-configmaps-9db859c9-2bea-45b8-89de-b2bd3d4c142a": Phase="Running", Reason="", readiness=true. Elapsed: 2.005070664s
Jan 30 11:53:30.613: INFO: Pod "pod-configmaps-9db859c9-2bea-45b8-89de-b2bd3d4c142a": Phase="Running", Reason="", readiness=false. Elapsed: 4.004256612s
Jan 30 11:53:32.614: INFO: Pod "pod-configmaps-9db859c9-2bea-45b8-89de-b2bd3d4c142a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.005042621s
STEP: Saw pod success 01/30/23 11:53:32.614
Jan 30 11:53:32.614: INFO: Pod "pod-configmaps-9db859c9-2bea-45b8-89de-b2bd3d4c142a" satisfied condition "Succeeded or Failed"
Jan 30 11:53:32.616: INFO: Trying to get logs from node pubt2-nks-for-dev1.dg.163.org pod pod-configmaps-9db859c9-2bea-45b8-89de-b2bd3d4c142a container agnhost-container: <nil>
STEP: delete the pod 01/30/23 11:53:32.62
Jan 30 11:53:32.624: INFO: Waiting for pod pod-configmaps-9db859c9-2bea-45b8-89de-b2bd3d4c142a to disappear
Jan 30 11:53:32.626: INFO: Pod pod-configmaps-9db859c9-2bea-45b8-89de-b2bd3d4c142a no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jan 30 11:53:32.626: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-5499" for this suite. 01/30/23 11:53:32.628
------------------------------
• [SLOW TEST] [6.037 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 11:53:26.593
    Jan 30 11:53:26.593: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename configmap 01/30/23 11:53:26.594
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:53:26.599
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:53:26.601
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:57
    STEP: Creating configMap with name configmap-test-volume-bfbf05e3-daa6-474e-980d-a799af54ded4 01/30/23 11:53:26.603
    STEP: Creating a pod to test consume configMaps 01/30/23 11:53:26.605
    Jan 30 11:53:26.609: INFO: Waiting up to 5m0s for pod "pod-configmaps-9db859c9-2bea-45b8-89de-b2bd3d4c142a" in namespace "configmap-5499" to be "Succeeded or Failed"
    Jan 30 11:53:26.610: INFO: Pod "pod-configmaps-9db859c9-2bea-45b8-89de-b2bd3d4c142a": Phase="Pending", Reason="", readiness=false. Elapsed: 1.552133ms
    Jan 30 11:53:28.614: INFO: Pod "pod-configmaps-9db859c9-2bea-45b8-89de-b2bd3d4c142a": Phase="Running", Reason="", readiness=true. Elapsed: 2.005070664s
    Jan 30 11:53:30.613: INFO: Pod "pod-configmaps-9db859c9-2bea-45b8-89de-b2bd3d4c142a": Phase="Running", Reason="", readiness=false. Elapsed: 4.004256612s
    Jan 30 11:53:32.614: INFO: Pod "pod-configmaps-9db859c9-2bea-45b8-89de-b2bd3d4c142a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.005042621s
    STEP: Saw pod success 01/30/23 11:53:32.614
    Jan 30 11:53:32.614: INFO: Pod "pod-configmaps-9db859c9-2bea-45b8-89de-b2bd3d4c142a" satisfied condition "Succeeded or Failed"
    Jan 30 11:53:32.616: INFO: Trying to get logs from node pubt2-nks-for-dev1.dg.163.org pod pod-configmaps-9db859c9-2bea-45b8-89de-b2bd3d4c142a container agnhost-container: <nil>
    STEP: delete the pod 01/30/23 11:53:32.62
    Jan 30 11:53:32.624: INFO: Waiting for pod pod-configmaps-9db859c9-2bea-45b8-89de-b2bd3d4c142a to disappear
    Jan 30 11:53:32.626: INFO: Pod pod-configmaps-9db859c9-2bea-45b8-89de-b2bd3d4c142a no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jan 30 11:53:32.626: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-5499" for this suite. 01/30/23 11:53:32.628
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:884
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 11:53:32.632
Jan 30 11:53:32.632: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename resourcequota 01/30/23 11:53:32.632
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:53:32.638
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:53:32.64
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:884
STEP: Creating a ResourceQuota 01/30/23 11:53:32.642
STEP: Getting a ResourceQuota 01/30/23 11:53:32.644
STEP: Updating a ResourceQuota 01/30/23 11:53:32.646
STEP: Verifying a ResourceQuota was modified 01/30/23 11:53:32.649
STEP: Deleting a ResourceQuota 01/30/23 11:53:32.651
STEP: Verifying the deleted ResourceQuota 01/30/23 11:53:32.653
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Jan 30 11:53:32.654: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-9846" for this suite. 01/30/23 11:53:32.656
------------------------------
• [0.027 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:884

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 11:53:32.632
    Jan 30 11:53:32.632: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename resourcequota 01/30/23 11:53:32.632
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:53:32.638
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:53:32.64
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to update and delete ResourceQuota. [Conformance]
      test/e2e/apimachinery/resource_quota.go:884
    STEP: Creating a ResourceQuota 01/30/23 11:53:32.642
    STEP: Getting a ResourceQuota 01/30/23 11:53:32.644
    STEP: Updating a ResourceQuota 01/30/23 11:53:32.646
    STEP: Verifying a ResourceQuota was modified 01/30/23 11:53:32.649
    STEP: Deleting a ResourceQuota 01/30/23 11:53:32.651
    STEP: Verifying the deleted ResourceQuota 01/30/23 11:53:32.653
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Jan 30 11:53:32.654: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-9846" for this suite. 01/30/23 11:53:32.656
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:129
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 11:53:32.66
Jan 30 11:53:32.660: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename security-context 01/30/23 11:53:32.661
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:53:32.666
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:53:32.668
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:129
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 01/30/23 11:53:32.67
Jan 30 11:53:32.674: INFO: Waiting up to 5m0s for pod "security-context-f254aabd-82d4-4541-b4e7-3c2c8017dce2" in namespace "security-context-6406" to be "Succeeded or Failed"
Jan 30 11:53:32.675: INFO: Pod "security-context-f254aabd-82d4-4541-b4e7-3c2c8017dce2": Phase="Pending", Reason="", readiness=false. Elapsed: 1.489321ms
Jan 30 11:53:34.678: INFO: Pod "security-context-f254aabd-82d4-4541-b4e7-3c2c8017dce2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004750149s
Jan 30 11:53:36.677: INFO: Pod "security-context-f254aabd-82d4-4541-b4e7-3c2c8017dce2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.003555444s
Jan 30 11:53:38.679: INFO: Pod "security-context-f254aabd-82d4-4541-b4e7-3c2c8017dce2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.005123205s
STEP: Saw pod success 01/30/23 11:53:38.679
Jan 30 11:53:38.679: INFO: Pod "security-context-f254aabd-82d4-4541-b4e7-3c2c8017dce2" satisfied condition "Succeeded or Failed"
Jan 30 11:53:38.681: INFO: Trying to get logs from node pubt2-nks-for-dev1.dg.163.org pod security-context-f254aabd-82d4-4541-b4e7-3c2c8017dce2 container test-container: <nil>
STEP: delete the pod 01/30/23 11:53:38.685
Jan 30 11:53:38.690: INFO: Waiting for pod security-context-f254aabd-82d4-4541-b4e7-3c2c8017dce2 to disappear
Jan 30 11:53:38.691: INFO: Pod security-context-f254aabd-82d4-4541-b4e7-3c2c8017dce2 no longer exists
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Jan 30 11:53:38.691: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-6406" for this suite. 01/30/23 11:53:38.693
------------------------------
• [SLOW TEST] [6.035 seconds]
[sig-node] Security Context
test/e2e/node/framework.go:23
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 11:53:32.66
    Jan 30 11:53:32.660: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename security-context 01/30/23 11:53:32.661
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:53:32.666
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:53:32.668
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
      test/e2e/node/security_context.go:129
    STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 01/30/23 11:53:32.67
    Jan 30 11:53:32.674: INFO: Waiting up to 5m0s for pod "security-context-f254aabd-82d4-4541-b4e7-3c2c8017dce2" in namespace "security-context-6406" to be "Succeeded or Failed"
    Jan 30 11:53:32.675: INFO: Pod "security-context-f254aabd-82d4-4541-b4e7-3c2c8017dce2": Phase="Pending", Reason="", readiness=false. Elapsed: 1.489321ms
    Jan 30 11:53:34.678: INFO: Pod "security-context-f254aabd-82d4-4541-b4e7-3c2c8017dce2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004750149s
    Jan 30 11:53:36.677: INFO: Pod "security-context-f254aabd-82d4-4541-b4e7-3c2c8017dce2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.003555444s
    Jan 30 11:53:38.679: INFO: Pod "security-context-f254aabd-82d4-4541-b4e7-3c2c8017dce2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.005123205s
    STEP: Saw pod success 01/30/23 11:53:38.679
    Jan 30 11:53:38.679: INFO: Pod "security-context-f254aabd-82d4-4541-b4e7-3c2c8017dce2" satisfied condition "Succeeded or Failed"
    Jan 30 11:53:38.681: INFO: Trying to get logs from node pubt2-nks-for-dev1.dg.163.org pod security-context-f254aabd-82d4-4541-b4e7-3c2c8017dce2 container test-container: <nil>
    STEP: delete the pod 01/30/23 11:53:38.685
    Jan 30 11:53:38.690: INFO: Waiting for pod security-context-f254aabd-82d4-4541-b4e7-3c2c8017dce2 to disappear
    Jan 30 11:53:38.691: INFO: Pod security-context-f254aabd-82d4-4541-b4e7-3c2c8017dce2 no longer exists
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Jan 30 11:53:38.691: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-6406" for this suite. 01/30/23 11:53:38.693
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:69
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 11:53:38.697
Jan 30 11:53:38.697: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename crd-publish-openapi 01/30/23 11:53:38.698
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:53:38.704
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:53:38.706
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:69
Jan 30 11:53:38.708: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: kubectl validation (kubectl create and apply) allows request with known and required properties 01/30/23 11:53:40.539
Jan 30 11:53:40.539: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=crd-publish-openapi-5990 --namespace=crd-publish-openapi-5990 create -f -'
Jan 30 11:53:41.711: INFO: stderr: ""
Jan 30 11:53:41.711: INFO: stdout: "e2e-test-crd-publish-openapi-5428-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Jan 30 11:53:41.711: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=crd-publish-openapi-5990 --namespace=crd-publish-openapi-5990 delete e2e-test-crd-publish-openapi-5428-crds test-foo'
Jan 30 11:53:41.774: INFO: stderr: ""
Jan 30 11:53:41.774: INFO: stdout: "e2e-test-crd-publish-openapi-5428-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Jan 30 11:53:41.774: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=crd-publish-openapi-5990 --namespace=crd-publish-openapi-5990 apply -f -'
Jan 30 11:53:41.941: INFO: stderr: ""
Jan 30 11:53:41.941: INFO: stdout: "e2e-test-crd-publish-openapi-5428-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Jan 30 11:53:41.941: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=crd-publish-openapi-5990 --namespace=crd-publish-openapi-5990 delete e2e-test-crd-publish-openapi-5428-crds test-foo'
Jan 30 11:53:42.004: INFO: stderr: ""
Jan 30 11:53:42.004: INFO: stdout: "e2e-test-crd-publish-openapi-5428-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values 01/30/23 11:53:42.004
Jan 30 11:53:42.004: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=crd-publish-openapi-5990 --namespace=crd-publish-openapi-5990 create -f -'
Jan 30 11:53:42.168: INFO: rc: 1
STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema 01/30/23 11:53:42.168
Jan 30 11:53:42.168: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=crd-publish-openapi-5990 --namespace=crd-publish-openapi-5990 create -f -'
Jan 30 11:53:42.331: INFO: rc: 1
Jan 30 11:53:42.331: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=crd-publish-openapi-5990 --namespace=crd-publish-openapi-5990 apply -f -'
Jan 30 11:53:42.497: INFO: rc: 1
STEP: kubectl validation (kubectl create and apply) rejects request without required properties 01/30/23 11:53:42.497
Jan 30 11:53:42.497: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=crd-publish-openapi-5990 --namespace=crd-publish-openapi-5990 create -f -'
Jan 30 11:53:42.664: INFO: rc: 1
Jan 30 11:53:42.664: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=crd-publish-openapi-5990 --namespace=crd-publish-openapi-5990 apply -f -'
Jan 30 11:53:42.834: INFO: rc: 1
STEP: kubectl explain works to explain CR properties 01/30/23 11:53:42.834
Jan 30 11:53:42.834: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=crd-publish-openapi-5990 explain e2e-test-crd-publish-openapi-5428-crds'
Jan 30 11:53:42.995: INFO: stderr: ""
Jan 30 11:53:42.995: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-5428-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively 01/30/23 11:53:42.995
Jan 30 11:53:42.995: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=crd-publish-openapi-5990 explain e2e-test-crd-publish-openapi-5428-crds.metadata'
Jan 30 11:53:43.157: INFO: stderr: ""
Jan 30 11:53:43.157: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-5428-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     return a 409.\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     Deprecated: selfLink is a legacy read-only field that is no longer\n     populated by the system.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Jan 30 11:53:43.158: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=crd-publish-openapi-5990 explain e2e-test-crd-publish-openapi-5428-crds.spec'
Jan 30 11:53:43.321: INFO: stderr: ""
Jan 30 11:53:43.321: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-5428-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Jan 30 11:53:43.321: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=crd-publish-openapi-5990 explain e2e-test-crd-publish-openapi-5428-crds.spec.bars'
Jan 30 11:53:43.486: INFO: stderr: ""
Jan 30 11:53:43.486: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-5428-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   feeling\t<string>\n     Whether Bar is feeling great.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist 01/30/23 11:53:43.486
Jan 30 11:53:43.486: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=crd-publish-openapi-5990 explain e2e-test-crd-publish-openapi-5428-crds.spec.bars2'
Jan 30 11:53:43.649: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 30 11:53:45.965: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-5990" for this suite. 01/30/23 11:53:45.972
------------------------------
• [SLOW TEST] [7.278 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:69

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 11:53:38.697
    Jan 30 11:53:38.697: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename crd-publish-openapi 01/30/23 11:53:38.698
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:53:38.704
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:53:38.706
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for CRD with validation schema [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:69
    Jan 30 11:53:38.708: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: kubectl validation (kubectl create and apply) allows request with known and required properties 01/30/23 11:53:40.539
    Jan 30 11:53:40.539: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=crd-publish-openapi-5990 --namespace=crd-publish-openapi-5990 create -f -'
    Jan 30 11:53:41.711: INFO: stderr: ""
    Jan 30 11:53:41.711: INFO: stdout: "e2e-test-crd-publish-openapi-5428-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
    Jan 30 11:53:41.711: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=crd-publish-openapi-5990 --namespace=crd-publish-openapi-5990 delete e2e-test-crd-publish-openapi-5428-crds test-foo'
    Jan 30 11:53:41.774: INFO: stderr: ""
    Jan 30 11:53:41.774: INFO: stdout: "e2e-test-crd-publish-openapi-5428-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
    Jan 30 11:53:41.774: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=crd-publish-openapi-5990 --namespace=crd-publish-openapi-5990 apply -f -'
    Jan 30 11:53:41.941: INFO: stderr: ""
    Jan 30 11:53:41.941: INFO: stdout: "e2e-test-crd-publish-openapi-5428-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
    Jan 30 11:53:41.941: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=crd-publish-openapi-5990 --namespace=crd-publish-openapi-5990 delete e2e-test-crd-publish-openapi-5428-crds test-foo'
    Jan 30 11:53:42.004: INFO: stderr: ""
    Jan 30 11:53:42.004: INFO: stdout: "e2e-test-crd-publish-openapi-5428-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
    STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values 01/30/23 11:53:42.004
    Jan 30 11:53:42.004: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=crd-publish-openapi-5990 --namespace=crd-publish-openapi-5990 create -f -'
    Jan 30 11:53:42.168: INFO: rc: 1
    STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema 01/30/23 11:53:42.168
    Jan 30 11:53:42.168: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=crd-publish-openapi-5990 --namespace=crd-publish-openapi-5990 create -f -'
    Jan 30 11:53:42.331: INFO: rc: 1
    Jan 30 11:53:42.331: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=crd-publish-openapi-5990 --namespace=crd-publish-openapi-5990 apply -f -'
    Jan 30 11:53:42.497: INFO: rc: 1
    STEP: kubectl validation (kubectl create and apply) rejects request without required properties 01/30/23 11:53:42.497
    Jan 30 11:53:42.497: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=crd-publish-openapi-5990 --namespace=crd-publish-openapi-5990 create -f -'
    Jan 30 11:53:42.664: INFO: rc: 1
    Jan 30 11:53:42.664: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=crd-publish-openapi-5990 --namespace=crd-publish-openapi-5990 apply -f -'
    Jan 30 11:53:42.834: INFO: rc: 1
    STEP: kubectl explain works to explain CR properties 01/30/23 11:53:42.834
    Jan 30 11:53:42.834: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=crd-publish-openapi-5990 explain e2e-test-crd-publish-openapi-5428-crds'
    Jan 30 11:53:42.995: INFO: stderr: ""
    Jan 30 11:53:42.995: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-5428-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
    STEP: kubectl explain works to explain CR properties recursively 01/30/23 11:53:42.995
    Jan 30 11:53:42.995: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=crd-publish-openapi-5990 explain e2e-test-crd-publish-openapi-5428-crds.metadata'
    Jan 30 11:53:43.157: INFO: stderr: ""
    Jan 30 11:53:43.157: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-5428-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     return a 409.\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     Deprecated: selfLink is a legacy read-only field that is no longer\n     populated by the system.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
    Jan 30 11:53:43.158: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=crd-publish-openapi-5990 explain e2e-test-crd-publish-openapi-5428-crds.spec'
    Jan 30 11:53:43.321: INFO: stderr: ""
    Jan 30 11:53:43.321: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-5428-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
    Jan 30 11:53:43.321: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=crd-publish-openapi-5990 explain e2e-test-crd-publish-openapi-5428-crds.spec.bars'
    Jan 30 11:53:43.486: INFO: stderr: ""
    Jan 30 11:53:43.486: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-5428-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   feeling\t<string>\n     Whether Bar is feeling great.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
    STEP: kubectl explain works to return error when explain is called on property that doesn't exist 01/30/23 11:53:43.486
    Jan 30 11:53:43.486: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=crd-publish-openapi-5990 explain e2e-test-crd-publish-openapi-5428-crds.spec.bars2'
    Jan 30 11:53:43.649: INFO: rc: 1
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 30 11:53:45.965: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-5990" for this suite. 01/30/23 11:53:45.972
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1
  A set of valid responses are returned for both pod and service Proxy [Conformance]
  test/e2e/network/proxy.go:380
[BeforeEach] version v1
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 11:53:45.976
Jan 30 11:53:45.976: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename proxy 01/30/23 11:53:45.976
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:53:45.983
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:53:45.985
[BeforeEach] version v1
  test/e2e/framework/metrics/init/init.go:31
[It] A set of valid responses are returned for both pod and service Proxy [Conformance]
  test/e2e/network/proxy.go:380
Jan 30 11:53:45.987: INFO: Creating pod...
Jan 30 11:53:45.991: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-179" to be "running"
Jan 30 11:53:45.992: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 1.525193ms
Jan 30 11:53:47.995: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004404506s
Jan 30 11:53:49.995: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 4.004345791s
Jan 30 11:53:49.995: INFO: Pod "agnhost" satisfied condition "running"
Jan 30 11:53:49.995: INFO: Creating service...
Jan 30 11:53:49.999: INFO: Starting http.Client for https://10.178.64.1:443/api/v1/namespaces/proxy-179/pods/agnhost/proxy?method=DELETE
Jan 30 11:53:50.001: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Jan 30 11:53:50.001: INFO: Starting http.Client for https://10.178.64.1:443/api/v1/namespaces/proxy-179/pods/agnhost/proxy?method=OPTIONS
Jan 30 11:53:50.003: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Jan 30 11:53:50.003: INFO: Starting http.Client for https://10.178.64.1:443/api/v1/namespaces/proxy-179/pods/agnhost/proxy?method=PATCH
Jan 30 11:53:50.005: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Jan 30 11:53:50.005: INFO: Starting http.Client for https://10.178.64.1:443/api/v1/namespaces/proxy-179/pods/agnhost/proxy?method=POST
Jan 30 11:53:50.007: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Jan 30 11:53:50.007: INFO: Starting http.Client for https://10.178.64.1:443/api/v1/namespaces/proxy-179/pods/agnhost/proxy?method=PUT
Jan 30 11:53:50.009: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Jan 30 11:53:50.009: INFO: Starting http.Client for https://10.178.64.1:443/api/v1/namespaces/proxy-179/services/e2e-proxy-test-service/proxy?method=DELETE
Jan 30 11:53:50.011: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Jan 30 11:53:50.011: INFO: Starting http.Client for https://10.178.64.1:443/api/v1/namespaces/proxy-179/services/e2e-proxy-test-service/proxy?method=OPTIONS
Jan 30 11:53:50.014: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Jan 30 11:53:50.014: INFO: Starting http.Client for https://10.178.64.1:443/api/v1/namespaces/proxy-179/services/e2e-proxy-test-service/proxy?method=PATCH
Jan 30 11:53:50.016: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Jan 30 11:53:50.016: INFO: Starting http.Client for https://10.178.64.1:443/api/v1/namespaces/proxy-179/services/e2e-proxy-test-service/proxy?method=POST
Jan 30 11:53:50.018: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Jan 30 11:53:50.018: INFO: Starting http.Client for https://10.178.64.1:443/api/v1/namespaces/proxy-179/services/e2e-proxy-test-service/proxy?method=PUT
Jan 30 11:53:50.020: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Jan 30 11:53:50.020: INFO: Starting http.Client for https://10.178.64.1:443/api/v1/namespaces/proxy-179/pods/agnhost/proxy?method=GET
Jan 30 11:53:50.021: INFO: http.Client request:GET StatusCode:301
Jan 30 11:53:50.021: INFO: Starting http.Client for https://10.178.64.1:443/api/v1/namespaces/proxy-179/services/e2e-proxy-test-service/proxy?method=GET
Jan 30 11:53:50.023: INFO: http.Client request:GET StatusCode:301
Jan 30 11:53:50.023: INFO: Starting http.Client for https://10.178.64.1:443/api/v1/namespaces/proxy-179/pods/agnhost/proxy?method=HEAD
Jan 30 11:53:50.024: INFO: http.Client request:HEAD StatusCode:301
Jan 30 11:53:50.024: INFO: Starting http.Client for https://10.178.64.1:443/api/v1/namespaces/proxy-179/services/e2e-proxy-test-service/proxy?method=HEAD
Jan 30 11:53:50.026: INFO: http.Client request:HEAD StatusCode:301
[AfterEach] version v1
  test/e2e/framework/node/init/init.go:32
Jan 30 11:53:50.026: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] version v1
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] version v1
  dump namespaces | framework.go:196
[DeferCleanup (Each)] version v1
  tear down framework | framework.go:193
STEP: Destroying namespace "proxy-179" for this suite. 01/30/23 11:53:50.028
------------------------------
• [4.055 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    A set of valid responses are returned for both pod and service Proxy [Conformance]
    test/e2e/network/proxy.go:380

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 11:53:45.976
    Jan 30 11:53:45.976: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename proxy 01/30/23 11:53:45.976
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:53:45.983
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:53:45.985
    [BeforeEach] version v1
      test/e2e/framework/metrics/init/init.go:31
    [It] A set of valid responses are returned for both pod and service Proxy [Conformance]
      test/e2e/network/proxy.go:380
    Jan 30 11:53:45.987: INFO: Creating pod...
    Jan 30 11:53:45.991: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-179" to be "running"
    Jan 30 11:53:45.992: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 1.525193ms
    Jan 30 11:53:47.995: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004404506s
    Jan 30 11:53:49.995: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 4.004345791s
    Jan 30 11:53:49.995: INFO: Pod "agnhost" satisfied condition "running"
    Jan 30 11:53:49.995: INFO: Creating service...
    Jan 30 11:53:49.999: INFO: Starting http.Client for https://10.178.64.1:443/api/v1/namespaces/proxy-179/pods/agnhost/proxy?method=DELETE
    Jan 30 11:53:50.001: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Jan 30 11:53:50.001: INFO: Starting http.Client for https://10.178.64.1:443/api/v1/namespaces/proxy-179/pods/agnhost/proxy?method=OPTIONS
    Jan 30 11:53:50.003: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Jan 30 11:53:50.003: INFO: Starting http.Client for https://10.178.64.1:443/api/v1/namespaces/proxy-179/pods/agnhost/proxy?method=PATCH
    Jan 30 11:53:50.005: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Jan 30 11:53:50.005: INFO: Starting http.Client for https://10.178.64.1:443/api/v1/namespaces/proxy-179/pods/agnhost/proxy?method=POST
    Jan 30 11:53:50.007: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Jan 30 11:53:50.007: INFO: Starting http.Client for https://10.178.64.1:443/api/v1/namespaces/proxy-179/pods/agnhost/proxy?method=PUT
    Jan 30 11:53:50.009: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Jan 30 11:53:50.009: INFO: Starting http.Client for https://10.178.64.1:443/api/v1/namespaces/proxy-179/services/e2e-proxy-test-service/proxy?method=DELETE
    Jan 30 11:53:50.011: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Jan 30 11:53:50.011: INFO: Starting http.Client for https://10.178.64.1:443/api/v1/namespaces/proxy-179/services/e2e-proxy-test-service/proxy?method=OPTIONS
    Jan 30 11:53:50.014: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Jan 30 11:53:50.014: INFO: Starting http.Client for https://10.178.64.1:443/api/v1/namespaces/proxy-179/services/e2e-proxy-test-service/proxy?method=PATCH
    Jan 30 11:53:50.016: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Jan 30 11:53:50.016: INFO: Starting http.Client for https://10.178.64.1:443/api/v1/namespaces/proxy-179/services/e2e-proxy-test-service/proxy?method=POST
    Jan 30 11:53:50.018: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Jan 30 11:53:50.018: INFO: Starting http.Client for https://10.178.64.1:443/api/v1/namespaces/proxy-179/services/e2e-proxy-test-service/proxy?method=PUT
    Jan 30 11:53:50.020: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Jan 30 11:53:50.020: INFO: Starting http.Client for https://10.178.64.1:443/api/v1/namespaces/proxy-179/pods/agnhost/proxy?method=GET
    Jan 30 11:53:50.021: INFO: http.Client request:GET StatusCode:301
    Jan 30 11:53:50.021: INFO: Starting http.Client for https://10.178.64.1:443/api/v1/namespaces/proxy-179/services/e2e-proxy-test-service/proxy?method=GET
    Jan 30 11:53:50.023: INFO: http.Client request:GET StatusCode:301
    Jan 30 11:53:50.023: INFO: Starting http.Client for https://10.178.64.1:443/api/v1/namespaces/proxy-179/pods/agnhost/proxy?method=HEAD
    Jan 30 11:53:50.024: INFO: http.Client request:HEAD StatusCode:301
    Jan 30 11:53:50.024: INFO: Starting http.Client for https://10.178.64.1:443/api/v1/namespaces/proxy-179/services/e2e-proxy-test-service/proxy?method=HEAD
    Jan 30 11:53:50.026: INFO: http.Client request:HEAD StatusCode:301
    [AfterEach] version v1
      test/e2e/framework/node/init/init.go:32
    Jan 30 11:53:50.026: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] version v1
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] version v1
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] version v1
      tear down framework | framework.go:193
    STEP: Destroying namespace "proxy-179" for this suite. 01/30/23 11:53:50.028
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:157
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 11:53:50.031
Jan 30 11:53:50.031: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename emptydir 01/30/23 11:53:50.032
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:53:50.038
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:53:50.04
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:157
STEP: Creating a pod to test emptydir volume type on node default medium 01/30/23 11:53:50.042
Jan 30 11:53:50.046: INFO: Waiting up to 5m0s for pod "pod-13370596-46ef-4131-9779-a36334fbb127" in namespace "emptydir-70" to be "Succeeded or Failed"
Jan 30 11:53:50.047: INFO: Pod "pod-13370596-46ef-4131-9779-a36334fbb127": Phase="Pending", Reason="", readiness=false. Elapsed: 1.543728ms
Jan 30 11:53:52.050: INFO: Pod "pod-13370596-46ef-4131-9779-a36334fbb127": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004224273s
Jan 30 11:53:54.050: INFO: Pod "pod-13370596-46ef-4131-9779-a36334fbb127": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004512414s
STEP: Saw pod success 01/30/23 11:53:54.05
Jan 30 11:53:54.050: INFO: Pod "pod-13370596-46ef-4131-9779-a36334fbb127" satisfied condition "Succeeded or Failed"
Jan 30 11:53:54.052: INFO: Trying to get logs from node pubt2-nks-for-dev1.dg.163.org pod pod-13370596-46ef-4131-9779-a36334fbb127 container test-container: <nil>
STEP: delete the pod 01/30/23 11:53:54.059
Jan 30 11:53:54.063: INFO: Waiting for pod pod-13370596-46ef-4131-9779-a36334fbb127 to disappear
Jan 30 11:53:54.065: INFO: Pod pod-13370596-46ef-4131-9779-a36334fbb127 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jan 30 11:53:54.065: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-70" for this suite. 01/30/23 11:53:54.067
------------------------------
• [4.038 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:157

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 11:53:50.031
    Jan 30 11:53:50.031: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename emptydir 01/30/23 11:53:50.032
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:53:50.038
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:53:50.04
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:157
    STEP: Creating a pod to test emptydir volume type on node default medium 01/30/23 11:53:50.042
    Jan 30 11:53:50.046: INFO: Waiting up to 5m0s for pod "pod-13370596-46ef-4131-9779-a36334fbb127" in namespace "emptydir-70" to be "Succeeded or Failed"
    Jan 30 11:53:50.047: INFO: Pod "pod-13370596-46ef-4131-9779-a36334fbb127": Phase="Pending", Reason="", readiness=false. Elapsed: 1.543728ms
    Jan 30 11:53:52.050: INFO: Pod "pod-13370596-46ef-4131-9779-a36334fbb127": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004224273s
    Jan 30 11:53:54.050: INFO: Pod "pod-13370596-46ef-4131-9779-a36334fbb127": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004512414s
    STEP: Saw pod success 01/30/23 11:53:54.05
    Jan 30 11:53:54.050: INFO: Pod "pod-13370596-46ef-4131-9779-a36334fbb127" satisfied condition "Succeeded or Failed"
    Jan 30 11:53:54.052: INFO: Trying to get logs from node pubt2-nks-for-dev1.dg.163.org pod pod-13370596-46ef-4131-9779-a36334fbb127 container test-container: <nil>
    STEP: delete the pod 01/30/23 11:53:54.059
    Jan 30 11:53:54.063: INFO: Waiting for pod pod-13370596-46ef-4131-9779-a36334fbb127 to disappear
    Jan 30 11:53:54.065: INFO: Pod pod-13370596-46ef-4131-9779-a36334fbb127 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jan 30 11:53:54.065: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-70" for this suite. 01/30/23 11:53:54.067
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] Downward API volume
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:162
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 11:53:54.069
Jan 30 11:53:54.069: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename downward-api 01/30/23 11:53:54.07
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:53:54.076
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:53:54.078
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:162
STEP: Creating the pod 01/30/23 11:53:54.08
Jan 30 11:53:54.084: INFO: Waiting up to 5m0s for pod "annotationupdate7a8e61cc-dc43-4c26-8e33-2ad4440b5396" in namespace "downward-api-827" to be "running and ready"
Jan 30 11:53:54.086: INFO: Pod "annotationupdate7a8e61cc-dc43-4c26-8e33-2ad4440b5396": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005332ms
Jan 30 11:53:54.086: INFO: The phase of Pod annotationupdate7a8e61cc-dc43-4c26-8e33-2ad4440b5396 is Pending, waiting for it to be Running (with Ready = true)
Jan 30 11:53:56.087: INFO: Pod "annotationupdate7a8e61cc-dc43-4c26-8e33-2ad4440b5396": Phase="Running", Reason="", readiness=true. Elapsed: 2.003910551s
Jan 30 11:53:56.087: INFO: The phase of Pod annotationupdate7a8e61cc-dc43-4c26-8e33-2ad4440b5396 is Running (Ready = true)
Jan 30 11:53:56.087: INFO: Pod "annotationupdate7a8e61cc-dc43-4c26-8e33-2ad4440b5396" satisfied condition "running and ready"
Jan 30 11:53:56.601: INFO: Successfully updated pod "annotationupdate7a8e61cc-dc43-4c26-8e33-2ad4440b5396"
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Jan 30 11:53:58.610: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-827" for this suite. 01/30/23 11:53:58.613
------------------------------
• [4.546 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:162

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 11:53:54.069
    Jan 30 11:53:54.069: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename downward-api 01/30/23 11:53:54.07
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:53:54.076
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:53:54.078
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should update annotations on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:162
    STEP: Creating the pod 01/30/23 11:53:54.08
    Jan 30 11:53:54.084: INFO: Waiting up to 5m0s for pod "annotationupdate7a8e61cc-dc43-4c26-8e33-2ad4440b5396" in namespace "downward-api-827" to be "running and ready"
    Jan 30 11:53:54.086: INFO: Pod "annotationupdate7a8e61cc-dc43-4c26-8e33-2ad4440b5396": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005332ms
    Jan 30 11:53:54.086: INFO: The phase of Pod annotationupdate7a8e61cc-dc43-4c26-8e33-2ad4440b5396 is Pending, waiting for it to be Running (with Ready = true)
    Jan 30 11:53:56.087: INFO: Pod "annotationupdate7a8e61cc-dc43-4c26-8e33-2ad4440b5396": Phase="Running", Reason="", readiness=true. Elapsed: 2.003910551s
    Jan 30 11:53:56.087: INFO: The phase of Pod annotationupdate7a8e61cc-dc43-4c26-8e33-2ad4440b5396 is Running (Ready = true)
    Jan 30 11:53:56.087: INFO: Pod "annotationupdate7a8e61cc-dc43-4c26-8e33-2ad4440b5396" satisfied condition "running and ready"
    Jan 30 11:53:56.601: INFO: Successfully updated pod "annotationupdate7a8e61cc-dc43-4c26-8e33-2ad4440b5396"
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Jan 30 11:53:58.610: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-827" for this suite. 01/30/23 11:53:58.613
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should validate Statefulset Status endpoints [Conformance]
  test/e2e/apps/statefulset.go:977
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 11:53:58.617
Jan 30 11:53:58.617: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename statefulset 01/30/23 11:53:58.618
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:53:58.624
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:53:58.626
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-8793 01/30/23 11:53:58.628
[It] should validate Statefulset Status endpoints [Conformance]
  test/e2e/apps/statefulset.go:977
STEP: Creating statefulset ss in namespace statefulset-8793 01/30/23 11:53:58.632
Jan 30 11:53:58.636: INFO: Found 0 stateful pods, waiting for 1
Jan 30 11:54:08.639: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Patch Statefulset to include a label 01/30/23 11:54:08.643
STEP: Getting /status 01/30/23 11:54:08.647
Jan 30 11:54:08.649: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
STEP: updating the StatefulSet Status 01/30/23 11:54:08.649
Jan 30 11:54:08.654: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the statefulset status to be updated 01/30/23 11:54:08.654
Jan 30 11:54:08.656: INFO: Observed &StatefulSet event: ADDED
Jan 30 11:54:08.656: INFO: Found Statefulset ss in namespace statefulset-8793 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Jan 30 11:54:08.656: INFO: Statefulset ss has an updated status
STEP: patching the Statefulset Status 01/30/23 11:54:08.656
Jan 30 11:54:08.656: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Jan 30 11:54:08.661: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Statefulset status to be patched 01/30/23 11:54:08.661
Jan 30 11:54:08.662: INFO: Observed &StatefulSet event: ADDED
Jan 30 11:54:08.662: INFO: Observed Statefulset ss in namespace statefulset-8793 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Jan 30 11:54:08.662: INFO: Observed &StatefulSet event: MODIFIED
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Jan 30 11:54:08.662: INFO: Deleting all statefulset in ns statefulset-8793
Jan 30 11:54:08.663: INFO: Scaling statefulset ss to 0
Jan 30 11:54:18.673: INFO: Waiting for statefulset status.replicas updated to 0
Jan 30 11:54:18.674: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Jan 30 11:54:18.680: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-8793" for this suite. 01/30/23 11:54:18.682
------------------------------
• [SLOW TEST] [20.067 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should validate Statefulset Status endpoints [Conformance]
    test/e2e/apps/statefulset.go:977

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 11:53:58.617
    Jan 30 11:53:58.617: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename statefulset 01/30/23 11:53:58.618
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:53:58.624
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:53:58.626
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-8793 01/30/23 11:53:58.628
    [It] should validate Statefulset Status endpoints [Conformance]
      test/e2e/apps/statefulset.go:977
    STEP: Creating statefulset ss in namespace statefulset-8793 01/30/23 11:53:58.632
    Jan 30 11:53:58.636: INFO: Found 0 stateful pods, waiting for 1
    Jan 30 11:54:08.639: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Patch Statefulset to include a label 01/30/23 11:54:08.643
    STEP: Getting /status 01/30/23 11:54:08.647
    Jan 30 11:54:08.649: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
    STEP: updating the StatefulSet Status 01/30/23 11:54:08.649
    Jan 30 11:54:08.654: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the statefulset status to be updated 01/30/23 11:54:08.654
    Jan 30 11:54:08.656: INFO: Observed &StatefulSet event: ADDED
    Jan 30 11:54:08.656: INFO: Found Statefulset ss in namespace statefulset-8793 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Jan 30 11:54:08.656: INFO: Statefulset ss has an updated status
    STEP: patching the Statefulset Status 01/30/23 11:54:08.656
    Jan 30 11:54:08.656: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Jan 30 11:54:08.661: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Statefulset status to be patched 01/30/23 11:54:08.661
    Jan 30 11:54:08.662: INFO: Observed &StatefulSet event: ADDED
    Jan 30 11:54:08.662: INFO: Observed Statefulset ss in namespace statefulset-8793 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Jan 30 11:54:08.662: INFO: Observed &StatefulSet event: MODIFIED
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Jan 30 11:54:08.662: INFO: Deleting all statefulset in ns statefulset-8793
    Jan 30 11:54:08.663: INFO: Scaling statefulset ss to 0
    Jan 30 11:54:18.673: INFO: Waiting for statefulset status.replicas updated to 0
    Jan 30 11:54:18.674: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Jan 30 11:54:18.680: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-8793" for this suite. 01/30/23 11:54:18.682
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 11:54:18.685
Jan 30 11:54:18.685: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename dns 01/30/23 11:54:18.686
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:54:18.692
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:54:18.694
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248
STEP: Creating a test headless service 01/30/23 11:54:18.696
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-7162.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-7162.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done
 01/30/23 11:54:18.698
STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-7162.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-7162.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done
 01/30/23 11:54:18.698
STEP: creating a pod to probe DNS 01/30/23 11:54:18.698
STEP: submitting the pod to kubernetes 01/30/23 11:54:18.699
Jan 30 11:54:18.703: INFO: Waiting up to 15m0s for pod "dns-test-ed9e89f7-7430-4e8a-99a1-e73a961bc8af" in namespace "dns-7162" to be "running"
Jan 30 11:54:18.705: INFO: Pod "dns-test-ed9e89f7-7430-4e8a-99a1-e73a961bc8af": Phase="Pending", Reason="", readiness=false. Elapsed: 1.619258ms
Jan 30 11:54:20.707: INFO: Pod "dns-test-ed9e89f7-7430-4e8a-99a1-e73a961bc8af": Phase="Running", Reason="", readiness=true. Elapsed: 2.004147093s
Jan 30 11:54:20.708: INFO: Pod "dns-test-ed9e89f7-7430-4e8a-99a1-e73a961bc8af" satisfied condition "running"
STEP: retrieving the pod 01/30/23 11:54:20.708
STEP: looking for the results for each expected name from probers 01/30/23 11:54:20.71
Jan 30 11:54:20.718: INFO: DNS probes using dns-7162/dns-test-ed9e89f7-7430-4e8a-99a1-e73a961bc8af succeeded

STEP: deleting the pod 01/30/23 11:54:20.718
STEP: deleting the test headless service 01/30/23 11:54:20.722
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Jan 30 11:54:20.726: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-7162" for this suite. 01/30/23 11:54:20.728
------------------------------
• [2.045 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 11:54:18.685
    Jan 30 11:54:18.685: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename dns 01/30/23 11:54:18.686
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:54:18.692
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:54:18.694
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for pods for Hostname [Conformance]
      test/e2e/network/dns.go:248
    STEP: Creating a test headless service 01/30/23 11:54:18.696
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-7162.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-7162.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done
     01/30/23 11:54:18.698
    STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-7162.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-7162.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done
     01/30/23 11:54:18.698
    STEP: creating a pod to probe DNS 01/30/23 11:54:18.698
    STEP: submitting the pod to kubernetes 01/30/23 11:54:18.699
    Jan 30 11:54:18.703: INFO: Waiting up to 15m0s for pod "dns-test-ed9e89f7-7430-4e8a-99a1-e73a961bc8af" in namespace "dns-7162" to be "running"
    Jan 30 11:54:18.705: INFO: Pod "dns-test-ed9e89f7-7430-4e8a-99a1-e73a961bc8af": Phase="Pending", Reason="", readiness=false. Elapsed: 1.619258ms
    Jan 30 11:54:20.707: INFO: Pod "dns-test-ed9e89f7-7430-4e8a-99a1-e73a961bc8af": Phase="Running", Reason="", readiness=true. Elapsed: 2.004147093s
    Jan 30 11:54:20.708: INFO: Pod "dns-test-ed9e89f7-7430-4e8a-99a1-e73a961bc8af" satisfied condition "running"
    STEP: retrieving the pod 01/30/23 11:54:20.708
    STEP: looking for the results for each expected name from probers 01/30/23 11:54:20.71
    Jan 30 11:54:20.718: INFO: DNS probes using dns-7162/dns-test-ed9e89f7-7430-4e8a-99a1-e73a961bc8af succeeded

    STEP: deleting the pod 01/30/23 11:54:20.718
    STEP: deleting the test headless service 01/30/23 11:54:20.722
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Jan 30 11:54:20.726: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-7162" for this suite. 01/30/23 11:54:20.728
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:221
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 11:54:20.731
Jan 30 11:54:20.731: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename projected 01/30/23 11:54:20.732
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:54:20.737
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:54:20.739
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:221
STEP: Creating a pod to test downward API volume plugin 01/30/23 11:54:20.741
Jan 30 11:54:20.746: INFO: Waiting up to 5m0s for pod "downwardapi-volume-91a4da41-b08c-4d31-ac76-a8400936263d" in namespace "projected-8455" to be "Succeeded or Failed"
Jan 30 11:54:20.747: INFO: Pod "downwardapi-volume-91a4da41-b08c-4d31-ac76-a8400936263d": Phase="Pending", Reason="", readiness=false. Elapsed: 1.574564ms
Jan 30 11:54:22.749: INFO: Pod "downwardapi-volume-91a4da41-b08c-4d31-ac76-a8400936263d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003610957s
Jan 30 11:54:24.751: INFO: Pod "downwardapi-volume-91a4da41-b08c-4d31-ac76-a8400936263d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004766562s
STEP: Saw pod success 01/30/23 11:54:24.751
Jan 30 11:54:24.751: INFO: Pod "downwardapi-volume-91a4da41-b08c-4d31-ac76-a8400936263d" satisfied condition "Succeeded or Failed"
Jan 30 11:54:24.753: INFO: Trying to get logs from node pubt2-nks-for-dev1.dg.163.org pod downwardapi-volume-91a4da41-b08c-4d31-ac76-a8400936263d container client-container: <nil>
STEP: delete the pod 01/30/23 11:54:24.768
Jan 30 11:54:24.775: INFO: Waiting for pod downwardapi-volume-91a4da41-b08c-4d31-ac76-a8400936263d to disappear
Jan 30 11:54:24.777: INFO: Pod downwardapi-volume-91a4da41-b08c-4d31-ac76-a8400936263d no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Jan 30 11:54:24.777: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-8455" for this suite. 01/30/23 11:54:24.78
------------------------------
• [4.051 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:221

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 11:54:20.731
    Jan 30 11:54:20.731: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename projected 01/30/23 11:54:20.732
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:54:20.737
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:54:20.739
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide container's cpu request [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:221
    STEP: Creating a pod to test downward API volume plugin 01/30/23 11:54:20.741
    Jan 30 11:54:20.746: INFO: Waiting up to 5m0s for pod "downwardapi-volume-91a4da41-b08c-4d31-ac76-a8400936263d" in namespace "projected-8455" to be "Succeeded or Failed"
    Jan 30 11:54:20.747: INFO: Pod "downwardapi-volume-91a4da41-b08c-4d31-ac76-a8400936263d": Phase="Pending", Reason="", readiness=false. Elapsed: 1.574564ms
    Jan 30 11:54:22.749: INFO: Pod "downwardapi-volume-91a4da41-b08c-4d31-ac76-a8400936263d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003610957s
    Jan 30 11:54:24.751: INFO: Pod "downwardapi-volume-91a4da41-b08c-4d31-ac76-a8400936263d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004766562s
    STEP: Saw pod success 01/30/23 11:54:24.751
    Jan 30 11:54:24.751: INFO: Pod "downwardapi-volume-91a4da41-b08c-4d31-ac76-a8400936263d" satisfied condition "Succeeded or Failed"
    Jan 30 11:54:24.753: INFO: Trying to get logs from node pubt2-nks-for-dev1.dg.163.org pod downwardapi-volume-91a4da41-b08c-4d31-ac76-a8400936263d container client-container: <nil>
    STEP: delete the pod 01/30/23 11:54:24.768
    Jan 30 11:54:24.775: INFO: Waiting for pod downwardapi-volume-91a4da41-b08c-4d31-ac76-a8400936263d to disappear
    Jan 30 11:54:24.777: INFO: Pod downwardapi-volume-91a4da41-b08c-4d31-ac76-a8400936263d no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Jan 30 11:54:24.777: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-8455" for this suite. 01/30/23 11:54:24.78
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute prestop http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:212
[BeforeEach] [sig-node] Container Lifecycle Hook
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 11:54:24.788
Jan 30 11:54:24.788: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename container-lifecycle-hook 01/30/23 11:54:24.789
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:54:24.796
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:54:24.798
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:77
STEP: create the container to handle the HTTPGet hook request. 01/30/23 11:54:24.803
Jan 30 11:54:24.807: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-9444" to be "running and ready"
Jan 30 11:54:24.810: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.111718ms
Jan 30 11:54:24.810: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jan 30 11:54:26.813: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.005102166s
Jan 30 11:54:26.813: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Jan 30 11:54:26.813: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:212
STEP: create the pod with lifecycle hook 01/30/23 11:54:26.814
Jan 30 11:54:26.817: INFO: Waiting up to 5m0s for pod "pod-with-prestop-http-hook" in namespace "container-lifecycle-hook-9444" to be "running and ready"
Jan 30 11:54:26.819: INFO: Pod "pod-with-prestop-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 1.742412ms
Jan 30 11:54:26.819: INFO: The phase of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
Jan 30 11:54:28.822: INFO: Pod "pod-with-prestop-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.004693296s
Jan 30 11:54:28.822: INFO: The phase of Pod pod-with-prestop-http-hook is Running (Ready = true)
Jan 30 11:54:28.822: INFO: Pod "pod-with-prestop-http-hook" satisfied condition "running and ready"
STEP: delete the pod with lifecycle hook 01/30/23 11:54:28.823
Jan 30 11:54:28.826: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jan 30 11:54:28.828: INFO: Pod pod-with-prestop-http-hook still exists
Jan 30 11:54:30.829: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jan 30 11:54:30.832: INFO: Pod pod-with-prestop-http-hook still exists
Jan 30 11:54:32.830: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jan 30 11:54:32.832: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook 01/30/23 11:54:32.832
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/node/init/init.go:32
Jan 30 11:54:32.846: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  tear down framework | framework.go:193
STEP: Destroying namespace "container-lifecycle-hook-9444" for this suite. 01/30/23 11:54:32.848
------------------------------
• [SLOW TEST] [8.063 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute prestop http hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:212

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 11:54:24.788
    Jan 30 11:54:24.788: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename container-lifecycle-hook 01/30/23 11:54:24.789
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:54:24.796
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:54:24.798
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:77
    STEP: create the container to handle the HTTPGet hook request. 01/30/23 11:54:24.803
    Jan 30 11:54:24.807: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-9444" to be "running and ready"
    Jan 30 11:54:24.810: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.111718ms
    Jan 30 11:54:24.810: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Jan 30 11:54:26.813: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.005102166s
    Jan 30 11:54:26.813: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Jan 30 11:54:26.813: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute prestop http hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:212
    STEP: create the pod with lifecycle hook 01/30/23 11:54:26.814
    Jan 30 11:54:26.817: INFO: Waiting up to 5m0s for pod "pod-with-prestop-http-hook" in namespace "container-lifecycle-hook-9444" to be "running and ready"
    Jan 30 11:54:26.819: INFO: Pod "pod-with-prestop-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 1.742412ms
    Jan 30 11:54:26.819: INFO: The phase of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
    Jan 30 11:54:28.822: INFO: Pod "pod-with-prestop-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.004693296s
    Jan 30 11:54:28.822: INFO: The phase of Pod pod-with-prestop-http-hook is Running (Ready = true)
    Jan 30 11:54:28.822: INFO: Pod "pod-with-prestop-http-hook" satisfied condition "running and ready"
    STEP: delete the pod with lifecycle hook 01/30/23 11:54:28.823
    Jan 30 11:54:28.826: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Jan 30 11:54:28.828: INFO: Pod pod-with-prestop-http-hook still exists
    Jan 30 11:54:30.829: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Jan 30 11:54:30.832: INFO: Pod pod-with-prestop-http-hook still exists
    Jan 30 11:54:32.830: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Jan 30 11:54:32.832: INFO: Pod pod-with-prestop-http-hook no longer exists
    STEP: check prestop hook 01/30/23 11:54:32.832
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/node/init/init.go:32
    Jan 30 11:54:32.846: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-lifecycle-hook-9444" for this suite. 01/30/23 11:54:32.848
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:299
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 11:54:32.851
Jan 30 11:54:32.851: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename namespaces 01/30/23 11:54:32.852
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:54:32.858
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:54:32.86
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:299
STEP: Read namespace status 01/30/23 11:54:32.862
Jan 30 11:54:32.863: INFO: Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
STEP: Patch namespace status 01/30/23 11:54:32.863
Jan 30 11:54:32.866: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
STEP: Update namespace status 01/30/23 11:54:32.866
Jan 30 11:54:32.871: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Jan 30 11:54:32.871: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-4603" for this suite. 01/30/23 11:54:32.873
------------------------------
• [0.024 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:299

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 11:54:32.851
    Jan 30 11:54:32.851: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename namespaces 01/30/23 11:54:32.852
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:54:32.858
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:54:32.86
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply changes to a namespace status [Conformance]
      test/e2e/apimachinery/namespace.go:299
    STEP: Read namespace status 01/30/23 11:54:32.862
    Jan 30 11:54:32.863: INFO: Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
    STEP: Patch namespace status 01/30/23 11:54:32.863
    Jan 30 11:54:32.866: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
    STEP: Update namespace status 01/30/23 11:54:32.866
    Jan 30 11:54:32.871: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan 30 11:54:32.871: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-4603" for this suite. 01/30/23 11:54:32.873
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:53
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 11:54:32.875
Jan 30 11:54:32.875: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename downward-api 01/30/23 11:54:32.876
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:54:32.884
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:54:32.885
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:53
STEP: Creating a pod to test downward API volume plugin 01/30/23 11:54:32.887
Jan 30 11:54:32.892: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4b114594-9b29-49d3-af1b-97b2059c9f49" in namespace "downward-api-562" to be "Succeeded or Failed"
Jan 30 11:54:32.893: INFO: Pod "downwardapi-volume-4b114594-9b29-49d3-af1b-97b2059c9f49": Phase="Pending", Reason="", readiness=false. Elapsed: 1.574022ms
Jan 30 11:54:34.896: INFO: Pod "downwardapi-volume-4b114594-9b29-49d3-af1b-97b2059c9f49": Phase="Running", Reason="", readiness=true. Elapsed: 2.004459649s
Jan 30 11:54:36.897: INFO: Pod "downwardapi-volume-4b114594-9b29-49d3-af1b-97b2059c9f49": Phase="Running", Reason="", readiness=false. Elapsed: 4.00528676s
Jan 30 11:54:38.897: INFO: Pod "downwardapi-volume-4b114594-9b29-49d3-af1b-97b2059c9f49": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.004801152s
STEP: Saw pod success 01/30/23 11:54:38.897
Jan 30 11:54:38.897: INFO: Pod "downwardapi-volume-4b114594-9b29-49d3-af1b-97b2059c9f49" satisfied condition "Succeeded or Failed"
Jan 30 11:54:38.898: INFO: Trying to get logs from node pubt2-nks-for-dev1.dg.163.org pod downwardapi-volume-4b114594-9b29-49d3-af1b-97b2059c9f49 container client-container: <nil>
STEP: delete the pod 01/30/23 11:54:38.903
Jan 30 11:54:38.908: INFO: Waiting for pod downwardapi-volume-4b114594-9b29-49d3-af1b-97b2059c9f49 to disappear
Jan 30 11:54:38.909: INFO: Pod downwardapi-volume-4b114594-9b29-49d3-af1b-97b2059c9f49 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Jan 30 11:54:38.909: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-562" for this suite. 01/30/23 11:54:38.911
------------------------------
• [SLOW TEST] [6.038 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 11:54:32.875
    Jan 30 11:54:32.875: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename downward-api 01/30/23 11:54:32.876
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:54:32.884
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:54:32.885
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide podname only [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:53
    STEP: Creating a pod to test downward API volume plugin 01/30/23 11:54:32.887
    Jan 30 11:54:32.892: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4b114594-9b29-49d3-af1b-97b2059c9f49" in namespace "downward-api-562" to be "Succeeded or Failed"
    Jan 30 11:54:32.893: INFO: Pod "downwardapi-volume-4b114594-9b29-49d3-af1b-97b2059c9f49": Phase="Pending", Reason="", readiness=false. Elapsed: 1.574022ms
    Jan 30 11:54:34.896: INFO: Pod "downwardapi-volume-4b114594-9b29-49d3-af1b-97b2059c9f49": Phase="Running", Reason="", readiness=true. Elapsed: 2.004459649s
    Jan 30 11:54:36.897: INFO: Pod "downwardapi-volume-4b114594-9b29-49d3-af1b-97b2059c9f49": Phase="Running", Reason="", readiness=false. Elapsed: 4.00528676s
    Jan 30 11:54:38.897: INFO: Pod "downwardapi-volume-4b114594-9b29-49d3-af1b-97b2059c9f49": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.004801152s
    STEP: Saw pod success 01/30/23 11:54:38.897
    Jan 30 11:54:38.897: INFO: Pod "downwardapi-volume-4b114594-9b29-49d3-af1b-97b2059c9f49" satisfied condition "Succeeded or Failed"
    Jan 30 11:54:38.898: INFO: Trying to get logs from node pubt2-nks-for-dev1.dg.163.org pod downwardapi-volume-4b114594-9b29-49d3-af1b-97b2059c9f49 container client-container: <nil>
    STEP: delete the pod 01/30/23 11:54:38.903
    Jan 30 11:54:38.908: INFO: Waiting for pod downwardapi-volume-4b114594-9b29-49d3-af1b-97b2059c9f49 to disappear
    Jan 30 11:54:38.909: INFO: Pod downwardapi-volume-4b114594-9b29-49d3-af1b-97b2059c9f49 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Jan 30 11:54:38.909: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-562" for this suite. 01/30/23 11:54:38.911
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Secrets
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:46
[BeforeEach] [sig-node] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 11:54:38.914
Jan 30 11:54:38.914: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename secrets 01/30/23 11:54:38.915
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:54:38.92
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:54:38.922
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:46
STEP: Creating secret with name secret-test-0768a7cf-5073-46e9-90f2-b4d72372368d 01/30/23 11:54:38.924
STEP: Creating a pod to test consume secrets 01/30/23 11:54:38.927
Jan 30 11:54:38.930: INFO: Waiting up to 5m0s for pod "pod-secrets-51076667-0fc2-42de-900c-d65747b21168" in namespace "secrets-9741" to be "Succeeded or Failed"
Jan 30 11:54:38.932: INFO: Pod "pod-secrets-51076667-0fc2-42de-900c-d65747b21168": Phase="Pending", Reason="", readiness=false. Elapsed: 1.481463ms
Jan 30 11:54:40.934: INFO: Pod "pod-secrets-51076667-0fc2-42de-900c-d65747b21168": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003663103s
Jan 30 11:54:42.934: INFO: Pod "pod-secrets-51076667-0fc2-42de-900c-d65747b21168": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00373315s
STEP: Saw pod success 01/30/23 11:54:42.934
Jan 30 11:54:42.934: INFO: Pod "pod-secrets-51076667-0fc2-42de-900c-d65747b21168" satisfied condition "Succeeded or Failed"
Jan 30 11:54:42.936: INFO: Trying to get logs from node pubt2-nks-for-dev1.dg.163.org pod pod-secrets-51076667-0fc2-42de-900c-d65747b21168 container secret-env-test: <nil>
STEP: delete the pod 01/30/23 11:54:42.941
Jan 30 11:54:42.945: INFO: Waiting for pod pod-secrets-51076667-0fc2-42de-900c-d65747b21168 to disappear
Jan 30 11:54:42.947: INFO: Pod pod-secrets-51076667-0fc2-42de-900c-d65747b21168 no longer exists
[AfterEach] [sig-node] Secrets
  test/e2e/framework/node/init/init.go:32
Jan 30 11:54:42.947: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-9741" for this suite. 01/30/23 11:54:42.949
------------------------------
• [4.037 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 11:54:38.914
    Jan 30 11:54:38.914: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename secrets 01/30/23 11:54:38.915
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:54:38.92
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:54:38.922
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in env vars [NodeConformance] [Conformance]
      test/e2e/common/node/secrets.go:46
    STEP: Creating secret with name secret-test-0768a7cf-5073-46e9-90f2-b4d72372368d 01/30/23 11:54:38.924
    STEP: Creating a pod to test consume secrets 01/30/23 11:54:38.927
    Jan 30 11:54:38.930: INFO: Waiting up to 5m0s for pod "pod-secrets-51076667-0fc2-42de-900c-d65747b21168" in namespace "secrets-9741" to be "Succeeded or Failed"
    Jan 30 11:54:38.932: INFO: Pod "pod-secrets-51076667-0fc2-42de-900c-d65747b21168": Phase="Pending", Reason="", readiness=false. Elapsed: 1.481463ms
    Jan 30 11:54:40.934: INFO: Pod "pod-secrets-51076667-0fc2-42de-900c-d65747b21168": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003663103s
    Jan 30 11:54:42.934: INFO: Pod "pod-secrets-51076667-0fc2-42de-900c-d65747b21168": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00373315s
    STEP: Saw pod success 01/30/23 11:54:42.934
    Jan 30 11:54:42.934: INFO: Pod "pod-secrets-51076667-0fc2-42de-900c-d65747b21168" satisfied condition "Succeeded or Failed"
    Jan 30 11:54:42.936: INFO: Trying to get logs from node pubt2-nks-for-dev1.dg.163.org pod pod-secrets-51076667-0fc2-42de-900c-d65747b21168 container secret-env-test: <nil>
    STEP: delete the pod 01/30/23 11:54:42.941
    Jan 30 11:54:42.945: INFO: Waiting for pod pod-secrets-51076667-0fc2-42de-900c-d65747b21168 to disappear
    Jan 30 11:54:42.947: INFO: Pod pod-secrets-51076667-0fc2-42de-900c-d65747b21168 no longer exists
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/node/init/init.go:32
    Jan 30 11:54:42.947: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-9741" for this suite. 01/30/23 11:54:42.949
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:448
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 11:54:42.952
Jan 30 11:54:42.952: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename resourcequota 01/30/23 11:54:42.952
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:54:42.959
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:54:42.961
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:448
STEP: Counting existing ResourceQuota 01/30/23 11:54:42.963
STEP: Creating a ResourceQuota 01/30/23 11:54:47.965
STEP: Ensuring resource quota status is calculated 01/30/23 11:54:47.968
STEP: Creating a ReplicaSet 01/30/23 11:54:49.972
STEP: Ensuring resource quota status captures replicaset creation 01/30/23 11:54:49.992
STEP: Deleting a ReplicaSet 01/30/23 11:54:51.998
STEP: Ensuring resource quota status released usage 01/30/23 11:54:52
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Jan 30 11:54:54.003: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-3497" for this suite. 01/30/23 11:54:54.005
------------------------------
• [SLOW TEST] [11.056 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:448

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 11:54:42.952
    Jan 30 11:54:42.952: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename resourcequota 01/30/23 11:54:42.952
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:54:42.959
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:54:42.961
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
      test/e2e/apimachinery/resource_quota.go:448
    STEP: Counting existing ResourceQuota 01/30/23 11:54:42.963
    STEP: Creating a ResourceQuota 01/30/23 11:54:47.965
    STEP: Ensuring resource quota status is calculated 01/30/23 11:54:47.968
    STEP: Creating a ReplicaSet 01/30/23 11:54:49.972
    STEP: Ensuring resource quota status captures replicaset creation 01/30/23 11:54:49.992
    STEP: Deleting a ReplicaSet 01/30/23 11:54:51.998
    STEP: Ensuring resource quota status released usage 01/30/23 11:54:52
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Jan 30 11:54:54.003: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-3497" for this suite. 01/30/23 11:54:54.005
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:618
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 11:54:54.008
Jan 30 11:54:54.008: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename pods 01/30/23 11:54:54.009
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:54:54.016
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:54:54.018
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:618
Jan 30 11:54:54.020: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: creating the pod 01/30/23 11:54:54.02
STEP: submitting the pod to kubernetes 01/30/23 11:54:54.02
Jan 30 11:54:54.025: INFO: Waiting up to 5m0s for pod "pod-logs-websocket-3be4fdda-3dbf-44e2-b732-5cbf1a6799da" in namespace "pods-8587" to be "running and ready"
Jan 30 11:54:54.026: INFO: Pod "pod-logs-websocket-3be4fdda-3dbf-44e2-b732-5cbf1a6799da": Phase="Pending", Reason="", readiness=false. Elapsed: 1.63074ms
Jan 30 11:54:54.026: INFO: The phase of Pod pod-logs-websocket-3be4fdda-3dbf-44e2-b732-5cbf1a6799da is Pending, waiting for it to be Running (with Ready = true)
Jan 30 11:54:56.028: INFO: Pod "pod-logs-websocket-3be4fdda-3dbf-44e2-b732-5cbf1a6799da": Phase="Running", Reason="", readiness=true. Elapsed: 2.003706891s
Jan 30 11:54:56.028: INFO: The phase of Pod pod-logs-websocket-3be4fdda-3dbf-44e2-b732-5cbf1a6799da is Running (Ready = true)
Jan 30 11:54:56.028: INFO: Pod "pod-logs-websocket-3be4fdda-3dbf-44e2-b732-5cbf1a6799da" satisfied condition "running and ready"
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Jan 30 11:54:56.038: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-8587" for this suite. 01/30/23 11:54:56.04
------------------------------
• [2.035 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:618

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 11:54:54.008
    Jan 30 11:54:54.008: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename pods 01/30/23 11:54:54.009
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:54:54.016
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:54:54.018
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:618
    Jan 30 11:54:54.020: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: creating the pod 01/30/23 11:54:54.02
    STEP: submitting the pod to kubernetes 01/30/23 11:54:54.02
    Jan 30 11:54:54.025: INFO: Waiting up to 5m0s for pod "pod-logs-websocket-3be4fdda-3dbf-44e2-b732-5cbf1a6799da" in namespace "pods-8587" to be "running and ready"
    Jan 30 11:54:54.026: INFO: Pod "pod-logs-websocket-3be4fdda-3dbf-44e2-b732-5cbf1a6799da": Phase="Pending", Reason="", readiness=false. Elapsed: 1.63074ms
    Jan 30 11:54:54.026: INFO: The phase of Pod pod-logs-websocket-3be4fdda-3dbf-44e2-b732-5cbf1a6799da is Pending, waiting for it to be Running (with Ready = true)
    Jan 30 11:54:56.028: INFO: Pod "pod-logs-websocket-3be4fdda-3dbf-44e2-b732-5cbf1a6799da": Phase="Running", Reason="", readiness=true. Elapsed: 2.003706891s
    Jan 30 11:54:56.028: INFO: The phase of Pod pod-logs-websocket-3be4fdda-3dbf-44e2-b732-5cbf1a6799da is Running (Ready = true)
    Jan 30 11:54:56.028: INFO: Pod "pod-logs-websocket-3be4fdda-3dbf-44e2-b732-5cbf1a6799da" satisfied condition "running and ready"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Jan 30 11:54:56.038: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-8587" for this suite. 01/30/23 11:54:56.04
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] ConfigMap
  should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:169
[BeforeEach] [sig-node] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 11:54:56.043
Jan 30 11:54:56.043: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename configmap 01/30/23 11:54:56.044
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:54:56.05
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:54:56.051
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:169
STEP: creating a ConfigMap 01/30/23 11:54:56.054
STEP: fetching the ConfigMap 01/30/23 11:54:56.056
STEP: patching the ConfigMap 01/30/23 11:54:56.057
STEP: listing all ConfigMaps in all namespaces with a label selector 01/30/23 11:54:56.061
STEP: deleting the ConfigMap by collection with a label selector 01/30/23 11:54:56.063
STEP: listing all ConfigMaps in test namespace 01/30/23 11:54:56.065
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jan 30 11:54:56.067: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-3779" for this suite. 01/30/23 11:54:56.069
------------------------------
• [0.028 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:169

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 11:54:56.043
    Jan 30 11:54:56.043: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename configmap 01/30/23 11:54:56.044
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:54:56.05
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:54:56.051
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should run through a ConfigMap lifecycle [Conformance]
      test/e2e/common/node/configmap.go:169
    STEP: creating a ConfigMap 01/30/23 11:54:56.054
    STEP: fetching the ConfigMap 01/30/23 11:54:56.056
    STEP: patching the ConfigMap 01/30/23 11:54:56.057
    STEP: listing all ConfigMaps in all namespaces with a label selector 01/30/23 11:54:56.061
    STEP: deleting the ConfigMap by collection with a label selector 01/30/23 11:54:56.063
    STEP: listing all ConfigMaps in test namespace 01/30/23 11:54:56.065
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jan 30 11:54:56.067: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-3779" for this suite. 01/30/23 11:54:56.069
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-auth] ServiceAccounts
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:531
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 11:54:56.072
Jan 30 11:54:56.072: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename svcaccounts 01/30/23 11:54:56.072
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:54:56.078
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:54:56.08
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:531
Jan 30 11:54:56.088: INFO: created pod
Jan 30 11:54:56.088: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-6649" to be "Succeeded or Failed"
Jan 30 11:54:56.089: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 1.607993ms
Jan 30 11:54:58.091: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003719857s
Jan 30 11:55:00.093: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005213679s
STEP: Saw pod success 01/30/23 11:55:00.093
Jan 30 11:55:00.093: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
Jan 30 11:55:30.094: INFO: polling logs
Jan 30 11:55:30.099: INFO: Pod logs: 
I0130 11:54:57.161033       1 log.go:198] OK: Got token
I0130 11:54:57.161069       1 log.go:198] validating with in-cluster discovery
I0130 11:54:57.161304       1 log.go:198] OK: got issuer https://kubernetes.default.svc.cluster.local
I0130 11:54:57.161326       1 log.go:198] Full, not-validated claims: 
openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-6649:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1675080296, NotBefore:1675079696, IssuedAt:1675079696, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-6649", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"ac1ccfdd-1132-4eb8-a823-8b35735dae71"}}}
I0130 11:54:57.170242       1 log.go:198] OK: Constructed OIDC provider for issuer https://kubernetes.default.svc.cluster.local
I0130 11:54:57.176760       1 log.go:198] OK: Validated signature on JWT
I0130 11:54:57.176831       1 log.go:198] OK: Got valid claims from token!
I0130 11:54:57.176852       1 log.go:198] Full, validated claims: 
&openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-6649:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1675080296, NotBefore:1675079696, IssuedAt:1675079696, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-6649", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"ac1ccfdd-1132-4eb8-a823-8b35735dae71"}}}

Jan 30 11:55:30.099: INFO: completed pod
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Jan 30 11:55:30.102: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-6649" for this suite. 01/30/23 11:55:30.104
------------------------------
• [SLOW TEST] [34.035 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:531

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 11:54:56.072
    Jan 30 11:54:56.072: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename svcaccounts 01/30/23 11:54:56.072
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:54:56.078
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:54:56.08
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
      test/e2e/auth/service_accounts.go:531
    Jan 30 11:54:56.088: INFO: created pod
    Jan 30 11:54:56.088: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-6649" to be "Succeeded or Failed"
    Jan 30 11:54:56.089: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 1.607993ms
    Jan 30 11:54:58.091: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003719857s
    Jan 30 11:55:00.093: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005213679s
    STEP: Saw pod success 01/30/23 11:55:00.093
    Jan 30 11:55:00.093: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
    Jan 30 11:55:30.094: INFO: polling logs
    Jan 30 11:55:30.099: INFO: Pod logs: 
    I0130 11:54:57.161033       1 log.go:198] OK: Got token
    I0130 11:54:57.161069       1 log.go:198] validating with in-cluster discovery
    I0130 11:54:57.161304       1 log.go:198] OK: got issuer https://kubernetes.default.svc.cluster.local
    I0130 11:54:57.161326       1 log.go:198] Full, not-validated claims: 
    openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-6649:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1675080296, NotBefore:1675079696, IssuedAt:1675079696, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-6649", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"ac1ccfdd-1132-4eb8-a823-8b35735dae71"}}}
    I0130 11:54:57.170242       1 log.go:198] OK: Constructed OIDC provider for issuer https://kubernetes.default.svc.cluster.local
    I0130 11:54:57.176760       1 log.go:198] OK: Validated signature on JWT
    I0130 11:54:57.176831       1 log.go:198] OK: Got valid claims from token!
    I0130 11:54:57.176852       1 log.go:198] Full, validated claims: 
    &openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-6649:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1675080296, NotBefore:1675079696, IssuedAt:1675079696, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-6649", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"ac1ccfdd-1132-4eb8-a823-8b35735dae71"}}}

    Jan 30 11:55:30.099: INFO: completed pod
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Jan 30 11:55:30.102: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-6649" for this suite. 01/30/23 11:55:30.104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes
  should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 11:55:30.107
Jan 30 11:55:30.107: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename emptydir-wrapper 01/30/23 11:55:30.108
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:55:30.115
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:55:30.117
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67
Jan 30 11:55:30.126: INFO: Waiting up to 5m0s for pod "pod-secrets-aa789a0d-dd92-4dcf-a4eb-28301a8cb3cb" in namespace "emptydir-wrapper-2153" to be "running and ready"
Jan 30 11:55:30.128: INFO: Pod "pod-secrets-aa789a0d-dd92-4dcf-a4eb-28301a8cb3cb": Phase="Pending", Reason="", readiness=false. Elapsed: 1.642191ms
Jan 30 11:55:30.128: INFO: The phase of Pod pod-secrets-aa789a0d-dd92-4dcf-a4eb-28301a8cb3cb is Pending, waiting for it to be Running (with Ready = true)
Jan 30 11:55:32.131: INFO: Pod "pod-secrets-aa789a0d-dd92-4dcf-a4eb-28301a8cb3cb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004119311s
Jan 30 11:55:32.131: INFO: The phase of Pod pod-secrets-aa789a0d-dd92-4dcf-a4eb-28301a8cb3cb is Pending, waiting for it to be Running (with Ready = true)
Jan 30 11:55:34.131: INFO: Pod "pod-secrets-aa789a0d-dd92-4dcf-a4eb-28301a8cb3cb": Phase="Running", Reason="", readiness=true. Elapsed: 4.004841914s
Jan 30 11:55:34.131: INFO: The phase of Pod pod-secrets-aa789a0d-dd92-4dcf-a4eb-28301a8cb3cb is Running (Ready = true)
Jan 30 11:55:34.131: INFO: Pod "pod-secrets-aa789a0d-dd92-4dcf-a4eb-28301a8cb3cb" satisfied condition "running and ready"
STEP: Cleaning up the secret 01/30/23 11:55:34.133
STEP: Cleaning up the configmap 01/30/23 11:55:34.135
STEP: Cleaning up the pod 01/30/23 11:55:34.138
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/node/init/init.go:32
Jan 30 11:55:34.142: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-wrapper-2153" for this suite. 01/30/23 11:55:34.145
------------------------------
• [4.039 seconds]
[sig-storage] EmptyDir wrapper volumes
test/e2e/storage/utils/framework.go:23
  should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 11:55:30.107
    Jan 30 11:55:30.107: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename emptydir-wrapper 01/30/23 11:55:30.108
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:55:30.115
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:55:30.117
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should not conflict [Conformance]
      test/e2e/storage/empty_dir_wrapper.go:67
    Jan 30 11:55:30.126: INFO: Waiting up to 5m0s for pod "pod-secrets-aa789a0d-dd92-4dcf-a4eb-28301a8cb3cb" in namespace "emptydir-wrapper-2153" to be "running and ready"
    Jan 30 11:55:30.128: INFO: Pod "pod-secrets-aa789a0d-dd92-4dcf-a4eb-28301a8cb3cb": Phase="Pending", Reason="", readiness=false. Elapsed: 1.642191ms
    Jan 30 11:55:30.128: INFO: The phase of Pod pod-secrets-aa789a0d-dd92-4dcf-a4eb-28301a8cb3cb is Pending, waiting for it to be Running (with Ready = true)
    Jan 30 11:55:32.131: INFO: Pod "pod-secrets-aa789a0d-dd92-4dcf-a4eb-28301a8cb3cb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004119311s
    Jan 30 11:55:32.131: INFO: The phase of Pod pod-secrets-aa789a0d-dd92-4dcf-a4eb-28301a8cb3cb is Pending, waiting for it to be Running (with Ready = true)
    Jan 30 11:55:34.131: INFO: Pod "pod-secrets-aa789a0d-dd92-4dcf-a4eb-28301a8cb3cb": Phase="Running", Reason="", readiness=true. Elapsed: 4.004841914s
    Jan 30 11:55:34.131: INFO: The phase of Pod pod-secrets-aa789a0d-dd92-4dcf-a4eb-28301a8cb3cb is Running (Ready = true)
    Jan 30 11:55:34.131: INFO: Pod "pod-secrets-aa789a0d-dd92-4dcf-a4eb-28301a8cb3cb" satisfied condition "running and ready"
    STEP: Cleaning up the secret 01/30/23 11:55:34.133
    STEP: Cleaning up the configmap 01/30/23 11:55:34.135
    STEP: Cleaning up the pod 01/30/23 11:55:34.138
    [AfterEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/node/init/init.go:32
    Jan 30 11:55:34.142: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-wrapper-2153" for this suite. 01/30/23 11:55:34.145
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:251
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 11:55:34.147
Jan 30 11:55:34.147: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename namespaces 01/30/23 11:55:34.148
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:55:34.154
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:55:34.156
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:251
STEP: Creating a test namespace 01/30/23 11:55:34.158
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:55:34.163
STEP: Creating a service in the namespace 01/30/23 11:55:34.165
STEP: Deleting the namespace 01/30/23 11:55:34.169
STEP: Waiting for the namespace to be removed. 01/30/23 11:55:34.171
STEP: Recreating the namespace 01/30/23 11:55:40.173
STEP: Verifying there is no service in the namespace 01/30/23 11:55:40.18
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Jan 30 11:55:40.182: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-2744" for this suite. 01/30/23 11:55:40.184
STEP: Destroying namespace "nsdeletetest-7506" for this suite. 01/30/23 11:55:40.186
Jan 30 11:55:40.187: INFO: Namespace nsdeletetest-7506 was already deleted
STEP: Destroying namespace "nsdeletetest-8132" for this suite. 01/30/23 11:55:40.187
------------------------------
• [SLOW TEST] [6.042 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:251

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 11:55:34.147
    Jan 30 11:55:34.147: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename namespaces 01/30/23 11:55:34.148
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:55:34.154
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:55:34.156
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should ensure that all services are removed when a namespace is deleted [Conformance]
      test/e2e/apimachinery/namespace.go:251
    STEP: Creating a test namespace 01/30/23 11:55:34.158
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:55:34.163
    STEP: Creating a service in the namespace 01/30/23 11:55:34.165
    STEP: Deleting the namespace 01/30/23 11:55:34.169
    STEP: Waiting for the namespace to be removed. 01/30/23 11:55:34.171
    STEP: Recreating the namespace 01/30/23 11:55:40.173
    STEP: Verifying there is no service in the namespace 01/30/23 11:55:40.18
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan 30 11:55:40.182: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-2744" for this suite. 01/30/23 11:55:40.184
    STEP: Destroying namespace "nsdeletetest-7506" for this suite. 01/30/23 11:55:40.186
    Jan 30 11:55:40.187: INFO: Namespace nsdeletetest-7506 was already deleted
    STEP: Destroying namespace "nsdeletetest-8132" for this suite. 01/30/23 11:55:40.187
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:243
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 11:55:40.19
Jan 30 11:55:40.190: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename namespaces 01/30/23 11:55:40.191
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:55:40.196
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:55:40.198
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:243
STEP: Creating a test namespace 01/30/23 11:55:40.202
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:55:40.217
STEP: Creating a pod in the namespace 01/30/23 11:55:40.219
STEP: Waiting for the pod to have running status 01/30/23 11:55:40.224
Jan 30 11:55:40.224: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "nsdeletetest-1913" to be "running"
Jan 30 11:55:40.225: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 1.583004ms
Jan 30 11:55:42.227: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.003579369s
Jan 30 11:55:42.227: INFO: Pod "test-pod" satisfied condition "running"
STEP: Deleting the namespace 01/30/23 11:55:42.227
STEP: Waiting for the namespace to be removed. 01/30/23 11:55:42.23
STEP: Recreating the namespace 01/30/23 11:55:53.232
STEP: Verifying there are no pods in the namespace 01/30/23 11:55:53.238
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Jan 30 11:55:53.240: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-4770" for this suite. 01/30/23 11:55:53.242
STEP: Destroying namespace "nsdeletetest-1913" for this suite. 01/30/23 11:55:53.244
Jan 30 11:55:53.246: INFO: Namespace nsdeletetest-1913 was already deleted
STEP: Destroying namespace "nsdeletetest-9025" for this suite. 01/30/23 11:55:53.246
------------------------------
• [SLOW TEST] [13.058 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:243

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 11:55:40.19
    Jan 30 11:55:40.190: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename namespaces 01/30/23 11:55:40.191
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:55:40.196
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:55:40.198
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should ensure that all pods are removed when a namespace is deleted [Conformance]
      test/e2e/apimachinery/namespace.go:243
    STEP: Creating a test namespace 01/30/23 11:55:40.202
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:55:40.217
    STEP: Creating a pod in the namespace 01/30/23 11:55:40.219
    STEP: Waiting for the pod to have running status 01/30/23 11:55:40.224
    Jan 30 11:55:40.224: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "nsdeletetest-1913" to be "running"
    Jan 30 11:55:40.225: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 1.583004ms
    Jan 30 11:55:42.227: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.003579369s
    Jan 30 11:55:42.227: INFO: Pod "test-pod" satisfied condition "running"
    STEP: Deleting the namespace 01/30/23 11:55:42.227
    STEP: Waiting for the namespace to be removed. 01/30/23 11:55:42.23
    STEP: Recreating the namespace 01/30/23 11:55:53.232
    STEP: Verifying there are no pods in the namespace 01/30/23 11:55:53.238
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan 30 11:55:53.240: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-4770" for this suite. 01/30/23 11:55:53.242
    STEP: Destroying namespace "nsdeletetest-1913" for this suite. 01/30/23 11:55:53.244
    Jan 30 11:55:53.246: INFO: Namespace nsdeletetest-1913 was already deleted
    STEP: Destroying namespace "nsdeletetest-9025" for this suite. 01/30/23 11:55:53.246
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-apps] Job
  should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:636
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 11:55:53.248
Jan 30 11:55:53.248: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename job 01/30/23 11:55:53.249
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:55:53.254
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:55:53.256
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:636
STEP: Creating a job 01/30/23 11:55:53.258
STEP: Ensure pods equal to parallelism count is attached to the job 01/30/23 11:55:53.26
STEP: patching /status 01/30/23 11:55:55.264
STEP: updating /status 01/30/23 11:55:55.268
STEP: get /status 01/30/23 11:55:55.292
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Jan 30 11:55:55.294: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-8157" for this suite. 01/30/23 11:55:55.296
------------------------------
• [2.050 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:636

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 11:55:53.248
    Jan 30 11:55:53.248: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename job 01/30/23 11:55:53.249
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:55:53.254
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:55:53.256
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply changes to a job status [Conformance]
      test/e2e/apps/job.go:636
    STEP: Creating a job 01/30/23 11:55:53.258
    STEP: Ensure pods equal to parallelism count is attached to the job 01/30/23 11:55:53.26
    STEP: patching /status 01/30/23 11:55:55.264
    STEP: updating /status 01/30/23 11:55:55.268
    STEP: get /status 01/30/23 11:55:55.292
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Jan 30 11:55:55.294: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-8157" for this suite. 01/30/23 11:55:55.296
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:443
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 11:55:55.299
Jan 30 11:55:55.299: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename sched-pred 01/30/23 11:55:55.299
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:55:55.305
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:55:55.307
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:97
Jan 30 11:55:55.309: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jan 30 11:55:55.312: INFO: Waiting for terminating namespaces to be deleted...
Jan 30 11:55:55.314: INFO: 
Logging pods the apiserver thinks is on node pubt2-nks-for-dev1.dg.163.org before test
Jan 30 11:55:55.319: INFO: suspend-false-to-true-79p7s from job-8157 started at 2023-01-30 11:55:53 +0000 UTC (1 container statuses recorded)
Jan 30 11:55:55.319: INFO: 	Container c ready: true, restart count 0
Jan 30 11:55:55.319: INFO: suspend-false-to-true-nhgmx from job-8157 started at 2023-01-30 11:55:53 +0000 UTC (1 container statuses recorded)
Jan 30 11:55:55.319: INFO: 	Container c ready: true, restart count 0
Jan 30 11:55:55.319: INFO: calico-node-5xgvr from kube-system started at 2023-01-30 11:14:53 +0000 UTC (1 container statuses recorded)
Jan 30 11:55:55.319: INFO: 	Container calico-node ready: true, restart count 0
Jan 30 11:55:55.319: INFO: cleanlog-w9znh from kube-system started at 2023-01-30 11:02:22 +0000 UTC (1 container statuses recorded)
Jan 30 11:55:55.319: INFO: 	Container cleanlog ready: true, restart count 0
Jan 30 11:55:55.319: INFO: coredns-5bf7dfc67-4chrr from kube-system started at 2023-01-30 11:33:54 +0000 UTC (1 container statuses recorded)
Jan 30 11:55:55.319: INFO: 	Container coredns ready: true, restart count 0
Jan 30 11:55:55.319: INFO: kube-proxy-fvt68 from kube-system started at 2023-01-30 11:02:22 +0000 UTC (1 container statuses recorded)
Jan 30 11:55:55.319: INFO: 	Container kube-proxy ready: true, restart count 0
Jan 30 11:55:55.319: INFO: node-local-dns-8jh9v from kube-system started at 2023-01-30 11:02:22 +0000 UTC (1 container statuses recorded)
Jan 30 11:55:55.319: INFO: 	Container node-cache ready: true, restart count 5
Jan 30 11:55:55.319: INFO: sonobuoy-systemd-logs-daemon-set-6d9e6fe3a8a74fcc-st9q5 from sonobuoy started at 2023-01-30 11:17:19 +0000 UTC (2 container statuses recorded)
Jan 30 11:55:55.319: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 30 11:55:55.319: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 30 11:55:55.319: INFO: 
Logging pods the apiserver thinks is on node pubt2-nks-for-dev3.dg.163.org before test
Jan 30 11:55:55.324: INFO: calico-kube-controllers-568668f974-5gtkq from kube-system started at 2023-01-30 09:59:07 +0000 UTC (1 container statuses recorded)
Jan 30 11:55:55.324: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Jan 30 11:55:55.324: INFO: calico-node-h7jwg from kube-system started at 2023-01-30 11:14:53 +0000 UTC (1 container statuses recorded)
Jan 30 11:55:55.324: INFO: 	Container calico-node ready: true, restart count 0
Jan 30 11:55:55.324: INFO: cleanlog-crksz from kube-system started at 2023-01-30 09:56:18 +0000 UTC (1 container statuses recorded)
Jan 30 11:55:55.324: INFO: 	Container cleanlog ready: true, restart count 0
Jan 30 11:55:55.324: INFO: coredns-5bf7dfc67-2gl8f from kube-system started at 2023-01-30 10:08:32 +0000 UTC (1 container statuses recorded)
Jan 30 11:55:55.324: INFO: 	Container coredns ready: true, restart count 0
Jan 30 11:55:55.324: INFO: etcd-main-pubt2-nks-for-dev3.dg.163.org from kube-system started at 2023-01-30 09:56:17 +0000 UTC (1 container statuses recorded)
Jan 30 11:55:55.324: INFO: 	Container etcd ready: true, restart count 1
Jan 30 11:55:55.324: INFO: kube-apiserver-pubt2-nks-for-dev3.dg.163.org from kube-system started at 2023-01-30 09:56:18 +0000 UTC (1 container statuses recorded)
Jan 30 11:55:55.324: INFO: 	Container kube-apiserver ready: true, restart count 1
Jan 30 11:55:55.324: INFO: kube-controller-manager-pubt2-nks-for-dev3.dg.163.org from kube-system started at 2023-01-30 09:56:18 +0000 UTC (1 container statuses recorded)
Jan 30 11:55:55.324: INFO: 	Container kube-controller-manager ready: true, restart count 1
Jan 30 11:55:55.324: INFO: kube-proxy-br2mf from kube-system started at 2023-01-30 09:58:40 +0000 UTC (1 container statuses recorded)
Jan 30 11:55:55.324: INFO: 	Container kube-proxy ready: true, restart count 0
Jan 30 11:55:55.324: INFO: kube-scheduler-pubt2-nks-for-dev3.dg.163.org from kube-system started at 2023-01-30 09:56:17 +0000 UTC (1 container statuses recorded)
Jan 30 11:55:55.324: INFO: 	Container kube-scheduler ready: true, restart count 1
Jan 30 11:55:55.324: INFO: node-local-dns-txwnd from kube-system started at 2023-01-30 10:09:12 +0000 UTC (1 container statuses recorded)
Jan 30 11:55:55.324: INFO: 	Container node-cache ready: true, restart count 0
Jan 30 11:55:55.324: INFO: sonobuoy from sonobuoy started at 2023-01-30 11:17:15 +0000 UTC (1 container statuses recorded)
Jan 30 11:55:55.324: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jan 30 11:55:55.324: INFO: sonobuoy-e2e-job-a0572830995342a7 from sonobuoy started at 2023-01-30 11:17:19 +0000 UTC (2 container statuses recorded)
Jan 30 11:55:55.324: INFO: 	Container e2e ready: true, restart count 0
Jan 30 11:55:55.324: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 30 11:55:55.324: INFO: sonobuoy-systemd-logs-daemon-set-6d9e6fe3a8a74fcc-nkw5p from sonobuoy started at 2023-01-30 11:17:19 +0000 UTC (2 container statuses recorded)
Jan 30 11:55:55.324: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 30 11:55:55.324: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:443
STEP: Trying to schedule Pod with nonempty NodeSelector. 01/30/23 11:55:55.324
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.173f14299b7c329a], Reason = [FailedScheduling], Message = [0/2 nodes are available: 2 node(s) didn't match Pod's node affinity/selector. preemption: 0/2 nodes are available: 2 Preemption is not helpful for scheduling..] 01/30/23 11:55:55.342
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/node/init/init.go:32
Jan 30 11:55:56.341: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:88
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-pred-6682" for this suite. 01/30/23 11:55:56.344
------------------------------
• [1.047 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:443

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 11:55:55.299
    Jan 30 11:55:55.299: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename sched-pred 01/30/23 11:55:55.299
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:55:55.305
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:55:55.307
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:97
    Jan 30 11:55:55.309: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Jan 30 11:55:55.312: INFO: Waiting for terminating namespaces to be deleted...
    Jan 30 11:55:55.314: INFO: 
    Logging pods the apiserver thinks is on node pubt2-nks-for-dev1.dg.163.org before test
    Jan 30 11:55:55.319: INFO: suspend-false-to-true-79p7s from job-8157 started at 2023-01-30 11:55:53 +0000 UTC (1 container statuses recorded)
    Jan 30 11:55:55.319: INFO: 	Container c ready: true, restart count 0
    Jan 30 11:55:55.319: INFO: suspend-false-to-true-nhgmx from job-8157 started at 2023-01-30 11:55:53 +0000 UTC (1 container statuses recorded)
    Jan 30 11:55:55.319: INFO: 	Container c ready: true, restart count 0
    Jan 30 11:55:55.319: INFO: calico-node-5xgvr from kube-system started at 2023-01-30 11:14:53 +0000 UTC (1 container statuses recorded)
    Jan 30 11:55:55.319: INFO: 	Container calico-node ready: true, restart count 0
    Jan 30 11:55:55.319: INFO: cleanlog-w9znh from kube-system started at 2023-01-30 11:02:22 +0000 UTC (1 container statuses recorded)
    Jan 30 11:55:55.319: INFO: 	Container cleanlog ready: true, restart count 0
    Jan 30 11:55:55.319: INFO: coredns-5bf7dfc67-4chrr from kube-system started at 2023-01-30 11:33:54 +0000 UTC (1 container statuses recorded)
    Jan 30 11:55:55.319: INFO: 	Container coredns ready: true, restart count 0
    Jan 30 11:55:55.319: INFO: kube-proxy-fvt68 from kube-system started at 2023-01-30 11:02:22 +0000 UTC (1 container statuses recorded)
    Jan 30 11:55:55.319: INFO: 	Container kube-proxy ready: true, restart count 0
    Jan 30 11:55:55.319: INFO: node-local-dns-8jh9v from kube-system started at 2023-01-30 11:02:22 +0000 UTC (1 container statuses recorded)
    Jan 30 11:55:55.319: INFO: 	Container node-cache ready: true, restart count 5
    Jan 30 11:55:55.319: INFO: sonobuoy-systemd-logs-daemon-set-6d9e6fe3a8a74fcc-st9q5 from sonobuoy started at 2023-01-30 11:17:19 +0000 UTC (2 container statuses recorded)
    Jan 30 11:55:55.319: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 30 11:55:55.319: INFO: 	Container systemd-logs ready: true, restart count 0
    Jan 30 11:55:55.319: INFO: 
    Logging pods the apiserver thinks is on node pubt2-nks-for-dev3.dg.163.org before test
    Jan 30 11:55:55.324: INFO: calico-kube-controllers-568668f974-5gtkq from kube-system started at 2023-01-30 09:59:07 +0000 UTC (1 container statuses recorded)
    Jan 30 11:55:55.324: INFO: 	Container calico-kube-controllers ready: true, restart count 0
    Jan 30 11:55:55.324: INFO: calico-node-h7jwg from kube-system started at 2023-01-30 11:14:53 +0000 UTC (1 container statuses recorded)
    Jan 30 11:55:55.324: INFO: 	Container calico-node ready: true, restart count 0
    Jan 30 11:55:55.324: INFO: cleanlog-crksz from kube-system started at 2023-01-30 09:56:18 +0000 UTC (1 container statuses recorded)
    Jan 30 11:55:55.324: INFO: 	Container cleanlog ready: true, restart count 0
    Jan 30 11:55:55.324: INFO: coredns-5bf7dfc67-2gl8f from kube-system started at 2023-01-30 10:08:32 +0000 UTC (1 container statuses recorded)
    Jan 30 11:55:55.324: INFO: 	Container coredns ready: true, restart count 0
    Jan 30 11:55:55.324: INFO: etcd-main-pubt2-nks-for-dev3.dg.163.org from kube-system started at 2023-01-30 09:56:17 +0000 UTC (1 container statuses recorded)
    Jan 30 11:55:55.324: INFO: 	Container etcd ready: true, restart count 1
    Jan 30 11:55:55.324: INFO: kube-apiserver-pubt2-nks-for-dev3.dg.163.org from kube-system started at 2023-01-30 09:56:18 +0000 UTC (1 container statuses recorded)
    Jan 30 11:55:55.324: INFO: 	Container kube-apiserver ready: true, restart count 1
    Jan 30 11:55:55.324: INFO: kube-controller-manager-pubt2-nks-for-dev3.dg.163.org from kube-system started at 2023-01-30 09:56:18 +0000 UTC (1 container statuses recorded)
    Jan 30 11:55:55.324: INFO: 	Container kube-controller-manager ready: true, restart count 1
    Jan 30 11:55:55.324: INFO: kube-proxy-br2mf from kube-system started at 2023-01-30 09:58:40 +0000 UTC (1 container statuses recorded)
    Jan 30 11:55:55.324: INFO: 	Container kube-proxy ready: true, restart count 0
    Jan 30 11:55:55.324: INFO: kube-scheduler-pubt2-nks-for-dev3.dg.163.org from kube-system started at 2023-01-30 09:56:17 +0000 UTC (1 container statuses recorded)
    Jan 30 11:55:55.324: INFO: 	Container kube-scheduler ready: true, restart count 1
    Jan 30 11:55:55.324: INFO: node-local-dns-txwnd from kube-system started at 2023-01-30 10:09:12 +0000 UTC (1 container statuses recorded)
    Jan 30 11:55:55.324: INFO: 	Container node-cache ready: true, restart count 0
    Jan 30 11:55:55.324: INFO: sonobuoy from sonobuoy started at 2023-01-30 11:17:15 +0000 UTC (1 container statuses recorded)
    Jan 30 11:55:55.324: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Jan 30 11:55:55.324: INFO: sonobuoy-e2e-job-a0572830995342a7 from sonobuoy started at 2023-01-30 11:17:19 +0000 UTC (2 container statuses recorded)
    Jan 30 11:55:55.324: INFO: 	Container e2e ready: true, restart count 0
    Jan 30 11:55:55.324: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 30 11:55:55.324: INFO: sonobuoy-systemd-logs-daemon-set-6d9e6fe3a8a74fcc-nkw5p from sonobuoy started at 2023-01-30 11:17:19 +0000 UTC (2 container statuses recorded)
    Jan 30 11:55:55.324: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 30 11:55:55.324: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates that NodeSelector is respected if not matching  [Conformance]
      test/e2e/scheduling/predicates.go:443
    STEP: Trying to schedule Pod with nonempty NodeSelector. 01/30/23 11:55:55.324
    STEP: Considering event: 
    Type = [Warning], Name = [restricted-pod.173f14299b7c329a], Reason = [FailedScheduling], Message = [0/2 nodes are available: 2 node(s) didn't match Pod's node affinity/selector. preemption: 0/2 nodes are available: 2 Preemption is not helpful for scheduling..] 01/30/23 11:55:55.342
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan 30 11:55:56.341: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:88
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-pred-6682" for this suite. 01/30/23 11:55:56.344
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:193
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 11:55:56.346
Jan 30 11:55:56.346: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename downward-api 01/30/23 11:55:56.347
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:55:56.353
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:55:56.355
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:193
STEP: Creating a pod to test downward API volume plugin 01/30/23 11:55:56.357
Jan 30 11:55:56.361: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ba9a5769-371f-4567-9209-c33fa8e0d46c" in namespace "downward-api-6580" to be "Succeeded or Failed"
Jan 30 11:55:56.363: INFO: Pod "downwardapi-volume-ba9a5769-371f-4567-9209-c33fa8e0d46c": Phase="Pending", Reason="", readiness=false. Elapsed: 1.586697ms
Jan 30 11:55:58.365: INFO: Pod "downwardapi-volume-ba9a5769-371f-4567-9209-c33fa8e0d46c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004235852s
Jan 30 11:56:00.367: INFO: Pod "downwardapi-volume-ba9a5769-371f-4567-9209-c33fa8e0d46c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005802364s
STEP: Saw pod success 01/30/23 11:56:00.367
Jan 30 11:56:00.367: INFO: Pod "downwardapi-volume-ba9a5769-371f-4567-9209-c33fa8e0d46c" satisfied condition "Succeeded or Failed"
Jan 30 11:56:00.369: INFO: Trying to get logs from node pubt2-nks-for-dev1.dg.163.org pod downwardapi-volume-ba9a5769-371f-4567-9209-c33fa8e0d46c container client-container: <nil>
STEP: delete the pod 01/30/23 11:56:00.373
Jan 30 11:56:00.377: INFO: Waiting for pod downwardapi-volume-ba9a5769-371f-4567-9209-c33fa8e0d46c to disappear
Jan 30 11:56:00.379: INFO: Pod downwardapi-volume-ba9a5769-371f-4567-9209-c33fa8e0d46c no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Jan 30 11:56:00.379: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-6580" for this suite. 01/30/23 11:56:00.381
------------------------------
• [4.037 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 11:55:56.346
    Jan 30 11:55:56.346: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename downward-api 01/30/23 11:55:56.347
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:55:56.353
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:55:56.355
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide container's cpu limit [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:193
    STEP: Creating a pod to test downward API volume plugin 01/30/23 11:55:56.357
    Jan 30 11:55:56.361: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ba9a5769-371f-4567-9209-c33fa8e0d46c" in namespace "downward-api-6580" to be "Succeeded or Failed"
    Jan 30 11:55:56.363: INFO: Pod "downwardapi-volume-ba9a5769-371f-4567-9209-c33fa8e0d46c": Phase="Pending", Reason="", readiness=false. Elapsed: 1.586697ms
    Jan 30 11:55:58.365: INFO: Pod "downwardapi-volume-ba9a5769-371f-4567-9209-c33fa8e0d46c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004235852s
    Jan 30 11:56:00.367: INFO: Pod "downwardapi-volume-ba9a5769-371f-4567-9209-c33fa8e0d46c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005802364s
    STEP: Saw pod success 01/30/23 11:56:00.367
    Jan 30 11:56:00.367: INFO: Pod "downwardapi-volume-ba9a5769-371f-4567-9209-c33fa8e0d46c" satisfied condition "Succeeded or Failed"
    Jan 30 11:56:00.369: INFO: Trying to get logs from node pubt2-nks-for-dev1.dg.163.org pod downwardapi-volume-ba9a5769-371f-4567-9209-c33fa8e0d46c container client-container: <nil>
    STEP: delete the pod 01/30/23 11:56:00.373
    Jan 30 11:56:00.377: INFO: Waiting for pod downwardapi-volume-ba9a5769-371f-4567-9209-c33fa8e0d46c to disappear
    Jan 30 11:56:00.379: INFO: Pod downwardapi-volume-ba9a5769-371f-4567-9209-c33fa8e0d46c no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Jan 30 11:56:00.379: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-6580" for this suite. 01/30/23 11:56:00.381
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:73
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 11:56:00.384
Jan 30 11:56:00.384: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename var-expansion 01/30/23 11:56:00.385
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:56:00.39
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:56:00.392
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:73
STEP: Creating a pod to test substitution in container's command 01/30/23 11:56:00.394
Jan 30 11:56:00.398: INFO: Waiting up to 5m0s for pod "var-expansion-289dff5d-a92b-4f47-a31c-a6175d6d9893" in namespace "var-expansion-7151" to be "Succeeded or Failed"
Jan 30 11:56:00.400: INFO: Pod "var-expansion-289dff5d-a92b-4f47-a31c-a6175d6d9893": Phase="Pending", Reason="", readiness=false. Elapsed: 1.64846ms
Jan 30 11:56:02.402: INFO: Pod "var-expansion-289dff5d-a92b-4f47-a31c-a6175d6d9893": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004063033s
Jan 30 11:56:04.402: INFO: Pod "var-expansion-289dff5d-a92b-4f47-a31c-a6175d6d9893": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004056304s
STEP: Saw pod success 01/30/23 11:56:04.402
Jan 30 11:56:04.402: INFO: Pod "var-expansion-289dff5d-a92b-4f47-a31c-a6175d6d9893" satisfied condition "Succeeded or Failed"
Jan 30 11:56:04.404: INFO: Trying to get logs from node pubt2-nks-for-dev1.dg.163.org pod var-expansion-289dff5d-a92b-4f47-a31c-a6175d6d9893 container dapi-container: <nil>
STEP: delete the pod 01/30/23 11:56:04.408
Jan 30 11:56:04.412: INFO: Waiting for pod var-expansion-289dff5d-a92b-4f47-a31c-a6175d6d9893 to disappear
Jan 30 11:56:04.414: INFO: Pod var-expansion-289dff5d-a92b-4f47-a31c-a6175d6d9893 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Jan 30 11:56:04.414: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-7151" for this suite. 01/30/23 11:56:04.417
------------------------------
• [4.035 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:73

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 11:56:00.384
    Jan 30 11:56:00.384: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename var-expansion 01/30/23 11:56:00.385
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:56:00.39
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:56:00.392
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow substituting values in a container's command [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:73
    STEP: Creating a pod to test substitution in container's command 01/30/23 11:56:00.394
    Jan 30 11:56:00.398: INFO: Waiting up to 5m0s for pod "var-expansion-289dff5d-a92b-4f47-a31c-a6175d6d9893" in namespace "var-expansion-7151" to be "Succeeded or Failed"
    Jan 30 11:56:00.400: INFO: Pod "var-expansion-289dff5d-a92b-4f47-a31c-a6175d6d9893": Phase="Pending", Reason="", readiness=false. Elapsed: 1.64846ms
    Jan 30 11:56:02.402: INFO: Pod "var-expansion-289dff5d-a92b-4f47-a31c-a6175d6d9893": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004063033s
    Jan 30 11:56:04.402: INFO: Pod "var-expansion-289dff5d-a92b-4f47-a31c-a6175d6d9893": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004056304s
    STEP: Saw pod success 01/30/23 11:56:04.402
    Jan 30 11:56:04.402: INFO: Pod "var-expansion-289dff5d-a92b-4f47-a31c-a6175d6d9893" satisfied condition "Succeeded or Failed"
    Jan 30 11:56:04.404: INFO: Trying to get logs from node pubt2-nks-for-dev1.dg.163.org pod var-expansion-289dff5d-a92b-4f47-a31c-a6175d6d9893 container dapi-container: <nil>
    STEP: delete the pod 01/30/23 11:56:04.408
    Jan 30 11:56:04.412: INFO: Waiting for pod var-expansion-289dff5d-a92b-4f47-a31c-a6175d6d9893 to disappear
    Jan 30 11:56:04.414: INFO: Pod var-expansion-289dff5d-a92b-4f47-a31c-a6175d6d9893 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Jan 30 11:56:04.414: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-7151" for this suite. 01/30/23 11:56:04.417
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:582
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 11:56:04.419
Jan 30 11:56:04.419: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename webhook 01/30/23 11:56:04.42
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:56:04.426
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:56:04.428
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 01/30/23 11:56:04.436
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/30/23 11:56:04.716
STEP: Deploying the webhook pod 01/30/23 11:56:04.72
STEP: Wait for the deployment to be ready 01/30/23 11:56:04.725
Jan 30 11:56:04.727: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 01/30/23 11:56:06.733
STEP: Verifying the service has paired with the endpoint 01/30/23 11:56:06.736
Jan 30 11:56:07.736: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:582
STEP: Listing all of the created validation webhooks 01/30/23 11:56:07.779
STEP: Creating a configMap that does not comply to the validation webhook rules 01/30/23 11:56:07.802
STEP: Deleting the collection of validation webhooks 01/30/23 11:56:07.822
STEP: Creating a configMap that does not comply to the validation webhook rules 01/30/23 11:56:07.837
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 30 11:56:07.841: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-6224" for this suite. 01/30/23 11:56:07.856
STEP: Destroying namespace "webhook-6224-markers" for this suite. 01/30/23 11:56:07.858
------------------------------
• [3.441 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:582

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 11:56:04.419
    Jan 30 11:56:04.419: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename webhook 01/30/23 11:56:04.42
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:56:04.426
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:56:04.428
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 01/30/23 11:56:04.436
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/30/23 11:56:04.716
    STEP: Deploying the webhook pod 01/30/23 11:56:04.72
    STEP: Wait for the deployment to be ready 01/30/23 11:56:04.725
    Jan 30 11:56:04.727: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 01/30/23 11:56:06.733
    STEP: Verifying the service has paired with the endpoint 01/30/23 11:56:06.736
    Jan 30 11:56:07.736: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] listing validating webhooks should work [Conformance]
      test/e2e/apimachinery/webhook.go:582
    STEP: Listing all of the created validation webhooks 01/30/23 11:56:07.779
    STEP: Creating a configMap that does not comply to the validation webhook rules 01/30/23 11:56:07.802
    STEP: Deleting the collection of validation webhooks 01/30/23 11:56:07.822
    STEP: Creating a configMap that does not comply to the validation webhook rules 01/30/23 11:56:07.837
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 30 11:56:07.841: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-6224" for this suite. 01/30/23 11:56:07.856
    STEP: Destroying namespace "webhook-6224-markers" for this suite. 01/30/23 11:56:07.858
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:381
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 11:56:07.863
Jan 30 11:56:07.863: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename webhook 01/30/23 11:56:07.864
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:56:07.87
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:56:07.872
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 01/30/23 11:56:07.88
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/30/23 11:56:08.269
STEP: Deploying the webhook pod 01/30/23 11:56:08.272
STEP: Wait for the deployment to be ready 01/30/23 11:56:08.277
Jan 30 11:56:08.280: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Jan 30 11:56:10.286: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 30, 11, 56, 8, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 30, 11, 56, 8, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 30, 11, 56, 8, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 30, 11, 56, 8, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5dbb7bc5bf\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 01/30/23 11:56:12.289
STEP: Verifying the service has paired with the endpoint 01/30/23 11:56:12.293
Jan 30 11:56:13.293: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:381
STEP: Setting timeout (1s) shorter than webhook latency (5s) 01/30/23 11:56:13.295
STEP: Registering slow webhook via the AdmissionRegistration API 01/30/23 11:56:13.295
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s) 01/30/23 11:56:13.304
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore 01/30/23 11:56:14.31
STEP: Registering slow webhook via the AdmissionRegistration API 01/30/23 11:56:14.31
STEP: Having no error when timeout is longer than webhook latency 01/30/23 11:56:15.323
STEP: Registering slow webhook via the AdmissionRegistration API 01/30/23 11:56:15.323
STEP: Having no error when timeout is empty (defaulted to 10s in v1) 01/30/23 11:56:20.34
STEP: Registering slow webhook via the AdmissionRegistration API 01/30/23 11:56:20.34
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 30 11:56:25.357: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-2633" for this suite. 01/30/23 11:56:25.372
STEP: Destroying namespace "webhook-2633-markers" for this suite. 01/30/23 11:56:25.375
------------------------------
• [SLOW TEST] [17.513 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:381

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 11:56:07.863
    Jan 30 11:56:07.863: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename webhook 01/30/23 11:56:07.864
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:56:07.87
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:56:07.872
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 01/30/23 11:56:07.88
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/30/23 11:56:08.269
    STEP: Deploying the webhook pod 01/30/23 11:56:08.272
    STEP: Wait for the deployment to be ready 01/30/23 11:56:08.277
    Jan 30 11:56:08.280: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    Jan 30 11:56:10.286: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 30, 11, 56, 8, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 30, 11, 56, 8, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 30, 11, 56, 8, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 30, 11, 56, 8, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5dbb7bc5bf\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 01/30/23 11:56:12.289
    STEP: Verifying the service has paired with the endpoint 01/30/23 11:56:12.293
    Jan 30 11:56:13.293: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should honor timeout [Conformance]
      test/e2e/apimachinery/webhook.go:381
    STEP: Setting timeout (1s) shorter than webhook latency (5s) 01/30/23 11:56:13.295
    STEP: Registering slow webhook via the AdmissionRegistration API 01/30/23 11:56:13.295
    STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s) 01/30/23 11:56:13.304
    STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore 01/30/23 11:56:14.31
    STEP: Registering slow webhook via the AdmissionRegistration API 01/30/23 11:56:14.31
    STEP: Having no error when timeout is longer than webhook latency 01/30/23 11:56:15.323
    STEP: Registering slow webhook via the AdmissionRegistration API 01/30/23 11:56:15.323
    STEP: Having no error when timeout is empty (defaulted to 10s in v1) 01/30/23 11:56:20.34
    STEP: Registering slow webhook via the AdmissionRegistration API 01/30/23 11:56:20.34
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 30 11:56:25.357: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-2633" for this suite. 01/30/23 11:56:25.372
    STEP: Destroying namespace "webhook-2633-markers" for this suite. 01/30/23 11:56:25.375
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:862
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 11:56:25.378
Jan 30 11:56:25.378: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename daemonsets 01/30/23 11:56:25.379
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:56:25.385
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:56:25.387
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:862
STEP: Creating simple DaemonSet "daemon-set" 01/30/23 11:56:25.4
STEP: Check that daemon pods launch on every node of the cluster. 01/30/23 11:56:25.402
Jan 30 11:56:25.406: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 30 11:56:25.406: INFO: Node pubt2-nks-for-dev1.dg.163.org is running 0 daemon pod, expected 1
Jan 30 11:56:26.410: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 30 11:56:26.410: INFO: Node pubt2-nks-for-dev1.dg.163.org is running 0 daemon pod, expected 1
Jan 30 11:56:27.411: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan 30 11:56:27.411: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
STEP: Getting /status 01/30/23 11:56:27.412
Jan 30 11:56:27.414: INFO: Daemon Set daemon-set has Conditions: []
STEP: updating the DaemonSet Status 01/30/23 11:56:27.414
Jan 30 11:56:27.419: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the daemon set status to be updated 01/30/23 11:56:27.419
Jan 30 11:56:27.421: INFO: Observed &DaemonSet event: ADDED
Jan 30 11:56:27.421: INFO: Observed &DaemonSet event: MODIFIED
Jan 30 11:56:27.421: INFO: Observed &DaemonSet event: MODIFIED
Jan 30 11:56:27.421: INFO: Observed &DaemonSet event: MODIFIED
Jan 30 11:56:27.421: INFO: Found daemon set daemon-set in namespace daemonsets-1981 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Jan 30 11:56:27.421: INFO: Daemon set daemon-set has an updated status
STEP: patching the DaemonSet Status 01/30/23 11:56:27.421
STEP: watching for the daemon set status to be patched 01/30/23 11:56:27.426
Jan 30 11:56:27.427: INFO: Observed &DaemonSet event: ADDED
Jan 30 11:56:27.427: INFO: Observed &DaemonSet event: MODIFIED
Jan 30 11:56:27.427: INFO: Observed &DaemonSet event: MODIFIED
Jan 30 11:56:27.428: INFO: Observed &DaemonSet event: MODIFIED
Jan 30 11:56:27.428: INFO: Observed daemon set daemon-set in namespace daemonsets-1981 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Jan 30 11:56:27.428: INFO: Observed &DaemonSet event: MODIFIED
Jan 30 11:56:27.428: INFO: Found daemon set daemon-set in namespace daemonsets-1981 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
Jan 30 11:56:27.428: INFO: Daemon set daemon-set has a patched status
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 01/30/23 11:56:27.429
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1981, will wait for the garbage collector to delete the pods 01/30/23 11:56:27.429
Jan 30 11:56:27.484: INFO: Deleting DaemonSet.extensions daemon-set took: 2.442972ms
Jan 30 11:56:27.584: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.317728ms
Jan 30 11:56:29.886: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 30 11:56:29.886: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jan 30 11:56:29.888: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"25711"},"items":null}

Jan 30 11:56:29.889: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"25711"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Jan 30 11:56:29.894: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-1981" for this suite. 01/30/23 11:56:29.896
------------------------------
• [4.520 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:862

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 11:56:25.378
    Jan 30 11:56:25.378: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename daemonsets 01/30/23 11:56:25.379
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:56:25.385
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:56:25.387
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should verify changes to a daemon set status [Conformance]
      test/e2e/apps/daemon_set.go:862
    STEP: Creating simple DaemonSet "daemon-set" 01/30/23 11:56:25.4
    STEP: Check that daemon pods launch on every node of the cluster. 01/30/23 11:56:25.402
    Jan 30 11:56:25.406: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 30 11:56:25.406: INFO: Node pubt2-nks-for-dev1.dg.163.org is running 0 daemon pod, expected 1
    Jan 30 11:56:26.410: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 30 11:56:26.410: INFO: Node pubt2-nks-for-dev1.dg.163.org is running 0 daemon pod, expected 1
    Jan 30 11:56:27.411: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jan 30 11:56:27.411: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    STEP: Getting /status 01/30/23 11:56:27.412
    Jan 30 11:56:27.414: INFO: Daemon Set daemon-set has Conditions: []
    STEP: updating the DaemonSet Status 01/30/23 11:56:27.414
    Jan 30 11:56:27.419: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the daemon set status to be updated 01/30/23 11:56:27.419
    Jan 30 11:56:27.421: INFO: Observed &DaemonSet event: ADDED
    Jan 30 11:56:27.421: INFO: Observed &DaemonSet event: MODIFIED
    Jan 30 11:56:27.421: INFO: Observed &DaemonSet event: MODIFIED
    Jan 30 11:56:27.421: INFO: Observed &DaemonSet event: MODIFIED
    Jan 30 11:56:27.421: INFO: Found daemon set daemon-set in namespace daemonsets-1981 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Jan 30 11:56:27.421: INFO: Daemon set daemon-set has an updated status
    STEP: patching the DaemonSet Status 01/30/23 11:56:27.421
    STEP: watching for the daemon set status to be patched 01/30/23 11:56:27.426
    Jan 30 11:56:27.427: INFO: Observed &DaemonSet event: ADDED
    Jan 30 11:56:27.427: INFO: Observed &DaemonSet event: MODIFIED
    Jan 30 11:56:27.427: INFO: Observed &DaemonSet event: MODIFIED
    Jan 30 11:56:27.428: INFO: Observed &DaemonSet event: MODIFIED
    Jan 30 11:56:27.428: INFO: Observed daemon set daemon-set in namespace daemonsets-1981 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Jan 30 11:56:27.428: INFO: Observed &DaemonSet event: MODIFIED
    Jan 30 11:56:27.428: INFO: Found daemon set daemon-set in namespace daemonsets-1981 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
    Jan 30 11:56:27.428: INFO: Daemon set daemon-set has a patched status
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 01/30/23 11:56:27.429
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1981, will wait for the garbage collector to delete the pods 01/30/23 11:56:27.429
    Jan 30 11:56:27.484: INFO: Deleting DaemonSet.extensions daemon-set took: 2.442972ms
    Jan 30 11:56:27.584: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.317728ms
    Jan 30 11:56:29.886: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 30 11:56:29.886: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Jan 30 11:56:29.888: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"25711"},"items":null}

    Jan 30 11:56:29.889: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"25711"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan 30 11:56:29.894: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-1981" for this suite. 01/30/23 11:56:29.896
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should provide secure master service  [Conformance]
  test/e2e/network/service.go:777
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 11:56:29.899
Jan 30 11:56:29.899: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename services 01/30/23 11:56:29.9
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:56:29.906
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:56:29.908
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should provide secure master service  [Conformance]
  test/e2e/network/service.go:777
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jan 30 11:56:29.911: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-1742" for this suite. 01/30/23 11:56:29.914
------------------------------
• [0.017 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should provide secure master service  [Conformance]
  test/e2e/network/service.go:777

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 11:56:29.899
    Jan 30 11:56:29.899: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename services 01/30/23 11:56:29.9
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:56:29.906
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:56:29.908
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should provide secure master service  [Conformance]
      test/e2e/network/service.go:777
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jan 30 11:56:29.911: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-1742" for this suite. 01/30/23 11:56:29.914
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info
  should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  test/e2e/kubectl/kubectl.go:1250
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 11:56:29.916
Jan 30 11:56:29.916: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename kubectl 01/30/23 11:56:29.917
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:56:29.922
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:56:29.924
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  test/e2e/kubectl/kubectl.go:1250
STEP: validating cluster-info 01/30/23 11:56:29.926
Jan 30 11:56:29.926: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=kubectl-259 cluster-info'
Jan 30 11:56:29.986: INFO: stderr: ""
Jan 30 11:56:29.986: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://10.178.64.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jan 30 11:56:29.986: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-259" for this suite. 01/30/23 11:56:29.988
------------------------------
• [0.075 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl cluster-info
  test/e2e/kubectl/kubectl.go:1244
    should check if Kubernetes control plane services is included in cluster-info  [Conformance]
    test/e2e/kubectl/kubectl.go:1250

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 11:56:29.916
    Jan 30 11:56:29.916: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename kubectl 01/30/23 11:56:29.917
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:56:29.922
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:56:29.924
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
      test/e2e/kubectl/kubectl.go:1250
    STEP: validating cluster-info 01/30/23 11:56:29.926
    Jan 30 11:56:29.926: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=kubectl-259 cluster-info'
    Jan 30 11:56:29.986: INFO: stderr: ""
    Jan 30 11:56:29.986: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://10.178.64.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jan 30 11:56:29.986: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-259" for this suite. 01/30/23 11:56:29.988
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:99
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 11:56:29.991
Jan 30 11:56:29.991: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename projected 01/30/23 11:56:29.992
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:56:29.997
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:56:29.999
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:99
STEP: Creating configMap with name projected-configmap-test-volume-map-b53d3e26-ef4d-4c30-86d4-ef680e122107 01/30/23 11:56:30.001
STEP: Creating a pod to test consume configMaps 01/30/23 11:56:30.004
Jan 30 11:56:30.008: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-74e37d31-663f-40f1-95d9-10cea204be81" in namespace "projected-8334" to be "Succeeded or Failed"
Jan 30 11:56:30.009: INFO: Pod "pod-projected-configmaps-74e37d31-663f-40f1-95d9-10cea204be81": Phase="Pending", Reason="", readiness=false. Elapsed: 1.625713ms
Jan 30 11:56:32.011: INFO: Pod "pod-projected-configmaps-74e37d31-663f-40f1-95d9-10cea204be81": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003547677s
Jan 30 11:56:34.013: INFO: Pod "pod-projected-configmaps-74e37d31-663f-40f1-95d9-10cea204be81": Phase="Pending", Reason="", readiness=false. Elapsed: 4.004894338s
Jan 30 11:56:36.012: INFO: Pod "pod-projected-configmaps-74e37d31-663f-40f1-95d9-10cea204be81": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.003850343s
STEP: Saw pod success 01/30/23 11:56:36.012
Jan 30 11:56:36.012: INFO: Pod "pod-projected-configmaps-74e37d31-663f-40f1-95d9-10cea204be81" satisfied condition "Succeeded or Failed"
Jan 30 11:56:36.013: INFO: Trying to get logs from node pubt2-nks-for-dev1.dg.163.org pod pod-projected-configmaps-74e37d31-663f-40f1-95d9-10cea204be81 container agnhost-container: <nil>
STEP: delete the pod 01/30/23 11:56:36.018
Jan 30 11:56:36.023: INFO: Waiting for pod pod-projected-configmaps-74e37d31-663f-40f1-95d9-10cea204be81 to disappear
Jan 30 11:56:36.024: INFO: Pod pod-projected-configmaps-74e37d31-663f-40f1-95d9-10cea204be81 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Jan 30 11:56:36.024: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-8334" for this suite. 01/30/23 11:56:36.026
------------------------------
• [SLOW TEST] [6.038 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:99

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 11:56:29.991
    Jan 30 11:56:29.991: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename projected 01/30/23 11:56:29.992
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:56:29.997
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:56:29.999
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:99
    STEP: Creating configMap with name projected-configmap-test-volume-map-b53d3e26-ef4d-4c30-86d4-ef680e122107 01/30/23 11:56:30.001
    STEP: Creating a pod to test consume configMaps 01/30/23 11:56:30.004
    Jan 30 11:56:30.008: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-74e37d31-663f-40f1-95d9-10cea204be81" in namespace "projected-8334" to be "Succeeded or Failed"
    Jan 30 11:56:30.009: INFO: Pod "pod-projected-configmaps-74e37d31-663f-40f1-95d9-10cea204be81": Phase="Pending", Reason="", readiness=false. Elapsed: 1.625713ms
    Jan 30 11:56:32.011: INFO: Pod "pod-projected-configmaps-74e37d31-663f-40f1-95d9-10cea204be81": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003547677s
    Jan 30 11:56:34.013: INFO: Pod "pod-projected-configmaps-74e37d31-663f-40f1-95d9-10cea204be81": Phase="Pending", Reason="", readiness=false. Elapsed: 4.004894338s
    Jan 30 11:56:36.012: INFO: Pod "pod-projected-configmaps-74e37d31-663f-40f1-95d9-10cea204be81": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.003850343s
    STEP: Saw pod success 01/30/23 11:56:36.012
    Jan 30 11:56:36.012: INFO: Pod "pod-projected-configmaps-74e37d31-663f-40f1-95d9-10cea204be81" satisfied condition "Succeeded or Failed"
    Jan 30 11:56:36.013: INFO: Trying to get logs from node pubt2-nks-for-dev1.dg.163.org pod pod-projected-configmaps-74e37d31-663f-40f1-95d9-10cea204be81 container agnhost-container: <nil>
    STEP: delete the pod 01/30/23 11:56:36.018
    Jan 30 11:56:36.023: INFO: Waiting for pod pod-projected-configmaps-74e37d31-663f-40f1-95d9-10cea204be81 to disappear
    Jan 30 11:56:36.024: INFO: Pod pod-projected-configmaps-74e37d31-663f-40f1-95d9-10cea204be81 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Jan 30 11:56:36.024: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-8334" for this suite. 01/30/23 11:56:36.026
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] Pods
  should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:226
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 11:56:36.029
Jan 30 11:56:36.029: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename pods 01/30/23 11:56:36.03
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:56:36.036
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:56:36.038
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:226
STEP: creating the pod 01/30/23 11:56:36.039
STEP: setting up watch 01/30/23 11:56:36.04
STEP: submitting the pod to kubernetes 01/30/23 11:56:36.142
STEP: verifying the pod is in kubernetes 01/30/23 11:56:36.146
STEP: verifying pod creation was observed 01/30/23 11:56:36.148
Jan 30 11:56:36.148: INFO: Waiting up to 5m0s for pod "pod-submit-remove-ea60fd96-74d4-4498-988d-441aed8977eb" in namespace "pods-6939" to be "running"
Jan 30 11:56:36.150: INFO: Pod "pod-submit-remove-ea60fd96-74d4-4498-988d-441aed8977eb": Phase="Pending", Reason="", readiness=false. Elapsed: 1.637605ms
Jan 30 11:56:38.152: INFO: Pod "pod-submit-remove-ea60fd96-74d4-4498-988d-441aed8977eb": Phase="Running", Reason="", readiness=true. Elapsed: 2.003790683s
Jan 30 11:56:38.152: INFO: Pod "pod-submit-remove-ea60fd96-74d4-4498-988d-441aed8977eb" satisfied condition "running"
STEP: deleting the pod gracefully 01/30/23 11:56:38.154
STEP: verifying pod deletion was observed 01/30/23 11:56:38.156
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Jan 30 11:56:41.126: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-6939" for this suite. 01/30/23 11:56:41.129
------------------------------
• [SLOW TEST] [5.102 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:226

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 11:56:36.029
    Jan 30 11:56:36.029: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename pods 01/30/23 11:56:36.03
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:56:36.036
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:56:36.038
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should be submitted and removed [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:226
    STEP: creating the pod 01/30/23 11:56:36.039
    STEP: setting up watch 01/30/23 11:56:36.04
    STEP: submitting the pod to kubernetes 01/30/23 11:56:36.142
    STEP: verifying the pod is in kubernetes 01/30/23 11:56:36.146
    STEP: verifying pod creation was observed 01/30/23 11:56:36.148
    Jan 30 11:56:36.148: INFO: Waiting up to 5m0s for pod "pod-submit-remove-ea60fd96-74d4-4498-988d-441aed8977eb" in namespace "pods-6939" to be "running"
    Jan 30 11:56:36.150: INFO: Pod "pod-submit-remove-ea60fd96-74d4-4498-988d-441aed8977eb": Phase="Pending", Reason="", readiness=false. Elapsed: 1.637605ms
    Jan 30 11:56:38.152: INFO: Pod "pod-submit-remove-ea60fd96-74d4-4498-988d-441aed8977eb": Phase="Running", Reason="", readiness=true. Elapsed: 2.003790683s
    Jan 30 11:56:38.152: INFO: Pod "pod-submit-remove-ea60fd96-74d4-4498-988d-441aed8977eb" satisfied condition "running"
    STEP: deleting the pod gracefully 01/30/23 11:56:38.154
    STEP: verifying pod deletion was observed 01/30/23 11:56:38.156
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Jan 30 11:56:41.126: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-6939" for this suite. 01/30/23 11:56:41.129
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2213
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 11:56:41.131
Jan 30 11:56:41.131: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename services 01/30/23 11:56:41.132
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:56:41.137
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:56:41.139
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2213
STEP: creating service in namespace services-3205 01/30/23 11:56:41.141
STEP: creating service affinity-clusterip-transition in namespace services-3205 01/30/23 11:56:41.141
STEP: creating replication controller affinity-clusterip-transition in namespace services-3205 01/30/23 11:56:41.144
I0130 11:56:41.147586      23 runners.go:193] Created replication controller with name: affinity-clusterip-transition, namespace: services-3205, replica count: 3
I0130 11:56:44.198822      23 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 30 11:56:44.202: INFO: Creating new exec pod
Jan 30 11:56:44.204: INFO: Waiting up to 5m0s for pod "execpod-affinitywznpm" in namespace "services-3205" to be "running"
Jan 30 11:56:44.206: INFO: Pod "execpod-affinitywznpm": Phase="Pending", Reason="", readiness=false. Elapsed: 1.543964ms
Jan 30 11:56:46.208: INFO: Pod "execpod-affinitywznpm": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003876316s
Jan 30 11:56:48.208: INFO: Pod "execpod-affinitywznpm": Phase="Running", Reason="", readiness=true. Elapsed: 4.003956681s
Jan 30 11:56:48.208: INFO: Pod "execpod-affinitywznpm" satisfied condition "running"
Jan 30 11:56:49.209: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=services-3205 exec execpod-affinitywznpm -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip-transition 80'
Jan 30 11:56:49.368: INFO: stderr: "+ nc -v -z -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
Jan 30 11:56:49.368: INFO: stdout: ""
Jan 30 11:56:49.368: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=services-3205 exec execpod-affinitywznpm -- /bin/sh -x -c nc -v -z -w 2 10.178.72.41 80'
Jan 30 11:56:49.498: INFO: stderr: "+ nc -v -z -w 2 10.178.72.41 80\nConnection to 10.178.72.41 80 port [tcp/http] succeeded!\n"
Jan 30 11:56:49.498: INFO: stdout: ""
Jan 30 11:56:49.503: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=services-3205 exec execpod-affinitywznpm -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.178.72.41:80/ ; done'
Jan 30 11:56:49.681: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.178.72.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.178.72.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.178.72.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.178.72.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.178.72.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.178.72.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.178.72.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.178.72.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.178.72.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.178.72.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.178.72.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.178.72.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.178.72.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.178.72.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.178.72.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.178.72.41:80/\n"
Jan 30 11:56:49.681: INFO: stdout: "\naffinity-clusterip-transition-n7gcq\naffinity-clusterip-transition-n7gcq\naffinity-clusterip-transition-n7gcq\naffinity-clusterip-transition-n7gcq\naffinity-clusterip-transition-n7gcq\naffinity-clusterip-transition-n7gcq\naffinity-clusterip-transition-n7gcq\naffinity-clusterip-transition-n7gcq\naffinity-clusterip-transition-n7gcq\naffinity-clusterip-transition-n7gcq\naffinity-clusterip-transition-n7gcq\naffinity-clusterip-transition-n7gcq\naffinity-clusterip-transition-n7gcq\naffinity-clusterip-transition-n7gcq\naffinity-clusterip-transition-n7gcq\naffinity-clusterip-transition-n7gcq"
Jan 30 11:56:49.681: INFO: Received response from host: affinity-clusterip-transition-n7gcq
Jan 30 11:56:49.681: INFO: Received response from host: affinity-clusterip-transition-n7gcq
Jan 30 11:56:49.681: INFO: Received response from host: affinity-clusterip-transition-n7gcq
Jan 30 11:56:49.681: INFO: Received response from host: affinity-clusterip-transition-n7gcq
Jan 30 11:56:49.681: INFO: Received response from host: affinity-clusterip-transition-n7gcq
Jan 30 11:56:49.681: INFO: Received response from host: affinity-clusterip-transition-n7gcq
Jan 30 11:56:49.681: INFO: Received response from host: affinity-clusterip-transition-n7gcq
Jan 30 11:56:49.681: INFO: Received response from host: affinity-clusterip-transition-n7gcq
Jan 30 11:56:49.681: INFO: Received response from host: affinity-clusterip-transition-n7gcq
Jan 30 11:56:49.681: INFO: Received response from host: affinity-clusterip-transition-n7gcq
Jan 30 11:56:49.681: INFO: Received response from host: affinity-clusterip-transition-n7gcq
Jan 30 11:56:49.681: INFO: Received response from host: affinity-clusterip-transition-n7gcq
Jan 30 11:56:49.681: INFO: Received response from host: affinity-clusterip-transition-n7gcq
Jan 30 11:56:49.681: INFO: Received response from host: affinity-clusterip-transition-n7gcq
Jan 30 11:56:49.681: INFO: Received response from host: affinity-clusterip-transition-n7gcq
Jan 30 11:56:49.681: INFO: Received response from host: affinity-clusterip-transition-n7gcq
Jan 30 11:57:19.681: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=services-3205 exec execpod-affinitywznpm -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.178.72.41:80/ ; done'
Jan 30 11:57:19.883: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.178.72.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.178.72.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.178.72.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.178.72.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.178.72.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.178.72.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.178.72.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.178.72.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.178.72.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.178.72.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.178.72.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.178.72.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.178.72.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.178.72.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.178.72.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.178.72.41:80/\n"
Jan 30 11:57:19.883: INFO: stdout: "\naffinity-clusterip-transition-6l9mq\naffinity-clusterip-transition-6l9mq\naffinity-clusterip-transition-n7gcq\naffinity-clusterip-transition-2bp68\naffinity-clusterip-transition-n7gcq\naffinity-clusterip-transition-6l9mq\naffinity-clusterip-transition-2bp68\naffinity-clusterip-transition-6l9mq\naffinity-clusterip-transition-2bp68\naffinity-clusterip-transition-2bp68\naffinity-clusterip-transition-6l9mq\naffinity-clusterip-transition-6l9mq\naffinity-clusterip-transition-n7gcq\naffinity-clusterip-transition-2bp68\naffinity-clusterip-transition-6l9mq\naffinity-clusterip-transition-n7gcq"
Jan 30 11:57:19.883: INFO: Received response from host: affinity-clusterip-transition-6l9mq
Jan 30 11:57:19.883: INFO: Received response from host: affinity-clusterip-transition-6l9mq
Jan 30 11:57:19.883: INFO: Received response from host: affinity-clusterip-transition-n7gcq
Jan 30 11:57:19.883: INFO: Received response from host: affinity-clusterip-transition-2bp68
Jan 30 11:57:19.883: INFO: Received response from host: affinity-clusterip-transition-n7gcq
Jan 30 11:57:19.883: INFO: Received response from host: affinity-clusterip-transition-6l9mq
Jan 30 11:57:19.883: INFO: Received response from host: affinity-clusterip-transition-2bp68
Jan 30 11:57:19.883: INFO: Received response from host: affinity-clusterip-transition-6l9mq
Jan 30 11:57:19.883: INFO: Received response from host: affinity-clusterip-transition-2bp68
Jan 30 11:57:19.883: INFO: Received response from host: affinity-clusterip-transition-2bp68
Jan 30 11:57:19.883: INFO: Received response from host: affinity-clusterip-transition-6l9mq
Jan 30 11:57:19.883: INFO: Received response from host: affinity-clusterip-transition-6l9mq
Jan 30 11:57:19.883: INFO: Received response from host: affinity-clusterip-transition-n7gcq
Jan 30 11:57:19.883: INFO: Received response from host: affinity-clusterip-transition-2bp68
Jan 30 11:57:19.883: INFO: Received response from host: affinity-clusterip-transition-6l9mq
Jan 30 11:57:19.883: INFO: Received response from host: affinity-clusterip-transition-n7gcq
Jan 30 11:57:19.889: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=services-3205 exec execpod-affinitywznpm -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.178.72.41:80/ ; done'
Jan 30 11:57:20.080: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.178.72.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.178.72.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.178.72.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.178.72.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.178.72.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.178.72.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.178.72.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.178.72.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.178.72.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.178.72.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.178.72.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.178.72.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.178.72.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.178.72.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.178.72.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.178.72.41:80/\n"
Jan 30 11:57:20.080: INFO: stdout: "\naffinity-clusterip-transition-2bp68\naffinity-clusterip-transition-2bp68\naffinity-clusterip-transition-2bp68\naffinity-clusterip-transition-2bp68\naffinity-clusterip-transition-2bp68\naffinity-clusterip-transition-2bp68\naffinity-clusterip-transition-2bp68\naffinity-clusterip-transition-2bp68\naffinity-clusterip-transition-2bp68\naffinity-clusterip-transition-2bp68\naffinity-clusterip-transition-2bp68\naffinity-clusterip-transition-2bp68\naffinity-clusterip-transition-2bp68\naffinity-clusterip-transition-2bp68\naffinity-clusterip-transition-2bp68\naffinity-clusterip-transition-2bp68"
Jan 30 11:57:20.080: INFO: Received response from host: affinity-clusterip-transition-2bp68
Jan 30 11:57:20.080: INFO: Received response from host: affinity-clusterip-transition-2bp68
Jan 30 11:57:20.080: INFO: Received response from host: affinity-clusterip-transition-2bp68
Jan 30 11:57:20.080: INFO: Received response from host: affinity-clusterip-transition-2bp68
Jan 30 11:57:20.080: INFO: Received response from host: affinity-clusterip-transition-2bp68
Jan 30 11:57:20.080: INFO: Received response from host: affinity-clusterip-transition-2bp68
Jan 30 11:57:20.080: INFO: Received response from host: affinity-clusterip-transition-2bp68
Jan 30 11:57:20.080: INFO: Received response from host: affinity-clusterip-transition-2bp68
Jan 30 11:57:20.080: INFO: Received response from host: affinity-clusterip-transition-2bp68
Jan 30 11:57:20.080: INFO: Received response from host: affinity-clusterip-transition-2bp68
Jan 30 11:57:20.080: INFO: Received response from host: affinity-clusterip-transition-2bp68
Jan 30 11:57:20.080: INFO: Received response from host: affinity-clusterip-transition-2bp68
Jan 30 11:57:20.080: INFO: Received response from host: affinity-clusterip-transition-2bp68
Jan 30 11:57:20.080: INFO: Received response from host: affinity-clusterip-transition-2bp68
Jan 30 11:57:20.080: INFO: Received response from host: affinity-clusterip-transition-2bp68
Jan 30 11:57:20.080: INFO: Received response from host: affinity-clusterip-transition-2bp68
Jan 30 11:57:20.080: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-3205, will wait for the garbage collector to delete the pods 01/30/23 11:57:20.084
Jan 30 11:57:20.140: INFO: Deleting ReplicationController affinity-clusterip-transition took: 2.860803ms
Jan 30 11:57:20.240: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 100.366434ms
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jan 30 11:57:22.946: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-3205" for this suite. 01/30/23 11:57:22.949
------------------------------
• [SLOW TEST] [41.820 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2213

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 11:56:41.131
    Jan 30 11:56:41.131: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename services 01/30/23 11:56:41.132
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:56:41.137
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:56:41.139
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2213
    STEP: creating service in namespace services-3205 01/30/23 11:56:41.141
    STEP: creating service affinity-clusterip-transition in namespace services-3205 01/30/23 11:56:41.141
    STEP: creating replication controller affinity-clusterip-transition in namespace services-3205 01/30/23 11:56:41.144
    I0130 11:56:41.147586      23 runners.go:193] Created replication controller with name: affinity-clusterip-transition, namespace: services-3205, replica count: 3
    I0130 11:56:44.198822      23 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan 30 11:56:44.202: INFO: Creating new exec pod
    Jan 30 11:56:44.204: INFO: Waiting up to 5m0s for pod "execpod-affinitywznpm" in namespace "services-3205" to be "running"
    Jan 30 11:56:44.206: INFO: Pod "execpod-affinitywznpm": Phase="Pending", Reason="", readiness=false. Elapsed: 1.543964ms
    Jan 30 11:56:46.208: INFO: Pod "execpod-affinitywznpm": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003876316s
    Jan 30 11:56:48.208: INFO: Pod "execpod-affinitywznpm": Phase="Running", Reason="", readiness=true. Elapsed: 4.003956681s
    Jan 30 11:56:48.208: INFO: Pod "execpod-affinitywznpm" satisfied condition "running"
    Jan 30 11:56:49.209: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=services-3205 exec execpod-affinitywznpm -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip-transition 80'
    Jan 30 11:56:49.368: INFO: stderr: "+ nc -v -z -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
    Jan 30 11:56:49.368: INFO: stdout: ""
    Jan 30 11:56:49.368: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=services-3205 exec execpod-affinitywznpm -- /bin/sh -x -c nc -v -z -w 2 10.178.72.41 80'
    Jan 30 11:56:49.498: INFO: stderr: "+ nc -v -z -w 2 10.178.72.41 80\nConnection to 10.178.72.41 80 port [tcp/http] succeeded!\n"
    Jan 30 11:56:49.498: INFO: stdout: ""
    Jan 30 11:56:49.503: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=services-3205 exec execpod-affinitywznpm -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.178.72.41:80/ ; done'
    Jan 30 11:56:49.681: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.178.72.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.178.72.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.178.72.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.178.72.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.178.72.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.178.72.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.178.72.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.178.72.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.178.72.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.178.72.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.178.72.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.178.72.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.178.72.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.178.72.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.178.72.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.178.72.41:80/\n"
    Jan 30 11:56:49.681: INFO: stdout: "\naffinity-clusterip-transition-n7gcq\naffinity-clusterip-transition-n7gcq\naffinity-clusterip-transition-n7gcq\naffinity-clusterip-transition-n7gcq\naffinity-clusterip-transition-n7gcq\naffinity-clusterip-transition-n7gcq\naffinity-clusterip-transition-n7gcq\naffinity-clusterip-transition-n7gcq\naffinity-clusterip-transition-n7gcq\naffinity-clusterip-transition-n7gcq\naffinity-clusterip-transition-n7gcq\naffinity-clusterip-transition-n7gcq\naffinity-clusterip-transition-n7gcq\naffinity-clusterip-transition-n7gcq\naffinity-clusterip-transition-n7gcq\naffinity-clusterip-transition-n7gcq"
    Jan 30 11:56:49.681: INFO: Received response from host: affinity-clusterip-transition-n7gcq
    Jan 30 11:56:49.681: INFO: Received response from host: affinity-clusterip-transition-n7gcq
    Jan 30 11:56:49.681: INFO: Received response from host: affinity-clusterip-transition-n7gcq
    Jan 30 11:56:49.681: INFO: Received response from host: affinity-clusterip-transition-n7gcq
    Jan 30 11:56:49.681: INFO: Received response from host: affinity-clusterip-transition-n7gcq
    Jan 30 11:56:49.681: INFO: Received response from host: affinity-clusterip-transition-n7gcq
    Jan 30 11:56:49.681: INFO: Received response from host: affinity-clusterip-transition-n7gcq
    Jan 30 11:56:49.681: INFO: Received response from host: affinity-clusterip-transition-n7gcq
    Jan 30 11:56:49.681: INFO: Received response from host: affinity-clusterip-transition-n7gcq
    Jan 30 11:56:49.681: INFO: Received response from host: affinity-clusterip-transition-n7gcq
    Jan 30 11:56:49.681: INFO: Received response from host: affinity-clusterip-transition-n7gcq
    Jan 30 11:56:49.681: INFO: Received response from host: affinity-clusterip-transition-n7gcq
    Jan 30 11:56:49.681: INFO: Received response from host: affinity-clusterip-transition-n7gcq
    Jan 30 11:56:49.681: INFO: Received response from host: affinity-clusterip-transition-n7gcq
    Jan 30 11:56:49.681: INFO: Received response from host: affinity-clusterip-transition-n7gcq
    Jan 30 11:56:49.681: INFO: Received response from host: affinity-clusterip-transition-n7gcq
    Jan 30 11:57:19.681: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=services-3205 exec execpod-affinitywznpm -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.178.72.41:80/ ; done'
    Jan 30 11:57:19.883: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.178.72.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.178.72.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.178.72.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.178.72.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.178.72.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.178.72.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.178.72.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.178.72.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.178.72.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.178.72.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.178.72.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.178.72.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.178.72.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.178.72.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.178.72.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.178.72.41:80/\n"
    Jan 30 11:57:19.883: INFO: stdout: "\naffinity-clusterip-transition-6l9mq\naffinity-clusterip-transition-6l9mq\naffinity-clusterip-transition-n7gcq\naffinity-clusterip-transition-2bp68\naffinity-clusterip-transition-n7gcq\naffinity-clusterip-transition-6l9mq\naffinity-clusterip-transition-2bp68\naffinity-clusterip-transition-6l9mq\naffinity-clusterip-transition-2bp68\naffinity-clusterip-transition-2bp68\naffinity-clusterip-transition-6l9mq\naffinity-clusterip-transition-6l9mq\naffinity-clusterip-transition-n7gcq\naffinity-clusterip-transition-2bp68\naffinity-clusterip-transition-6l9mq\naffinity-clusterip-transition-n7gcq"
    Jan 30 11:57:19.883: INFO: Received response from host: affinity-clusterip-transition-6l9mq
    Jan 30 11:57:19.883: INFO: Received response from host: affinity-clusterip-transition-6l9mq
    Jan 30 11:57:19.883: INFO: Received response from host: affinity-clusterip-transition-n7gcq
    Jan 30 11:57:19.883: INFO: Received response from host: affinity-clusterip-transition-2bp68
    Jan 30 11:57:19.883: INFO: Received response from host: affinity-clusterip-transition-n7gcq
    Jan 30 11:57:19.883: INFO: Received response from host: affinity-clusterip-transition-6l9mq
    Jan 30 11:57:19.883: INFO: Received response from host: affinity-clusterip-transition-2bp68
    Jan 30 11:57:19.883: INFO: Received response from host: affinity-clusterip-transition-6l9mq
    Jan 30 11:57:19.883: INFO: Received response from host: affinity-clusterip-transition-2bp68
    Jan 30 11:57:19.883: INFO: Received response from host: affinity-clusterip-transition-2bp68
    Jan 30 11:57:19.883: INFO: Received response from host: affinity-clusterip-transition-6l9mq
    Jan 30 11:57:19.883: INFO: Received response from host: affinity-clusterip-transition-6l9mq
    Jan 30 11:57:19.883: INFO: Received response from host: affinity-clusterip-transition-n7gcq
    Jan 30 11:57:19.883: INFO: Received response from host: affinity-clusterip-transition-2bp68
    Jan 30 11:57:19.883: INFO: Received response from host: affinity-clusterip-transition-6l9mq
    Jan 30 11:57:19.883: INFO: Received response from host: affinity-clusterip-transition-n7gcq
    Jan 30 11:57:19.889: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=services-3205 exec execpod-affinitywznpm -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.178.72.41:80/ ; done'
    Jan 30 11:57:20.080: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.178.72.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.178.72.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.178.72.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.178.72.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.178.72.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.178.72.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.178.72.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.178.72.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.178.72.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.178.72.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.178.72.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.178.72.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.178.72.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.178.72.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.178.72.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.178.72.41:80/\n"
    Jan 30 11:57:20.080: INFO: stdout: "\naffinity-clusterip-transition-2bp68\naffinity-clusterip-transition-2bp68\naffinity-clusterip-transition-2bp68\naffinity-clusterip-transition-2bp68\naffinity-clusterip-transition-2bp68\naffinity-clusterip-transition-2bp68\naffinity-clusterip-transition-2bp68\naffinity-clusterip-transition-2bp68\naffinity-clusterip-transition-2bp68\naffinity-clusterip-transition-2bp68\naffinity-clusterip-transition-2bp68\naffinity-clusterip-transition-2bp68\naffinity-clusterip-transition-2bp68\naffinity-clusterip-transition-2bp68\naffinity-clusterip-transition-2bp68\naffinity-clusterip-transition-2bp68"
    Jan 30 11:57:20.080: INFO: Received response from host: affinity-clusterip-transition-2bp68
    Jan 30 11:57:20.080: INFO: Received response from host: affinity-clusterip-transition-2bp68
    Jan 30 11:57:20.080: INFO: Received response from host: affinity-clusterip-transition-2bp68
    Jan 30 11:57:20.080: INFO: Received response from host: affinity-clusterip-transition-2bp68
    Jan 30 11:57:20.080: INFO: Received response from host: affinity-clusterip-transition-2bp68
    Jan 30 11:57:20.080: INFO: Received response from host: affinity-clusterip-transition-2bp68
    Jan 30 11:57:20.080: INFO: Received response from host: affinity-clusterip-transition-2bp68
    Jan 30 11:57:20.080: INFO: Received response from host: affinity-clusterip-transition-2bp68
    Jan 30 11:57:20.080: INFO: Received response from host: affinity-clusterip-transition-2bp68
    Jan 30 11:57:20.080: INFO: Received response from host: affinity-clusterip-transition-2bp68
    Jan 30 11:57:20.080: INFO: Received response from host: affinity-clusterip-transition-2bp68
    Jan 30 11:57:20.080: INFO: Received response from host: affinity-clusterip-transition-2bp68
    Jan 30 11:57:20.080: INFO: Received response from host: affinity-clusterip-transition-2bp68
    Jan 30 11:57:20.080: INFO: Received response from host: affinity-clusterip-transition-2bp68
    Jan 30 11:57:20.080: INFO: Received response from host: affinity-clusterip-transition-2bp68
    Jan 30 11:57:20.080: INFO: Received response from host: affinity-clusterip-transition-2bp68
    Jan 30 11:57:20.080: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-3205, will wait for the garbage collector to delete the pods 01/30/23 11:57:20.084
    Jan 30 11:57:20.140: INFO: Deleting ReplicationController affinity-clusterip-transition took: 2.860803ms
    Jan 30 11:57:20.240: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 100.366434ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jan 30 11:57:22.946: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-3205" for this suite. 01/30/23 11:57:22.949
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates
  should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176
[BeforeEach] [sig-node] PodTemplates
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 11:57:22.951
Jan 30 11:57:22.951: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename podtemplate 01/30/23 11:57:22.952
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:57:22.958
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:57:22.96
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:31
[It] should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176
STEP: Create a pod template 01/30/23 11:57:22.962
STEP: Replace a pod template 01/30/23 11:57:22.965
Jan 30 11:57:22.969: INFO: Found updated podtemplate annotation: "true"

[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/node/init/init.go:32
Jan 30 11:57:22.969: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] PodTemplates
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] PodTemplates
  tear down framework | framework.go:193
STEP: Destroying namespace "podtemplate-1075" for this suite. 01/30/23 11:57:22.971
------------------------------
• [0.025 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 11:57:22.951
    Jan 30 11:57:22.951: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename podtemplate 01/30/23 11:57:22.952
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:57:22.958
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:57:22.96
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:31
    [It] should replace a pod template [Conformance]
      test/e2e/common/node/podtemplates.go:176
    STEP: Create a pod template 01/30/23 11:57:22.962
    STEP: Replace a pod template 01/30/23 11:57:22.965
    Jan 30 11:57:22.969: INFO: Found updated podtemplate annotation: "true"

    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/node/init/init.go:32
    Jan 30 11:57:22.969: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] PodTemplates
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] PodTemplates
      tear down framework | framework.go:193
    STEP: Destroying namespace "podtemplate-1075" for this suite. 01/30/23 11:57:22.971
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:125
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 11:57:22.977
Jan 30 11:57:22.977: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename secrets 01/30/23 11:57:22.978
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:57:22.983
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:57:22.985
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:125
STEP: Creating secret with name secret-test-d393a3a9-df0c-4b49-b6ea-497ef56a1a5c 01/30/23 11:57:22.987
STEP: Creating a pod to test consume secrets 01/30/23 11:57:22.989
Jan 30 11:57:22.993: INFO: Waiting up to 5m0s for pod "pod-secrets-75e3b6c4-eab2-418a-bc0a-6de704c498fd" in namespace "secrets-4533" to be "Succeeded or Failed"
Jan 30 11:57:22.994: INFO: Pod "pod-secrets-75e3b6c4-eab2-418a-bc0a-6de704c498fd": Phase="Pending", Reason="", readiness=false. Elapsed: 1.643097ms
Jan 30 11:57:24.997: INFO: Pod "pod-secrets-75e3b6c4-eab2-418a-bc0a-6de704c498fd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004282881s
Jan 30 11:57:26.998: INFO: Pod "pod-secrets-75e3b6c4-eab2-418a-bc0a-6de704c498fd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.005054646s
Jan 30 11:57:28.997: INFO: Pod "pod-secrets-75e3b6c4-eab2-418a-bc0a-6de704c498fd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.004355219s
STEP: Saw pod success 01/30/23 11:57:28.997
Jan 30 11:57:28.997: INFO: Pod "pod-secrets-75e3b6c4-eab2-418a-bc0a-6de704c498fd" satisfied condition "Succeeded or Failed"
Jan 30 11:57:28.999: INFO: Trying to get logs from node pubt2-nks-for-dev1.dg.163.org pod pod-secrets-75e3b6c4-eab2-418a-bc0a-6de704c498fd container secret-volume-test: <nil>
STEP: delete the pod 01/30/23 11:57:29.004
Jan 30 11:57:29.008: INFO: Waiting for pod pod-secrets-75e3b6c4-eab2-418a-bc0a-6de704c498fd to disappear
Jan 30 11:57:29.010: INFO: Pod pod-secrets-75e3b6c4-eab2-418a-bc0a-6de704c498fd no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Jan 30 11:57:29.010: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-4533" for this suite. 01/30/23 11:57:29.012
------------------------------
• [SLOW TEST] [6.037 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:125

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 11:57:22.977
    Jan 30 11:57:22.977: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename secrets 01/30/23 11:57:22.978
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:57:22.983
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:57:22.985
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:125
    STEP: Creating secret with name secret-test-d393a3a9-df0c-4b49-b6ea-497ef56a1a5c 01/30/23 11:57:22.987
    STEP: Creating a pod to test consume secrets 01/30/23 11:57:22.989
    Jan 30 11:57:22.993: INFO: Waiting up to 5m0s for pod "pod-secrets-75e3b6c4-eab2-418a-bc0a-6de704c498fd" in namespace "secrets-4533" to be "Succeeded or Failed"
    Jan 30 11:57:22.994: INFO: Pod "pod-secrets-75e3b6c4-eab2-418a-bc0a-6de704c498fd": Phase="Pending", Reason="", readiness=false. Elapsed: 1.643097ms
    Jan 30 11:57:24.997: INFO: Pod "pod-secrets-75e3b6c4-eab2-418a-bc0a-6de704c498fd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004282881s
    Jan 30 11:57:26.998: INFO: Pod "pod-secrets-75e3b6c4-eab2-418a-bc0a-6de704c498fd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.005054646s
    Jan 30 11:57:28.997: INFO: Pod "pod-secrets-75e3b6c4-eab2-418a-bc0a-6de704c498fd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.004355219s
    STEP: Saw pod success 01/30/23 11:57:28.997
    Jan 30 11:57:28.997: INFO: Pod "pod-secrets-75e3b6c4-eab2-418a-bc0a-6de704c498fd" satisfied condition "Succeeded or Failed"
    Jan 30 11:57:28.999: INFO: Trying to get logs from node pubt2-nks-for-dev1.dg.163.org pod pod-secrets-75e3b6c4-eab2-418a-bc0a-6de704c498fd container secret-volume-test: <nil>
    STEP: delete the pod 01/30/23 11:57:29.004
    Jan 30 11:57:29.008: INFO: Waiting for pod pod-secrets-75e3b6c4-eab2-418a-bc0a-6de704c498fd to disappear
    Jan 30 11:57:29.010: INFO: Pod pod-secrets-75e3b6c4-eab2-418a-bc0a-6de704c498fd no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Jan 30 11:57:29.010: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-4533" for this suite. 01/30/23 11:57:29.012
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:375
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 11:57:29.014
Jan 30 11:57:29.014: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename projected 01/30/23 11:57:29.015
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:57:29.021
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:57:29.023
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:375
STEP: Creating configMap with name projected-configmap-test-volume-44be92f2-28ef-499e-9e34-1b6d9e470796 01/30/23 11:57:29.025
STEP: Creating a pod to test consume configMaps 01/30/23 11:57:29.027
Jan 30 11:57:29.031: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-d72e6394-4575-4ac7-ad1e-8d4e2326ae6c" in namespace "projected-3747" to be "Succeeded or Failed"
Jan 30 11:57:29.032: INFO: Pod "pod-projected-configmaps-d72e6394-4575-4ac7-ad1e-8d4e2326ae6c": Phase="Pending", Reason="", readiness=false. Elapsed: 1.63692ms
Jan 30 11:57:31.035: INFO: Pod "pod-projected-configmaps-d72e6394-4575-4ac7-ad1e-8d4e2326ae6c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004603984s
Jan 30 11:57:33.035: INFO: Pod "pod-projected-configmaps-d72e6394-4575-4ac7-ad1e-8d4e2326ae6c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.004597933s
Jan 30 11:57:35.037: INFO: Pod "pod-projected-configmaps-d72e6394-4575-4ac7-ad1e-8d4e2326ae6c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.006101942s
STEP: Saw pod success 01/30/23 11:57:35.037
Jan 30 11:57:35.037: INFO: Pod "pod-projected-configmaps-d72e6394-4575-4ac7-ad1e-8d4e2326ae6c" satisfied condition "Succeeded or Failed"
Jan 30 11:57:35.038: INFO: Trying to get logs from node pubt2-nks-for-dev1.dg.163.org pod pod-projected-configmaps-d72e6394-4575-4ac7-ad1e-8d4e2326ae6c container projected-configmap-volume-test: <nil>
STEP: delete the pod 01/30/23 11:57:35.043
Jan 30 11:57:35.048: INFO: Waiting for pod pod-projected-configmaps-d72e6394-4575-4ac7-ad1e-8d4e2326ae6c to disappear
Jan 30 11:57:35.050: INFO: Pod pod-projected-configmaps-d72e6394-4575-4ac7-ad1e-8d4e2326ae6c no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Jan 30 11:57:35.050: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-3747" for this suite. 01/30/23 11:57:35.052
------------------------------
• [SLOW TEST] [6.040 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:375

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 11:57:29.014
    Jan 30 11:57:29.014: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename projected 01/30/23 11:57:29.015
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:57:29.021
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:57:29.023
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:375
    STEP: Creating configMap with name projected-configmap-test-volume-44be92f2-28ef-499e-9e34-1b6d9e470796 01/30/23 11:57:29.025
    STEP: Creating a pod to test consume configMaps 01/30/23 11:57:29.027
    Jan 30 11:57:29.031: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-d72e6394-4575-4ac7-ad1e-8d4e2326ae6c" in namespace "projected-3747" to be "Succeeded or Failed"
    Jan 30 11:57:29.032: INFO: Pod "pod-projected-configmaps-d72e6394-4575-4ac7-ad1e-8d4e2326ae6c": Phase="Pending", Reason="", readiness=false. Elapsed: 1.63692ms
    Jan 30 11:57:31.035: INFO: Pod "pod-projected-configmaps-d72e6394-4575-4ac7-ad1e-8d4e2326ae6c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004603984s
    Jan 30 11:57:33.035: INFO: Pod "pod-projected-configmaps-d72e6394-4575-4ac7-ad1e-8d4e2326ae6c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.004597933s
    Jan 30 11:57:35.037: INFO: Pod "pod-projected-configmaps-d72e6394-4575-4ac7-ad1e-8d4e2326ae6c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.006101942s
    STEP: Saw pod success 01/30/23 11:57:35.037
    Jan 30 11:57:35.037: INFO: Pod "pod-projected-configmaps-d72e6394-4575-4ac7-ad1e-8d4e2326ae6c" satisfied condition "Succeeded or Failed"
    Jan 30 11:57:35.038: INFO: Trying to get logs from node pubt2-nks-for-dev1.dg.163.org pod pod-projected-configmaps-d72e6394-4575-4ac7-ad1e-8d4e2326ae6c container projected-configmap-volume-test: <nil>
    STEP: delete the pod 01/30/23 11:57:35.043
    Jan 30 11:57:35.048: INFO: Waiting for pod pod-projected-configmaps-d72e6394-4575-4ac7-ad1e-8d4e2326ae6c to disappear
    Jan 30 11:57:35.050: INFO: Pod pod-projected-configmaps-d72e6394-4575-4ac7-ad1e-8d4e2326ae6c no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Jan 30 11:57:35.050: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-3747" for this suite. 01/30/23 11:57:35.052
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:268
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 11:57:35.054
Jan 30 11:57:35.054: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename namespaces 01/30/23 11:57:35.055
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:57:35.062
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:57:35.063
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:268
STEP: creating a Namespace 01/30/23 11:57:35.066
STEP: patching the Namespace 01/30/23 11:57:35.071
STEP: get the Namespace and ensuring it has the label 01/30/23 11:57:35.074
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Jan 30 11:57:35.075: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-8183" for this suite. 01/30/23 11:57:35.077
STEP: Destroying namespace "nspatchtest-4057308d-deed-4c0c-987c-617026388b19-5841" for this suite. 01/30/23 11:57:35.08
------------------------------
• [0.027 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:268

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 11:57:35.054
    Jan 30 11:57:35.054: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename namespaces 01/30/23 11:57:35.055
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:57:35.062
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:57:35.063
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should patch a Namespace [Conformance]
      test/e2e/apimachinery/namespace.go:268
    STEP: creating a Namespace 01/30/23 11:57:35.066
    STEP: patching the Namespace 01/30/23 11:57:35.071
    STEP: get the Namespace and ensuring it has the label 01/30/23 11:57:35.074
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan 30 11:57:35.075: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-8183" for this suite. 01/30/23 11:57:35.077
    STEP: Destroying namespace "nspatchtest-4057308d-deed-4c0c-987c-617026388b19-5841" for this suite. 01/30/23 11:57:35.08
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Ephemeral Containers [NodeConformance]
  will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 11:57:35.082
Jan 30 11:57:35.082: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename ephemeral-containers-test 01/30/23 11:57:35.083
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:57:35.088
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:57:35.09
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/common/node/ephemeral_containers.go:38
[It] will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45
STEP: creating a target pod 01/30/23 11:57:35.092
Jan 30 11:57:35.096: INFO: Waiting up to 5m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-3581" to be "running and ready"
Jan 30 11:57:35.097: INFO: Pod "ephemeral-containers-target-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 1.525687ms
Jan 30 11:57:35.097: INFO: The phase of Pod ephemeral-containers-target-pod is Pending, waiting for it to be Running (with Ready = true)
Jan 30 11:57:37.099: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.003595444s
Jan 30 11:57:37.099: INFO: The phase of Pod ephemeral-containers-target-pod is Running (Ready = true)
Jan 30 11:57:37.099: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "running and ready"
STEP: adding an ephemeral container 01/30/23 11:57:37.101
Jan 30 11:57:37.112: INFO: Waiting up to 1m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-3581" to be "container debugger running"
Jan 30 11:57:37.113: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 1.494908ms
Jan 30 11:57:39.116: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.004606751s
Jan 30 11:57:41.117: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.005412514s
Jan 30 11:57:41.117: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "container debugger running"
STEP: checking pod container endpoints 01/30/23 11:57:41.117
Jan 30 11:57:41.117: INFO: ExecWithOptions {Command:[/bin/echo marco] Namespace:ephemeral-containers-test-3581 PodName:ephemeral-containers-target-pod ContainerName:debugger Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 30 11:57:41.117: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
Jan 30 11:57:41.118: INFO: ExecWithOptions: Clientset creation
Jan 30 11:57:41.118: INFO: ExecWithOptions: execute(POST https://10.178.64.1:443/api/v1/namespaces/ephemeral-containers-test-3581/pods/ephemeral-containers-target-pod/exec?command=%2Fbin%2Fecho&command=marco&container=debugger&container=debugger&stderr=true&stdout=true)
Jan 30 11:57:41.661: INFO: Exec stderr: ""
[AfterEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Jan 30 11:57:41.666: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "ephemeral-containers-test-3581" for this suite. 01/30/23 11:57:41.668
------------------------------
• [SLOW TEST] [6.588 seconds]
[sig-node] Ephemeral Containers [NodeConformance]
test/e2e/common/node/framework.go:23
  will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 11:57:35.082
    Jan 30 11:57:35.082: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename ephemeral-containers-test 01/30/23 11:57:35.083
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:57:35.088
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:57:35.09
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/common/node/ephemeral_containers.go:38
    [It] will start an ephemeral container in an existing pod [Conformance]
      test/e2e/common/node/ephemeral_containers.go:45
    STEP: creating a target pod 01/30/23 11:57:35.092
    Jan 30 11:57:35.096: INFO: Waiting up to 5m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-3581" to be "running and ready"
    Jan 30 11:57:35.097: INFO: Pod "ephemeral-containers-target-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 1.525687ms
    Jan 30 11:57:35.097: INFO: The phase of Pod ephemeral-containers-target-pod is Pending, waiting for it to be Running (with Ready = true)
    Jan 30 11:57:37.099: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.003595444s
    Jan 30 11:57:37.099: INFO: The phase of Pod ephemeral-containers-target-pod is Running (Ready = true)
    Jan 30 11:57:37.099: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "running and ready"
    STEP: adding an ephemeral container 01/30/23 11:57:37.101
    Jan 30 11:57:37.112: INFO: Waiting up to 1m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-3581" to be "container debugger running"
    Jan 30 11:57:37.113: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 1.494908ms
    Jan 30 11:57:39.116: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.004606751s
    Jan 30 11:57:41.117: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.005412514s
    Jan 30 11:57:41.117: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "container debugger running"
    STEP: checking pod container endpoints 01/30/23 11:57:41.117
    Jan 30 11:57:41.117: INFO: ExecWithOptions {Command:[/bin/echo marco] Namespace:ephemeral-containers-test-3581 PodName:ephemeral-containers-target-pod ContainerName:debugger Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 30 11:57:41.117: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    Jan 30 11:57:41.118: INFO: ExecWithOptions: Clientset creation
    Jan 30 11:57:41.118: INFO: ExecWithOptions: execute(POST https://10.178.64.1:443/api/v1/namespaces/ephemeral-containers-test-3581/pods/ephemeral-containers-target-pod/exec?command=%2Fbin%2Fecho&command=marco&container=debugger&container=debugger&stderr=true&stdout=true)
    Jan 30 11:57:41.661: INFO: Exec stderr: ""
    [AfterEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Jan 30 11:57:41.666: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "ephemeral-containers-test-3581" for this suite. 01/30/23 11:57:41.668
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-cli] Kubectl client Kubectl logs
  should be able to retrieve and filter logs  [Conformance]
  test/e2e/kubectl/kubectl.go:1592
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 11:57:41.671
Jan 30 11:57:41.671: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename kubectl 01/30/23 11:57:41.671
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:57:41.677
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:57:41.679
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Kubectl logs
  test/e2e/kubectl/kubectl.go:1572
STEP: creating an pod 01/30/23 11:57:41.681
Jan 30 11:57:41.681: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=kubectl-2803 run logs-generator --image=harbor.cloud.netease.com/qzprod-k8s/e2e/agnhost:2.43 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
Jan 30 11:57:41.743: INFO: stderr: ""
Jan 30 11:57:41.743: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  test/e2e/kubectl/kubectl.go:1592
STEP: Waiting for log generator to start. 01/30/23 11:57:41.744
Jan 30 11:57:41.744: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Jan 30 11:57:41.744: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-2803" to be "running and ready, or succeeded"
Jan 30 11:57:41.746: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 1.890102ms
Jan 30 11:57:41.746: INFO: Error evaluating pod condition running and ready, or succeeded: want pod 'logs-generator' on 'pubt2-nks-for-dev1.dg.163.org' to be 'Running' but was 'Pending'
Jan 30 11:57:43.748: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003998809s
Jan 30 11:57:43.748: INFO: Error evaluating pod condition running and ready, or succeeded: want pod 'logs-generator' on 'pubt2-nks-for-dev1.dg.163.org' to be 'Running' but was 'Pending'
Jan 30 11:57:45.749: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 4.005307387s
Jan 30 11:57:45.749: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Jan 30 11:57:45.749: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings 01/30/23 11:57:45.749
Jan 30 11:57:45.749: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=kubectl-2803 logs logs-generator logs-generator'
Jan 30 11:57:45.814: INFO: stderr: ""
Jan 30 11:57:45.814: INFO: stdout: "I0130 11:57:43.419840       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/default/pods/5mp 580\nI0130 11:57:43.620199       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/default/pods/8p2m 502\nI0130 11:57:43.820487       1 logs_generator.go:76] 2 POST /api/v1/namespaces/kube-system/pods/w5jg 522\nI0130 11:57:44.020783       1 logs_generator.go:76] 3 GET /api/v1/namespaces/ns/pods/m4lr 456\nI0130 11:57:44.219995       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/kube-system/pods/8ktr 271\nI0130 11:57:44.420280       1 logs_generator.go:76] 5 GET /api/v1/namespaces/kube-system/pods/h2n 387\nI0130 11:57:44.620567       1 logs_generator.go:76] 6 POST /api/v1/namespaces/default/pods/5n9n 381\nI0130 11:57:44.820842       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/kube-system/pods/jzj7 361\nI0130 11:57:45.020130       1 logs_generator.go:76] 8 POST /api/v1/namespaces/ns/pods/648 328\nI0130 11:57:45.220416       1 logs_generator.go:76] 9 POST /api/v1/namespaces/default/pods/g998 326\nI0130 11:57:45.420698       1 logs_generator.go:76] 10 GET /api/v1/namespaces/default/pods/9rt 422\nI0130 11:57:45.619925       1 logs_generator.go:76] 11 PUT /api/v1/namespaces/kube-system/pods/jt2 438\n"
STEP: limiting log lines 01/30/23 11:57:45.814
Jan 30 11:57:45.814: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=kubectl-2803 logs logs-generator logs-generator --tail=1'
Jan 30 11:57:45.876: INFO: stderr: ""
Jan 30 11:57:45.876: INFO: stdout: "I0130 11:57:45.820206       1 logs_generator.go:76] 12 GET /api/v1/namespaces/default/pods/6w6s 467\n"
Jan 30 11:57:45.876: INFO: got output "I0130 11:57:45.820206       1 logs_generator.go:76] 12 GET /api/v1/namespaces/default/pods/6w6s 467\n"
STEP: limiting log bytes 01/30/23 11:57:45.876
Jan 30 11:57:45.876: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=kubectl-2803 logs logs-generator logs-generator --limit-bytes=1'
Jan 30 11:57:45.937: INFO: stderr: ""
Jan 30 11:57:45.937: INFO: stdout: "I"
Jan 30 11:57:45.937: INFO: got output "I"
STEP: exposing timestamps 01/30/23 11:57:45.937
Jan 30 11:57:45.937: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=kubectl-2803 logs logs-generator logs-generator --tail=1 --timestamps'
Jan 30 11:57:45.997: INFO: stderr: ""
Jan 30 11:57:45.997: INFO: stdout: "2023-01-30T11:57:45.820248195Z I0130 11:57:45.820206       1 logs_generator.go:76] 12 GET /api/v1/namespaces/default/pods/6w6s 467\n"
Jan 30 11:57:45.997: INFO: got output "2023-01-30T11:57:45.820248195Z I0130 11:57:45.820206       1 logs_generator.go:76] 12 GET /api/v1/namespaces/default/pods/6w6s 467\n"
STEP: restricting to a time range 01/30/23 11:57:45.997
Jan 30 11:57:48.498: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=kubectl-2803 logs logs-generator logs-generator --since=1s'
Jan 30 11:57:48.564: INFO: stderr: ""
Jan 30 11:57:48.564: INFO: stdout: "I0130 11:57:47.620697       1 logs_generator.go:76] 21 GET /api/v1/namespaces/kube-system/pods/266f 273\nI0130 11:57:47.819918       1 logs_generator.go:76] 22 GET /api/v1/namespaces/default/pods/c7qk 531\nI0130 11:57:48.020202       1 logs_generator.go:76] 23 POST /api/v1/namespaces/ns/pods/6m6 317\nI0130 11:57:48.220505       1 logs_generator.go:76] 24 GET /api/v1/namespaces/default/pods/4ghn 575\nI0130 11:57:48.420784       1 logs_generator.go:76] 25 PUT /api/v1/namespaces/kube-system/pods/gldq 548\n"
Jan 30 11:57:48.564: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=kubectl-2803 logs logs-generator logs-generator --since=24h'
Jan 30 11:57:48.627: INFO: stderr: ""
Jan 30 11:57:48.627: INFO: stdout: "I0130 11:57:43.419840       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/default/pods/5mp 580\nI0130 11:57:43.620199       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/default/pods/8p2m 502\nI0130 11:57:43.820487       1 logs_generator.go:76] 2 POST /api/v1/namespaces/kube-system/pods/w5jg 522\nI0130 11:57:44.020783       1 logs_generator.go:76] 3 GET /api/v1/namespaces/ns/pods/m4lr 456\nI0130 11:57:44.219995       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/kube-system/pods/8ktr 271\nI0130 11:57:44.420280       1 logs_generator.go:76] 5 GET /api/v1/namespaces/kube-system/pods/h2n 387\nI0130 11:57:44.620567       1 logs_generator.go:76] 6 POST /api/v1/namespaces/default/pods/5n9n 381\nI0130 11:57:44.820842       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/kube-system/pods/jzj7 361\nI0130 11:57:45.020130       1 logs_generator.go:76] 8 POST /api/v1/namespaces/ns/pods/648 328\nI0130 11:57:45.220416       1 logs_generator.go:76] 9 POST /api/v1/namespaces/default/pods/g998 326\nI0130 11:57:45.420698       1 logs_generator.go:76] 10 GET /api/v1/namespaces/default/pods/9rt 422\nI0130 11:57:45.619925       1 logs_generator.go:76] 11 PUT /api/v1/namespaces/kube-system/pods/jt2 438\nI0130 11:57:45.820206       1 logs_generator.go:76] 12 GET /api/v1/namespaces/default/pods/6w6s 467\nI0130 11:57:46.020490       1 logs_generator.go:76] 13 PUT /api/v1/namespaces/default/pods/4lqt 438\nI0130 11:57:46.220770       1 logs_generator.go:76] 14 GET /api/v1/namespaces/ns/pods/dq5s 461\nI0130 11:57:46.419991       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/kube-system/pods/nx24 513\nI0130 11:57:46.620277       1 logs_generator.go:76] 16 GET /api/v1/namespaces/kube-system/pods/lk6t 245\nI0130 11:57:46.820559       1 logs_generator.go:76] 17 GET /api/v1/namespaces/kube-system/pods/sspx 543\nI0130 11:57:47.020864       1 logs_generator.go:76] 18 PUT /api/v1/namespaces/default/pods/7m4n 331\nI0130 11:57:47.220144       1 logs_generator.go:76] 19 GET /api/v1/namespaces/default/pods/pzj 591\nI0130 11:57:47.420422       1 logs_generator.go:76] 20 POST /api/v1/namespaces/default/pods/n5cs 225\nI0130 11:57:47.620697       1 logs_generator.go:76] 21 GET /api/v1/namespaces/kube-system/pods/266f 273\nI0130 11:57:47.819918       1 logs_generator.go:76] 22 GET /api/v1/namespaces/default/pods/c7qk 531\nI0130 11:57:48.020202       1 logs_generator.go:76] 23 POST /api/v1/namespaces/ns/pods/6m6 317\nI0130 11:57:48.220505       1 logs_generator.go:76] 24 GET /api/v1/namespaces/default/pods/4ghn 575\nI0130 11:57:48.420784       1 logs_generator.go:76] 25 PUT /api/v1/namespaces/kube-system/pods/gldq 548\nI0130 11:57:48.620043       1 logs_generator.go:76] 26 POST /api/v1/namespaces/ns/pods/g45l 413\n"
[AfterEach] Kubectl logs
  test/e2e/kubectl/kubectl.go:1577
Jan 30 11:57:48.627: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=kubectl-2803 delete pod logs-generator'
Jan 30 11:57:49.500: INFO: stderr: ""
Jan 30 11:57:49.500: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jan 30 11:57:49.500: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-2803" for this suite. 01/30/23 11:57:49.502
------------------------------
• [SLOW TEST] [7.834 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl logs
  test/e2e/kubectl/kubectl.go:1569
    should be able to retrieve and filter logs  [Conformance]
    test/e2e/kubectl/kubectl.go:1592

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 11:57:41.671
    Jan 30 11:57:41.671: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename kubectl 01/30/23 11:57:41.671
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:57:41.677
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:57:41.679
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Kubectl logs
      test/e2e/kubectl/kubectl.go:1572
    STEP: creating an pod 01/30/23 11:57:41.681
    Jan 30 11:57:41.681: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=kubectl-2803 run logs-generator --image=harbor.cloud.netease.com/qzprod-k8s/e2e/agnhost:2.43 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
    Jan 30 11:57:41.743: INFO: stderr: ""
    Jan 30 11:57:41.743: INFO: stdout: "pod/logs-generator created\n"
    [It] should be able to retrieve and filter logs  [Conformance]
      test/e2e/kubectl/kubectl.go:1592
    STEP: Waiting for log generator to start. 01/30/23 11:57:41.744
    Jan 30 11:57:41.744: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
    Jan 30 11:57:41.744: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-2803" to be "running and ready, or succeeded"
    Jan 30 11:57:41.746: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 1.890102ms
    Jan 30 11:57:41.746: INFO: Error evaluating pod condition running and ready, or succeeded: want pod 'logs-generator' on 'pubt2-nks-for-dev1.dg.163.org' to be 'Running' but was 'Pending'
    Jan 30 11:57:43.748: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003998809s
    Jan 30 11:57:43.748: INFO: Error evaluating pod condition running and ready, or succeeded: want pod 'logs-generator' on 'pubt2-nks-for-dev1.dg.163.org' to be 'Running' but was 'Pending'
    Jan 30 11:57:45.749: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 4.005307387s
    Jan 30 11:57:45.749: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
    Jan 30 11:57:45.749: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
    STEP: checking for a matching strings 01/30/23 11:57:45.749
    Jan 30 11:57:45.749: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=kubectl-2803 logs logs-generator logs-generator'
    Jan 30 11:57:45.814: INFO: stderr: ""
    Jan 30 11:57:45.814: INFO: stdout: "I0130 11:57:43.419840       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/default/pods/5mp 580\nI0130 11:57:43.620199       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/default/pods/8p2m 502\nI0130 11:57:43.820487       1 logs_generator.go:76] 2 POST /api/v1/namespaces/kube-system/pods/w5jg 522\nI0130 11:57:44.020783       1 logs_generator.go:76] 3 GET /api/v1/namespaces/ns/pods/m4lr 456\nI0130 11:57:44.219995       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/kube-system/pods/8ktr 271\nI0130 11:57:44.420280       1 logs_generator.go:76] 5 GET /api/v1/namespaces/kube-system/pods/h2n 387\nI0130 11:57:44.620567       1 logs_generator.go:76] 6 POST /api/v1/namespaces/default/pods/5n9n 381\nI0130 11:57:44.820842       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/kube-system/pods/jzj7 361\nI0130 11:57:45.020130       1 logs_generator.go:76] 8 POST /api/v1/namespaces/ns/pods/648 328\nI0130 11:57:45.220416       1 logs_generator.go:76] 9 POST /api/v1/namespaces/default/pods/g998 326\nI0130 11:57:45.420698       1 logs_generator.go:76] 10 GET /api/v1/namespaces/default/pods/9rt 422\nI0130 11:57:45.619925       1 logs_generator.go:76] 11 PUT /api/v1/namespaces/kube-system/pods/jt2 438\n"
    STEP: limiting log lines 01/30/23 11:57:45.814
    Jan 30 11:57:45.814: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=kubectl-2803 logs logs-generator logs-generator --tail=1'
    Jan 30 11:57:45.876: INFO: stderr: ""
    Jan 30 11:57:45.876: INFO: stdout: "I0130 11:57:45.820206       1 logs_generator.go:76] 12 GET /api/v1/namespaces/default/pods/6w6s 467\n"
    Jan 30 11:57:45.876: INFO: got output "I0130 11:57:45.820206       1 logs_generator.go:76] 12 GET /api/v1/namespaces/default/pods/6w6s 467\n"
    STEP: limiting log bytes 01/30/23 11:57:45.876
    Jan 30 11:57:45.876: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=kubectl-2803 logs logs-generator logs-generator --limit-bytes=1'
    Jan 30 11:57:45.937: INFO: stderr: ""
    Jan 30 11:57:45.937: INFO: stdout: "I"
    Jan 30 11:57:45.937: INFO: got output "I"
    STEP: exposing timestamps 01/30/23 11:57:45.937
    Jan 30 11:57:45.937: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=kubectl-2803 logs logs-generator logs-generator --tail=1 --timestamps'
    Jan 30 11:57:45.997: INFO: stderr: ""
    Jan 30 11:57:45.997: INFO: stdout: "2023-01-30T11:57:45.820248195Z I0130 11:57:45.820206       1 logs_generator.go:76] 12 GET /api/v1/namespaces/default/pods/6w6s 467\n"
    Jan 30 11:57:45.997: INFO: got output "2023-01-30T11:57:45.820248195Z I0130 11:57:45.820206       1 logs_generator.go:76] 12 GET /api/v1/namespaces/default/pods/6w6s 467\n"
    STEP: restricting to a time range 01/30/23 11:57:45.997
    Jan 30 11:57:48.498: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=kubectl-2803 logs logs-generator logs-generator --since=1s'
    Jan 30 11:57:48.564: INFO: stderr: ""
    Jan 30 11:57:48.564: INFO: stdout: "I0130 11:57:47.620697       1 logs_generator.go:76] 21 GET /api/v1/namespaces/kube-system/pods/266f 273\nI0130 11:57:47.819918       1 logs_generator.go:76] 22 GET /api/v1/namespaces/default/pods/c7qk 531\nI0130 11:57:48.020202       1 logs_generator.go:76] 23 POST /api/v1/namespaces/ns/pods/6m6 317\nI0130 11:57:48.220505       1 logs_generator.go:76] 24 GET /api/v1/namespaces/default/pods/4ghn 575\nI0130 11:57:48.420784       1 logs_generator.go:76] 25 PUT /api/v1/namespaces/kube-system/pods/gldq 548\n"
    Jan 30 11:57:48.564: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=kubectl-2803 logs logs-generator logs-generator --since=24h'
    Jan 30 11:57:48.627: INFO: stderr: ""
    Jan 30 11:57:48.627: INFO: stdout: "I0130 11:57:43.419840       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/default/pods/5mp 580\nI0130 11:57:43.620199       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/default/pods/8p2m 502\nI0130 11:57:43.820487       1 logs_generator.go:76] 2 POST /api/v1/namespaces/kube-system/pods/w5jg 522\nI0130 11:57:44.020783       1 logs_generator.go:76] 3 GET /api/v1/namespaces/ns/pods/m4lr 456\nI0130 11:57:44.219995       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/kube-system/pods/8ktr 271\nI0130 11:57:44.420280       1 logs_generator.go:76] 5 GET /api/v1/namespaces/kube-system/pods/h2n 387\nI0130 11:57:44.620567       1 logs_generator.go:76] 6 POST /api/v1/namespaces/default/pods/5n9n 381\nI0130 11:57:44.820842       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/kube-system/pods/jzj7 361\nI0130 11:57:45.020130       1 logs_generator.go:76] 8 POST /api/v1/namespaces/ns/pods/648 328\nI0130 11:57:45.220416       1 logs_generator.go:76] 9 POST /api/v1/namespaces/default/pods/g998 326\nI0130 11:57:45.420698       1 logs_generator.go:76] 10 GET /api/v1/namespaces/default/pods/9rt 422\nI0130 11:57:45.619925       1 logs_generator.go:76] 11 PUT /api/v1/namespaces/kube-system/pods/jt2 438\nI0130 11:57:45.820206       1 logs_generator.go:76] 12 GET /api/v1/namespaces/default/pods/6w6s 467\nI0130 11:57:46.020490       1 logs_generator.go:76] 13 PUT /api/v1/namespaces/default/pods/4lqt 438\nI0130 11:57:46.220770       1 logs_generator.go:76] 14 GET /api/v1/namespaces/ns/pods/dq5s 461\nI0130 11:57:46.419991       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/kube-system/pods/nx24 513\nI0130 11:57:46.620277       1 logs_generator.go:76] 16 GET /api/v1/namespaces/kube-system/pods/lk6t 245\nI0130 11:57:46.820559       1 logs_generator.go:76] 17 GET /api/v1/namespaces/kube-system/pods/sspx 543\nI0130 11:57:47.020864       1 logs_generator.go:76] 18 PUT /api/v1/namespaces/default/pods/7m4n 331\nI0130 11:57:47.220144       1 logs_generator.go:76] 19 GET /api/v1/namespaces/default/pods/pzj 591\nI0130 11:57:47.420422       1 logs_generator.go:76] 20 POST /api/v1/namespaces/default/pods/n5cs 225\nI0130 11:57:47.620697       1 logs_generator.go:76] 21 GET /api/v1/namespaces/kube-system/pods/266f 273\nI0130 11:57:47.819918       1 logs_generator.go:76] 22 GET /api/v1/namespaces/default/pods/c7qk 531\nI0130 11:57:48.020202       1 logs_generator.go:76] 23 POST /api/v1/namespaces/ns/pods/6m6 317\nI0130 11:57:48.220505       1 logs_generator.go:76] 24 GET /api/v1/namespaces/default/pods/4ghn 575\nI0130 11:57:48.420784       1 logs_generator.go:76] 25 PUT /api/v1/namespaces/kube-system/pods/gldq 548\nI0130 11:57:48.620043       1 logs_generator.go:76] 26 POST /api/v1/namespaces/ns/pods/g45l 413\n"
    [AfterEach] Kubectl logs
      test/e2e/kubectl/kubectl.go:1577
    Jan 30 11:57:48.627: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=kubectl-2803 delete pod logs-generator'
    Jan 30 11:57:49.500: INFO: stderr: ""
    Jan 30 11:57:49.500: INFO: stdout: "pod \"logs-generator\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jan 30 11:57:49.500: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-2803" for this suite. 01/30/23 11:57:49.502
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:423
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 11:57:49.505
Jan 30 11:57:49.505: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename configmap 01/30/23 11:57:49.506
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:57:49.512
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:57:49.514
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:423
STEP: Creating configMap with name configmap-test-volume-43ce055b-17b3-43d1-949d-ce2223982386 01/30/23 11:57:49.516
STEP: Creating a pod to test consume configMaps 01/30/23 11:57:49.518
Jan 30 11:57:49.522: INFO: Waiting up to 5m0s for pod "pod-configmaps-1c376582-6249-480b-b62e-b2c3d0cb4602" in namespace "configmap-6707" to be "Succeeded or Failed"
Jan 30 11:57:49.523: INFO: Pod "pod-configmaps-1c376582-6249-480b-b62e-b2c3d0cb4602": Phase="Pending", Reason="", readiness=false. Elapsed: 1.610424ms
Jan 30 11:57:51.526: INFO: Pod "pod-configmaps-1c376582-6249-480b-b62e-b2c3d0cb4602": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003769704s
Jan 30 11:57:53.526: INFO: Pod "pod-configmaps-1c376582-6249-480b-b62e-b2c3d0cb4602": Phase="Pending", Reason="", readiness=false. Elapsed: 4.003921968s
Jan 30 11:57:55.527: INFO: Pod "pod-configmaps-1c376582-6249-480b-b62e-b2c3d0cb4602": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.004816251s
STEP: Saw pod success 01/30/23 11:57:55.527
Jan 30 11:57:55.527: INFO: Pod "pod-configmaps-1c376582-6249-480b-b62e-b2c3d0cb4602" satisfied condition "Succeeded or Failed"
Jan 30 11:57:55.528: INFO: Trying to get logs from node pubt2-nks-for-dev1.dg.163.org pod pod-configmaps-1c376582-6249-480b-b62e-b2c3d0cb4602 container configmap-volume-test: <nil>
STEP: delete the pod 01/30/23 11:57:55.533
Jan 30 11:57:55.537: INFO: Waiting for pod pod-configmaps-1c376582-6249-480b-b62e-b2c3d0cb4602 to disappear
Jan 30 11:57:55.538: INFO: Pod pod-configmaps-1c376582-6249-480b-b62e-b2c3d0cb4602 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jan 30 11:57:55.539: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-6707" for this suite. 01/30/23 11:57:55.541
------------------------------
• [SLOW TEST] [6.038 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:423

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 11:57:49.505
    Jan 30 11:57:49.505: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename configmap 01/30/23 11:57:49.506
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:57:49.512
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:57:49.514
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:423
    STEP: Creating configMap with name configmap-test-volume-43ce055b-17b3-43d1-949d-ce2223982386 01/30/23 11:57:49.516
    STEP: Creating a pod to test consume configMaps 01/30/23 11:57:49.518
    Jan 30 11:57:49.522: INFO: Waiting up to 5m0s for pod "pod-configmaps-1c376582-6249-480b-b62e-b2c3d0cb4602" in namespace "configmap-6707" to be "Succeeded or Failed"
    Jan 30 11:57:49.523: INFO: Pod "pod-configmaps-1c376582-6249-480b-b62e-b2c3d0cb4602": Phase="Pending", Reason="", readiness=false. Elapsed: 1.610424ms
    Jan 30 11:57:51.526: INFO: Pod "pod-configmaps-1c376582-6249-480b-b62e-b2c3d0cb4602": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003769704s
    Jan 30 11:57:53.526: INFO: Pod "pod-configmaps-1c376582-6249-480b-b62e-b2c3d0cb4602": Phase="Pending", Reason="", readiness=false. Elapsed: 4.003921968s
    Jan 30 11:57:55.527: INFO: Pod "pod-configmaps-1c376582-6249-480b-b62e-b2c3d0cb4602": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.004816251s
    STEP: Saw pod success 01/30/23 11:57:55.527
    Jan 30 11:57:55.527: INFO: Pod "pod-configmaps-1c376582-6249-480b-b62e-b2c3d0cb4602" satisfied condition "Succeeded or Failed"
    Jan 30 11:57:55.528: INFO: Trying to get logs from node pubt2-nks-for-dev1.dg.163.org pod pod-configmaps-1c376582-6249-480b-b62e-b2c3d0cb4602 container configmap-volume-test: <nil>
    STEP: delete the pod 01/30/23 11:57:55.533
    Jan 30 11:57:55.537: INFO: Waiting for pod pod-configmaps-1c376582-6249-480b-b62e-b2c3d0cb4602 to disappear
    Jan 30 11:57:55.538: INFO: Pod pod-configmaps-1c376582-6249-480b-b62e-b2c3d0cb4602 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jan 30 11:57:55.539: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-6707" for this suite. 01/30/23 11:57:55.541
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:413
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 11:57:55.543
Jan 30 11:57:55.543: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename webhook 01/30/23 11:57:55.544
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:57:55.549
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:57:55.551
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 01/30/23 11:57:55.559
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/30/23 11:57:55.866
STEP: Deploying the webhook pod 01/30/23 11:57:55.87
STEP: Wait for the deployment to be ready 01/30/23 11:57:55.875
Jan 30 11:57:55.878: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 01/30/23 11:57:57.884
STEP: Verifying the service has paired with the endpoint 01/30/23 11:57:57.888
Jan 30 11:57:58.888: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:413
STEP: Creating a validating webhook configuration 01/30/23 11:57:58.89
STEP: Creating a configMap that does not comply to the validation webhook rules 01/30/23 11:57:58.9
STEP: Updating a validating webhook configuration's rules to not include the create operation 01/30/23 11:57:58.906
STEP: Creating a configMap that does not comply to the validation webhook rules 01/30/23 11:57:58.912
STEP: Patching a validating webhook configuration's rules to include the create operation 01/30/23 11:57:58.916
STEP: Creating a configMap that does not comply to the validation webhook rules 01/30/23 11:57:58.921
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 30 11:57:58.927: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-62" for this suite. 01/30/23 11:57:58.942
STEP: Destroying namespace "webhook-62-markers" for this suite. 01/30/23 11:57:58.944
------------------------------
• [3.403 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:413

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 11:57:55.543
    Jan 30 11:57:55.543: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename webhook 01/30/23 11:57:55.544
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:57:55.549
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:57:55.551
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 01/30/23 11:57:55.559
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/30/23 11:57:55.866
    STEP: Deploying the webhook pod 01/30/23 11:57:55.87
    STEP: Wait for the deployment to be ready 01/30/23 11:57:55.875
    Jan 30 11:57:55.878: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 01/30/23 11:57:57.884
    STEP: Verifying the service has paired with the endpoint 01/30/23 11:57:57.888
    Jan 30 11:57:58.888: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] patching/updating a validating webhook should work [Conformance]
      test/e2e/apimachinery/webhook.go:413
    STEP: Creating a validating webhook configuration 01/30/23 11:57:58.89
    STEP: Creating a configMap that does not comply to the validation webhook rules 01/30/23 11:57:58.9
    STEP: Updating a validating webhook configuration's rules to not include the create operation 01/30/23 11:57:58.906
    STEP: Creating a configMap that does not comply to the validation webhook rules 01/30/23 11:57:58.912
    STEP: Patching a validating webhook configuration's rules to include the create operation 01/30/23 11:57:58.916
    STEP: Creating a configMap that does not comply to the validation webhook rules 01/30/23 11:57:58.921
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 30 11:57:58.927: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-62" for this suite. 01/30/23 11:57:58.942
    STEP: Destroying namespace "webhook-62-markers" for this suite. 01/30/23 11:57:58.944
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Projected downwardAPI
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:162
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 11:57:58.946
Jan 30 11:57:58.946: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename projected 01/30/23 11:57:58.947
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:57:58.953
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:57:58.955
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:162
STEP: Creating the pod 01/30/23 11:57:58.957
Jan 30 11:57:58.961: INFO: Waiting up to 5m0s for pod "annotationupdate44d43969-9cf1-497c-b74e-c2bff0bff09c" in namespace "projected-6471" to be "running and ready"
Jan 30 11:57:58.962: INFO: Pod "annotationupdate44d43969-9cf1-497c-b74e-c2bff0bff09c": Phase="Pending", Reason="", readiness=false. Elapsed: 1.486272ms
Jan 30 11:57:58.962: INFO: The phase of Pod annotationupdate44d43969-9cf1-497c-b74e-c2bff0bff09c is Pending, waiting for it to be Running (with Ready = true)
Jan 30 11:58:00.965: INFO: Pod "annotationupdate44d43969-9cf1-497c-b74e-c2bff0bff09c": Phase="Running", Reason="", readiness=true. Elapsed: 2.004546424s
Jan 30 11:58:00.965: INFO: The phase of Pod annotationupdate44d43969-9cf1-497c-b74e-c2bff0bff09c is Running (Ready = true)
Jan 30 11:58:00.965: INFO: Pod "annotationupdate44d43969-9cf1-497c-b74e-c2bff0bff09c" satisfied condition "running and ready"
Jan 30 11:58:01.479: INFO: Successfully updated pod "annotationupdate44d43969-9cf1-497c-b74e-c2bff0bff09c"
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Jan 30 11:58:03.490: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-6471" for this suite. 01/30/23 11:58:03.492
------------------------------
• [4.548 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:162

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 11:57:58.946
    Jan 30 11:57:58.946: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename projected 01/30/23 11:57:58.947
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:57:58.953
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:57:58.955
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should update annotations on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:162
    STEP: Creating the pod 01/30/23 11:57:58.957
    Jan 30 11:57:58.961: INFO: Waiting up to 5m0s for pod "annotationupdate44d43969-9cf1-497c-b74e-c2bff0bff09c" in namespace "projected-6471" to be "running and ready"
    Jan 30 11:57:58.962: INFO: Pod "annotationupdate44d43969-9cf1-497c-b74e-c2bff0bff09c": Phase="Pending", Reason="", readiness=false. Elapsed: 1.486272ms
    Jan 30 11:57:58.962: INFO: The phase of Pod annotationupdate44d43969-9cf1-497c-b74e-c2bff0bff09c is Pending, waiting for it to be Running (with Ready = true)
    Jan 30 11:58:00.965: INFO: Pod "annotationupdate44d43969-9cf1-497c-b74e-c2bff0bff09c": Phase="Running", Reason="", readiness=true. Elapsed: 2.004546424s
    Jan 30 11:58:00.965: INFO: The phase of Pod annotationupdate44d43969-9cf1-497c-b74e-c2bff0bff09c is Running (Ready = true)
    Jan 30 11:58:00.965: INFO: Pod "annotationupdate44d43969-9cf1-497c-b74e-c2bff0bff09c" satisfied condition "running and ready"
    Jan 30 11:58:01.479: INFO: Successfully updated pod "annotationupdate44d43969-9cf1-497c-b74e-c2bff0bff09c"
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Jan 30 11:58:03.490: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-6471" for this suite. 01/30/23 11:58:03.492
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling an agnhost Pod with hostAliases
  should write entries to /etc/hosts [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:148
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 11:58:03.496
Jan 30 11:58:03.496: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename kubelet-test 01/30/23 11:58:03.496
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:58:03.502
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:58:03.504
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should write entries to /etc/hosts [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:148
STEP: Waiting for pod completion 01/30/23 11:58:03.511
Jan 30 11:58:03.511: INFO: Waiting up to 3m0s for pod "agnhost-host-aliasesbcf2d2d9-68e4-4b73-9fad-73214fd6d531" in namespace "kubelet-test-2622" to be "completed"
Jan 30 11:58:03.512: INFO: Pod "agnhost-host-aliasesbcf2d2d9-68e4-4b73-9fad-73214fd6d531": Phase="Pending", Reason="", readiness=false. Elapsed: 1.556086ms
Jan 30 11:58:05.515: INFO: Pod "agnhost-host-aliasesbcf2d2d9-68e4-4b73-9fad-73214fd6d531": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004033473s
Jan 30 11:58:07.516: INFO: Pod "agnhost-host-aliasesbcf2d2d9-68e4-4b73-9fad-73214fd6d531": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005466271s
Jan 30 11:58:07.516: INFO: Pod "agnhost-host-aliasesbcf2d2d9-68e4-4b73-9fad-73214fd6d531" satisfied condition "completed"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Jan 30 11:58:07.521: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-2622" for this suite. 01/30/23 11:58:07.523
------------------------------
• [4.030 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling an agnhost Pod with hostAliases
  test/e2e/common/node/kubelet.go:140
    should write entries to /etc/hosts [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:148

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 11:58:03.496
    Jan 30 11:58:03.496: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename kubelet-test 01/30/23 11:58:03.496
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:58:03.502
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:58:03.504
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should write entries to /etc/hosts [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:148
    STEP: Waiting for pod completion 01/30/23 11:58:03.511
    Jan 30 11:58:03.511: INFO: Waiting up to 3m0s for pod "agnhost-host-aliasesbcf2d2d9-68e4-4b73-9fad-73214fd6d531" in namespace "kubelet-test-2622" to be "completed"
    Jan 30 11:58:03.512: INFO: Pod "agnhost-host-aliasesbcf2d2d9-68e4-4b73-9fad-73214fd6d531": Phase="Pending", Reason="", readiness=false. Elapsed: 1.556086ms
    Jan 30 11:58:05.515: INFO: Pod "agnhost-host-aliasesbcf2d2d9-68e4-4b73-9fad-73214fd6d531": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004033473s
    Jan 30 11:58:07.516: INFO: Pod "agnhost-host-aliasesbcf2d2d9-68e4-4b73-9fad-73214fd6d531": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005466271s
    Jan 30 11:58:07.516: INFO: Pod "agnhost-host-aliasesbcf2d2d9-68e4-4b73-9fad-73214fd6d531" satisfied condition "completed"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Jan 30 11:58:07.521: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-2622" for this suite. 01/30/23 11:58:07.523
  << End Captured GinkgoWriter Output
------------------------------
[sig-cli] Kubectl client Update Demo
  should scale a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:352
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 11:58:07.526
Jan 30 11:58:07.526: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename kubectl 01/30/23 11:58:07.527
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:58:07.533
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:58:07.534
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Update Demo
  test/e2e/kubectl/kubectl.go:326
[It] should scale a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:352
STEP: creating a replication controller 01/30/23 11:58:07.536
Jan 30 11:58:07.537: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=kubectl-7176 create -f -'
Jan 30 11:58:08.672: INFO: stderr: ""
Jan 30 11:58:08.672: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up. 01/30/23 11:58:08.672
Jan 30 11:58:08.672: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=kubectl-7176 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan 30 11:58:08.736: INFO: stderr: ""
Jan 30 11:58:08.736: INFO: stdout: "update-demo-nautilus-4bz2f update-demo-nautilus-qn2v5 "
Jan 30 11:58:08.736: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=kubectl-7176 get pods update-demo-nautilus-4bz2f -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 30 11:58:08.792: INFO: stderr: ""
Jan 30 11:58:08.792: INFO: stdout: ""
Jan 30 11:58:08.792: INFO: update-demo-nautilus-4bz2f is created but not running
Jan 30 11:58:13.793: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=kubectl-7176 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan 30 11:58:13.859: INFO: stderr: ""
Jan 30 11:58:13.859: INFO: stdout: "update-demo-nautilus-4bz2f update-demo-nautilus-qn2v5 "
Jan 30 11:58:13.859: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=kubectl-7176 get pods update-demo-nautilus-4bz2f -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 30 11:58:13.918: INFO: stderr: ""
Jan 30 11:58:13.918: INFO: stdout: ""
Jan 30 11:58:13.918: INFO: update-demo-nautilus-4bz2f is created but not running
Jan 30 11:58:18.918: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=kubectl-7176 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan 30 11:58:18.985: INFO: stderr: ""
Jan 30 11:58:18.985: INFO: stdout: "update-demo-nautilus-4bz2f update-demo-nautilus-qn2v5 "
Jan 30 11:58:18.985: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=kubectl-7176 get pods update-demo-nautilus-4bz2f -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 30 11:58:19.041: INFO: stderr: ""
Jan 30 11:58:19.041: INFO: stdout: "true"
Jan 30 11:58:19.041: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=kubectl-7176 get pods update-demo-nautilus-4bz2f -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan 30 11:58:19.098: INFO: stderr: ""
Jan 30 11:58:19.098: INFO: stdout: "harbor.cloud.netease.com/qzprod-k8s/e2e/nautilus:1.7"
Jan 30 11:58:19.098: INFO: validating pod update-demo-nautilus-4bz2f
Jan 30 11:58:19.100: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 30 11:58:19.101: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 30 11:58:19.101: INFO: update-demo-nautilus-4bz2f is verified up and running
Jan 30 11:58:19.101: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=kubectl-7176 get pods update-demo-nautilus-qn2v5 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 30 11:58:19.160: INFO: stderr: ""
Jan 30 11:58:19.160: INFO: stdout: "true"
Jan 30 11:58:19.160: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=kubectl-7176 get pods update-demo-nautilus-qn2v5 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan 30 11:58:19.217: INFO: stderr: ""
Jan 30 11:58:19.217: INFO: stdout: "harbor.cloud.netease.com/qzprod-k8s/e2e/nautilus:1.7"
Jan 30 11:58:19.217: INFO: validating pod update-demo-nautilus-qn2v5
Jan 30 11:58:19.220: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 30 11:58:19.220: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 30 11:58:19.220: INFO: update-demo-nautilus-qn2v5 is verified up and running
STEP: scaling down the replication controller 01/30/23 11:58:19.22
Jan 30 11:58:19.221: INFO: scanned /root for discovery docs: <nil>
Jan 30 11:58:19.221: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=kubectl-7176 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
Jan 30 11:58:20.287: INFO: stderr: ""
Jan 30 11:58:20.287: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up. 01/30/23 11:58:20.287
Jan 30 11:58:20.288: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=kubectl-7176 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan 30 11:58:20.347: INFO: stderr: ""
Jan 30 11:58:20.347: INFO: stdout: "update-demo-nautilus-4bz2f "
Jan 30 11:58:20.347: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=kubectl-7176 get pods update-demo-nautilus-4bz2f -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 30 11:58:20.404: INFO: stderr: ""
Jan 30 11:58:20.404: INFO: stdout: "true"
Jan 30 11:58:20.404: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=kubectl-7176 get pods update-demo-nautilus-4bz2f -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan 30 11:58:20.461: INFO: stderr: ""
Jan 30 11:58:20.461: INFO: stdout: "harbor.cloud.netease.com/qzprod-k8s/e2e/nautilus:1.7"
Jan 30 11:58:20.461: INFO: validating pod update-demo-nautilus-4bz2f
Jan 30 11:58:20.463: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 30 11:58:20.463: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 30 11:58:20.463: INFO: update-demo-nautilus-4bz2f is verified up and running
STEP: scaling up the replication controller 01/30/23 11:58:20.463
Jan 30 11:58:20.464: INFO: scanned /root for discovery docs: <nil>
Jan 30 11:58:20.464: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=kubectl-7176 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
Jan 30 11:58:21.533: INFO: stderr: ""
Jan 30 11:58:21.533: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up. 01/30/23 11:58:21.533
Jan 30 11:58:21.533: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=kubectl-7176 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan 30 11:58:21.596: INFO: stderr: ""
Jan 30 11:58:21.596: INFO: stdout: "update-demo-nautilus-4bz2f update-demo-nautilus-w2dz6 "
Jan 30 11:58:21.596: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=kubectl-7176 get pods update-demo-nautilus-4bz2f -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 30 11:58:21.653: INFO: stderr: ""
Jan 30 11:58:21.653: INFO: stdout: "true"
Jan 30 11:58:21.653: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=kubectl-7176 get pods update-demo-nautilus-4bz2f -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan 30 11:58:21.710: INFO: stderr: ""
Jan 30 11:58:21.710: INFO: stdout: "harbor.cloud.netease.com/qzprod-k8s/e2e/nautilus:1.7"
Jan 30 11:58:21.710: INFO: validating pod update-demo-nautilus-4bz2f
Jan 30 11:58:21.712: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 30 11:58:21.713: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 30 11:58:21.713: INFO: update-demo-nautilus-4bz2f is verified up and running
Jan 30 11:58:21.713: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=kubectl-7176 get pods update-demo-nautilus-w2dz6 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 30 11:58:21.769: INFO: stderr: ""
Jan 30 11:58:21.769: INFO: stdout: ""
Jan 30 11:58:21.769: INFO: update-demo-nautilus-w2dz6 is created but not running
Jan 30 11:58:26.769: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=kubectl-7176 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan 30 11:58:26.832: INFO: stderr: ""
Jan 30 11:58:26.832: INFO: stdout: "update-demo-nautilus-4bz2f update-demo-nautilus-w2dz6 "
Jan 30 11:58:26.832: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=kubectl-7176 get pods update-demo-nautilus-4bz2f -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 30 11:58:26.889: INFO: stderr: ""
Jan 30 11:58:26.889: INFO: stdout: "true"
Jan 30 11:58:26.889: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=kubectl-7176 get pods update-demo-nautilus-4bz2f -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan 30 11:58:26.947: INFO: stderr: ""
Jan 30 11:58:26.947: INFO: stdout: "harbor.cloud.netease.com/qzprod-k8s/e2e/nautilus:1.7"
Jan 30 11:58:26.947: INFO: validating pod update-demo-nautilus-4bz2f
Jan 30 11:58:26.949: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 30 11:58:26.949: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 30 11:58:26.949: INFO: update-demo-nautilus-4bz2f is verified up and running
Jan 30 11:58:26.949: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=kubectl-7176 get pods update-demo-nautilus-w2dz6 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 30 11:58:27.010: INFO: stderr: ""
Jan 30 11:58:27.010: INFO: stdout: "true"
Jan 30 11:58:27.010: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=kubectl-7176 get pods update-demo-nautilus-w2dz6 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan 30 11:58:27.068: INFO: stderr: ""
Jan 30 11:58:27.068: INFO: stdout: "harbor.cloud.netease.com/qzprod-k8s/e2e/nautilus:1.7"
Jan 30 11:58:27.068: INFO: validating pod update-demo-nautilus-w2dz6
Jan 30 11:58:27.070: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 30 11:58:27.070: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 30 11:58:27.070: INFO: update-demo-nautilus-w2dz6 is verified up and running
STEP: using delete to clean up resources 01/30/23 11:58:27.07
Jan 30 11:58:27.071: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=kubectl-7176 delete --grace-period=0 --force -f -'
Jan 30 11:58:27.131: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 30 11:58:27.131: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Jan 30 11:58:27.131: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=kubectl-7176 get rc,svc -l name=update-demo --no-headers'
Jan 30 11:58:27.195: INFO: stderr: "No resources found in kubectl-7176 namespace.\n"
Jan 30 11:58:27.195: INFO: stdout: ""
Jan 30 11:58:27.195: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=kubectl-7176 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jan 30 11:58:27.257: INFO: stderr: ""
Jan 30 11:58:27.257: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jan 30 11:58:27.257: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-7176" for this suite. 01/30/23 11:58:27.259
------------------------------
• [SLOW TEST] [19.736 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Update Demo
  test/e2e/kubectl/kubectl.go:324
    should scale a replication controller  [Conformance]
    test/e2e/kubectl/kubectl.go:352

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 11:58:07.526
    Jan 30 11:58:07.526: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename kubectl 01/30/23 11:58:07.527
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:58:07.533
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:58:07.534
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Update Demo
      test/e2e/kubectl/kubectl.go:326
    [It] should scale a replication controller  [Conformance]
      test/e2e/kubectl/kubectl.go:352
    STEP: creating a replication controller 01/30/23 11:58:07.536
    Jan 30 11:58:07.537: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=kubectl-7176 create -f -'
    Jan 30 11:58:08.672: INFO: stderr: ""
    Jan 30 11:58:08.672: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 01/30/23 11:58:08.672
    Jan 30 11:58:08.672: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=kubectl-7176 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jan 30 11:58:08.736: INFO: stderr: ""
    Jan 30 11:58:08.736: INFO: stdout: "update-demo-nautilus-4bz2f update-demo-nautilus-qn2v5 "
    Jan 30 11:58:08.736: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=kubectl-7176 get pods update-demo-nautilus-4bz2f -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan 30 11:58:08.792: INFO: stderr: ""
    Jan 30 11:58:08.792: INFO: stdout: ""
    Jan 30 11:58:08.792: INFO: update-demo-nautilus-4bz2f is created but not running
    Jan 30 11:58:13.793: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=kubectl-7176 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jan 30 11:58:13.859: INFO: stderr: ""
    Jan 30 11:58:13.859: INFO: stdout: "update-demo-nautilus-4bz2f update-demo-nautilus-qn2v5 "
    Jan 30 11:58:13.859: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=kubectl-7176 get pods update-demo-nautilus-4bz2f -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan 30 11:58:13.918: INFO: stderr: ""
    Jan 30 11:58:13.918: INFO: stdout: ""
    Jan 30 11:58:13.918: INFO: update-demo-nautilus-4bz2f is created but not running
    Jan 30 11:58:18.918: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=kubectl-7176 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jan 30 11:58:18.985: INFO: stderr: ""
    Jan 30 11:58:18.985: INFO: stdout: "update-demo-nautilus-4bz2f update-demo-nautilus-qn2v5 "
    Jan 30 11:58:18.985: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=kubectl-7176 get pods update-demo-nautilus-4bz2f -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan 30 11:58:19.041: INFO: stderr: ""
    Jan 30 11:58:19.041: INFO: stdout: "true"
    Jan 30 11:58:19.041: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=kubectl-7176 get pods update-demo-nautilus-4bz2f -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jan 30 11:58:19.098: INFO: stderr: ""
    Jan 30 11:58:19.098: INFO: stdout: "harbor.cloud.netease.com/qzprod-k8s/e2e/nautilus:1.7"
    Jan 30 11:58:19.098: INFO: validating pod update-demo-nautilus-4bz2f
    Jan 30 11:58:19.100: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jan 30 11:58:19.101: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jan 30 11:58:19.101: INFO: update-demo-nautilus-4bz2f is verified up and running
    Jan 30 11:58:19.101: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=kubectl-7176 get pods update-demo-nautilus-qn2v5 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan 30 11:58:19.160: INFO: stderr: ""
    Jan 30 11:58:19.160: INFO: stdout: "true"
    Jan 30 11:58:19.160: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=kubectl-7176 get pods update-demo-nautilus-qn2v5 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jan 30 11:58:19.217: INFO: stderr: ""
    Jan 30 11:58:19.217: INFO: stdout: "harbor.cloud.netease.com/qzprod-k8s/e2e/nautilus:1.7"
    Jan 30 11:58:19.217: INFO: validating pod update-demo-nautilus-qn2v5
    Jan 30 11:58:19.220: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jan 30 11:58:19.220: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jan 30 11:58:19.220: INFO: update-demo-nautilus-qn2v5 is verified up and running
    STEP: scaling down the replication controller 01/30/23 11:58:19.22
    Jan 30 11:58:19.221: INFO: scanned /root for discovery docs: <nil>
    Jan 30 11:58:19.221: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=kubectl-7176 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
    Jan 30 11:58:20.287: INFO: stderr: ""
    Jan 30 11:58:20.287: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 01/30/23 11:58:20.287
    Jan 30 11:58:20.288: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=kubectl-7176 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jan 30 11:58:20.347: INFO: stderr: ""
    Jan 30 11:58:20.347: INFO: stdout: "update-demo-nautilus-4bz2f "
    Jan 30 11:58:20.347: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=kubectl-7176 get pods update-demo-nautilus-4bz2f -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan 30 11:58:20.404: INFO: stderr: ""
    Jan 30 11:58:20.404: INFO: stdout: "true"
    Jan 30 11:58:20.404: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=kubectl-7176 get pods update-demo-nautilus-4bz2f -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jan 30 11:58:20.461: INFO: stderr: ""
    Jan 30 11:58:20.461: INFO: stdout: "harbor.cloud.netease.com/qzprod-k8s/e2e/nautilus:1.7"
    Jan 30 11:58:20.461: INFO: validating pod update-demo-nautilus-4bz2f
    Jan 30 11:58:20.463: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jan 30 11:58:20.463: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jan 30 11:58:20.463: INFO: update-demo-nautilus-4bz2f is verified up and running
    STEP: scaling up the replication controller 01/30/23 11:58:20.463
    Jan 30 11:58:20.464: INFO: scanned /root for discovery docs: <nil>
    Jan 30 11:58:20.464: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=kubectl-7176 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
    Jan 30 11:58:21.533: INFO: stderr: ""
    Jan 30 11:58:21.533: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 01/30/23 11:58:21.533
    Jan 30 11:58:21.533: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=kubectl-7176 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jan 30 11:58:21.596: INFO: stderr: ""
    Jan 30 11:58:21.596: INFO: stdout: "update-demo-nautilus-4bz2f update-demo-nautilus-w2dz6 "
    Jan 30 11:58:21.596: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=kubectl-7176 get pods update-demo-nautilus-4bz2f -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan 30 11:58:21.653: INFO: stderr: ""
    Jan 30 11:58:21.653: INFO: stdout: "true"
    Jan 30 11:58:21.653: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=kubectl-7176 get pods update-demo-nautilus-4bz2f -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jan 30 11:58:21.710: INFO: stderr: ""
    Jan 30 11:58:21.710: INFO: stdout: "harbor.cloud.netease.com/qzprod-k8s/e2e/nautilus:1.7"
    Jan 30 11:58:21.710: INFO: validating pod update-demo-nautilus-4bz2f
    Jan 30 11:58:21.712: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jan 30 11:58:21.713: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jan 30 11:58:21.713: INFO: update-demo-nautilus-4bz2f is verified up and running
    Jan 30 11:58:21.713: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=kubectl-7176 get pods update-demo-nautilus-w2dz6 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan 30 11:58:21.769: INFO: stderr: ""
    Jan 30 11:58:21.769: INFO: stdout: ""
    Jan 30 11:58:21.769: INFO: update-demo-nautilus-w2dz6 is created but not running
    Jan 30 11:58:26.769: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=kubectl-7176 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jan 30 11:58:26.832: INFO: stderr: ""
    Jan 30 11:58:26.832: INFO: stdout: "update-demo-nautilus-4bz2f update-demo-nautilus-w2dz6 "
    Jan 30 11:58:26.832: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=kubectl-7176 get pods update-demo-nautilus-4bz2f -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan 30 11:58:26.889: INFO: stderr: ""
    Jan 30 11:58:26.889: INFO: stdout: "true"
    Jan 30 11:58:26.889: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=kubectl-7176 get pods update-demo-nautilus-4bz2f -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jan 30 11:58:26.947: INFO: stderr: ""
    Jan 30 11:58:26.947: INFO: stdout: "harbor.cloud.netease.com/qzprod-k8s/e2e/nautilus:1.7"
    Jan 30 11:58:26.947: INFO: validating pod update-demo-nautilus-4bz2f
    Jan 30 11:58:26.949: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jan 30 11:58:26.949: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jan 30 11:58:26.949: INFO: update-demo-nautilus-4bz2f is verified up and running
    Jan 30 11:58:26.949: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=kubectl-7176 get pods update-demo-nautilus-w2dz6 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan 30 11:58:27.010: INFO: stderr: ""
    Jan 30 11:58:27.010: INFO: stdout: "true"
    Jan 30 11:58:27.010: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=kubectl-7176 get pods update-demo-nautilus-w2dz6 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jan 30 11:58:27.068: INFO: stderr: ""
    Jan 30 11:58:27.068: INFO: stdout: "harbor.cloud.netease.com/qzprod-k8s/e2e/nautilus:1.7"
    Jan 30 11:58:27.068: INFO: validating pod update-demo-nautilus-w2dz6
    Jan 30 11:58:27.070: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jan 30 11:58:27.070: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jan 30 11:58:27.070: INFO: update-demo-nautilus-w2dz6 is verified up and running
    STEP: using delete to clean up resources 01/30/23 11:58:27.07
    Jan 30 11:58:27.071: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=kubectl-7176 delete --grace-period=0 --force -f -'
    Jan 30 11:58:27.131: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jan 30 11:58:27.131: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
    Jan 30 11:58:27.131: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=kubectl-7176 get rc,svc -l name=update-demo --no-headers'
    Jan 30 11:58:27.195: INFO: stderr: "No resources found in kubectl-7176 namespace.\n"
    Jan 30 11:58:27.195: INFO: stdout: ""
    Jan 30 11:58:27.195: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=kubectl-7176 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Jan 30 11:58:27.257: INFO: stderr: ""
    Jan 30 11:58:27.257: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jan 30 11:58:27.257: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-7176" for this suite. 01/30/23 11:58:27.259
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Aggregator
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100
[BeforeEach] [sig-api-machinery] Aggregator
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 11:58:27.263
Jan 30 11:58:27.264: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename aggregator 01/30/23 11:58:27.264
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:58:27.27
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:58:27.272
[BeforeEach] [sig-api-machinery] Aggregator
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] Aggregator
  test/e2e/apimachinery/aggregator.go:78
Jan 30 11:58:27.274: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
[It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100
STEP: Registering the sample API server. 01/30/23 11:58:27.275
Jan 30 11:58:27.562: INFO: new replicaset for deployment "sample-apiserver-deployment" is yet to be created
Jan 30 11:58:29.582: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 30, 11, 58, 27, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 30, 11, 58, 27, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 30, 11, 58, 27, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 30, 11, 58, 27, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-75996699d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 30 11:58:31.585: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 30, 11, 58, 27, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 30, 11, 58, 27, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 30, 11, 58, 27, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 30, 11, 58, 27, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-75996699d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 30 11:58:33.584: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 30, 11, 58, 27, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 30, 11, 58, 27, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 30, 11, 58, 27, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 30, 11, 58, 27, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-75996699d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 30 11:58:35.584: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 30, 11, 58, 27, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 30, 11, 58, 27, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 30, 11, 58, 27, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 30, 11, 58, 27, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-75996699d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 30 11:58:37.584: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 30, 11, 58, 27, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 30, 11, 58, 27, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 30, 11, 58, 27, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 30, 11, 58, 27, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-75996699d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 30 11:58:39.584: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 30, 11, 58, 27, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 30, 11, 58, 27, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 30, 11, 58, 27, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 30, 11, 58, 27, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-75996699d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 30 11:58:41.584: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 30, 11, 58, 27, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 30, 11, 58, 27, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 30, 11, 58, 27, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 30, 11, 58, 27, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-75996699d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 30 11:58:43.584: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 30, 11, 58, 27, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 30, 11, 58, 27, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 30, 11, 58, 27, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 30, 11, 58, 27, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-75996699d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 30 11:58:45.586: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 30, 11, 58, 27, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 30, 11, 58, 27, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 30, 11, 58, 27, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 30, 11, 58, 27, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-75996699d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 30 11:58:47.585: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 30, 11, 58, 27, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 30, 11, 58, 27, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 30, 11, 58, 27, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 30, 11, 58, 27, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-75996699d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 30 11:58:49.585: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 30, 11, 58, 27, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 30, 11, 58, 27, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 30, 11, 58, 27, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 30, 11, 58, 27, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-75996699d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 30 11:58:51.585: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 30, 11, 58, 27, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 30, 11, 58, 27, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 30, 11, 58, 27, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 30, 11, 58, 27, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-75996699d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 30 11:58:53.701: INFO: Waited 113.982314ms for the sample-apiserver to be ready to handle requests.
STEP: Read Status for v1alpha1.wardle.example.com 01/30/23 11:58:53.728
STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}' 01/30/23 11:58:53.73
STEP: List APIServices 01/30/23 11:58:53.733
Jan 30 11:58:53.739: INFO: Found v1alpha1.wardle.example.com in APIServiceList
[AfterEach] [sig-api-machinery] Aggregator
  test/e2e/apimachinery/aggregator.go:68
[AfterEach] [sig-api-machinery] Aggregator
  test/e2e/framework/node/init/init.go:32
Jan 30 11:58:53.841: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Aggregator
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Aggregator
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Aggregator
  tear down framework | framework.go:193
STEP: Destroying namespace "aggregator-5840" for this suite. 01/30/23 11:58:53.893
------------------------------
• [SLOW TEST] [26.679 seconds]
[sig-api-machinery] Aggregator
test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Aggregator
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 11:58:27.263
    Jan 30 11:58:27.264: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename aggregator 01/30/23 11:58:27.264
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:58:27.27
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:58:27.272
    [BeforeEach] [sig-api-machinery] Aggregator
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] Aggregator
      test/e2e/apimachinery/aggregator.go:78
    Jan 30 11:58:27.274: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    [It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
      test/e2e/apimachinery/aggregator.go:100
    STEP: Registering the sample API server. 01/30/23 11:58:27.275
    Jan 30 11:58:27.562: INFO: new replicaset for deployment "sample-apiserver-deployment" is yet to be created
    Jan 30 11:58:29.582: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 30, 11, 58, 27, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 30, 11, 58, 27, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 30, 11, 58, 27, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 30, 11, 58, 27, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-75996699d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 30 11:58:31.585: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 30, 11, 58, 27, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 30, 11, 58, 27, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 30, 11, 58, 27, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 30, 11, 58, 27, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-75996699d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 30 11:58:33.584: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 30, 11, 58, 27, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 30, 11, 58, 27, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 30, 11, 58, 27, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 30, 11, 58, 27, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-75996699d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 30 11:58:35.584: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 30, 11, 58, 27, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 30, 11, 58, 27, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 30, 11, 58, 27, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 30, 11, 58, 27, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-75996699d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 30 11:58:37.584: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 30, 11, 58, 27, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 30, 11, 58, 27, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 30, 11, 58, 27, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 30, 11, 58, 27, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-75996699d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 30 11:58:39.584: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 30, 11, 58, 27, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 30, 11, 58, 27, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 30, 11, 58, 27, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 30, 11, 58, 27, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-75996699d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 30 11:58:41.584: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 30, 11, 58, 27, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 30, 11, 58, 27, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 30, 11, 58, 27, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 30, 11, 58, 27, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-75996699d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 30 11:58:43.584: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 30, 11, 58, 27, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 30, 11, 58, 27, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 30, 11, 58, 27, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 30, 11, 58, 27, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-75996699d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 30 11:58:45.586: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 30, 11, 58, 27, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 30, 11, 58, 27, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 30, 11, 58, 27, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 30, 11, 58, 27, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-75996699d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 30 11:58:47.585: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 30, 11, 58, 27, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 30, 11, 58, 27, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 30, 11, 58, 27, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 30, 11, 58, 27, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-75996699d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 30 11:58:49.585: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 30, 11, 58, 27, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 30, 11, 58, 27, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 30, 11, 58, 27, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 30, 11, 58, 27, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-75996699d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 30 11:58:51.585: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 30, 11, 58, 27, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 30, 11, 58, 27, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 30, 11, 58, 27, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 30, 11, 58, 27, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-75996699d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 30 11:58:53.701: INFO: Waited 113.982314ms for the sample-apiserver to be ready to handle requests.
    STEP: Read Status for v1alpha1.wardle.example.com 01/30/23 11:58:53.728
    STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}' 01/30/23 11:58:53.73
    STEP: List APIServices 01/30/23 11:58:53.733
    Jan 30 11:58:53.739: INFO: Found v1alpha1.wardle.example.com in APIServiceList
    [AfterEach] [sig-api-machinery] Aggregator
      test/e2e/apimachinery/aggregator.go:68
    [AfterEach] [sig-api-machinery] Aggregator
      test/e2e/framework/node/init/init.go:32
    Jan 30 11:58:53.841: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Aggregator
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Aggregator
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Aggregator
      tear down framework | framework.go:193
    STEP: Destroying namespace "aggregator-5840" for this suite. 01/30/23 11:58:53.893
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:79
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 11:58:53.944
Jan 30 11:58:53.944: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename secrets 01/30/23 11:58:53.945
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:58:53.951
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:58:53.953
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:79
STEP: Creating secret with name secret-test-map-6b230d9d-9ba3-484f-b1dd-e63bf5ba3d3f 01/30/23 11:58:53.955
STEP: Creating a pod to test consume secrets 01/30/23 11:58:53.957
Jan 30 11:58:53.962: INFO: Waiting up to 5m0s for pod "pod-secrets-7db27595-abad-40fe-b144-84090e1b2001" in namespace "secrets-914" to be "Succeeded or Failed"
Jan 30 11:58:53.963: INFO: Pod "pod-secrets-7db27595-abad-40fe-b144-84090e1b2001": Phase="Pending", Reason="", readiness=false. Elapsed: 1.611ms
Jan 30 11:58:55.965: INFO: Pod "pod-secrets-7db27595-abad-40fe-b144-84090e1b2001": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003681558s
Jan 30 11:58:57.965: INFO: Pod "pod-secrets-7db27595-abad-40fe-b144-84090e1b2001": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003588955s
STEP: Saw pod success 01/30/23 11:58:57.965
Jan 30 11:58:57.965: INFO: Pod "pod-secrets-7db27595-abad-40fe-b144-84090e1b2001" satisfied condition "Succeeded or Failed"
Jan 30 11:58:57.967: INFO: Trying to get logs from node pubt2-nks-for-dev1.dg.163.org pod pod-secrets-7db27595-abad-40fe-b144-84090e1b2001 container secret-volume-test: <nil>
STEP: delete the pod 01/30/23 11:58:57.976
Jan 30 11:58:57.980: INFO: Waiting for pod pod-secrets-7db27595-abad-40fe-b144-84090e1b2001 to disappear
Jan 30 11:58:57.982: INFO: Pod pod-secrets-7db27595-abad-40fe-b144-84090e1b2001 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Jan 30 11:58:57.982: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-914" for this suite. 01/30/23 11:58:57.984
------------------------------
• [4.042 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:79

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 11:58:53.944
    Jan 30 11:58:53.944: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename secrets 01/30/23 11:58:53.945
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:58:53.951
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:58:53.953
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:79
    STEP: Creating secret with name secret-test-map-6b230d9d-9ba3-484f-b1dd-e63bf5ba3d3f 01/30/23 11:58:53.955
    STEP: Creating a pod to test consume secrets 01/30/23 11:58:53.957
    Jan 30 11:58:53.962: INFO: Waiting up to 5m0s for pod "pod-secrets-7db27595-abad-40fe-b144-84090e1b2001" in namespace "secrets-914" to be "Succeeded or Failed"
    Jan 30 11:58:53.963: INFO: Pod "pod-secrets-7db27595-abad-40fe-b144-84090e1b2001": Phase="Pending", Reason="", readiness=false. Elapsed: 1.611ms
    Jan 30 11:58:55.965: INFO: Pod "pod-secrets-7db27595-abad-40fe-b144-84090e1b2001": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003681558s
    Jan 30 11:58:57.965: INFO: Pod "pod-secrets-7db27595-abad-40fe-b144-84090e1b2001": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003588955s
    STEP: Saw pod success 01/30/23 11:58:57.965
    Jan 30 11:58:57.965: INFO: Pod "pod-secrets-7db27595-abad-40fe-b144-84090e1b2001" satisfied condition "Succeeded or Failed"
    Jan 30 11:58:57.967: INFO: Trying to get logs from node pubt2-nks-for-dev1.dg.163.org pod pod-secrets-7db27595-abad-40fe-b144-84090e1b2001 container secret-volume-test: <nil>
    STEP: delete the pod 01/30/23 11:58:57.976
    Jan 30 11:58:57.980: INFO: Waiting for pod pod-secrets-7db27595-abad-40fe-b144-84090e1b2001 to disappear
    Jan 30 11:58:57.982: INFO: Pod pod-secrets-7db27595-abad-40fe-b144-84090e1b2001 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Jan 30 11:58:57.982: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-914" for this suite. 01/30/23 11:58:57.984
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] Daemon set [Serial]
  should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:294
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 11:58:57.986
Jan 30 11:58:57.986: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename daemonsets 01/30/23 11:58:57.987
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:58:57.993
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:58:57.994
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:294
STEP: Creating a simple DaemonSet "daemon-set" 01/30/23 11:58:58.004
STEP: Check that daemon pods launch on every node of the cluster. 01/30/23 11:58:58.007
Jan 30 11:58:58.010: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 30 11:58:58.010: INFO: Node pubt2-nks-for-dev1.dg.163.org is running 0 daemon pod, expected 1
Jan 30 11:58:59.015: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 30 11:58:59.015: INFO: Node pubt2-nks-for-dev1.dg.163.org is running 0 daemon pod, expected 1
Jan 30 11:59:00.016: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan 30 11:59:00.016: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. 01/30/23 11:59:00.017
Jan 30 11:59:00.028: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 30 11:59:00.028: INFO: Node pubt2-nks-for-dev3.dg.163.org is running 0 daemon pod, expected 1
Jan 30 11:59:01.034: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 30 11:59:01.034: INFO: Node pubt2-nks-for-dev3.dg.163.org is running 0 daemon pod, expected 1
Jan 30 11:59:02.033: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan 30 11:59:02.033: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
STEP: Wait for the failed daemon pod to be completely deleted. 01/30/23 11:59:02.033
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 01/30/23 11:59:02.036
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6903, will wait for the garbage collector to delete the pods 01/30/23 11:59:02.036
Jan 30 11:59:02.091: INFO: Deleting DaemonSet.extensions daemon-set took: 2.454396ms
Jan 30 11:59:02.191: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.764217ms
Jan 30 11:59:04.394: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 30 11:59:04.394: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jan 30 11:59:04.395: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"26905"},"items":null}

Jan 30 11:59:04.397: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"26905"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Jan 30 11:59:04.402: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-6903" for this suite. 01/30/23 11:59:04.404
------------------------------
• [SLOW TEST] [6.420 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:294

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 11:58:57.986
    Jan 30 11:58:57.986: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename daemonsets 01/30/23 11:58:57.987
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:58:57.993
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:58:57.994
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should retry creating failed daemon pods [Conformance]
      test/e2e/apps/daemon_set.go:294
    STEP: Creating a simple DaemonSet "daemon-set" 01/30/23 11:58:58.004
    STEP: Check that daemon pods launch on every node of the cluster. 01/30/23 11:58:58.007
    Jan 30 11:58:58.010: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 30 11:58:58.010: INFO: Node pubt2-nks-for-dev1.dg.163.org is running 0 daemon pod, expected 1
    Jan 30 11:58:59.015: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 30 11:58:59.015: INFO: Node pubt2-nks-for-dev1.dg.163.org is running 0 daemon pod, expected 1
    Jan 30 11:59:00.016: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jan 30 11:59:00.016: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. 01/30/23 11:59:00.017
    Jan 30 11:59:00.028: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan 30 11:59:00.028: INFO: Node pubt2-nks-for-dev3.dg.163.org is running 0 daemon pod, expected 1
    Jan 30 11:59:01.034: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan 30 11:59:01.034: INFO: Node pubt2-nks-for-dev3.dg.163.org is running 0 daemon pod, expected 1
    Jan 30 11:59:02.033: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jan 30 11:59:02.033: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    STEP: Wait for the failed daemon pod to be completely deleted. 01/30/23 11:59:02.033
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 01/30/23 11:59:02.036
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6903, will wait for the garbage collector to delete the pods 01/30/23 11:59:02.036
    Jan 30 11:59:02.091: INFO: Deleting DaemonSet.extensions daemon-set took: 2.454396ms
    Jan 30 11:59:02.191: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.764217ms
    Jan 30 11:59:04.394: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 30 11:59:04.394: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Jan 30 11:59:04.395: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"26905"},"items":null}

    Jan 30 11:59:04.397: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"26905"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan 30 11:59:04.402: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-6903" for this suite. 01/30/23 11:59:04.404
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:47
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 11:59:04.407
Jan 30 11:59:04.407: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename configmap 01/30/23 11:59:04.407
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:59:04.413
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:59:04.415
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:47
STEP: Creating configMap with name configmap-test-volume-0aa48722-fe9f-4451-90f8-d9e09b102440 01/30/23 11:59:04.417
STEP: Creating a pod to test consume configMaps 01/30/23 11:59:04.419
Jan 30 11:59:04.423: INFO: Waiting up to 5m0s for pod "pod-configmaps-26e38baa-a0ac-49e0-9532-bb0c5c1ee7ad" in namespace "configmap-9060" to be "Succeeded or Failed"
Jan 30 11:59:04.424: INFO: Pod "pod-configmaps-26e38baa-a0ac-49e0-9532-bb0c5c1ee7ad": Phase="Pending", Reason="", readiness=false. Elapsed: 1.505076ms
Jan 30 11:59:06.427: INFO: Pod "pod-configmaps-26e38baa-a0ac-49e0-9532-bb0c5c1ee7ad": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004378789s
Jan 30 11:59:08.428: INFO: Pod "pod-configmaps-26e38baa-a0ac-49e0-9532-bb0c5c1ee7ad": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005087015s
STEP: Saw pod success 01/30/23 11:59:08.428
Jan 30 11:59:08.428: INFO: Pod "pod-configmaps-26e38baa-a0ac-49e0-9532-bb0c5c1ee7ad" satisfied condition "Succeeded or Failed"
Jan 30 11:59:08.430: INFO: Trying to get logs from node pubt2-nks-for-dev1.dg.163.org pod pod-configmaps-26e38baa-a0ac-49e0-9532-bb0c5c1ee7ad container agnhost-container: <nil>
STEP: delete the pod 01/30/23 11:59:08.435
Jan 30 11:59:08.439: INFO: Waiting for pod pod-configmaps-26e38baa-a0ac-49e0-9532-bb0c5c1ee7ad to disappear
Jan 30 11:59:08.441: INFO: Pod pod-configmaps-26e38baa-a0ac-49e0-9532-bb0c5c1ee7ad no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jan 30 11:59:08.441: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-9060" for this suite. 01/30/23 11:59:08.443
------------------------------
• [4.039 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:47

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 11:59:04.407
    Jan 30 11:59:04.407: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename configmap 01/30/23 11:59:04.407
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:59:04.413
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:59:04.415
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:47
    STEP: Creating configMap with name configmap-test-volume-0aa48722-fe9f-4451-90f8-d9e09b102440 01/30/23 11:59:04.417
    STEP: Creating a pod to test consume configMaps 01/30/23 11:59:04.419
    Jan 30 11:59:04.423: INFO: Waiting up to 5m0s for pod "pod-configmaps-26e38baa-a0ac-49e0-9532-bb0c5c1ee7ad" in namespace "configmap-9060" to be "Succeeded or Failed"
    Jan 30 11:59:04.424: INFO: Pod "pod-configmaps-26e38baa-a0ac-49e0-9532-bb0c5c1ee7ad": Phase="Pending", Reason="", readiness=false. Elapsed: 1.505076ms
    Jan 30 11:59:06.427: INFO: Pod "pod-configmaps-26e38baa-a0ac-49e0-9532-bb0c5c1ee7ad": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004378789s
    Jan 30 11:59:08.428: INFO: Pod "pod-configmaps-26e38baa-a0ac-49e0-9532-bb0c5c1ee7ad": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005087015s
    STEP: Saw pod success 01/30/23 11:59:08.428
    Jan 30 11:59:08.428: INFO: Pod "pod-configmaps-26e38baa-a0ac-49e0-9532-bb0c5c1ee7ad" satisfied condition "Succeeded or Failed"
    Jan 30 11:59:08.430: INFO: Trying to get logs from node pubt2-nks-for-dev1.dg.163.org pod pod-configmaps-26e38baa-a0ac-49e0-9532-bb0c5c1ee7ad container agnhost-container: <nil>
    STEP: delete the pod 01/30/23 11:59:08.435
    Jan 30 11:59:08.439: INFO: Waiting for pod pod-configmaps-26e38baa-a0ac-49e0-9532-bb0c5c1ee7ad to disappear
    Jan 30 11:59:08.441: INFO: Pod pod-configmaps-26e38baa-a0ac-49e0-9532-bb0c5c1ee7ad no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jan 30 11:59:08.441: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-9060" for this suite. 01/30/23 11:59:08.443
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-apps] Daemon set [Serial]
  should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:432
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 11:59:08.446
Jan 30 11:59:08.446: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename daemonsets 01/30/23 11:59:08.446
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:59:08.452
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:59:08.454
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:432
Jan 30 11:59:08.466: INFO: Create a RollingUpdate DaemonSet
Jan 30 11:59:08.468: INFO: Check that daemon pods launch on every node of the cluster
Jan 30 11:59:08.487: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 30 11:59:08.487: INFO: Node pubt2-nks-for-dev1.dg.163.org is running 0 daemon pod, expected 1
Jan 30 11:59:09.493: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 30 11:59:09.493: INFO: Node pubt2-nks-for-dev1.dg.163.org is running 0 daemon pod, expected 1
Jan 30 11:59:10.493: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan 30 11:59:10.493: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
Jan 30 11:59:10.493: INFO: Update the DaemonSet to trigger a rollout
Jan 30 11:59:10.498: INFO: Updating DaemonSet daemon-set
Jan 30 11:59:14.507: INFO: Roll back the DaemonSet before rollout is complete
Jan 30 11:59:14.512: INFO: Updating DaemonSet daemon-set
Jan 30 11:59:14.512: INFO: Make sure DaemonSet rollback is complete
Jan 30 11:59:14.513: INFO: Wrong image for pod: daemon-set-rl8ws. Expected: harbor.cloud.netease.com/qzprod-k8s/e2e/httpd:2.4.38-4, got: foo:non-existent.
Jan 30 11:59:14.513: INFO: Pod daemon-set-rl8ws is not available
Jan 30 11:59:21.518: INFO: Pod daemon-set-k7ql8 is not available
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 01/30/23 11:59:21.524
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5835, will wait for the garbage collector to delete the pods 01/30/23 11:59:21.525
Jan 30 11:59:21.580: INFO: Deleting DaemonSet.extensions daemon-set took: 2.703456ms
Jan 30 11:59:21.680: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.358233ms
Jan 30 11:59:23.782: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 30 11:59:23.782: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jan 30 11:59:23.784: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"27103"},"items":null}

Jan 30 11:59:23.785: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"27103"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Jan 30 11:59:23.791: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-5835" for this suite. 01/30/23 11:59:23.793
------------------------------
• [SLOW TEST] [15.349 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:432

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 11:59:08.446
    Jan 30 11:59:08.446: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename daemonsets 01/30/23 11:59:08.446
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:59:08.452
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:59:08.454
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should rollback without unnecessary restarts [Conformance]
      test/e2e/apps/daemon_set.go:432
    Jan 30 11:59:08.466: INFO: Create a RollingUpdate DaemonSet
    Jan 30 11:59:08.468: INFO: Check that daemon pods launch on every node of the cluster
    Jan 30 11:59:08.487: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 30 11:59:08.487: INFO: Node pubt2-nks-for-dev1.dg.163.org is running 0 daemon pod, expected 1
    Jan 30 11:59:09.493: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 30 11:59:09.493: INFO: Node pubt2-nks-for-dev1.dg.163.org is running 0 daemon pod, expected 1
    Jan 30 11:59:10.493: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jan 30 11:59:10.493: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    Jan 30 11:59:10.493: INFO: Update the DaemonSet to trigger a rollout
    Jan 30 11:59:10.498: INFO: Updating DaemonSet daemon-set
    Jan 30 11:59:14.507: INFO: Roll back the DaemonSet before rollout is complete
    Jan 30 11:59:14.512: INFO: Updating DaemonSet daemon-set
    Jan 30 11:59:14.512: INFO: Make sure DaemonSet rollback is complete
    Jan 30 11:59:14.513: INFO: Wrong image for pod: daemon-set-rl8ws. Expected: harbor.cloud.netease.com/qzprod-k8s/e2e/httpd:2.4.38-4, got: foo:non-existent.
    Jan 30 11:59:14.513: INFO: Pod daemon-set-rl8ws is not available
    Jan 30 11:59:21.518: INFO: Pod daemon-set-k7ql8 is not available
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 01/30/23 11:59:21.524
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5835, will wait for the garbage collector to delete the pods 01/30/23 11:59:21.525
    Jan 30 11:59:21.580: INFO: Deleting DaemonSet.extensions daemon-set took: 2.703456ms
    Jan 30 11:59:21.680: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.358233ms
    Jan 30 11:59:23.782: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 30 11:59:23.782: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Jan 30 11:59:23.784: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"27103"},"items":null}

    Jan 30 11:59:23.785: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"27103"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan 30 11:59:23.791: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-5835" for this suite. 01/30/23 11:59:23.793
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:135
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 11:59:23.796
Jan 30 11:59:23.796: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename container-probe 01/30/23 11:59:23.797
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:59:23.803
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:59:23.805
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:135
STEP: Creating pod busybox-33364634-621a-40d6-914b-d918c549eed1 in namespace container-probe-2854 01/30/23 11:59:23.807
Jan 30 11:59:23.810: INFO: Waiting up to 5m0s for pod "busybox-33364634-621a-40d6-914b-d918c549eed1" in namespace "container-probe-2854" to be "not pending"
Jan 30 11:59:23.812: INFO: Pod "busybox-33364634-621a-40d6-914b-d918c549eed1": Phase="Pending", Reason="", readiness=false. Elapsed: 1.525991ms
Jan 30 11:59:25.815: INFO: Pod "busybox-33364634-621a-40d6-914b-d918c549eed1": Phase="Running", Reason="", readiness=true. Elapsed: 2.004571485s
Jan 30 11:59:25.815: INFO: Pod "busybox-33364634-621a-40d6-914b-d918c549eed1" satisfied condition "not pending"
Jan 30 11:59:25.815: INFO: Started pod busybox-33364634-621a-40d6-914b-d918c549eed1 in namespace container-probe-2854
STEP: checking the pod's current state and verifying that restartCount is present 01/30/23 11:59:25.815
Jan 30 11:59:25.817: INFO: Initial restart count of pod busybox-33364634-621a-40d6-914b-d918c549eed1 is 0
Jan 30 12:00:15.892: INFO: Restart count of pod container-probe-2854/busybox-33364634-621a-40d6-914b-d918c549eed1 is now 1 (50.075207503s elapsed)
STEP: deleting the pod 01/30/23 12:00:15.892
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Jan 30 12:00:15.896: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-2854" for this suite. 01/30/23 12:00:15.898
------------------------------
• [SLOW TEST] [52.105 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:135

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 11:59:23.796
    Jan 30 11:59:23.796: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename container-probe 01/30/23 11:59:23.797
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 11:59:23.803
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 11:59:23.805
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:135
    STEP: Creating pod busybox-33364634-621a-40d6-914b-d918c549eed1 in namespace container-probe-2854 01/30/23 11:59:23.807
    Jan 30 11:59:23.810: INFO: Waiting up to 5m0s for pod "busybox-33364634-621a-40d6-914b-d918c549eed1" in namespace "container-probe-2854" to be "not pending"
    Jan 30 11:59:23.812: INFO: Pod "busybox-33364634-621a-40d6-914b-d918c549eed1": Phase="Pending", Reason="", readiness=false. Elapsed: 1.525991ms
    Jan 30 11:59:25.815: INFO: Pod "busybox-33364634-621a-40d6-914b-d918c549eed1": Phase="Running", Reason="", readiness=true. Elapsed: 2.004571485s
    Jan 30 11:59:25.815: INFO: Pod "busybox-33364634-621a-40d6-914b-d918c549eed1" satisfied condition "not pending"
    Jan 30 11:59:25.815: INFO: Started pod busybox-33364634-621a-40d6-914b-d918c549eed1 in namespace container-probe-2854
    STEP: checking the pod's current state and verifying that restartCount is present 01/30/23 11:59:25.815
    Jan 30 11:59:25.817: INFO: Initial restart count of pod busybox-33364634-621a-40d6-914b-d918c549eed1 is 0
    Jan 30 12:00:15.892: INFO: Restart count of pod container-probe-2854/busybox-33364634-621a-40d6-914b-d918c549eed1 is now 1 (50.075207503s elapsed)
    STEP: deleting the pod 01/30/23 12:00:15.892
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Jan 30 12:00:15.896: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-2854" for this suite. 01/30/23 12:00:15.898
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:117
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 12:00:15.901
Jan 30 12:00:15.901: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename emptydir 01/30/23 12:00:15.902
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:00:15.908
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:00:15.91
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:117
STEP: Creating a pod to test emptydir 0777 on tmpfs 01/30/23 12:00:15.912
Jan 30 12:00:15.916: INFO: Waiting up to 5m0s for pod "pod-1d998ce6-338c-458d-8d6a-f275122302e5" in namespace "emptydir-7826" to be "Succeeded or Failed"
Jan 30 12:00:15.918: INFO: Pod "pod-1d998ce6-338c-458d-8d6a-f275122302e5": Phase="Pending", Reason="", readiness=false. Elapsed: 1.488799ms
Jan 30 12:00:17.920: INFO: Pod "pod-1d998ce6-338c-458d-8d6a-f275122302e5": Phase="Running", Reason="", readiness=true. Elapsed: 2.003437151s
Jan 30 12:00:19.922: INFO: Pod "pod-1d998ce6-338c-458d-8d6a-f275122302e5": Phase="Running", Reason="", readiness=false. Elapsed: 4.005550034s
Jan 30 12:00:21.922: INFO: Pod "pod-1d998ce6-338c-458d-8d6a-f275122302e5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.005824563s
STEP: Saw pod success 01/30/23 12:00:21.922
Jan 30 12:00:21.922: INFO: Pod "pod-1d998ce6-338c-458d-8d6a-f275122302e5" satisfied condition "Succeeded or Failed"
Jan 30 12:00:21.924: INFO: Trying to get logs from node pubt2-nks-for-dev1.dg.163.org pod pod-1d998ce6-338c-458d-8d6a-f275122302e5 container test-container: <nil>
STEP: delete the pod 01/30/23 12:00:21.928
Jan 30 12:00:21.933: INFO: Waiting for pod pod-1d998ce6-338c-458d-8d6a-f275122302e5 to disappear
Jan 30 12:00:21.934: INFO: Pod pod-1d998ce6-338c-458d-8d6a-f275122302e5 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jan 30 12:00:21.934: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-7826" for this suite. 01/30/23 12:00:21.936
------------------------------
• [SLOW TEST] [6.037 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:117

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 12:00:15.901
    Jan 30 12:00:15.901: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename emptydir 01/30/23 12:00:15.902
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:00:15.908
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:00:15.91
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:117
    STEP: Creating a pod to test emptydir 0777 on tmpfs 01/30/23 12:00:15.912
    Jan 30 12:00:15.916: INFO: Waiting up to 5m0s for pod "pod-1d998ce6-338c-458d-8d6a-f275122302e5" in namespace "emptydir-7826" to be "Succeeded or Failed"
    Jan 30 12:00:15.918: INFO: Pod "pod-1d998ce6-338c-458d-8d6a-f275122302e5": Phase="Pending", Reason="", readiness=false. Elapsed: 1.488799ms
    Jan 30 12:00:17.920: INFO: Pod "pod-1d998ce6-338c-458d-8d6a-f275122302e5": Phase="Running", Reason="", readiness=true. Elapsed: 2.003437151s
    Jan 30 12:00:19.922: INFO: Pod "pod-1d998ce6-338c-458d-8d6a-f275122302e5": Phase="Running", Reason="", readiness=false. Elapsed: 4.005550034s
    Jan 30 12:00:21.922: INFO: Pod "pod-1d998ce6-338c-458d-8d6a-f275122302e5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.005824563s
    STEP: Saw pod success 01/30/23 12:00:21.922
    Jan 30 12:00:21.922: INFO: Pod "pod-1d998ce6-338c-458d-8d6a-f275122302e5" satisfied condition "Succeeded or Failed"
    Jan 30 12:00:21.924: INFO: Trying to get logs from node pubt2-nks-for-dev1.dg.163.org pod pod-1d998ce6-338c-458d-8d6a-f275122302e5 container test-container: <nil>
    STEP: delete the pod 01/30/23 12:00:21.928
    Jan 30 12:00:21.933: INFO: Waiting for pod pod-1d998ce6-338c-458d-8d6a-f275122302e5 to disappear
    Jan 30 12:00:21.934: INFO: Pod pod-1d998ce6-338c-458d-8d6a-f275122302e5 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jan 30 12:00:21.934: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-7826" for this suite. 01/30/23 12:00:21.936
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Security Context when creating containers with AllowPrivilegeEscalation
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:609
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 12:00:21.939
Jan 30 12:00:21.939: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename security-context-test 01/30/23 12:00:21.94
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:00:21.945
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:00:21.947
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:50
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:609
Jan 30 12:00:21.953: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-041eded1-6f9a-401c-afbf-88657245013f" in namespace "security-context-test-2629" to be "Succeeded or Failed"
Jan 30 12:00:21.955: INFO: Pod "alpine-nnp-false-041eded1-6f9a-401c-afbf-88657245013f": Phase="Pending", Reason="", readiness=false. Elapsed: 1.653303ms
Jan 30 12:00:23.958: INFO: Pod "alpine-nnp-false-041eded1-6f9a-401c-afbf-88657245013f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00461024s
Jan 30 12:00:25.957: INFO: Pod "alpine-nnp-false-041eded1-6f9a-401c-afbf-88657245013f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.00441075s
Jan 30 12:00:27.958: INFO: Pod "alpine-nnp-false-041eded1-6f9a-401c-afbf-88657245013f": Phase="Pending", Reason="", readiness=false. Elapsed: 6.004468805s
Jan 30 12:00:29.958: INFO: Pod "alpine-nnp-false-041eded1-6f9a-401c-afbf-88657245013f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.004922999s
Jan 30 12:00:29.958: INFO: Pod "alpine-nnp-false-041eded1-6f9a-401c-afbf-88657245013f" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Jan 30 12:00:29.963: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-test-2629" for this suite. 01/30/23 12:00:29.965
------------------------------
• [SLOW TEST] [8.028 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  when creating containers with AllowPrivilegeEscalation
  test/e2e/common/node/security_context.go:555
    should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:609

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 12:00:21.939
    Jan 30 12:00:21.939: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename security-context-test 01/30/23 12:00:21.94
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:00:21.945
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:00:21.947
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:50
    [It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:609
    Jan 30 12:00:21.953: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-041eded1-6f9a-401c-afbf-88657245013f" in namespace "security-context-test-2629" to be "Succeeded or Failed"
    Jan 30 12:00:21.955: INFO: Pod "alpine-nnp-false-041eded1-6f9a-401c-afbf-88657245013f": Phase="Pending", Reason="", readiness=false. Elapsed: 1.653303ms
    Jan 30 12:00:23.958: INFO: Pod "alpine-nnp-false-041eded1-6f9a-401c-afbf-88657245013f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00461024s
    Jan 30 12:00:25.957: INFO: Pod "alpine-nnp-false-041eded1-6f9a-401c-afbf-88657245013f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.00441075s
    Jan 30 12:00:27.958: INFO: Pod "alpine-nnp-false-041eded1-6f9a-401c-afbf-88657245013f": Phase="Pending", Reason="", readiness=false. Elapsed: 6.004468805s
    Jan 30 12:00:29.958: INFO: Pod "alpine-nnp-false-041eded1-6f9a-401c-afbf-88657245013f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.004922999s
    Jan 30 12:00:29.958: INFO: Pod "alpine-nnp-false-041eded1-6f9a-401c-afbf-88657245013f" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Jan 30 12:00:29.963: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-test-2629" for this suite. 01/30/23 12:00:29.965
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:37
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 12:00:29.968
Jan 30 12:00:29.968: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename sysctl 01/30/23 12:00:29.969
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:00:29.975
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:00:29.977
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:67
[It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123
STEP: Creating a pod with one valid and two invalid sysctls 01/30/23 12:00:29.979
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Jan 30 12:00:29.983: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "sysctl-2298" for this suite. 01/30/23 12:00:29.985
------------------------------
• [0.019 seconds]
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
test/e2e/common/node/framework.go:23
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:37
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 12:00:29.968
    Jan 30 12:00:29.968: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename sysctl 01/30/23 12:00:29.969
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:00:29.975
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:00:29.977
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:67
    [It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
      test/e2e/common/node/sysctl.go:123
    STEP: Creating a pod with one valid and two invalid sysctls 01/30/23 12:00:29.979
    [AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Jan 30 12:00:29.983: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sysctl-2298" for this suite. 01/30/23 12:00:29.985
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] server version
  should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39
[BeforeEach] [sig-api-machinery] server version
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 12:00:29.988
Jan 30 12:00:29.988: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename server-version 01/30/23 12:00:29.988
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:00:29.993
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:00:29.995
[BeforeEach] [sig-api-machinery] server version
  test/e2e/framework/metrics/init/init.go:31
[It] should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39
STEP: Request ServerVersion 01/30/23 12:00:29.997
STEP: Confirm major version 01/30/23 12:00:29.998
Jan 30 12:00:29.998: INFO: Major version: 1
STEP: Confirm minor version 01/30/23 12:00:29.998
Jan 30 12:00:29.998: INFO: cleanMinorVersion: 26
Jan 30 12:00:29.998: INFO: Minor version: 26+
[AfterEach] [sig-api-machinery] server version
  test/e2e/framework/node/init/init.go:32
Jan 30 12:00:29.998: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] server version
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] server version
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] server version
  tear down framework | framework.go:193
STEP: Destroying namespace "server-version-2793" for this suite. 01/30/23 12:00:30
------------------------------
• [0.015 seconds]
[sig-api-machinery] server version
test/e2e/apimachinery/framework.go:23
  should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] server version
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 12:00:29.988
    Jan 30 12:00:29.988: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename server-version 01/30/23 12:00:29.988
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:00:29.993
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:00:29.995
    [BeforeEach] [sig-api-machinery] server version
      test/e2e/framework/metrics/init/init.go:31
    [It] should find the server version [Conformance]
      test/e2e/apimachinery/server_version.go:39
    STEP: Request ServerVersion 01/30/23 12:00:29.997
    STEP: Confirm major version 01/30/23 12:00:29.998
    Jan 30 12:00:29.998: INFO: Major version: 1
    STEP: Confirm minor version 01/30/23 12:00:29.998
    Jan 30 12:00:29.998: INFO: cleanMinorVersion: 26
    Jan 30 12:00:29.998: INFO: Minor version: 26+
    [AfterEach] [sig-api-machinery] server version
      test/e2e/framework/node/init/init.go:32
    Jan 30 12:00:29.998: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] server version
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] server version
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] server version
      tear down framework | framework.go:193
    STEP: Destroying namespace "server-version-2793" for this suite. 01/30/23 12:00:30
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:74
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 12:00:30.003
Jan 30 12:00:30.003: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename configmap 01/30/23 12:00:30.003
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:00:30.009
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:00:30.011
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:74
STEP: Creating configMap with name configmap-test-volume-485740ad-fcfe-477c-bec5-c867ec4e0aac 01/30/23 12:00:30.012
STEP: Creating a pod to test consume configMaps 01/30/23 12:00:30.014
Jan 30 12:00:30.018: INFO: Waiting up to 5m0s for pod "pod-configmaps-a765ad6d-0f74-4e75-9097-e938bffd556f" in namespace "configmap-6961" to be "Succeeded or Failed"
Jan 30 12:00:30.020: INFO: Pod "pod-configmaps-a765ad6d-0f74-4e75-9097-e938bffd556f": Phase="Pending", Reason="", readiness=false. Elapsed: 1.495428ms
Jan 30 12:00:32.023: INFO: Pod "pod-configmaps-a765ad6d-0f74-4e75-9097-e938bffd556f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005011401s
Jan 30 12:00:34.022: INFO: Pod "pod-configmaps-a765ad6d-0f74-4e75-9097-e938bffd556f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004391373s
STEP: Saw pod success 01/30/23 12:00:34.023
Jan 30 12:00:34.023: INFO: Pod "pod-configmaps-a765ad6d-0f74-4e75-9097-e938bffd556f" satisfied condition "Succeeded or Failed"
Jan 30 12:00:34.024: INFO: Trying to get logs from node pubt2-nks-for-dev1.dg.163.org pod pod-configmaps-a765ad6d-0f74-4e75-9097-e938bffd556f container agnhost-container: <nil>
STEP: delete the pod 01/30/23 12:00:34.029
Jan 30 12:00:34.033: INFO: Waiting for pod pod-configmaps-a765ad6d-0f74-4e75-9097-e938bffd556f to disappear
Jan 30 12:00:34.035: INFO: Pod pod-configmaps-a765ad6d-0f74-4e75-9097-e938bffd556f no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jan 30 12:00:34.035: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-6961" for this suite. 01/30/23 12:00:34.037
------------------------------
• [4.037 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:74

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 12:00:30.003
    Jan 30 12:00:30.003: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename configmap 01/30/23 12:00:30.003
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:00:30.009
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:00:30.011
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:74
    STEP: Creating configMap with name configmap-test-volume-485740ad-fcfe-477c-bec5-c867ec4e0aac 01/30/23 12:00:30.012
    STEP: Creating a pod to test consume configMaps 01/30/23 12:00:30.014
    Jan 30 12:00:30.018: INFO: Waiting up to 5m0s for pod "pod-configmaps-a765ad6d-0f74-4e75-9097-e938bffd556f" in namespace "configmap-6961" to be "Succeeded or Failed"
    Jan 30 12:00:30.020: INFO: Pod "pod-configmaps-a765ad6d-0f74-4e75-9097-e938bffd556f": Phase="Pending", Reason="", readiness=false. Elapsed: 1.495428ms
    Jan 30 12:00:32.023: INFO: Pod "pod-configmaps-a765ad6d-0f74-4e75-9097-e938bffd556f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005011401s
    Jan 30 12:00:34.022: INFO: Pod "pod-configmaps-a765ad6d-0f74-4e75-9097-e938bffd556f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004391373s
    STEP: Saw pod success 01/30/23 12:00:34.023
    Jan 30 12:00:34.023: INFO: Pod "pod-configmaps-a765ad6d-0f74-4e75-9097-e938bffd556f" satisfied condition "Succeeded or Failed"
    Jan 30 12:00:34.024: INFO: Trying to get logs from node pubt2-nks-for-dev1.dg.163.org pod pod-configmaps-a765ad6d-0f74-4e75-9097-e938bffd556f container agnhost-container: <nil>
    STEP: delete the pod 01/30/23 12:00:34.029
    Jan 30 12:00:34.033: INFO: Waiting for pod pod-configmaps-a765ad6d-0f74-4e75-9097-e938bffd556f to disappear
    Jan 30 12:00:34.035: INFO: Pod pod-configmaps-a765ad6d-0f74-4e75-9097-e938bffd556f no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jan 30 12:00:34.035: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-6961" for this suite. 01/30/23 12:00:34.037
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin]
  should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 12:00:34.04
Jan 30 12:00:34.040: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename certificates 01/30/23 12:00:34.041
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:00:34.047
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:00:34.049
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200
STEP: getting /apis 01/30/23 12:00:34.546
STEP: getting /apis/certificates.k8s.io 01/30/23 12:00:34.548
STEP: getting /apis/certificates.k8s.io/v1 01/30/23 12:00:34.549
STEP: creating 01/30/23 12:00:34.55
STEP: getting 01/30/23 12:00:34.556
STEP: listing 01/30/23 12:00:34.557
STEP: watching 01/30/23 12:00:34.559
Jan 30 12:00:34.559: INFO: starting watch
STEP: patching 01/30/23 12:00:34.56
STEP: updating 01/30/23 12:00:34.563
Jan 30 12:00:34.566: INFO: waiting for watch events with expected annotations
Jan 30 12:00:34.566: INFO: saw patched and updated annotations
STEP: getting /approval 01/30/23 12:00:34.566
STEP: patching /approval 01/30/23 12:00:34.567
STEP: updating /approval 01/30/23 12:00:34.57
STEP: getting /status 01/30/23 12:00:34.573
STEP: patching /status 01/30/23 12:00:34.575
STEP: updating /status 01/30/23 12:00:34.579
STEP: deleting 01/30/23 12:00:34.583
STEP: deleting a collection 01/30/23 12:00:34.588
[AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 30 12:00:34.593: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "certificates-7326" for this suite. 01/30/23 12:00:34.595
------------------------------
• [0.557 seconds]
[sig-auth] Certificates API [Privileged:ClusterAdmin]
test/e2e/auth/framework.go:23
  should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 12:00:34.04
    Jan 30 12:00:34.040: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename certificates 01/30/23 12:00:34.041
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:00:34.047
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:00:34.049
    [BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] should support CSR API operations [Conformance]
      test/e2e/auth/certificates.go:200
    STEP: getting /apis 01/30/23 12:00:34.546
    STEP: getting /apis/certificates.k8s.io 01/30/23 12:00:34.548
    STEP: getting /apis/certificates.k8s.io/v1 01/30/23 12:00:34.549
    STEP: creating 01/30/23 12:00:34.55
    STEP: getting 01/30/23 12:00:34.556
    STEP: listing 01/30/23 12:00:34.557
    STEP: watching 01/30/23 12:00:34.559
    Jan 30 12:00:34.559: INFO: starting watch
    STEP: patching 01/30/23 12:00:34.56
    STEP: updating 01/30/23 12:00:34.563
    Jan 30 12:00:34.566: INFO: waiting for watch events with expected annotations
    Jan 30 12:00:34.566: INFO: saw patched and updated annotations
    STEP: getting /approval 01/30/23 12:00:34.566
    STEP: patching /approval 01/30/23 12:00:34.567
    STEP: updating /approval 01/30/23 12:00:34.57
    STEP: getting /status 01/30/23 12:00:34.573
    STEP: patching /status 01/30/23 12:00:34.575
    STEP: updating /status 01/30/23 12:00:34.579
    STEP: deleting 01/30/23 12:00:34.583
    STEP: deleting a collection 01/30/23 12:00:34.588
    [AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 30 12:00:34.593: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "certificates-7326" for this suite. 01/30/23 12:00:34.595
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl label
  should update the label on a resource  [Conformance]
  test/e2e/kubectl/kubectl.go:1509
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 12:00:34.598
Jan 30 12:00:34.598: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename kubectl 01/30/23 12:00:34.599
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:00:34.604
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:00:34.606
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Kubectl label
  test/e2e/kubectl/kubectl.go:1494
STEP: creating the pod 01/30/23 12:00:34.608
Jan 30 12:00:34.608: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=kubectl-8501 create -f -'
Jan 30 12:00:36.122: INFO: stderr: ""
Jan 30 12:00:36.122: INFO: stdout: "pod/pause created\n"
Jan 30 12:00:36.122: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Jan 30 12:00:36.122: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-8501" to be "running and ready"
Jan 30 12:00:36.124: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 1.775795ms
Jan 30 12:00:36.124: INFO: Error evaluating pod condition running and ready: want pod 'pause' on 'pubt2-nks-for-dev1.dg.163.org' to be 'Running' but was 'Pending'
Jan 30 12:00:38.126: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.00470712s
Jan 30 12:00:38.126: INFO: Pod "pause" satisfied condition "running and ready"
Jan 30 12:00:38.126: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  test/e2e/kubectl/kubectl.go:1509
STEP: adding the label testing-label with value testing-label-value to a pod 01/30/23 12:00:38.127
Jan 30 12:00:38.127: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=kubectl-8501 label pods pause testing-label=testing-label-value'
Jan 30 12:00:38.194: INFO: stderr: ""
Jan 30 12:00:38.194: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value 01/30/23 12:00:38.194
Jan 30 12:00:38.194: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=kubectl-8501 get pod pause -L testing-label'
Jan 30 12:00:38.250: INFO: stderr: ""
Jan 30 12:00:38.250: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
STEP: removing the label testing-label of a pod 01/30/23 12:00:38.25
Jan 30 12:00:38.250: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=kubectl-8501 label pods pause testing-label-'
Jan 30 12:00:38.315: INFO: stderr: ""
Jan 30 12:00:38.315: INFO: stdout: "pod/pause unlabeled\n"
STEP: verifying the pod doesn't have the label testing-label 01/30/23 12:00:38.315
Jan 30 12:00:38.315: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=kubectl-8501 get pod pause -L testing-label'
Jan 30 12:00:38.374: INFO: stderr: ""
Jan 30 12:00:38.374: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    \n"
[AfterEach] Kubectl label
  test/e2e/kubectl/kubectl.go:1500
STEP: using delete to clean up resources 01/30/23 12:00:38.374
Jan 30 12:00:38.374: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=kubectl-8501 delete --grace-period=0 --force -f -'
Jan 30 12:00:38.435: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 30 12:00:38.435: INFO: stdout: "pod \"pause\" force deleted\n"
Jan 30 12:00:38.435: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=kubectl-8501 get rc,svc -l name=pause --no-headers'
Jan 30 12:00:38.498: INFO: stderr: "No resources found in kubectl-8501 namespace.\n"
Jan 30 12:00:38.498: INFO: stdout: ""
Jan 30 12:00:38.498: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=kubectl-8501 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jan 30 12:00:38.555: INFO: stderr: ""
Jan 30 12:00:38.555: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jan 30 12:00:38.555: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-8501" for this suite. 01/30/23 12:00:38.558
------------------------------
• [3.962 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl label
  test/e2e/kubectl/kubectl.go:1492
    should update the label on a resource  [Conformance]
    test/e2e/kubectl/kubectl.go:1509

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 12:00:34.598
    Jan 30 12:00:34.598: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename kubectl 01/30/23 12:00:34.599
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:00:34.604
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:00:34.606
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Kubectl label
      test/e2e/kubectl/kubectl.go:1494
    STEP: creating the pod 01/30/23 12:00:34.608
    Jan 30 12:00:34.608: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=kubectl-8501 create -f -'
    Jan 30 12:00:36.122: INFO: stderr: ""
    Jan 30 12:00:36.122: INFO: stdout: "pod/pause created\n"
    Jan 30 12:00:36.122: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
    Jan 30 12:00:36.122: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-8501" to be "running and ready"
    Jan 30 12:00:36.124: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 1.775795ms
    Jan 30 12:00:36.124: INFO: Error evaluating pod condition running and ready: want pod 'pause' on 'pubt2-nks-for-dev1.dg.163.org' to be 'Running' but was 'Pending'
    Jan 30 12:00:38.126: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.00470712s
    Jan 30 12:00:38.126: INFO: Pod "pause" satisfied condition "running and ready"
    Jan 30 12:00:38.126: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
    [It] should update the label on a resource  [Conformance]
      test/e2e/kubectl/kubectl.go:1509
    STEP: adding the label testing-label with value testing-label-value to a pod 01/30/23 12:00:38.127
    Jan 30 12:00:38.127: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=kubectl-8501 label pods pause testing-label=testing-label-value'
    Jan 30 12:00:38.194: INFO: stderr: ""
    Jan 30 12:00:38.194: INFO: stdout: "pod/pause labeled\n"
    STEP: verifying the pod has the label testing-label with the value testing-label-value 01/30/23 12:00:38.194
    Jan 30 12:00:38.194: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=kubectl-8501 get pod pause -L testing-label'
    Jan 30 12:00:38.250: INFO: stderr: ""
    Jan 30 12:00:38.250: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
    STEP: removing the label testing-label of a pod 01/30/23 12:00:38.25
    Jan 30 12:00:38.250: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=kubectl-8501 label pods pause testing-label-'
    Jan 30 12:00:38.315: INFO: stderr: ""
    Jan 30 12:00:38.315: INFO: stdout: "pod/pause unlabeled\n"
    STEP: verifying the pod doesn't have the label testing-label 01/30/23 12:00:38.315
    Jan 30 12:00:38.315: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=kubectl-8501 get pod pause -L testing-label'
    Jan 30 12:00:38.374: INFO: stderr: ""
    Jan 30 12:00:38.374: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    \n"
    [AfterEach] Kubectl label
      test/e2e/kubectl/kubectl.go:1500
    STEP: using delete to clean up resources 01/30/23 12:00:38.374
    Jan 30 12:00:38.374: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=kubectl-8501 delete --grace-period=0 --force -f -'
    Jan 30 12:00:38.435: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jan 30 12:00:38.435: INFO: stdout: "pod \"pause\" force deleted\n"
    Jan 30 12:00:38.435: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=kubectl-8501 get rc,svc -l name=pause --no-headers'
    Jan 30 12:00:38.498: INFO: stderr: "No resources found in kubectl-8501 namespace.\n"
    Jan 30 12:00:38.498: INFO: stdout: ""
    Jan 30 12:00:38.498: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=kubectl-8501 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Jan 30 12:00:38.555: INFO: stderr: ""
    Jan 30 12:00:38.555: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jan 30 12:00:38.555: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-8501" for this suite. 01/30/23 12:00:38.558
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:787
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 12:00:38.562
Jan 30 12:00:38.562: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename services 01/30/23 12:00:38.563
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:00:38.568
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:00:38.57
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:787
STEP: creating service endpoint-test2 in namespace services-634 01/30/23 12:00:38.572
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-634 to expose endpoints map[] 01/30/23 12:00:38.575
Jan 30 12:00:38.577: INFO: Failed go get Endpoints object: endpoints "endpoint-test2" not found
Jan 30 12:00:39.581: INFO: successfully validated that service endpoint-test2 in namespace services-634 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-634 01/30/23 12:00:39.581
Jan 30 12:00:39.585: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-634" to be "running and ready"
Jan 30 12:00:39.587: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 1.55487ms
Jan 30 12:00:39.587: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Jan 30 12:00:41.589: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.004065216s
Jan 30 12:00:41.589: INFO: The phase of Pod pod1 is Running (Ready = true)
Jan 30 12:00:41.589: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-634 to expose endpoints map[pod1:[80]] 01/30/23 12:00:41.591
Jan 30 12:00:41.596: INFO: successfully validated that service endpoint-test2 in namespace services-634 exposes endpoints map[pod1:[80]]
STEP: Checking if the Service forwards traffic to pod1 01/30/23 12:00:41.596
Jan 30 12:00:41.596: INFO: Creating new exec pod
Jan 30 12:00:41.598: INFO: Waiting up to 5m0s for pod "execpod6qdq8" in namespace "services-634" to be "running"
Jan 30 12:00:41.600: INFO: Pod "execpod6qdq8": Phase="Pending", Reason="", readiness=false. Elapsed: 1.446732ms
Jan 30 12:00:43.602: INFO: Pod "execpod6qdq8": Phase="Running", Reason="", readiness=true. Elapsed: 2.003787381s
Jan 30 12:00:43.602: INFO: Pod "execpod6qdq8" satisfied condition "running"
Jan 30 12:00:44.603: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=services-634 exec execpod6qdq8 -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
Jan 30 12:00:44.751: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Jan 30 12:00:44.751: INFO: stdout: ""
Jan 30 12:00:44.752: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=services-634 exec execpod6qdq8 -- /bin/sh -x -c nc -v -z -w 2 10.178.73.46 80'
Jan 30 12:00:44.888: INFO: stderr: "+ nc -v -z -w 2 10.178.73.46 80\nConnection to 10.178.73.46 80 port [tcp/http] succeeded!\n"
Jan 30 12:00:44.888: INFO: stdout: ""
STEP: Creating pod pod2 in namespace services-634 01/30/23 12:00:44.888
Jan 30 12:00:44.891: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-634" to be "running and ready"
Jan 30 12:00:44.893: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 1.63854ms
Jan 30 12:00:44.893: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Jan 30 12:00:46.895: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.003999955s
Jan 30 12:00:46.895: INFO: The phase of Pod pod2 is Running (Ready = true)
Jan 30 12:00:46.895: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-634 to expose endpoints map[pod1:[80] pod2:[80]] 01/30/23 12:00:46.897
Jan 30 12:00:46.904: INFO: successfully validated that service endpoint-test2 in namespace services-634 exposes endpoints map[pod1:[80] pod2:[80]]
STEP: Checking if the Service forwards traffic to pod1 and pod2 01/30/23 12:00:46.904
Jan 30 12:00:47.905: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=services-634 exec execpod6qdq8 -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
Jan 30 12:00:48.036: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Jan 30 12:00:48.036: INFO: stdout: ""
Jan 30 12:00:48.036: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=services-634 exec execpod6qdq8 -- /bin/sh -x -c nc -v -z -w 2 10.178.73.46 80'
Jan 30 12:00:48.175: INFO: stderr: "+ nc -v -z -w 2 10.178.73.46 80\nConnection to 10.178.73.46 80 port [tcp/http] succeeded!\n"
Jan 30 12:00:48.175: INFO: stdout: ""
STEP: Deleting pod pod1 in namespace services-634 01/30/23 12:00:48.175
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-634 to expose endpoints map[pod2:[80]] 01/30/23 12:00:48.179
Jan 30 12:00:48.184: INFO: successfully validated that service endpoint-test2 in namespace services-634 exposes endpoints map[pod2:[80]]
STEP: Checking if the Service forwards traffic to pod2 01/30/23 12:00:48.185
Jan 30 12:00:49.185: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=services-634 exec execpod6qdq8 -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
Jan 30 12:00:49.315: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Jan 30 12:00:49.315: INFO: stdout: ""
Jan 30 12:00:49.315: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=services-634 exec execpod6qdq8 -- /bin/sh -x -c nc -v -z -w 2 10.178.73.46 80'
Jan 30 12:00:49.438: INFO: stderr: "+ nc -v -z -w 2 10.178.73.46 80\nConnection to 10.178.73.46 80 port [tcp/http] succeeded!\n"
Jan 30 12:00:49.438: INFO: stdout: ""
STEP: Deleting pod pod2 in namespace services-634 01/30/23 12:00:49.438
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-634 to expose endpoints map[] 01/30/23 12:00:49.443
Jan 30 12:00:50.449: INFO: successfully validated that service endpoint-test2 in namespace services-634 exposes endpoints map[]
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jan 30 12:00:50.454: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-634" for this suite. 01/30/23 12:00:50.457
------------------------------
• [SLOW TEST] [11.897 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:787

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 12:00:38.562
    Jan 30 12:00:38.562: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename services 01/30/23 12:00:38.563
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:00:38.568
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:00:38.57
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should serve a basic endpoint from pods  [Conformance]
      test/e2e/network/service.go:787
    STEP: creating service endpoint-test2 in namespace services-634 01/30/23 12:00:38.572
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-634 to expose endpoints map[] 01/30/23 12:00:38.575
    Jan 30 12:00:38.577: INFO: Failed go get Endpoints object: endpoints "endpoint-test2" not found
    Jan 30 12:00:39.581: INFO: successfully validated that service endpoint-test2 in namespace services-634 exposes endpoints map[]
    STEP: Creating pod pod1 in namespace services-634 01/30/23 12:00:39.581
    Jan 30 12:00:39.585: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-634" to be "running and ready"
    Jan 30 12:00:39.587: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 1.55487ms
    Jan 30 12:00:39.587: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Jan 30 12:00:41.589: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.004065216s
    Jan 30 12:00:41.589: INFO: The phase of Pod pod1 is Running (Ready = true)
    Jan 30 12:00:41.589: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-634 to expose endpoints map[pod1:[80]] 01/30/23 12:00:41.591
    Jan 30 12:00:41.596: INFO: successfully validated that service endpoint-test2 in namespace services-634 exposes endpoints map[pod1:[80]]
    STEP: Checking if the Service forwards traffic to pod1 01/30/23 12:00:41.596
    Jan 30 12:00:41.596: INFO: Creating new exec pod
    Jan 30 12:00:41.598: INFO: Waiting up to 5m0s for pod "execpod6qdq8" in namespace "services-634" to be "running"
    Jan 30 12:00:41.600: INFO: Pod "execpod6qdq8": Phase="Pending", Reason="", readiness=false. Elapsed: 1.446732ms
    Jan 30 12:00:43.602: INFO: Pod "execpod6qdq8": Phase="Running", Reason="", readiness=true. Elapsed: 2.003787381s
    Jan 30 12:00:43.602: INFO: Pod "execpod6qdq8" satisfied condition "running"
    Jan 30 12:00:44.603: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=services-634 exec execpod6qdq8 -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
    Jan 30 12:00:44.751: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Jan 30 12:00:44.751: INFO: stdout: ""
    Jan 30 12:00:44.752: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=services-634 exec execpod6qdq8 -- /bin/sh -x -c nc -v -z -w 2 10.178.73.46 80'
    Jan 30 12:00:44.888: INFO: stderr: "+ nc -v -z -w 2 10.178.73.46 80\nConnection to 10.178.73.46 80 port [tcp/http] succeeded!\n"
    Jan 30 12:00:44.888: INFO: stdout: ""
    STEP: Creating pod pod2 in namespace services-634 01/30/23 12:00:44.888
    Jan 30 12:00:44.891: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-634" to be "running and ready"
    Jan 30 12:00:44.893: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 1.63854ms
    Jan 30 12:00:44.893: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Jan 30 12:00:46.895: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.003999955s
    Jan 30 12:00:46.895: INFO: The phase of Pod pod2 is Running (Ready = true)
    Jan 30 12:00:46.895: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-634 to expose endpoints map[pod1:[80] pod2:[80]] 01/30/23 12:00:46.897
    Jan 30 12:00:46.904: INFO: successfully validated that service endpoint-test2 in namespace services-634 exposes endpoints map[pod1:[80] pod2:[80]]
    STEP: Checking if the Service forwards traffic to pod1 and pod2 01/30/23 12:00:46.904
    Jan 30 12:00:47.905: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=services-634 exec execpod6qdq8 -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
    Jan 30 12:00:48.036: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Jan 30 12:00:48.036: INFO: stdout: ""
    Jan 30 12:00:48.036: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=services-634 exec execpod6qdq8 -- /bin/sh -x -c nc -v -z -w 2 10.178.73.46 80'
    Jan 30 12:00:48.175: INFO: stderr: "+ nc -v -z -w 2 10.178.73.46 80\nConnection to 10.178.73.46 80 port [tcp/http] succeeded!\n"
    Jan 30 12:00:48.175: INFO: stdout: ""
    STEP: Deleting pod pod1 in namespace services-634 01/30/23 12:00:48.175
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-634 to expose endpoints map[pod2:[80]] 01/30/23 12:00:48.179
    Jan 30 12:00:48.184: INFO: successfully validated that service endpoint-test2 in namespace services-634 exposes endpoints map[pod2:[80]]
    STEP: Checking if the Service forwards traffic to pod2 01/30/23 12:00:48.185
    Jan 30 12:00:49.185: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=services-634 exec execpod6qdq8 -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
    Jan 30 12:00:49.315: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Jan 30 12:00:49.315: INFO: stdout: ""
    Jan 30 12:00:49.315: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=services-634 exec execpod6qdq8 -- /bin/sh -x -c nc -v -z -w 2 10.178.73.46 80'
    Jan 30 12:00:49.438: INFO: stderr: "+ nc -v -z -w 2 10.178.73.46 80\nConnection to 10.178.73.46 80 port [tcp/http] succeeded!\n"
    Jan 30 12:00:49.438: INFO: stdout: ""
    STEP: Deleting pod pod2 in namespace services-634 01/30/23 12:00:49.438
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-634 to expose endpoints map[] 01/30/23 12:00:49.443
    Jan 30 12:00:50.449: INFO: successfully validated that service endpoint-test2 in namespace services-634 exposes endpoints map[]
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jan 30 12:00:50.454: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-634" for this suite. 01/30/23 12:00:50.457
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:205
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 12:00:50.463
Jan 30 12:00:50.463: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename secrets 01/30/23 12:00:50.463
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:00:50.47
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:00:50.472
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:205
STEP: Creating secret with name s-test-opt-del-dc101a63-4158-40a6-9f18-f8a17d2691e4 01/30/23 12:00:50.476
STEP: Creating secret with name s-test-opt-upd-54c655a5-d530-4396-a5ff-ef855ef9e5dc 01/30/23 12:00:50.478
STEP: Creating the pod 01/30/23 12:00:50.48
Jan 30 12:00:50.484: INFO: Waiting up to 5m0s for pod "pod-secrets-9cb0ddd1-2ce6-4834-b3a7-d423bb01abdd" in namespace "secrets-129" to be "running and ready"
Jan 30 12:00:50.486: INFO: Pod "pod-secrets-9cb0ddd1-2ce6-4834-b3a7-d423bb01abdd": Phase="Pending", Reason="", readiness=false. Elapsed: 1.618991ms
Jan 30 12:00:50.486: INFO: The phase of Pod pod-secrets-9cb0ddd1-2ce6-4834-b3a7-d423bb01abdd is Pending, waiting for it to be Running (with Ready = true)
Jan 30 12:00:52.489: INFO: Pod "pod-secrets-9cb0ddd1-2ce6-4834-b3a7-d423bb01abdd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004567729s
Jan 30 12:00:52.489: INFO: The phase of Pod pod-secrets-9cb0ddd1-2ce6-4834-b3a7-d423bb01abdd is Pending, waiting for it to be Running (with Ready = true)
Jan 30 12:00:54.490: INFO: Pod "pod-secrets-9cb0ddd1-2ce6-4834-b3a7-d423bb01abdd": Phase="Running", Reason="", readiness=true. Elapsed: 4.005935213s
Jan 30 12:00:54.490: INFO: The phase of Pod pod-secrets-9cb0ddd1-2ce6-4834-b3a7-d423bb01abdd is Running (Ready = true)
Jan 30 12:00:54.490: INFO: Pod "pod-secrets-9cb0ddd1-2ce6-4834-b3a7-d423bb01abdd" satisfied condition "running and ready"
STEP: Deleting secret s-test-opt-del-dc101a63-4158-40a6-9f18-f8a17d2691e4 01/30/23 12:00:54.505
STEP: Updating secret s-test-opt-upd-54c655a5-d530-4396-a5ff-ef855ef9e5dc 01/30/23 12:00:54.508
STEP: Creating secret with name s-test-opt-create-57c18037-ab59-4e58-aae3-536edebfe5d7 01/30/23 12:00:54.51
STEP: waiting to observe update in volume 01/30/23 12:00:54.512
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Jan 30 12:01:58.742: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-129" for this suite. 01/30/23 12:01:58.744
------------------------------
• [SLOW TEST] [68.284 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:205

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 12:00:50.463
    Jan 30 12:00:50.463: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename secrets 01/30/23 12:00:50.463
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:00:50.47
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:00:50.472
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:205
    STEP: Creating secret with name s-test-opt-del-dc101a63-4158-40a6-9f18-f8a17d2691e4 01/30/23 12:00:50.476
    STEP: Creating secret with name s-test-opt-upd-54c655a5-d530-4396-a5ff-ef855ef9e5dc 01/30/23 12:00:50.478
    STEP: Creating the pod 01/30/23 12:00:50.48
    Jan 30 12:00:50.484: INFO: Waiting up to 5m0s for pod "pod-secrets-9cb0ddd1-2ce6-4834-b3a7-d423bb01abdd" in namespace "secrets-129" to be "running and ready"
    Jan 30 12:00:50.486: INFO: Pod "pod-secrets-9cb0ddd1-2ce6-4834-b3a7-d423bb01abdd": Phase="Pending", Reason="", readiness=false. Elapsed: 1.618991ms
    Jan 30 12:00:50.486: INFO: The phase of Pod pod-secrets-9cb0ddd1-2ce6-4834-b3a7-d423bb01abdd is Pending, waiting for it to be Running (with Ready = true)
    Jan 30 12:00:52.489: INFO: Pod "pod-secrets-9cb0ddd1-2ce6-4834-b3a7-d423bb01abdd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004567729s
    Jan 30 12:00:52.489: INFO: The phase of Pod pod-secrets-9cb0ddd1-2ce6-4834-b3a7-d423bb01abdd is Pending, waiting for it to be Running (with Ready = true)
    Jan 30 12:00:54.490: INFO: Pod "pod-secrets-9cb0ddd1-2ce6-4834-b3a7-d423bb01abdd": Phase="Running", Reason="", readiness=true. Elapsed: 4.005935213s
    Jan 30 12:00:54.490: INFO: The phase of Pod pod-secrets-9cb0ddd1-2ce6-4834-b3a7-d423bb01abdd is Running (Ready = true)
    Jan 30 12:00:54.490: INFO: Pod "pod-secrets-9cb0ddd1-2ce6-4834-b3a7-d423bb01abdd" satisfied condition "running and ready"
    STEP: Deleting secret s-test-opt-del-dc101a63-4158-40a6-9f18-f8a17d2691e4 01/30/23 12:00:54.505
    STEP: Updating secret s-test-opt-upd-54c655a5-d530-4396-a5ff-ef855ef9e5dc 01/30/23 12:00:54.508
    STEP: Creating secret with name s-test-opt-create-57c18037-ab59-4e58-aae3-536edebfe5d7 01/30/23 12:00:54.51
    STEP: waiting to observe update in volume 01/30/23 12:00:54.512
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Jan 30 12:01:58.742: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-129" for this suite. 01/30/23 12:01:58.744
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:167
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 12:01:58.748
Jan 30 12:01:58.748: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename emptydir 01/30/23 12:01:58.749
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:01:58.755
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:01:58.757
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:167
STEP: Creating a pod to test emptydir 0644 on node default medium 01/30/23 12:01:58.759
Jan 30 12:01:58.764: INFO: Waiting up to 5m0s for pod "pod-da7aaff8-f52f-42c2-9617-28dcad80d978" in namespace "emptydir-5956" to be "Succeeded or Failed"
Jan 30 12:01:58.765: INFO: Pod "pod-da7aaff8-f52f-42c2-9617-28dcad80d978": Phase="Pending", Reason="", readiness=false. Elapsed: 1.557915ms
Jan 30 12:02:00.767: INFO: Pod "pod-da7aaff8-f52f-42c2-9617-28dcad80d978": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003669703s
Jan 30 12:02:02.767: INFO: Pod "pod-da7aaff8-f52f-42c2-9617-28dcad80d978": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003621682s
STEP: Saw pod success 01/30/23 12:02:02.767
Jan 30 12:02:02.767: INFO: Pod "pod-da7aaff8-f52f-42c2-9617-28dcad80d978" satisfied condition "Succeeded or Failed"
Jan 30 12:02:02.769: INFO: Trying to get logs from node pubt2-nks-for-dev1.dg.163.org pod pod-da7aaff8-f52f-42c2-9617-28dcad80d978 container test-container: <nil>
STEP: delete the pod 01/30/23 12:02:02.774
Jan 30 12:02:02.778: INFO: Waiting for pod pod-da7aaff8-f52f-42c2-9617-28dcad80d978 to disappear
Jan 30 12:02:02.780: INFO: Pod pod-da7aaff8-f52f-42c2-9617-28dcad80d978 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jan 30 12:02:02.780: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-5956" for this suite. 01/30/23 12:02:02.782
------------------------------
• [4.036 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:167

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 12:01:58.748
    Jan 30 12:01:58.748: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename emptydir 01/30/23 12:01:58.749
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:01:58.755
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:01:58.757
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:167
    STEP: Creating a pod to test emptydir 0644 on node default medium 01/30/23 12:01:58.759
    Jan 30 12:01:58.764: INFO: Waiting up to 5m0s for pod "pod-da7aaff8-f52f-42c2-9617-28dcad80d978" in namespace "emptydir-5956" to be "Succeeded or Failed"
    Jan 30 12:01:58.765: INFO: Pod "pod-da7aaff8-f52f-42c2-9617-28dcad80d978": Phase="Pending", Reason="", readiness=false. Elapsed: 1.557915ms
    Jan 30 12:02:00.767: INFO: Pod "pod-da7aaff8-f52f-42c2-9617-28dcad80d978": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003669703s
    Jan 30 12:02:02.767: INFO: Pod "pod-da7aaff8-f52f-42c2-9617-28dcad80d978": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003621682s
    STEP: Saw pod success 01/30/23 12:02:02.767
    Jan 30 12:02:02.767: INFO: Pod "pod-da7aaff8-f52f-42c2-9617-28dcad80d978" satisfied condition "Succeeded or Failed"
    Jan 30 12:02:02.769: INFO: Trying to get logs from node pubt2-nks-for-dev1.dg.163.org pod pod-da7aaff8-f52f-42c2-9617-28dcad80d978 container test-container: <nil>
    STEP: delete the pod 01/30/23 12:02:02.774
    Jan 30 12:02:02.778: INFO: Waiting for pod pod-da7aaff8-f52f-42c2-9617-28dcad80d978 to disappear
    Jan 30 12:02:02.780: INFO: Pod pod-da7aaff8-f52f-42c2-9617-28dcad80d978 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jan 30 12:02:02.780: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-5956" for this suite. 01/30/23 12:02:02.782
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:68
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 12:02:02.785
Jan 30 12:02:02.785: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename downward-api 01/30/23 12:02:02.785
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:02:02.791
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:02:02.793
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:68
STEP: Creating a pod to test downward API volume plugin 01/30/23 12:02:02.795
Jan 30 12:02:02.799: INFO: Waiting up to 5m0s for pod "downwardapi-volume-13f7308e-cbd3-4803-9b5a-85ad6edbfdf9" in namespace "downward-api-8849" to be "Succeeded or Failed"
Jan 30 12:02:02.801: INFO: Pod "downwardapi-volume-13f7308e-cbd3-4803-9b5a-85ad6edbfdf9": Phase="Pending", Reason="", readiness=false. Elapsed: 1.517558ms
Jan 30 12:02:04.804: INFO: Pod "downwardapi-volume-13f7308e-cbd3-4803-9b5a-85ad6edbfdf9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005064155s
Jan 30 12:02:06.804: INFO: Pod "downwardapi-volume-13f7308e-cbd3-4803-9b5a-85ad6edbfdf9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005038016s
STEP: Saw pod success 01/30/23 12:02:06.804
Jan 30 12:02:06.804: INFO: Pod "downwardapi-volume-13f7308e-cbd3-4803-9b5a-85ad6edbfdf9" satisfied condition "Succeeded or Failed"
Jan 30 12:02:06.806: INFO: Trying to get logs from node pubt2-nks-for-dev1.dg.163.org pod downwardapi-volume-13f7308e-cbd3-4803-9b5a-85ad6edbfdf9 container client-container: <nil>
STEP: delete the pod 01/30/23 12:02:06.81
Jan 30 12:02:06.815: INFO: Waiting for pod downwardapi-volume-13f7308e-cbd3-4803-9b5a-85ad6edbfdf9 to disappear
Jan 30 12:02:06.816: INFO: Pod downwardapi-volume-13f7308e-cbd3-4803-9b5a-85ad6edbfdf9 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Jan 30 12:02:06.816: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-8849" for this suite. 01/30/23 12:02:06.818
------------------------------
• [4.036 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 12:02:02.785
    Jan 30 12:02:02.785: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename downward-api 01/30/23 12:02:02.785
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:02:02.791
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:02:02.793
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:68
    STEP: Creating a pod to test downward API volume plugin 01/30/23 12:02:02.795
    Jan 30 12:02:02.799: INFO: Waiting up to 5m0s for pod "downwardapi-volume-13f7308e-cbd3-4803-9b5a-85ad6edbfdf9" in namespace "downward-api-8849" to be "Succeeded or Failed"
    Jan 30 12:02:02.801: INFO: Pod "downwardapi-volume-13f7308e-cbd3-4803-9b5a-85ad6edbfdf9": Phase="Pending", Reason="", readiness=false. Elapsed: 1.517558ms
    Jan 30 12:02:04.804: INFO: Pod "downwardapi-volume-13f7308e-cbd3-4803-9b5a-85ad6edbfdf9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005064155s
    Jan 30 12:02:06.804: INFO: Pod "downwardapi-volume-13f7308e-cbd3-4803-9b5a-85ad6edbfdf9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005038016s
    STEP: Saw pod success 01/30/23 12:02:06.804
    Jan 30 12:02:06.804: INFO: Pod "downwardapi-volume-13f7308e-cbd3-4803-9b5a-85ad6edbfdf9" satisfied condition "Succeeded or Failed"
    Jan 30 12:02:06.806: INFO: Trying to get logs from node pubt2-nks-for-dev1.dg.163.org pod downwardapi-volume-13f7308e-cbd3-4803-9b5a-85ad6edbfdf9 container client-container: <nil>
    STEP: delete the pod 01/30/23 12:02:06.81
    Jan 30 12:02:06.815: INFO: Waiting for pod downwardapi-volume-13f7308e-cbd3-4803-9b5a-85ad6edbfdf9 to disappear
    Jan 30 12:02:06.816: INFO: Pod downwardapi-volume-13f7308e-cbd3-4803-9b5a-85ad6edbfdf9 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Jan 30 12:02:06.816: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-8849" for this suite. 01/30/23 12:02:06.818
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions
  should check if v1 is in available api versions  [Conformance]
  test/e2e/kubectl/kubectl.go:824
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 12:02:06.821
Jan 30 12:02:06.821: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename kubectl 01/30/23 12:02:06.822
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:02:06.828
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:02:06.83
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if v1 is in available api versions  [Conformance]
  test/e2e/kubectl/kubectl.go:824
STEP: validating api versions 01/30/23 12:02:06.832
Jan 30 12:02:06.832: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=kubectl-2559 api-versions'
Jan 30 12:02:06.896: INFO: stderr: ""
Jan 30 12:02:06.896: INFO: stdout: "admissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling/v1\nautoscaling/v2\nbatch/v1\ncertificates.k8s.io/v1\ncoordination.k8s.io/v1\ncrd.projectcalico.org/v1\ndiscovery.k8s.io/v1\nevents.k8s.io/v1\nflowcontrol.apiserver.k8s.io/v1beta2\nflowcontrol.apiserver.k8s.io/v1beta3\nnetworking.k8s.io/v1\nnode.k8s.io/v1\npolicy/v1\nrbac.authorization.k8s.io/v1\nscheduling.k8s.io/v1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jan 30 12:02:06.896: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-2559" for this suite. 01/30/23 12:02:06.898
------------------------------
• [0.080 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl api-versions
  test/e2e/kubectl/kubectl.go:818
    should check if v1 is in available api versions  [Conformance]
    test/e2e/kubectl/kubectl.go:824

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 12:02:06.821
    Jan 30 12:02:06.821: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename kubectl 01/30/23 12:02:06.822
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:02:06.828
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:02:06.83
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if v1 is in available api versions  [Conformance]
      test/e2e/kubectl/kubectl.go:824
    STEP: validating api versions 01/30/23 12:02:06.832
    Jan 30 12:02:06.832: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=kubectl-2559 api-versions'
    Jan 30 12:02:06.896: INFO: stderr: ""
    Jan 30 12:02:06.896: INFO: stdout: "admissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling/v1\nautoscaling/v2\nbatch/v1\ncertificates.k8s.io/v1\ncoordination.k8s.io/v1\ncrd.projectcalico.org/v1\ndiscovery.k8s.io/v1\nevents.k8s.io/v1\nflowcontrol.apiserver.k8s.io/v1beta2\nflowcontrol.apiserver.k8s.io/v1beta3\nnetworking.k8s.io/v1\nnode.k8s.io/v1\npolicy/v1\nrbac.authorization.k8s.io/v1\nscheduling.k8s.io/v1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jan 30 12:02:06.896: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-2559" for this suite. 01/30/23 12:02:06.898
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:97
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 12:02:06.901
Jan 30 12:02:06.901: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename emptydir 01/30/23 12:02:06.902
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:02:06.908
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:02:06.91
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:97
STEP: Creating a pod to test emptydir 0644 on tmpfs 01/30/23 12:02:06.912
Jan 30 12:02:06.916: INFO: Waiting up to 5m0s for pod "pod-ded5f6a8-a7c5-40ab-ae63-e34677a4c7dc" in namespace "emptydir-514" to be "Succeeded or Failed"
Jan 30 12:02:06.918: INFO: Pod "pod-ded5f6a8-a7c5-40ab-ae63-e34677a4c7dc": Phase="Pending", Reason="", readiness=false. Elapsed: 1.563551ms
Jan 30 12:02:08.919: INFO: Pod "pod-ded5f6a8-a7c5-40ab-ae63-e34677a4c7dc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003346815s
Jan 30 12:02:10.921: INFO: Pod "pod-ded5f6a8-a7c5-40ab-ae63-e34677a4c7dc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005136907s
STEP: Saw pod success 01/30/23 12:02:10.921
Jan 30 12:02:10.921: INFO: Pod "pod-ded5f6a8-a7c5-40ab-ae63-e34677a4c7dc" satisfied condition "Succeeded or Failed"
Jan 30 12:02:10.923: INFO: Trying to get logs from node pubt2-nks-for-dev1.dg.163.org pod pod-ded5f6a8-a7c5-40ab-ae63-e34677a4c7dc container test-container: <nil>
STEP: delete the pod 01/30/23 12:02:10.927
Jan 30 12:02:10.932: INFO: Waiting for pod pod-ded5f6a8-a7c5-40ab-ae63-e34677a4c7dc to disappear
Jan 30 12:02:10.933: INFO: Pod pod-ded5f6a8-a7c5-40ab-ae63-e34677a4c7dc no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jan 30 12:02:10.933: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-514" for this suite. 01/30/23 12:02:10.936
------------------------------
• [4.037 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:97

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 12:02:06.901
    Jan 30 12:02:06.901: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename emptydir 01/30/23 12:02:06.902
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:02:06.908
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:02:06.91
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:97
    STEP: Creating a pod to test emptydir 0644 on tmpfs 01/30/23 12:02:06.912
    Jan 30 12:02:06.916: INFO: Waiting up to 5m0s for pod "pod-ded5f6a8-a7c5-40ab-ae63-e34677a4c7dc" in namespace "emptydir-514" to be "Succeeded or Failed"
    Jan 30 12:02:06.918: INFO: Pod "pod-ded5f6a8-a7c5-40ab-ae63-e34677a4c7dc": Phase="Pending", Reason="", readiness=false. Elapsed: 1.563551ms
    Jan 30 12:02:08.919: INFO: Pod "pod-ded5f6a8-a7c5-40ab-ae63-e34677a4c7dc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003346815s
    Jan 30 12:02:10.921: INFO: Pod "pod-ded5f6a8-a7c5-40ab-ae63-e34677a4c7dc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005136907s
    STEP: Saw pod success 01/30/23 12:02:10.921
    Jan 30 12:02:10.921: INFO: Pod "pod-ded5f6a8-a7c5-40ab-ae63-e34677a4c7dc" satisfied condition "Succeeded or Failed"
    Jan 30 12:02:10.923: INFO: Trying to get logs from node pubt2-nks-for-dev1.dg.163.org pod pod-ded5f6a8-a7c5-40ab-ae63-e34677a4c7dc container test-container: <nil>
    STEP: delete the pod 01/30/23 12:02:10.927
    Jan 30 12:02:10.932: INFO: Waiting for pod pod-ded5f6a8-a7c5-40ab-ae63-e34677a4c7dc to disappear
    Jan 30 12:02:10.933: INFO: Pod pod-ded5f6a8-a7c5-40ab-ae63-e34677a4c7dc no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jan 30 12:02:10.933: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-514" for this suite. 01/30/23 12:02:10.936
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:108
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 12:02:10.939
Jan 30 12:02:10.939: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename disruption 01/30/23 12:02:10.94
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:02:10.946
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:02:10.948
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[It] should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:108
STEP: creating the pdb 01/30/23 12:02:10.95
STEP: Waiting for the pdb to be processed 01/30/23 12:02:10.952
STEP: updating the pdb 01/30/23 12:02:12.955
STEP: Waiting for the pdb to be processed 01/30/23 12:02:12.961
STEP: patching the pdb 01/30/23 12:02:14.966
STEP: Waiting for the pdb to be processed 01/30/23 12:02:14.972
STEP: Waiting for the pdb to be deleted 01/30/23 12:02:16.98
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Jan 30 12:02:16.982: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-1207" for this suite. 01/30/23 12:02:16.984
------------------------------
• [SLOW TEST] [6.048 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:108

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 12:02:10.939
    Jan 30 12:02:10.939: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename disruption 01/30/23 12:02:10.94
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:02:10.946
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:02:10.948
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [It] should create a PodDisruptionBudget [Conformance]
      test/e2e/apps/disruption.go:108
    STEP: creating the pdb 01/30/23 12:02:10.95
    STEP: Waiting for the pdb to be processed 01/30/23 12:02:10.952
    STEP: updating the pdb 01/30/23 12:02:12.955
    STEP: Waiting for the pdb to be processed 01/30/23 12:02:12.961
    STEP: patching the pdb 01/30/23 12:02:14.966
    STEP: Waiting for the pdb to be processed 01/30/23 12:02:14.972
    STEP: Waiting for the pdb to be deleted 01/30/23 12:02:16.98
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Jan 30 12:02:16.982: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-1207" for this suite. 01/30/23 12:02:16.984
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-network] Proxy version v1
  should proxy through a service and a pod  [Conformance]
  test/e2e/network/proxy.go:101
[BeforeEach] version v1
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 12:02:16.987
Jan 30 12:02:16.987: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename proxy 01/30/23 12:02:16.988
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:02:16.993
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:02:16.995
[BeforeEach] version v1
  test/e2e/framework/metrics/init/init.go:31
[It] should proxy through a service and a pod  [Conformance]
  test/e2e/network/proxy.go:101
STEP: starting an echo server on multiple ports 01/30/23 12:02:17.001
STEP: creating replication controller proxy-service-rnsxp in namespace proxy-4838 01/30/23 12:02:17.001
I0130 12:02:17.004913      23 runners.go:193] Created replication controller with name: proxy-service-rnsxp, namespace: proxy-4838, replica count: 1
I0130 12:02:18.056310      23 runners.go:193] proxy-service-rnsxp Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0130 12:02:19.056582      23 runners.go:193] proxy-service-rnsxp Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0130 12:02:20.057662      23 runners.go:193] proxy-service-rnsxp Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 30 12:02:20.059: INFO: setup took 3.06152924s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts 01/30/23 12:02:20.059
Jan 30 12:02:20.063: INFO: (0) /api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8:162/proxy/: bar (200; 3.86409ms)
Jan 30 12:02:20.063: INFO: (0) /api/v1/namespaces/proxy-4838/services/proxy-service-rnsxp:portname2/proxy/: bar (200; 3.996379ms)
Jan 30 12:02:20.063: INFO: (0) /api/v1/namespaces/proxy-4838/services/proxy-service-rnsxp:portname1/proxy/: foo (200; 4.001257ms)
Jan 30 12:02:20.063: INFO: (0) /api/v1/namespaces/proxy-4838/services/http:proxy-service-rnsxp:portname1/proxy/: foo (200; 4.049286ms)
Jan 30 12:02:20.063: INFO: (0) /api/v1/namespaces/proxy-4838/pods/http:proxy-service-rnsxp-gwgn8:162/proxy/: bar (200; 4.13885ms)
Jan 30 12:02:20.063: INFO: (0) /api/v1/namespaces/proxy-4838/services/http:proxy-service-rnsxp:portname2/proxy/: bar (200; 4.130778ms)
Jan 30 12:02:20.063: INFO: (0) /api/v1/namespaces/proxy-4838/pods/http:proxy-service-rnsxp-gwgn8:160/proxy/: foo (200; 4.211638ms)
Jan 30 12:02:20.063: INFO: (0) /api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8:160/proxy/: foo (200; 4.295606ms)
Jan 30 12:02:20.064: INFO: (0) /api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8/proxy/: <a href="/api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8/proxy/rewriteme">test</a> (200; 5.202766ms)
Jan 30 12:02:20.064: INFO: (0) /api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8:1080/proxy/: <a href="/api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8:1080/proxy/rewriteme">test<... (200; 5.242863ms)
Jan 30 12:02:20.064: INFO: (0) /api/v1/namespaces/proxy-4838/pods/http:proxy-service-rnsxp-gwgn8:1080/proxy/: <a href="/api/v1/namespaces/proxy-4838/pods/http:proxy-service-rnsxp-gwgn8:1080/proxy/rewriteme">... (200; 5.263024ms)
Jan 30 12:02:20.067: INFO: (0) /api/v1/namespaces/proxy-4838/pods/https:proxy-service-rnsxp-gwgn8:443/proxy/: <a href="/api/v1/namespaces/proxy-4838/pods/https:proxy-service-rnsxp-gwgn8:443/proxy/tlsrewritem... (200; 8.259326ms)
Jan 30 12:02:20.068: INFO: (0) /api/v1/namespaces/proxy-4838/pods/https:proxy-service-rnsxp-gwgn8:460/proxy/: tls baz (200; 8.766165ms)
Jan 30 12:02:20.068: INFO: (0) /api/v1/namespaces/proxy-4838/services/https:proxy-service-rnsxp:tlsportname2/proxy/: tls qux (200; 8.726518ms)
Jan 30 12:02:20.068: INFO: (0) /api/v1/namespaces/proxy-4838/services/https:proxy-service-rnsxp:tlsportname1/proxy/: tls baz (200; 8.815761ms)
Jan 30 12:02:20.068: INFO: (0) /api/v1/namespaces/proxy-4838/pods/https:proxy-service-rnsxp-gwgn8:462/proxy/: tls qux (200; 8.80622ms)
Jan 30 12:02:20.070: INFO: (1) /api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8:160/proxy/: foo (200; 2.137785ms)
Jan 30 12:02:20.071: INFO: (1) /api/v1/namespaces/proxy-4838/services/http:proxy-service-rnsxp:portname1/proxy/: foo (200; 2.897577ms)
Jan 30 12:02:20.071: INFO: (1) /api/v1/namespaces/proxy-4838/services/http:proxy-service-rnsxp:portname2/proxy/: bar (200; 3.216078ms)
Jan 30 12:02:20.071: INFO: (1) /api/v1/namespaces/proxy-4838/services/proxy-service-rnsxp:portname1/proxy/: foo (200; 3.248574ms)
Jan 30 12:02:20.071: INFO: (1) /api/v1/namespaces/proxy-4838/services/https:proxy-service-rnsxp:tlsportname1/proxy/: tls baz (200; 3.258609ms)
Jan 30 12:02:20.071: INFO: (1) /api/v1/namespaces/proxy-4838/pods/http:proxy-service-rnsxp-gwgn8:160/proxy/: foo (200; 3.288027ms)
Jan 30 12:02:20.071: INFO: (1) /api/v1/namespaces/proxy-4838/pods/https:proxy-service-rnsxp-gwgn8:443/proxy/: <a href="/api/v1/namespaces/proxy-4838/pods/https:proxy-service-rnsxp-gwgn8:443/proxy/tlsrewritem... (200; 3.349291ms)
Jan 30 12:02:20.071: INFO: (1) /api/v1/namespaces/proxy-4838/pods/http:proxy-service-rnsxp-gwgn8:1080/proxy/: <a href="/api/v1/namespaces/proxy-4838/pods/http:proxy-service-rnsxp-gwgn8:1080/proxy/rewriteme">... (200; 3.328168ms)
Jan 30 12:02:20.071: INFO: (1) /api/v1/namespaces/proxy-4838/services/proxy-service-rnsxp:portname2/proxy/: bar (200; 3.413705ms)
Jan 30 12:02:20.071: INFO: (1) /api/v1/namespaces/proxy-4838/pods/https:proxy-service-rnsxp-gwgn8:460/proxy/: tls baz (200; 3.494332ms)
Jan 30 12:02:20.071: INFO: (1) /api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8:162/proxy/: bar (200; 3.515438ms)
Jan 30 12:02:20.071: INFO: (1) /api/v1/namespaces/proxy-4838/services/https:proxy-service-rnsxp:tlsportname2/proxy/: tls qux (200; 3.578766ms)
Jan 30 12:02:20.071: INFO: (1) /api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8/proxy/: <a href="/api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8/proxy/rewriteme">test</a> (200; 3.600099ms)
Jan 30 12:02:20.071: INFO: (1) /api/v1/namespaces/proxy-4838/pods/http:proxy-service-rnsxp-gwgn8:162/proxy/: bar (200; 3.576761ms)
Jan 30 12:02:20.071: INFO: (1) /api/v1/namespaces/proxy-4838/pods/https:proxy-service-rnsxp-gwgn8:462/proxy/: tls qux (200; 3.642793ms)
Jan 30 12:02:20.071: INFO: (1) /api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8:1080/proxy/: <a href="/api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8:1080/proxy/rewriteme">test<... (200; 3.638967ms)
Jan 30 12:02:20.074: INFO: (2) /api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8:160/proxy/: foo (200; 2.814832ms)
Jan 30 12:02:20.074: INFO: (2) /api/v1/namespaces/proxy-4838/services/http:proxy-service-rnsxp:portname1/proxy/: foo (200; 2.88795ms)
Jan 30 12:02:20.074: INFO: (2) /api/v1/namespaces/proxy-4838/services/https:proxy-service-rnsxp:tlsportname1/proxy/: tls baz (200; 2.876649ms)
Jan 30 12:02:20.074: INFO: (2) /api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8:1080/proxy/: <a href="/api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8:1080/proxy/rewriteme">test<... (200; 2.829019ms)
Jan 30 12:02:20.075: INFO: (2) /api/v1/namespaces/proxy-4838/services/proxy-service-rnsxp:portname2/proxy/: bar (200; 2.98507ms)
Jan 30 12:02:20.075: INFO: (2) /api/v1/namespaces/proxy-4838/pods/http:proxy-service-rnsxp-gwgn8:1080/proxy/: <a href="/api/v1/namespaces/proxy-4838/pods/http:proxy-service-rnsxp-gwgn8:1080/proxy/rewriteme">... (200; 2.98671ms)
Jan 30 12:02:20.075: INFO: (2) /api/v1/namespaces/proxy-4838/pods/https:proxy-service-rnsxp-gwgn8:460/proxy/: tls baz (200; 3.031595ms)
Jan 30 12:02:20.075: INFO: (2) /api/v1/namespaces/proxy-4838/services/https:proxy-service-rnsxp:tlsportname2/proxy/: tls qux (200; 3.108632ms)
Jan 30 12:02:20.075: INFO: (2) /api/v1/namespaces/proxy-4838/services/proxy-service-rnsxp:portname1/proxy/: foo (200; 3.13293ms)
Jan 30 12:02:20.075: INFO: (2) /api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8/proxy/: <a href="/api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8/proxy/rewriteme">test</a> (200; 3.164602ms)
Jan 30 12:02:20.075: INFO: (2) /api/v1/namespaces/proxy-4838/services/http:proxy-service-rnsxp:portname2/proxy/: bar (200; 3.334243ms)
Jan 30 12:02:20.075: INFO: (2) /api/v1/namespaces/proxy-4838/pods/http:proxy-service-rnsxp-gwgn8:160/proxy/: foo (200; 3.345929ms)
Jan 30 12:02:20.075: INFO: (2) /api/v1/namespaces/proxy-4838/pods/http:proxy-service-rnsxp-gwgn8:162/proxy/: bar (200; 3.321571ms)
Jan 30 12:02:20.075: INFO: (2) /api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8:162/proxy/: bar (200; 3.403234ms)
Jan 30 12:02:20.075: INFO: (2) /api/v1/namespaces/proxy-4838/pods/https:proxy-service-rnsxp-gwgn8:462/proxy/: tls qux (200; 3.451419ms)
Jan 30 12:02:20.075: INFO: (2) /api/v1/namespaces/proxy-4838/pods/https:proxy-service-rnsxp-gwgn8:443/proxy/: <a href="/api/v1/namespaces/proxy-4838/pods/https:proxy-service-rnsxp-gwgn8:443/proxy/tlsrewritem... (200; 3.538076ms)
Jan 30 12:02:20.077: INFO: (3) /api/v1/namespaces/proxy-4838/pods/http:proxy-service-rnsxp-gwgn8:1080/proxy/: <a href="/api/v1/namespaces/proxy-4838/pods/http:proxy-service-rnsxp-gwgn8:1080/proxy/rewriteme">... (200; 2.204622ms)
Jan 30 12:02:20.077: INFO: (3) /api/v1/namespaces/proxy-4838/pods/https:proxy-service-rnsxp-gwgn8:462/proxy/: tls qux (200; 2.231532ms)
Jan 30 12:02:20.077: INFO: (3) /api/v1/namespaces/proxy-4838/pods/https:proxy-service-rnsxp-gwgn8:460/proxy/: tls baz (200; 2.224377ms)
Jan 30 12:02:20.078: INFO: (3) /api/v1/namespaces/proxy-4838/services/https:proxy-service-rnsxp:tlsportname1/proxy/: tls baz (200; 2.685614ms)
Jan 30 12:02:20.078: INFO: (3) /api/v1/namespaces/proxy-4838/services/proxy-service-rnsxp:portname2/proxy/: bar (200; 2.74744ms)
Jan 30 12:02:20.078: INFO: (3) /api/v1/namespaces/proxy-4838/pods/http:proxy-service-rnsxp-gwgn8:160/proxy/: foo (200; 3.062023ms)
Jan 30 12:02:20.078: INFO: (3) /api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8:162/proxy/: bar (200; 3.332567ms)
Jan 30 12:02:20.079: INFO: (3) /api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8:160/proxy/: foo (200; 3.38967ms)
Jan 30 12:02:20.079: INFO: (3) /api/v1/namespaces/proxy-4838/pods/http:proxy-service-rnsxp-gwgn8:162/proxy/: bar (200; 3.402721ms)
Jan 30 12:02:20.079: INFO: (3) /api/v1/namespaces/proxy-4838/services/proxy-service-rnsxp:portname1/proxy/: foo (200; 3.424508ms)
Jan 30 12:02:20.079: INFO: (3) /api/v1/namespaces/proxy-4838/pods/https:proxy-service-rnsxp-gwgn8:443/proxy/: <a href="/api/v1/namespaces/proxy-4838/pods/https:proxy-service-rnsxp-gwgn8:443/proxy/tlsrewritem... (200; 3.417263ms)
Jan 30 12:02:20.079: INFO: (3) /api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8/proxy/: <a href="/api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8/proxy/rewriteme">test</a> (200; 3.448375ms)
Jan 30 12:02:20.079: INFO: (3) /api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8:1080/proxy/: <a href="/api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8:1080/proxy/rewriteme">test<... (200; 3.447971ms)
Jan 30 12:02:20.079: INFO: (3) /api/v1/namespaces/proxy-4838/services/https:proxy-service-rnsxp:tlsportname2/proxy/: tls qux (200; 3.472436ms)
Jan 30 12:02:20.079: INFO: (3) /api/v1/namespaces/proxy-4838/services/http:proxy-service-rnsxp:portname1/proxy/: foo (200; 3.478245ms)
Jan 30 12:02:20.079: INFO: (3) /api/v1/namespaces/proxy-4838/services/http:proxy-service-rnsxp:portname2/proxy/: bar (200; 3.495363ms)
Jan 30 12:02:20.081: INFO: (4) /api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8:162/proxy/: bar (200; 2.260858ms)
Jan 30 12:02:20.081: INFO: (4) /api/v1/namespaces/proxy-4838/pods/http:proxy-service-rnsxp-gwgn8:1080/proxy/: <a href="/api/v1/namespaces/proxy-4838/pods/http:proxy-service-rnsxp-gwgn8:1080/proxy/rewriteme">... (200; 2.556656ms)
Jan 30 12:02:20.081: INFO: (4) /api/v1/namespaces/proxy-4838/pods/http:proxy-service-rnsxp-gwgn8:160/proxy/: foo (200; 2.587249ms)
Jan 30 12:02:20.081: INFO: (4) /api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8/proxy/: <a href="/api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8/proxy/rewriteme">test</a> (200; 2.616444ms)
Jan 30 12:02:20.081: INFO: (4) /api/v1/namespaces/proxy-4838/pods/https:proxy-service-rnsxp-gwgn8:462/proxy/: tls qux (200; 2.652801ms)
Jan 30 12:02:20.081: INFO: (4) /api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8:1080/proxy/: <a href="/api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8:1080/proxy/rewriteme">test<... (200; 2.667707ms)
Jan 30 12:02:20.081: INFO: (4) /api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8:160/proxy/: foo (200; 2.632116ms)
Jan 30 12:02:20.081: INFO: (4) /api/v1/namespaces/proxy-4838/pods/http:proxy-service-rnsxp-gwgn8:162/proxy/: bar (200; 2.676987ms)
Jan 30 12:02:20.081: INFO: (4) /api/v1/namespaces/proxy-4838/pods/https:proxy-service-rnsxp-gwgn8:460/proxy/: tls baz (200; 2.629981ms)
Jan 30 12:02:20.081: INFO: (4) /api/v1/namespaces/proxy-4838/pods/https:proxy-service-rnsxp-gwgn8:443/proxy/: <a href="/api/v1/namespaces/proxy-4838/pods/https:proxy-service-rnsxp-gwgn8:443/proxy/tlsrewritem... (200; 2.697426ms)
Jan 30 12:02:20.081: INFO: (4) /api/v1/namespaces/proxy-4838/services/https:proxy-service-rnsxp:tlsportname1/proxy/: tls baz (200; 2.696494ms)
Jan 30 12:02:20.082: INFO: (4) /api/v1/namespaces/proxy-4838/services/http:proxy-service-rnsxp:portname1/proxy/: foo (200; 3.232742ms)
Jan 30 12:02:20.082: INFO: (4) /api/v1/namespaces/proxy-4838/services/proxy-service-rnsxp:portname2/proxy/: bar (200; 3.255972ms)
Jan 30 12:02:20.082: INFO: (4) /api/v1/namespaces/proxy-4838/services/proxy-service-rnsxp:portname1/proxy/: foo (200; 3.245034ms)
Jan 30 12:02:20.082: INFO: (4) /api/v1/namespaces/proxy-4838/services/http:proxy-service-rnsxp:portname2/proxy/: bar (200; 3.24211ms)
Jan 30 12:02:20.082: INFO: (4) /api/v1/namespaces/proxy-4838/services/https:proxy-service-rnsxp:tlsportname2/proxy/: tls qux (200; 3.339128ms)
Jan 30 12:02:20.084: INFO: (5) /api/v1/namespaces/proxy-4838/pods/https:proxy-service-rnsxp-gwgn8:460/proxy/: tls baz (200; 2.064438ms)
Jan 30 12:02:20.084: INFO: (5) /api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8:160/proxy/: foo (200; 2.044176ms)
Jan 30 12:02:20.085: INFO: (5) /api/v1/namespaces/proxy-4838/services/http:proxy-service-rnsxp:portname1/proxy/: foo (200; 2.640358ms)
Jan 30 12:02:20.085: INFO: (5) /api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8:1080/proxy/: <a href="/api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8:1080/proxy/rewriteme">test<... (200; 2.634275ms)
Jan 30 12:02:20.085: INFO: (5) /api/v1/namespaces/proxy-4838/services/https:proxy-service-rnsxp:tlsportname2/proxy/: tls qux (200; 3.022583ms)
Jan 30 12:02:20.085: INFO: (5) /api/v1/namespaces/proxy-4838/pods/http:proxy-service-rnsxp-gwgn8:1080/proxy/: <a href="/api/v1/namespaces/proxy-4838/pods/http:proxy-service-rnsxp-gwgn8:1080/proxy/rewriteme">... (200; 2.949544ms)
Jan 30 12:02:20.085: INFO: (5) /api/v1/namespaces/proxy-4838/pods/https:proxy-service-rnsxp-gwgn8:443/proxy/: <a href="/api/v1/namespaces/proxy-4838/pods/https:proxy-service-rnsxp-gwgn8:443/proxy/tlsrewritem... (200; 2.947605ms)
Jan 30 12:02:20.085: INFO: (5) /api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8:162/proxy/: bar (200; 2.977655ms)
Jan 30 12:02:20.085: INFO: (5) /api/v1/namespaces/proxy-4838/services/proxy-service-rnsxp:portname2/proxy/: bar (200; 2.980414ms)
Jan 30 12:02:20.085: INFO: (5) /api/v1/namespaces/proxy-4838/services/proxy-service-rnsxp:portname1/proxy/: foo (200; 3.037969ms)
Jan 30 12:02:20.085: INFO: (5) /api/v1/namespaces/proxy-4838/services/https:proxy-service-rnsxp:tlsportname1/proxy/: tls baz (200; 3.106323ms)
Jan 30 12:02:20.085: INFO: (5) /api/v1/namespaces/proxy-4838/services/http:proxy-service-rnsxp:portname2/proxy/: bar (200; 3.123128ms)
Jan 30 12:02:20.085: INFO: (5) /api/v1/namespaces/proxy-4838/pods/http:proxy-service-rnsxp-gwgn8:162/proxy/: bar (200; 3.179313ms)
Jan 30 12:02:20.085: INFO: (5) /api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8/proxy/: <a href="/api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8/proxy/rewriteme">test</a> (200; 3.210369ms)
Jan 30 12:02:20.085: INFO: (5) /api/v1/namespaces/proxy-4838/pods/http:proxy-service-rnsxp-gwgn8:160/proxy/: foo (200; 3.24231ms)
Jan 30 12:02:20.085: INFO: (5) /api/v1/namespaces/proxy-4838/pods/https:proxy-service-rnsxp-gwgn8:462/proxy/: tls qux (200; 3.216871ms)
Jan 30 12:02:20.087: INFO: (6) /api/v1/namespaces/proxy-4838/pods/http:proxy-service-rnsxp-gwgn8:160/proxy/: foo (200; 1.851599ms)
Jan 30 12:02:20.087: INFO: (6) /api/v1/namespaces/proxy-4838/pods/https:proxy-service-rnsxp-gwgn8:460/proxy/: tls baz (200; 1.9364ms)
Jan 30 12:02:20.089: INFO: (6) /api/v1/namespaces/proxy-4838/services/http:proxy-service-rnsxp:portname2/proxy/: bar (200; 3.276394ms)
Jan 30 12:02:20.089: INFO: (6) /api/v1/namespaces/proxy-4838/services/proxy-service-rnsxp:portname1/proxy/: foo (200; 3.254758ms)
Jan 30 12:02:20.089: INFO: (6) /api/v1/namespaces/proxy-4838/services/http:proxy-service-rnsxp:portname1/proxy/: foo (200; 3.255754ms)
Jan 30 12:02:20.089: INFO: (6) /api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8/proxy/: <a href="/api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8/proxy/rewriteme">test</a> (200; 3.305847ms)
Jan 30 12:02:20.089: INFO: (6) /api/v1/namespaces/proxy-4838/pods/http:proxy-service-rnsxp-gwgn8:1080/proxy/: <a href="/api/v1/namespaces/proxy-4838/pods/http:proxy-service-rnsxp-gwgn8:1080/proxy/rewriteme">... (200; 3.314062ms)
Jan 30 12:02:20.089: INFO: (6) /api/v1/namespaces/proxy-4838/services/https:proxy-service-rnsxp:tlsportname1/proxy/: tls baz (200; 3.354624ms)
Jan 30 12:02:20.089: INFO: (6) /api/v1/namespaces/proxy-4838/services/https:proxy-service-rnsxp:tlsportname2/proxy/: tls qux (200; 3.380262ms)
Jan 30 12:02:20.089: INFO: (6) /api/v1/namespaces/proxy-4838/pods/https:proxy-service-rnsxp-gwgn8:462/proxy/: tls qux (200; 3.362462ms)
Jan 30 12:02:20.089: INFO: (6) /api/v1/namespaces/proxy-4838/pods/https:proxy-service-rnsxp-gwgn8:443/proxy/: <a href="/api/v1/namespaces/proxy-4838/pods/https:proxy-service-rnsxp-gwgn8:443/proxy/tlsrewritem... (200; 3.344217ms)
Jan 30 12:02:20.089: INFO: (6) /api/v1/namespaces/proxy-4838/pods/http:proxy-service-rnsxp-gwgn8:162/proxy/: bar (200; 3.382122ms)
Jan 30 12:02:20.089: INFO: (6) /api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8:162/proxy/: bar (200; 3.379498ms)
Jan 30 12:02:20.089: INFO: (6) /api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8:1080/proxy/: <a href="/api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8:1080/proxy/rewriteme">test<... (200; 3.573069ms)
Jan 30 12:02:20.089: INFO: (6) /api/v1/namespaces/proxy-4838/services/proxy-service-rnsxp:portname2/proxy/: bar (200; 3.598828ms)
Jan 30 12:02:20.089: INFO: (6) /api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8:160/proxy/: foo (200; 3.54442ms)
Jan 30 12:02:20.091: INFO: (7) /api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8:1080/proxy/: <a href="/api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8:1080/proxy/rewriteme">test<... (200; 1.949196ms)
Jan 30 12:02:20.091: INFO: (7) /api/v1/namespaces/proxy-4838/pods/https:proxy-service-rnsxp-gwgn8:460/proxy/: tls baz (200; 2.231916ms)
Jan 30 12:02:20.091: INFO: (7) /api/v1/namespaces/proxy-4838/pods/http:proxy-service-rnsxp-gwgn8:160/proxy/: foo (200; 2.236161ms)
Jan 30 12:02:20.091: INFO: (7) /api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8:162/proxy/: bar (200; 2.394591ms)
Jan 30 12:02:20.092: INFO: (7) /api/v1/namespaces/proxy-4838/pods/https:proxy-service-rnsxp-gwgn8:462/proxy/: tls qux (200; 2.537342ms)
Jan 30 12:02:20.092: INFO: (7) /api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8/proxy/: <a href="/api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8/proxy/rewriteme">test</a> (200; 2.517086ms)
Jan 30 12:02:20.092: INFO: (7) /api/v1/namespaces/proxy-4838/services/https:proxy-service-rnsxp:tlsportname2/proxy/: tls qux (200; 3.119242ms)
Jan 30 12:02:20.092: INFO: (7) /api/v1/namespaces/proxy-4838/services/http:proxy-service-rnsxp:portname2/proxy/: bar (200; 3.124934ms)
Jan 30 12:02:20.092: INFO: (7) /api/v1/namespaces/proxy-4838/services/proxy-service-rnsxp:portname1/proxy/: foo (200; 3.114517ms)
Jan 30 12:02:20.092: INFO: (7) /api/v1/namespaces/proxy-4838/pods/https:proxy-service-rnsxp-gwgn8:443/proxy/: <a href="/api/v1/namespaces/proxy-4838/pods/https:proxy-service-rnsxp-gwgn8:443/proxy/tlsrewritem... (200; 3.19861ms)
Jan 30 12:02:20.092: INFO: (7) /api/v1/namespaces/proxy-4838/services/https:proxy-service-rnsxp:tlsportname1/proxy/: tls baz (200; 3.182814ms)
Jan 30 12:02:20.092: INFO: (7) /api/v1/namespaces/proxy-4838/services/proxy-service-rnsxp:portname2/proxy/: bar (200; 3.163732ms)
Jan 30 12:02:20.092: INFO: (7) /api/v1/namespaces/proxy-4838/pods/http:proxy-service-rnsxp-gwgn8:1080/proxy/: <a href="/api/v1/namespaces/proxy-4838/pods/http:proxy-service-rnsxp-gwgn8:1080/proxy/rewriteme">... (200; 3.137984ms)
Jan 30 12:02:20.092: INFO: (7) /api/v1/namespaces/proxy-4838/services/http:proxy-service-rnsxp:portname1/proxy/: foo (200; 3.257544ms)
Jan 30 12:02:20.092: INFO: (7) /api/v1/namespaces/proxy-4838/pods/http:proxy-service-rnsxp-gwgn8:162/proxy/: bar (200; 3.321681ms)
Jan 30 12:02:20.092: INFO: (7) /api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8:160/proxy/: foo (200; 3.327967ms)
Jan 30 12:02:20.095: INFO: (8) /api/v1/namespaces/proxy-4838/pods/https:proxy-service-rnsxp-gwgn8:443/proxy/: <a href="/api/v1/namespaces/proxy-4838/pods/https:proxy-service-rnsxp-gwgn8:443/proxy/tlsrewritem... (200; 2.528524ms)
Jan 30 12:02:20.095: INFO: (8) /api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8:1080/proxy/: <a href="/api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8:1080/proxy/rewriteme">test<... (200; 2.533801ms)
Jan 30 12:02:20.095: INFO: (8) /api/v1/namespaces/proxy-4838/services/https:proxy-service-rnsxp:tlsportname1/proxy/: tls baz (200; 2.622508ms)
Jan 30 12:02:20.095: INFO: (8) /api/v1/namespaces/proxy-4838/pods/https:proxy-service-rnsxp-gwgn8:462/proxy/: tls qux (200; 2.768118ms)
Jan 30 12:02:20.095: INFO: (8) /api/v1/namespaces/proxy-4838/services/proxy-service-rnsxp:portname2/proxy/: bar (200; 2.778002ms)
Jan 30 12:02:20.095: INFO: (8) /api/v1/namespaces/proxy-4838/services/http:proxy-service-rnsxp:portname2/proxy/: bar (200; 2.874662ms)
Jan 30 12:02:20.095: INFO: (8) /api/v1/namespaces/proxy-4838/services/http:proxy-service-rnsxp:portname1/proxy/: foo (200; 2.926312ms)
Jan 30 12:02:20.095: INFO: (8) /api/v1/namespaces/proxy-4838/pods/http:proxy-service-rnsxp-gwgn8:1080/proxy/: <a href="/api/v1/namespaces/proxy-4838/pods/http:proxy-service-rnsxp-gwgn8:1080/proxy/rewriteme">... (200; 2.974176ms)
Jan 30 12:02:20.096: INFO: (8) /api/v1/namespaces/proxy-4838/services/proxy-service-rnsxp:portname1/proxy/: foo (200; 3.044079ms)
Jan 30 12:02:20.096: INFO: (8) /api/v1/namespaces/proxy-4838/services/https:proxy-service-rnsxp:tlsportname2/proxy/: tls qux (200; 3.265421ms)
Jan 30 12:02:20.096: INFO: (8) /api/v1/namespaces/proxy-4838/pods/http:proxy-service-rnsxp-gwgn8:162/proxy/: bar (200; 3.266081ms)
Jan 30 12:02:20.096: INFO: (8) /api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8:162/proxy/: bar (200; 3.29945ms)
Jan 30 12:02:20.096: INFO: (8) /api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8:160/proxy/: foo (200; 3.316995ms)
Jan 30 12:02:20.096: INFO: (8) /api/v1/namespaces/proxy-4838/pods/http:proxy-service-rnsxp-gwgn8:160/proxy/: foo (200; 3.382556ms)
Jan 30 12:02:20.096: INFO: (8) /api/v1/namespaces/proxy-4838/pods/https:proxy-service-rnsxp-gwgn8:460/proxy/: tls baz (200; 3.392283ms)
Jan 30 12:02:20.096: INFO: (8) /api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8/proxy/: <a href="/api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8/proxy/rewriteme">test</a> (200; 3.418686ms)
Jan 30 12:02:20.098: INFO: (9) /api/v1/namespaces/proxy-4838/pods/http:proxy-service-rnsxp-gwgn8:1080/proxy/: <a href="/api/v1/namespaces/proxy-4838/pods/http:proxy-service-rnsxp-gwgn8:1080/proxy/rewriteme">... (200; 2.375635ms)
Jan 30 12:02:20.099: INFO: (9) /api/v1/namespaces/proxy-4838/services/http:proxy-service-rnsxp:portname2/proxy/: bar (200; 2.961634ms)
Jan 30 12:02:20.099: INFO: (9) /api/v1/namespaces/proxy-4838/services/proxy-service-rnsxp:portname1/proxy/: foo (200; 3.055455ms)
Jan 30 12:02:20.099: INFO: (9) /api/v1/namespaces/proxy-4838/pods/https:proxy-service-rnsxp-gwgn8:443/proxy/: <a href="/api/v1/namespaces/proxy-4838/pods/https:proxy-service-rnsxp-gwgn8:443/proxy/tlsrewritem... (200; 3.08042ms)
Jan 30 12:02:20.099: INFO: (9) /api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8:162/proxy/: bar (200; 3.116227ms)
Jan 30 12:02:20.099: INFO: (9) /api/v1/namespaces/proxy-4838/services/https:proxy-service-rnsxp:tlsportname1/proxy/: tls baz (200; 3.195809ms)
Jan 30 12:02:20.099: INFO: (9) /api/v1/namespaces/proxy-4838/services/https:proxy-service-rnsxp:tlsportname2/proxy/: tls qux (200; 3.22908ms)
Jan 30 12:02:20.099: INFO: (9) /api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8:1080/proxy/: <a href="/api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8:1080/proxy/rewriteme">test<... (200; 3.274335ms)
Jan 30 12:02:20.099: INFO: (9) /api/v1/namespaces/proxy-4838/pods/http:proxy-service-rnsxp-gwgn8:162/proxy/: bar (200; 3.429355ms)
Jan 30 12:02:20.099: INFO: (9) /api/v1/namespaces/proxy-4838/services/http:proxy-service-rnsxp:portname1/proxy/: foo (200; 3.39343ms)
Jan 30 12:02:20.099: INFO: (9) /api/v1/namespaces/proxy-4838/pods/https:proxy-service-rnsxp-gwgn8:462/proxy/: tls qux (200; 3.404537ms)
Jan 30 12:02:20.099: INFO: (9) /api/v1/namespaces/proxy-4838/pods/https:proxy-service-rnsxp-gwgn8:460/proxy/: tls baz (200; 3.416615ms)
Jan 30 12:02:20.099: INFO: (9) /api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8/proxy/: <a href="/api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8/proxy/rewriteme">test</a> (200; 3.443969ms)
Jan 30 12:02:20.099: INFO: (9) /api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8:160/proxy/: foo (200; 3.484985ms)
Jan 30 12:02:20.099: INFO: (9) /api/v1/namespaces/proxy-4838/services/proxy-service-rnsxp:portname2/proxy/: bar (200; 3.420189ms)
Jan 30 12:02:20.099: INFO: (9) /api/v1/namespaces/proxy-4838/pods/http:proxy-service-rnsxp-gwgn8:160/proxy/: foo (200; 3.551046ms)
Jan 30 12:02:20.102: INFO: (10) /api/v1/namespaces/proxy-4838/pods/http:proxy-service-rnsxp-gwgn8:1080/proxy/: <a href="/api/v1/namespaces/proxy-4838/pods/http:proxy-service-rnsxp-gwgn8:1080/proxy/rewriteme">... (200; 2.319864ms)
Jan 30 12:02:20.102: INFO: (10) /api/v1/namespaces/proxy-4838/pods/https:proxy-service-rnsxp-gwgn8:460/proxy/: tls baz (200; 2.344883ms)
Jan 30 12:02:20.102: INFO: (10) /api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8:1080/proxy/: <a href="/api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8:1080/proxy/rewriteme">test<... (200; 2.469396ms)
Jan 30 12:02:20.102: INFO: (10) /api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8:162/proxy/: bar (200; 2.503132ms)
Jan 30 12:02:20.102: INFO: (10) /api/v1/namespaces/proxy-4838/pods/https:proxy-service-rnsxp-gwgn8:462/proxy/: tls qux (200; 2.519266ms)
Jan 30 12:02:20.102: INFO: (10) /api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8:160/proxy/: foo (200; 2.707564ms)
Jan 30 12:02:20.102: INFO: (10) /api/v1/namespaces/proxy-4838/services/proxy-service-rnsxp:portname1/proxy/: foo (200; 2.756384ms)
Jan 30 12:02:20.102: INFO: (10) /api/v1/namespaces/proxy-4838/services/http:proxy-service-rnsxp:portname2/proxy/: bar (200; 2.868168ms)
Jan 30 12:02:20.103: INFO: (10) /api/v1/namespaces/proxy-4838/services/https:proxy-service-rnsxp:tlsportname2/proxy/: tls qux (200; 2.97779ms)
Jan 30 12:02:20.103: INFO: (10) /api/v1/namespaces/proxy-4838/services/http:proxy-service-rnsxp:portname1/proxy/: foo (200; 2.970948ms)
Jan 30 12:02:20.103: INFO: (10) /api/v1/namespaces/proxy-4838/services/proxy-service-rnsxp:portname2/proxy/: bar (200; 2.968969ms)
Jan 30 12:02:20.103: INFO: (10) /api/v1/namespaces/proxy-4838/pods/http:proxy-service-rnsxp-gwgn8:160/proxy/: foo (200; 3.193274ms)
Jan 30 12:02:20.103: INFO: (10) /api/v1/namespaces/proxy-4838/pods/https:proxy-service-rnsxp-gwgn8:443/proxy/: <a href="/api/v1/namespaces/proxy-4838/pods/https:proxy-service-rnsxp-gwgn8:443/proxy/tlsrewritem... (200; 3.213854ms)
Jan 30 12:02:20.103: INFO: (10) /api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8/proxy/: <a href="/api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8/proxy/rewriteme">test</a> (200; 3.185103ms)
Jan 30 12:02:20.103: INFO: (10) /api/v1/namespaces/proxy-4838/services/https:proxy-service-rnsxp:tlsportname1/proxy/: tls baz (200; 3.149393ms)
Jan 30 12:02:20.103: INFO: (10) /api/v1/namespaces/proxy-4838/pods/http:proxy-service-rnsxp-gwgn8:162/proxy/: bar (200; 3.208731ms)
Jan 30 12:02:20.105: INFO: (11) /api/v1/namespaces/proxy-4838/services/proxy-service-rnsxp:portname1/proxy/: foo (200; 2.451398ms)
Jan 30 12:02:20.105: INFO: (11) /api/v1/namespaces/proxy-4838/services/http:proxy-service-rnsxp:portname1/proxy/: foo (200; 2.540359ms)
Jan 30 12:02:20.106: INFO: (11) /api/v1/namespaces/proxy-4838/pods/http:proxy-service-rnsxp-gwgn8:160/proxy/: foo (200; 2.731012ms)
Jan 30 12:02:20.106: INFO: (11) /api/v1/namespaces/proxy-4838/pods/https:proxy-service-rnsxp-gwgn8:462/proxy/: tls qux (200; 2.747784ms)
Jan 30 12:02:20.106: INFO: (11) /api/v1/namespaces/proxy-4838/services/https:proxy-service-rnsxp:tlsportname2/proxy/: tls qux (200; 2.811256ms)
Jan 30 12:02:20.106: INFO: (11) /api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8/proxy/: <a href="/api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8/proxy/rewriteme">test</a> (200; 2.805841ms)
Jan 30 12:02:20.106: INFO: (11) /api/v1/namespaces/proxy-4838/services/proxy-service-rnsxp:portname2/proxy/: bar (200; 2.869944ms)
Jan 30 12:02:20.106: INFO: (11) /api/v1/namespaces/proxy-4838/services/https:proxy-service-rnsxp:tlsportname1/proxy/: tls baz (200; 2.814383ms)
Jan 30 12:02:20.106: INFO: (11) /api/v1/namespaces/proxy-4838/services/http:proxy-service-rnsxp:portname2/proxy/: bar (200; 2.815395ms)
Jan 30 12:02:20.106: INFO: (11) /api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8:1080/proxy/: <a href="/api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8:1080/proxy/rewriteme">test<... (200; 2.852989ms)
Jan 30 12:02:20.106: INFO: (11) /api/v1/namespaces/proxy-4838/pods/http:proxy-service-rnsxp-gwgn8:1080/proxy/: <a href="/api/v1/namespaces/proxy-4838/pods/http:proxy-service-rnsxp-gwgn8:1080/proxy/rewriteme">... (200; 2.846392ms)
Jan 30 12:02:20.106: INFO: (11) /api/v1/namespaces/proxy-4838/pods/https:proxy-service-rnsxp-gwgn8:443/proxy/: <a href="/api/v1/namespaces/proxy-4838/pods/https:proxy-service-rnsxp-gwgn8:443/proxy/tlsrewritem... (200; 2.881455ms)
Jan 30 12:02:20.106: INFO: (11) /api/v1/namespaces/proxy-4838/pods/https:proxy-service-rnsxp-gwgn8:460/proxy/: tls baz (200; 2.894165ms)
Jan 30 12:02:20.106: INFO: (11) /api/v1/namespaces/proxy-4838/pods/http:proxy-service-rnsxp-gwgn8:162/proxy/: bar (200; 2.945399ms)
Jan 30 12:02:20.106: INFO: (11) /api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8:160/proxy/: foo (200; 3.0454ms)
Jan 30 12:02:20.106: INFO: (11) /api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8:162/proxy/: bar (200; 3.054808ms)
Jan 30 12:02:20.108: INFO: (12) /api/v1/namespaces/proxy-4838/pods/http:proxy-service-rnsxp-gwgn8:162/proxy/: bar (200; 1.920575ms)
Jan 30 12:02:20.108: INFO: (12) /api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8:160/proxy/: foo (200; 1.910892ms)
Jan 30 12:02:20.108: INFO: (12) /api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8:162/proxy/: bar (200; 2.417206ms)
Jan 30 12:02:20.108: INFO: (12) /api/v1/namespaces/proxy-4838/services/https:proxy-service-rnsxp:tlsportname1/proxy/: tls baz (200; 2.546941ms)
Jan 30 12:02:20.108: INFO: (12) /api/v1/namespaces/proxy-4838/pods/https:proxy-service-rnsxp-gwgn8:443/proxy/: <a href="/api/v1/namespaces/proxy-4838/pods/https:proxy-service-rnsxp-gwgn8:443/proxy/tlsrewritem... (200; 2.563678ms)
Jan 30 12:02:20.108: INFO: (12) /api/v1/namespaces/proxy-4838/pods/http:proxy-service-rnsxp-gwgn8:160/proxy/: foo (200; 2.574865ms)
Jan 30 12:02:20.109: INFO: (12) /api/v1/namespaces/proxy-4838/services/proxy-service-rnsxp:portname2/proxy/: bar (200; 2.651071ms)
Jan 30 12:02:20.109: INFO: (12) /api/v1/namespaces/proxy-4838/pods/https:proxy-service-rnsxp-gwgn8:460/proxy/: tls baz (200; 2.746888ms)
Jan 30 12:02:20.109: INFO: (12) /api/v1/namespaces/proxy-4838/pods/https:proxy-service-rnsxp-gwgn8:462/proxy/: tls qux (200; 2.895378ms)
Jan 30 12:02:20.109: INFO: (12) /api/v1/namespaces/proxy-4838/services/http:proxy-service-rnsxp:portname2/proxy/: bar (200; 2.899626ms)
Jan 30 12:02:20.109: INFO: (12) /api/v1/namespaces/proxy-4838/services/http:proxy-service-rnsxp:portname1/proxy/: foo (200; 2.919446ms)
Jan 30 12:02:20.109: INFO: (12) /api/v1/namespaces/proxy-4838/services/proxy-service-rnsxp:portname1/proxy/: foo (200; 2.890155ms)
Jan 30 12:02:20.109: INFO: (12) /api/v1/namespaces/proxy-4838/pods/http:proxy-service-rnsxp-gwgn8:1080/proxy/: <a href="/api/v1/namespaces/proxy-4838/pods/http:proxy-service-rnsxp-gwgn8:1080/proxy/rewriteme">... (200; 3.022503ms)
Jan 30 12:02:20.109: INFO: (12) /api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8/proxy/: <a href="/api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8/proxy/rewriteme">test</a> (200; 3.044131ms)
Jan 30 12:02:20.109: INFO: (12) /api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8:1080/proxy/: <a href="/api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8:1080/proxy/rewriteme">test<... (200; 3.057814ms)
Jan 30 12:02:20.109: INFO: (12) /api/v1/namespaces/proxy-4838/services/https:proxy-service-rnsxp:tlsportname2/proxy/: tls qux (200; 3.060944ms)
Jan 30 12:02:20.111: INFO: (13) /api/v1/namespaces/proxy-4838/pods/http:proxy-service-rnsxp-gwgn8:160/proxy/: foo (200; 2.167102ms)
Jan 30 12:02:20.111: INFO: (13) /api/v1/namespaces/proxy-4838/pods/https:proxy-service-rnsxp-gwgn8:462/proxy/: tls qux (200; 2.182345ms)
Jan 30 12:02:20.111: INFO: (13) /api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8:160/proxy/: foo (200; 2.207303ms)
Jan 30 12:02:20.111: INFO: (13) /api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8:1080/proxy/: <a href="/api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8:1080/proxy/rewriteme">test<... (200; 2.327398ms)
Jan 30 12:02:20.112: INFO: (13) /api/v1/namespaces/proxy-4838/services/http:proxy-service-rnsxp:portname1/proxy/: foo (200; 2.583423ms)
Jan 30 12:02:20.112: INFO: (13) /api/v1/namespaces/proxy-4838/services/proxy-service-rnsxp:portname1/proxy/: foo (200; 2.707679ms)
Jan 30 12:02:20.112: INFO: (13) /api/v1/namespaces/proxy-4838/pods/https:proxy-service-rnsxp-gwgn8:460/proxy/: tls baz (200; 2.783102ms)
Jan 30 12:02:20.112: INFO: (13) /api/v1/namespaces/proxy-4838/services/proxy-service-rnsxp:portname2/proxy/: bar (200; 2.796663ms)
Jan 30 12:02:20.112: INFO: (13) /api/v1/namespaces/proxy-4838/services/https:proxy-service-rnsxp:tlsportname2/proxy/: tls qux (200; 2.834007ms)
Jan 30 12:02:20.112: INFO: (13) /api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8/proxy/: <a href="/api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8/proxy/rewriteme">test</a> (200; 2.852818ms)
Jan 30 12:02:20.112: INFO: (13) /api/v1/namespaces/proxy-4838/services/https:proxy-service-rnsxp:tlsportname1/proxy/: tls baz (200; 2.884749ms)
Jan 30 12:02:20.112: INFO: (13) /api/v1/namespaces/proxy-4838/pods/http:proxy-service-rnsxp-gwgn8:1080/proxy/: <a href="/api/v1/namespaces/proxy-4838/pods/http:proxy-service-rnsxp-gwgn8:1080/proxy/rewriteme">... (200; 2.919679ms)
Jan 30 12:02:20.112: INFO: (13) /api/v1/namespaces/proxy-4838/pods/https:proxy-service-rnsxp-gwgn8:443/proxy/: <a href="/api/v1/namespaces/proxy-4838/pods/https:proxy-service-rnsxp-gwgn8:443/proxy/tlsrewritem... (200; 3.148687ms)
Jan 30 12:02:20.112: INFO: (13) /api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8:162/proxy/: bar (200; 3.164164ms)
Jan 30 12:02:20.112: INFO: (13) /api/v1/namespaces/proxy-4838/services/http:proxy-service-rnsxp:portname2/proxy/: bar (200; 3.077785ms)
Jan 30 12:02:20.112: INFO: (13) /api/v1/namespaces/proxy-4838/pods/http:proxy-service-rnsxp-gwgn8:162/proxy/: bar (200; 3.236506ms)
Jan 30 12:02:20.115: INFO: (14) /api/v1/namespaces/proxy-4838/pods/http:proxy-service-rnsxp-gwgn8:162/proxy/: bar (200; 2.262429ms)
Jan 30 12:02:20.115: INFO: (14) /api/v1/namespaces/proxy-4838/pods/http:proxy-service-rnsxp-gwgn8:160/proxy/: foo (200; 2.294105ms)
Jan 30 12:02:20.115: INFO: (14) /api/v1/namespaces/proxy-4838/pods/https:proxy-service-rnsxp-gwgn8:462/proxy/: tls qux (200; 2.350172ms)
Jan 30 12:02:20.115: INFO: (14) /api/v1/namespaces/proxy-4838/services/http:proxy-service-rnsxp:portname2/proxy/: bar (200; 2.477061ms)
Jan 30 12:02:20.115: INFO: (14) /api/v1/namespaces/proxy-4838/services/proxy-service-rnsxp:portname1/proxy/: foo (200; 2.464079ms)
Jan 30 12:02:20.115: INFO: (14) /api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8:160/proxy/: foo (200; 2.54147ms)
Jan 30 12:02:20.115: INFO: (14) /api/v1/namespaces/proxy-4838/pods/https:proxy-service-rnsxp-gwgn8:460/proxy/: tls baz (200; 2.665656ms)
Jan 30 12:02:20.115: INFO: (14) /api/v1/namespaces/proxy-4838/services/proxy-service-rnsxp:portname2/proxy/: bar (200; 3.021495ms)
Jan 30 12:02:20.115: INFO: (14) /api/v1/namespaces/proxy-4838/services/http:proxy-service-rnsxp:portname1/proxy/: foo (200; 2.985603ms)
Jan 30 12:02:20.115: INFO: (14) /api/v1/namespaces/proxy-4838/services/https:proxy-service-rnsxp:tlsportname2/proxy/: tls qux (200; 2.999605ms)
Jan 30 12:02:20.115: INFO: (14) /api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8:162/proxy/: bar (200; 3.067221ms)
Jan 30 12:02:20.115: INFO: (14) /api/v1/namespaces/proxy-4838/services/https:proxy-service-rnsxp:tlsportname1/proxy/: tls baz (200; 3.013504ms)
Jan 30 12:02:20.115: INFO: (14) /api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8/proxy/: <a href="/api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8/proxy/rewriteme">test</a> (200; 3.030115ms)
Jan 30 12:02:20.115: INFO: (14) /api/v1/namespaces/proxy-4838/pods/https:proxy-service-rnsxp-gwgn8:443/proxy/: <a href="/api/v1/namespaces/proxy-4838/pods/https:proxy-service-rnsxp-gwgn8:443/proxy/tlsrewritem... (200; 3.07162ms)
Jan 30 12:02:20.115: INFO: (14) /api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8:1080/proxy/: <a href="/api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8:1080/proxy/rewriteme">test<... (200; 3.034137ms)
Jan 30 12:02:20.115: INFO: (14) /api/v1/namespaces/proxy-4838/pods/http:proxy-service-rnsxp-gwgn8:1080/proxy/: <a href="/api/v1/namespaces/proxy-4838/pods/http:proxy-service-rnsxp-gwgn8:1080/proxy/rewriteme">... (200; 3.112411ms)
Jan 30 12:02:20.118: INFO: (15) /api/v1/namespaces/proxy-4838/services/proxy-service-rnsxp:portname1/proxy/: foo (200; 2.42285ms)
Jan 30 12:02:20.118: INFO: (15) /api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8/proxy/: <a href="/api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8/proxy/rewriteme">test</a> (200; 2.485299ms)
Jan 30 12:02:20.118: INFO: (15) /api/v1/namespaces/proxy-4838/pods/https:proxy-service-rnsxp-gwgn8:462/proxy/: tls qux (200; 2.537644ms)
Jan 30 12:02:20.118: INFO: (15) /api/v1/namespaces/proxy-4838/pods/https:proxy-service-rnsxp-gwgn8:443/proxy/: <a href="/api/v1/namespaces/proxy-4838/pods/https:proxy-service-rnsxp-gwgn8:443/proxy/tlsrewritem... (200; 2.536486ms)
Jan 30 12:02:20.118: INFO: (15) /api/v1/namespaces/proxy-4838/services/http:proxy-service-rnsxp:portname2/proxy/: bar (200; 2.75827ms)
Jan 30 12:02:20.118: INFO: (15) /api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8:160/proxy/: foo (200; 2.903032ms)
Jan 30 12:02:20.118: INFO: (15) /api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8:162/proxy/: bar (200; 2.854792ms)
Jan 30 12:02:20.118: INFO: (15) /api/v1/namespaces/proxy-4838/pods/https:proxy-service-rnsxp-gwgn8:460/proxy/: tls baz (200; 2.907533ms)
Jan 30 12:02:20.118: INFO: (15) /api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8:1080/proxy/: <a href="/api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8:1080/proxy/rewriteme">test<... (200; 2.931726ms)
Jan 30 12:02:20.119: INFO: (15) /api/v1/namespaces/proxy-4838/pods/http:proxy-service-rnsxp-gwgn8:1080/proxy/: <a href="/api/v1/namespaces/proxy-4838/pods/http:proxy-service-rnsxp-gwgn8:1080/proxy/rewriteme">... (200; 3.003026ms)
Jan 30 12:02:20.119: INFO: (15) /api/v1/namespaces/proxy-4838/pods/http:proxy-service-rnsxp-gwgn8:162/proxy/: bar (200; 2.977494ms)
Jan 30 12:02:20.119: INFO: (15) /api/v1/namespaces/proxy-4838/services/https:proxy-service-rnsxp:tlsportname2/proxy/: tls qux (200; 3.083681ms)
Jan 30 12:02:20.119: INFO: (15) /api/v1/namespaces/proxy-4838/services/http:proxy-service-rnsxp:portname1/proxy/: foo (200; 3.098538ms)
Jan 30 12:02:20.119: INFO: (15) /api/v1/namespaces/proxy-4838/services/proxy-service-rnsxp:portname2/proxy/: bar (200; 3.133549ms)
Jan 30 12:02:20.119: INFO: (15) /api/v1/namespaces/proxy-4838/services/https:proxy-service-rnsxp:tlsportname1/proxy/: tls baz (200; 3.138649ms)
Jan 30 12:02:20.119: INFO: (15) /api/v1/namespaces/proxy-4838/pods/http:proxy-service-rnsxp-gwgn8:160/proxy/: foo (200; 3.18582ms)
Jan 30 12:02:20.120: INFO: (16) /api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8:162/proxy/: bar (200; 1.746966ms)
Jan 30 12:02:20.121: INFO: (16) /api/v1/namespaces/proxy-4838/pods/http:proxy-service-rnsxp-gwgn8:1080/proxy/: <a href="/api/v1/namespaces/proxy-4838/pods/http:proxy-service-rnsxp-gwgn8:1080/proxy/rewriteme">... (200; 2.104598ms)
Jan 30 12:02:20.122: INFO: (16) /api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8/proxy/: <a href="/api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8/proxy/rewriteme">test</a> (200; 3.047971ms)
Jan 30 12:02:20.122: INFO: (16) /api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8:160/proxy/: foo (200; 3.079301ms)
Jan 30 12:02:20.122: INFO: (16) /api/v1/namespaces/proxy-4838/services/proxy-service-rnsxp:portname2/proxy/: bar (200; 3.098015ms)
Jan 30 12:02:20.122: INFO: (16) /api/v1/namespaces/proxy-4838/services/https:proxy-service-rnsxp:tlsportname1/proxy/: tls baz (200; 3.150929ms)
Jan 30 12:02:20.122: INFO: (16) /api/v1/namespaces/proxy-4838/services/http:proxy-service-rnsxp:portname2/proxy/: bar (200; 3.119641ms)
Jan 30 12:02:20.122: INFO: (16) /api/v1/namespaces/proxy-4838/services/proxy-service-rnsxp:portname1/proxy/: foo (200; 3.167865ms)
Jan 30 12:02:20.122: INFO: (16) /api/v1/namespaces/proxy-4838/pods/http:proxy-service-rnsxp-gwgn8:162/proxy/: bar (200; 3.36007ms)
Jan 30 12:02:20.122: INFO: (16) /api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8:1080/proxy/: <a href="/api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8:1080/proxy/rewriteme">test<... (200; 3.397543ms)
Jan 30 12:02:20.122: INFO: (16) /api/v1/namespaces/proxy-4838/services/https:proxy-service-rnsxp:tlsportname2/proxy/: tls qux (200; 3.389924ms)
Jan 30 12:02:20.122: INFO: (16) /api/v1/namespaces/proxy-4838/pods/https:proxy-service-rnsxp-gwgn8:460/proxy/: tls baz (200; 3.425363ms)
Jan 30 12:02:20.122: INFO: (16) /api/v1/namespaces/proxy-4838/pods/http:proxy-service-rnsxp-gwgn8:160/proxy/: foo (200; 3.466403ms)
Jan 30 12:02:20.122: INFO: (16) /api/v1/namespaces/proxy-4838/pods/https:proxy-service-rnsxp-gwgn8:462/proxy/: tls qux (200; 3.446995ms)
Jan 30 12:02:20.122: INFO: (16) /api/v1/namespaces/proxy-4838/services/http:proxy-service-rnsxp:portname1/proxy/: foo (200; 3.508791ms)
Jan 30 12:02:20.122: INFO: (16) /api/v1/namespaces/proxy-4838/pods/https:proxy-service-rnsxp-gwgn8:443/proxy/: <a href="/api/v1/namespaces/proxy-4838/pods/https:proxy-service-rnsxp-gwgn8:443/proxy/tlsrewritem... (200; 3.50431ms)
Jan 30 12:02:20.125: INFO: (17) /api/v1/namespaces/proxy-4838/services/proxy-service-rnsxp:portname2/proxy/: bar (200; 2.551689ms)
Jan 30 12:02:20.125: INFO: (17) /api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8:1080/proxy/: <a href="/api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8:1080/proxy/rewriteme">test<... (200; 2.530783ms)
Jan 30 12:02:20.125: INFO: (17) /api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8:162/proxy/: bar (200; 2.513421ms)
Jan 30 12:02:20.125: INFO: (17) /api/v1/namespaces/proxy-4838/services/https:proxy-service-rnsxp:tlsportname1/proxy/: tls baz (200; 2.844104ms)
Jan 30 12:02:20.125: INFO: (17) /api/v1/namespaces/proxy-4838/services/http:proxy-service-rnsxp:portname2/proxy/: bar (200; 3.055474ms)
Jan 30 12:02:20.125: INFO: (17) /api/v1/namespaces/proxy-4838/pods/https:proxy-service-rnsxp-gwgn8:460/proxy/: tls baz (200; 3.0855ms)
Jan 30 12:02:20.125: INFO: (17) /api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8/proxy/: <a href="/api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8/proxy/rewriteme">test</a> (200; 3.114705ms)
Jan 30 12:02:20.125: INFO: (17) /api/v1/namespaces/proxy-4838/services/proxy-service-rnsxp:portname1/proxy/: foo (200; 3.0846ms)
Jan 30 12:02:20.125: INFO: (17) /api/v1/namespaces/proxy-4838/services/https:proxy-service-rnsxp:tlsportname2/proxy/: tls qux (200; 3.129383ms)
Jan 30 12:02:20.125: INFO: (17) /api/v1/namespaces/proxy-4838/pods/http:proxy-service-rnsxp-gwgn8:162/proxy/: bar (200; 3.17722ms)
Jan 30 12:02:20.126: INFO: (17) /api/v1/namespaces/proxy-4838/services/http:proxy-service-rnsxp:portname1/proxy/: foo (200; 3.136285ms)
Jan 30 12:02:20.126: INFO: (17) /api/v1/namespaces/proxy-4838/pods/https:proxy-service-rnsxp-gwgn8:443/proxy/: <a href="/api/v1/namespaces/proxy-4838/pods/https:proxy-service-rnsxp-gwgn8:443/proxy/tlsrewritem... (200; 3.185053ms)
Jan 30 12:02:20.126: INFO: (17) /api/v1/namespaces/proxy-4838/pods/http:proxy-service-rnsxp-gwgn8:1080/proxy/: <a href="/api/v1/namespaces/proxy-4838/pods/http:proxy-service-rnsxp-gwgn8:1080/proxy/rewriteme">... (200; 3.248766ms)
Jan 30 12:02:20.126: INFO: (17) /api/v1/namespaces/proxy-4838/pods/http:proxy-service-rnsxp-gwgn8:160/proxy/: foo (200; 3.265316ms)
Jan 30 12:02:20.126: INFO: (17) /api/v1/namespaces/proxy-4838/pods/https:proxy-service-rnsxp-gwgn8:462/proxy/: tls qux (200; 3.338356ms)
Jan 30 12:02:20.126: INFO: (17) /api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8:160/proxy/: foo (200; 3.324456ms)
Jan 30 12:02:20.128: INFO: (18) /api/v1/namespaces/proxy-4838/services/http:proxy-service-rnsxp:portname1/proxy/: foo (200; 2.673017ms)
Jan 30 12:02:20.128: INFO: (18) /api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8/proxy/: <a href="/api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8/proxy/rewriteme">test</a> (200; 2.71233ms)
Jan 30 12:02:20.128: INFO: (18) /api/v1/namespaces/proxy-4838/pods/http:proxy-service-rnsxp-gwgn8:162/proxy/: bar (200; 2.693873ms)
Jan 30 12:02:20.129: INFO: (18) /api/v1/namespaces/proxy-4838/pods/https:proxy-service-rnsxp-gwgn8:460/proxy/: tls baz (200; 3.109412ms)
Jan 30 12:02:20.129: INFO: (18) /api/v1/namespaces/proxy-4838/services/proxy-service-rnsxp:portname1/proxy/: foo (200; 3.169009ms)
Jan 30 12:02:20.129: INFO: (18) /api/v1/namespaces/proxy-4838/pods/http:proxy-service-rnsxp-gwgn8:1080/proxy/: <a href="/api/v1/namespaces/proxy-4838/pods/http:proxy-service-rnsxp-gwgn8:1080/proxy/rewriteme">... (200; 3.182784ms)
Jan 30 12:02:20.129: INFO: (18) /api/v1/namespaces/proxy-4838/services/https:proxy-service-rnsxp:tlsportname1/proxy/: tls baz (200; 3.217193ms)
Jan 30 12:02:20.129: INFO: (18) /api/v1/namespaces/proxy-4838/services/proxy-service-rnsxp:portname2/proxy/: bar (200; 3.17416ms)
Jan 30 12:02:20.129: INFO: (18) /api/v1/namespaces/proxy-4838/pods/http:proxy-service-rnsxp-gwgn8:160/proxy/: foo (200; 3.243941ms)
Jan 30 12:02:20.129: INFO: (18) /api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8:162/proxy/: bar (200; 3.266699ms)
Jan 30 12:02:20.129: INFO: (18) /api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8:160/proxy/: foo (200; 3.275446ms)
Jan 30 12:02:20.129: INFO: (18) /api/v1/namespaces/proxy-4838/services/http:proxy-service-rnsxp:portname2/proxy/: bar (200; 3.383051ms)
Jan 30 12:02:20.129: INFO: (18) /api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8:1080/proxy/: <a href="/api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8:1080/proxy/rewriteme">test<... (200; 3.383268ms)
Jan 30 12:02:20.129: INFO: (18) /api/v1/namespaces/proxy-4838/pods/https:proxy-service-rnsxp-gwgn8:462/proxy/: tls qux (200; 3.363689ms)
Jan 30 12:02:20.129: INFO: (18) /api/v1/namespaces/proxy-4838/pods/https:proxy-service-rnsxp-gwgn8:443/proxy/: <a href="/api/v1/namespaces/proxy-4838/pods/https:proxy-service-rnsxp-gwgn8:443/proxy/tlsrewritem... (200; 3.353435ms)
Jan 30 12:02:20.129: INFO: (18) /api/v1/namespaces/proxy-4838/services/https:proxy-service-rnsxp:tlsportname2/proxy/: tls qux (200; 3.423481ms)
Jan 30 12:02:20.131: INFO: (19) /api/v1/namespaces/proxy-4838/pods/http:proxy-service-rnsxp-gwgn8:1080/proxy/: <a href="/api/v1/namespaces/proxy-4838/pods/http:proxy-service-rnsxp-gwgn8:1080/proxy/rewriteme">... (200; 1.933778ms)
Jan 30 12:02:20.131: INFO: (19) /api/v1/namespaces/proxy-4838/pods/https:proxy-service-rnsxp-gwgn8:460/proxy/: tls baz (200; 2.074361ms)
Jan 30 12:02:20.131: INFO: (19) /api/v1/namespaces/proxy-4838/pods/http:proxy-service-rnsxp-gwgn8:160/proxy/: foo (200; 2.068491ms)
Jan 30 12:02:20.131: INFO: (19) /api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8:162/proxy/: bar (200; 2.126517ms)
Jan 30 12:02:20.131: INFO: (19) /api/v1/namespaces/proxy-4838/pods/https:proxy-service-rnsxp-gwgn8:443/proxy/: <a href="/api/v1/namespaces/proxy-4838/pods/https:proxy-service-rnsxp-gwgn8:443/proxy/tlsrewritem... (200; 2.167476ms)
Jan 30 12:02:20.132: INFO: (19) /api/v1/namespaces/proxy-4838/services/proxy-service-rnsxp:portname2/proxy/: bar (200; 2.859624ms)
Jan 30 12:02:20.132: INFO: (19) /api/v1/namespaces/proxy-4838/services/proxy-service-rnsxp:portname1/proxy/: foo (200; 2.882393ms)
Jan 30 12:02:20.132: INFO: (19) /api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8/proxy/: <a href="/api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8/proxy/rewriteme">test</a> (200; 2.850396ms)
Jan 30 12:02:20.132: INFO: (19) /api/v1/namespaces/proxy-4838/services/https:proxy-service-rnsxp:tlsportname2/proxy/: tls qux (200; 2.883224ms)
Jan 30 12:02:20.132: INFO: (19) /api/v1/namespaces/proxy-4838/services/http:proxy-service-rnsxp:portname1/proxy/: foo (200; 2.954026ms)
Jan 30 12:02:20.132: INFO: (19) /api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8:1080/proxy/: <a href="/api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8:1080/proxy/rewriteme">test<... (200; 2.969008ms)
Jan 30 12:02:20.132: INFO: (19) /api/v1/namespaces/proxy-4838/services/http:proxy-service-rnsxp:portname2/proxy/: bar (200; 3.054947ms)
Jan 30 12:02:20.132: INFO: (19) /api/v1/namespaces/proxy-4838/services/https:proxy-service-rnsxp:tlsportname1/proxy/: tls baz (200; 3.067134ms)
Jan 30 12:02:20.132: INFO: (19) /api/v1/namespaces/proxy-4838/pods/http:proxy-service-rnsxp-gwgn8:162/proxy/: bar (200; 3.119736ms)
Jan 30 12:02:20.132: INFO: (19) /api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8:160/proxy/: foo (200; 3.125445ms)
Jan 30 12:02:20.133: INFO: (19) /api/v1/namespaces/proxy-4838/pods/https:proxy-service-rnsxp-gwgn8:462/proxy/: tls qux (200; 3.344738ms)
STEP: deleting ReplicationController proxy-service-rnsxp in namespace proxy-4838, will wait for the garbage collector to delete the pods 01/30/23 12:02:20.133
Jan 30 12:02:20.187: INFO: Deleting ReplicationController proxy-service-rnsxp took: 2.316883ms
Jan 30 12:02:20.288: INFO: Terminating ReplicationController proxy-service-rnsxp pods took: 100.777029ms
[AfterEach] version v1
  test/e2e/framework/node/init/init.go:32
Jan 30 12:02:22.788: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] version v1
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] version v1
  dump namespaces | framework.go:196
[DeferCleanup (Each)] version v1
  tear down framework | framework.go:193
STEP: Destroying namespace "proxy-4838" for this suite. 01/30/23 12:02:22.791
------------------------------
• [SLOW TEST] [5.806 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    should proxy through a service and a pod  [Conformance]
    test/e2e/network/proxy.go:101

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 12:02:16.987
    Jan 30 12:02:16.987: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename proxy 01/30/23 12:02:16.988
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:02:16.993
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:02:16.995
    [BeforeEach] version v1
      test/e2e/framework/metrics/init/init.go:31
    [It] should proxy through a service and a pod  [Conformance]
      test/e2e/network/proxy.go:101
    STEP: starting an echo server on multiple ports 01/30/23 12:02:17.001
    STEP: creating replication controller proxy-service-rnsxp in namespace proxy-4838 01/30/23 12:02:17.001
    I0130 12:02:17.004913      23 runners.go:193] Created replication controller with name: proxy-service-rnsxp, namespace: proxy-4838, replica count: 1
    I0130 12:02:18.056310      23 runners.go:193] proxy-service-rnsxp Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0130 12:02:19.056582      23 runners.go:193] proxy-service-rnsxp Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
    I0130 12:02:20.057662      23 runners.go:193] proxy-service-rnsxp Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan 30 12:02:20.059: INFO: setup took 3.06152924s, starting test cases
    STEP: running 16 cases, 20 attempts per case, 320 total attempts 01/30/23 12:02:20.059
    Jan 30 12:02:20.063: INFO: (0) /api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8:162/proxy/: bar (200; 3.86409ms)
    Jan 30 12:02:20.063: INFO: (0) /api/v1/namespaces/proxy-4838/services/proxy-service-rnsxp:portname2/proxy/: bar (200; 3.996379ms)
    Jan 30 12:02:20.063: INFO: (0) /api/v1/namespaces/proxy-4838/services/proxy-service-rnsxp:portname1/proxy/: foo (200; 4.001257ms)
    Jan 30 12:02:20.063: INFO: (0) /api/v1/namespaces/proxy-4838/services/http:proxy-service-rnsxp:portname1/proxy/: foo (200; 4.049286ms)
    Jan 30 12:02:20.063: INFO: (0) /api/v1/namespaces/proxy-4838/pods/http:proxy-service-rnsxp-gwgn8:162/proxy/: bar (200; 4.13885ms)
    Jan 30 12:02:20.063: INFO: (0) /api/v1/namespaces/proxy-4838/services/http:proxy-service-rnsxp:portname2/proxy/: bar (200; 4.130778ms)
    Jan 30 12:02:20.063: INFO: (0) /api/v1/namespaces/proxy-4838/pods/http:proxy-service-rnsxp-gwgn8:160/proxy/: foo (200; 4.211638ms)
    Jan 30 12:02:20.063: INFO: (0) /api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8:160/proxy/: foo (200; 4.295606ms)
    Jan 30 12:02:20.064: INFO: (0) /api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8/proxy/: <a href="/api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8/proxy/rewriteme">test</a> (200; 5.202766ms)
    Jan 30 12:02:20.064: INFO: (0) /api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8:1080/proxy/: <a href="/api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8:1080/proxy/rewriteme">test<... (200; 5.242863ms)
    Jan 30 12:02:20.064: INFO: (0) /api/v1/namespaces/proxy-4838/pods/http:proxy-service-rnsxp-gwgn8:1080/proxy/: <a href="/api/v1/namespaces/proxy-4838/pods/http:proxy-service-rnsxp-gwgn8:1080/proxy/rewriteme">... (200; 5.263024ms)
    Jan 30 12:02:20.067: INFO: (0) /api/v1/namespaces/proxy-4838/pods/https:proxy-service-rnsxp-gwgn8:443/proxy/: <a href="/api/v1/namespaces/proxy-4838/pods/https:proxy-service-rnsxp-gwgn8:443/proxy/tlsrewritem... (200; 8.259326ms)
    Jan 30 12:02:20.068: INFO: (0) /api/v1/namespaces/proxy-4838/pods/https:proxy-service-rnsxp-gwgn8:460/proxy/: tls baz (200; 8.766165ms)
    Jan 30 12:02:20.068: INFO: (0) /api/v1/namespaces/proxy-4838/services/https:proxy-service-rnsxp:tlsportname2/proxy/: tls qux (200; 8.726518ms)
    Jan 30 12:02:20.068: INFO: (0) /api/v1/namespaces/proxy-4838/services/https:proxy-service-rnsxp:tlsportname1/proxy/: tls baz (200; 8.815761ms)
    Jan 30 12:02:20.068: INFO: (0) /api/v1/namespaces/proxy-4838/pods/https:proxy-service-rnsxp-gwgn8:462/proxy/: tls qux (200; 8.80622ms)
    Jan 30 12:02:20.070: INFO: (1) /api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8:160/proxy/: foo (200; 2.137785ms)
    Jan 30 12:02:20.071: INFO: (1) /api/v1/namespaces/proxy-4838/services/http:proxy-service-rnsxp:portname1/proxy/: foo (200; 2.897577ms)
    Jan 30 12:02:20.071: INFO: (1) /api/v1/namespaces/proxy-4838/services/http:proxy-service-rnsxp:portname2/proxy/: bar (200; 3.216078ms)
    Jan 30 12:02:20.071: INFO: (1) /api/v1/namespaces/proxy-4838/services/proxy-service-rnsxp:portname1/proxy/: foo (200; 3.248574ms)
    Jan 30 12:02:20.071: INFO: (1) /api/v1/namespaces/proxy-4838/services/https:proxy-service-rnsxp:tlsportname1/proxy/: tls baz (200; 3.258609ms)
    Jan 30 12:02:20.071: INFO: (1) /api/v1/namespaces/proxy-4838/pods/http:proxy-service-rnsxp-gwgn8:160/proxy/: foo (200; 3.288027ms)
    Jan 30 12:02:20.071: INFO: (1) /api/v1/namespaces/proxy-4838/pods/https:proxy-service-rnsxp-gwgn8:443/proxy/: <a href="/api/v1/namespaces/proxy-4838/pods/https:proxy-service-rnsxp-gwgn8:443/proxy/tlsrewritem... (200; 3.349291ms)
    Jan 30 12:02:20.071: INFO: (1) /api/v1/namespaces/proxy-4838/pods/http:proxy-service-rnsxp-gwgn8:1080/proxy/: <a href="/api/v1/namespaces/proxy-4838/pods/http:proxy-service-rnsxp-gwgn8:1080/proxy/rewriteme">... (200; 3.328168ms)
    Jan 30 12:02:20.071: INFO: (1) /api/v1/namespaces/proxy-4838/services/proxy-service-rnsxp:portname2/proxy/: bar (200; 3.413705ms)
    Jan 30 12:02:20.071: INFO: (1) /api/v1/namespaces/proxy-4838/pods/https:proxy-service-rnsxp-gwgn8:460/proxy/: tls baz (200; 3.494332ms)
    Jan 30 12:02:20.071: INFO: (1) /api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8:162/proxy/: bar (200; 3.515438ms)
    Jan 30 12:02:20.071: INFO: (1) /api/v1/namespaces/proxy-4838/services/https:proxy-service-rnsxp:tlsportname2/proxy/: tls qux (200; 3.578766ms)
    Jan 30 12:02:20.071: INFO: (1) /api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8/proxy/: <a href="/api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8/proxy/rewriteme">test</a> (200; 3.600099ms)
    Jan 30 12:02:20.071: INFO: (1) /api/v1/namespaces/proxy-4838/pods/http:proxy-service-rnsxp-gwgn8:162/proxy/: bar (200; 3.576761ms)
    Jan 30 12:02:20.071: INFO: (1) /api/v1/namespaces/proxy-4838/pods/https:proxy-service-rnsxp-gwgn8:462/proxy/: tls qux (200; 3.642793ms)
    Jan 30 12:02:20.071: INFO: (1) /api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8:1080/proxy/: <a href="/api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8:1080/proxy/rewriteme">test<... (200; 3.638967ms)
    Jan 30 12:02:20.074: INFO: (2) /api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8:160/proxy/: foo (200; 2.814832ms)
    Jan 30 12:02:20.074: INFO: (2) /api/v1/namespaces/proxy-4838/services/http:proxy-service-rnsxp:portname1/proxy/: foo (200; 2.88795ms)
    Jan 30 12:02:20.074: INFO: (2) /api/v1/namespaces/proxy-4838/services/https:proxy-service-rnsxp:tlsportname1/proxy/: tls baz (200; 2.876649ms)
    Jan 30 12:02:20.074: INFO: (2) /api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8:1080/proxy/: <a href="/api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8:1080/proxy/rewriteme">test<... (200; 2.829019ms)
    Jan 30 12:02:20.075: INFO: (2) /api/v1/namespaces/proxy-4838/services/proxy-service-rnsxp:portname2/proxy/: bar (200; 2.98507ms)
    Jan 30 12:02:20.075: INFO: (2) /api/v1/namespaces/proxy-4838/pods/http:proxy-service-rnsxp-gwgn8:1080/proxy/: <a href="/api/v1/namespaces/proxy-4838/pods/http:proxy-service-rnsxp-gwgn8:1080/proxy/rewriteme">... (200; 2.98671ms)
    Jan 30 12:02:20.075: INFO: (2) /api/v1/namespaces/proxy-4838/pods/https:proxy-service-rnsxp-gwgn8:460/proxy/: tls baz (200; 3.031595ms)
    Jan 30 12:02:20.075: INFO: (2) /api/v1/namespaces/proxy-4838/services/https:proxy-service-rnsxp:tlsportname2/proxy/: tls qux (200; 3.108632ms)
    Jan 30 12:02:20.075: INFO: (2) /api/v1/namespaces/proxy-4838/services/proxy-service-rnsxp:portname1/proxy/: foo (200; 3.13293ms)
    Jan 30 12:02:20.075: INFO: (2) /api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8/proxy/: <a href="/api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8/proxy/rewriteme">test</a> (200; 3.164602ms)
    Jan 30 12:02:20.075: INFO: (2) /api/v1/namespaces/proxy-4838/services/http:proxy-service-rnsxp:portname2/proxy/: bar (200; 3.334243ms)
    Jan 30 12:02:20.075: INFO: (2) /api/v1/namespaces/proxy-4838/pods/http:proxy-service-rnsxp-gwgn8:160/proxy/: foo (200; 3.345929ms)
    Jan 30 12:02:20.075: INFO: (2) /api/v1/namespaces/proxy-4838/pods/http:proxy-service-rnsxp-gwgn8:162/proxy/: bar (200; 3.321571ms)
    Jan 30 12:02:20.075: INFO: (2) /api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8:162/proxy/: bar (200; 3.403234ms)
    Jan 30 12:02:20.075: INFO: (2) /api/v1/namespaces/proxy-4838/pods/https:proxy-service-rnsxp-gwgn8:462/proxy/: tls qux (200; 3.451419ms)
    Jan 30 12:02:20.075: INFO: (2) /api/v1/namespaces/proxy-4838/pods/https:proxy-service-rnsxp-gwgn8:443/proxy/: <a href="/api/v1/namespaces/proxy-4838/pods/https:proxy-service-rnsxp-gwgn8:443/proxy/tlsrewritem... (200; 3.538076ms)
    Jan 30 12:02:20.077: INFO: (3) /api/v1/namespaces/proxy-4838/pods/http:proxy-service-rnsxp-gwgn8:1080/proxy/: <a href="/api/v1/namespaces/proxy-4838/pods/http:proxy-service-rnsxp-gwgn8:1080/proxy/rewriteme">... (200; 2.204622ms)
    Jan 30 12:02:20.077: INFO: (3) /api/v1/namespaces/proxy-4838/pods/https:proxy-service-rnsxp-gwgn8:462/proxy/: tls qux (200; 2.231532ms)
    Jan 30 12:02:20.077: INFO: (3) /api/v1/namespaces/proxy-4838/pods/https:proxy-service-rnsxp-gwgn8:460/proxy/: tls baz (200; 2.224377ms)
    Jan 30 12:02:20.078: INFO: (3) /api/v1/namespaces/proxy-4838/services/https:proxy-service-rnsxp:tlsportname1/proxy/: tls baz (200; 2.685614ms)
    Jan 30 12:02:20.078: INFO: (3) /api/v1/namespaces/proxy-4838/services/proxy-service-rnsxp:portname2/proxy/: bar (200; 2.74744ms)
    Jan 30 12:02:20.078: INFO: (3) /api/v1/namespaces/proxy-4838/pods/http:proxy-service-rnsxp-gwgn8:160/proxy/: foo (200; 3.062023ms)
    Jan 30 12:02:20.078: INFO: (3) /api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8:162/proxy/: bar (200; 3.332567ms)
    Jan 30 12:02:20.079: INFO: (3) /api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8:160/proxy/: foo (200; 3.38967ms)
    Jan 30 12:02:20.079: INFO: (3) /api/v1/namespaces/proxy-4838/pods/http:proxy-service-rnsxp-gwgn8:162/proxy/: bar (200; 3.402721ms)
    Jan 30 12:02:20.079: INFO: (3) /api/v1/namespaces/proxy-4838/services/proxy-service-rnsxp:portname1/proxy/: foo (200; 3.424508ms)
    Jan 30 12:02:20.079: INFO: (3) /api/v1/namespaces/proxy-4838/pods/https:proxy-service-rnsxp-gwgn8:443/proxy/: <a href="/api/v1/namespaces/proxy-4838/pods/https:proxy-service-rnsxp-gwgn8:443/proxy/tlsrewritem... (200; 3.417263ms)
    Jan 30 12:02:20.079: INFO: (3) /api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8/proxy/: <a href="/api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8/proxy/rewriteme">test</a> (200; 3.448375ms)
    Jan 30 12:02:20.079: INFO: (3) /api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8:1080/proxy/: <a href="/api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8:1080/proxy/rewriteme">test<... (200; 3.447971ms)
    Jan 30 12:02:20.079: INFO: (3) /api/v1/namespaces/proxy-4838/services/https:proxy-service-rnsxp:tlsportname2/proxy/: tls qux (200; 3.472436ms)
    Jan 30 12:02:20.079: INFO: (3) /api/v1/namespaces/proxy-4838/services/http:proxy-service-rnsxp:portname1/proxy/: foo (200; 3.478245ms)
    Jan 30 12:02:20.079: INFO: (3) /api/v1/namespaces/proxy-4838/services/http:proxy-service-rnsxp:portname2/proxy/: bar (200; 3.495363ms)
    Jan 30 12:02:20.081: INFO: (4) /api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8:162/proxy/: bar (200; 2.260858ms)
    Jan 30 12:02:20.081: INFO: (4) /api/v1/namespaces/proxy-4838/pods/http:proxy-service-rnsxp-gwgn8:1080/proxy/: <a href="/api/v1/namespaces/proxy-4838/pods/http:proxy-service-rnsxp-gwgn8:1080/proxy/rewriteme">... (200; 2.556656ms)
    Jan 30 12:02:20.081: INFO: (4) /api/v1/namespaces/proxy-4838/pods/http:proxy-service-rnsxp-gwgn8:160/proxy/: foo (200; 2.587249ms)
    Jan 30 12:02:20.081: INFO: (4) /api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8/proxy/: <a href="/api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8/proxy/rewriteme">test</a> (200; 2.616444ms)
    Jan 30 12:02:20.081: INFO: (4) /api/v1/namespaces/proxy-4838/pods/https:proxy-service-rnsxp-gwgn8:462/proxy/: tls qux (200; 2.652801ms)
    Jan 30 12:02:20.081: INFO: (4) /api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8:1080/proxy/: <a href="/api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8:1080/proxy/rewriteme">test<... (200; 2.667707ms)
    Jan 30 12:02:20.081: INFO: (4) /api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8:160/proxy/: foo (200; 2.632116ms)
    Jan 30 12:02:20.081: INFO: (4) /api/v1/namespaces/proxy-4838/pods/http:proxy-service-rnsxp-gwgn8:162/proxy/: bar (200; 2.676987ms)
    Jan 30 12:02:20.081: INFO: (4) /api/v1/namespaces/proxy-4838/pods/https:proxy-service-rnsxp-gwgn8:460/proxy/: tls baz (200; 2.629981ms)
    Jan 30 12:02:20.081: INFO: (4) /api/v1/namespaces/proxy-4838/pods/https:proxy-service-rnsxp-gwgn8:443/proxy/: <a href="/api/v1/namespaces/proxy-4838/pods/https:proxy-service-rnsxp-gwgn8:443/proxy/tlsrewritem... (200; 2.697426ms)
    Jan 30 12:02:20.081: INFO: (4) /api/v1/namespaces/proxy-4838/services/https:proxy-service-rnsxp:tlsportname1/proxy/: tls baz (200; 2.696494ms)
    Jan 30 12:02:20.082: INFO: (4) /api/v1/namespaces/proxy-4838/services/http:proxy-service-rnsxp:portname1/proxy/: foo (200; 3.232742ms)
    Jan 30 12:02:20.082: INFO: (4) /api/v1/namespaces/proxy-4838/services/proxy-service-rnsxp:portname2/proxy/: bar (200; 3.255972ms)
    Jan 30 12:02:20.082: INFO: (4) /api/v1/namespaces/proxy-4838/services/proxy-service-rnsxp:portname1/proxy/: foo (200; 3.245034ms)
    Jan 30 12:02:20.082: INFO: (4) /api/v1/namespaces/proxy-4838/services/http:proxy-service-rnsxp:portname2/proxy/: bar (200; 3.24211ms)
    Jan 30 12:02:20.082: INFO: (4) /api/v1/namespaces/proxy-4838/services/https:proxy-service-rnsxp:tlsportname2/proxy/: tls qux (200; 3.339128ms)
    Jan 30 12:02:20.084: INFO: (5) /api/v1/namespaces/proxy-4838/pods/https:proxy-service-rnsxp-gwgn8:460/proxy/: tls baz (200; 2.064438ms)
    Jan 30 12:02:20.084: INFO: (5) /api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8:160/proxy/: foo (200; 2.044176ms)
    Jan 30 12:02:20.085: INFO: (5) /api/v1/namespaces/proxy-4838/services/http:proxy-service-rnsxp:portname1/proxy/: foo (200; 2.640358ms)
    Jan 30 12:02:20.085: INFO: (5) /api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8:1080/proxy/: <a href="/api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8:1080/proxy/rewriteme">test<... (200; 2.634275ms)
    Jan 30 12:02:20.085: INFO: (5) /api/v1/namespaces/proxy-4838/services/https:proxy-service-rnsxp:tlsportname2/proxy/: tls qux (200; 3.022583ms)
    Jan 30 12:02:20.085: INFO: (5) /api/v1/namespaces/proxy-4838/pods/http:proxy-service-rnsxp-gwgn8:1080/proxy/: <a href="/api/v1/namespaces/proxy-4838/pods/http:proxy-service-rnsxp-gwgn8:1080/proxy/rewriteme">... (200; 2.949544ms)
    Jan 30 12:02:20.085: INFO: (5) /api/v1/namespaces/proxy-4838/pods/https:proxy-service-rnsxp-gwgn8:443/proxy/: <a href="/api/v1/namespaces/proxy-4838/pods/https:proxy-service-rnsxp-gwgn8:443/proxy/tlsrewritem... (200; 2.947605ms)
    Jan 30 12:02:20.085: INFO: (5) /api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8:162/proxy/: bar (200; 2.977655ms)
    Jan 30 12:02:20.085: INFO: (5) /api/v1/namespaces/proxy-4838/services/proxy-service-rnsxp:portname2/proxy/: bar (200; 2.980414ms)
    Jan 30 12:02:20.085: INFO: (5) /api/v1/namespaces/proxy-4838/services/proxy-service-rnsxp:portname1/proxy/: foo (200; 3.037969ms)
    Jan 30 12:02:20.085: INFO: (5) /api/v1/namespaces/proxy-4838/services/https:proxy-service-rnsxp:tlsportname1/proxy/: tls baz (200; 3.106323ms)
    Jan 30 12:02:20.085: INFO: (5) /api/v1/namespaces/proxy-4838/services/http:proxy-service-rnsxp:portname2/proxy/: bar (200; 3.123128ms)
    Jan 30 12:02:20.085: INFO: (5) /api/v1/namespaces/proxy-4838/pods/http:proxy-service-rnsxp-gwgn8:162/proxy/: bar (200; 3.179313ms)
    Jan 30 12:02:20.085: INFO: (5) /api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8/proxy/: <a href="/api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8/proxy/rewriteme">test</a> (200; 3.210369ms)
    Jan 30 12:02:20.085: INFO: (5) /api/v1/namespaces/proxy-4838/pods/http:proxy-service-rnsxp-gwgn8:160/proxy/: foo (200; 3.24231ms)
    Jan 30 12:02:20.085: INFO: (5) /api/v1/namespaces/proxy-4838/pods/https:proxy-service-rnsxp-gwgn8:462/proxy/: tls qux (200; 3.216871ms)
    Jan 30 12:02:20.087: INFO: (6) /api/v1/namespaces/proxy-4838/pods/http:proxy-service-rnsxp-gwgn8:160/proxy/: foo (200; 1.851599ms)
    Jan 30 12:02:20.087: INFO: (6) /api/v1/namespaces/proxy-4838/pods/https:proxy-service-rnsxp-gwgn8:460/proxy/: tls baz (200; 1.9364ms)
    Jan 30 12:02:20.089: INFO: (6) /api/v1/namespaces/proxy-4838/services/http:proxy-service-rnsxp:portname2/proxy/: bar (200; 3.276394ms)
    Jan 30 12:02:20.089: INFO: (6) /api/v1/namespaces/proxy-4838/services/proxy-service-rnsxp:portname1/proxy/: foo (200; 3.254758ms)
    Jan 30 12:02:20.089: INFO: (6) /api/v1/namespaces/proxy-4838/services/http:proxy-service-rnsxp:portname1/proxy/: foo (200; 3.255754ms)
    Jan 30 12:02:20.089: INFO: (6) /api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8/proxy/: <a href="/api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8/proxy/rewriteme">test</a> (200; 3.305847ms)
    Jan 30 12:02:20.089: INFO: (6) /api/v1/namespaces/proxy-4838/pods/http:proxy-service-rnsxp-gwgn8:1080/proxy/: <a href="/api/v1/namespaces/proxy-4838/pods/http:proxy-service-rnsxp-gwgn8:1080/proxy/rewriteme">... (200; 3.314062ms)
    Jan 30 12:02:20.089: INFO: (6) /api/v1/namespaces/proxy-4838/services/https:proxy-service-rnsxp:tlsportname1/proxy/: tls baz (200; 3.354624ms)
    Jan 30 12:02:20.089: INFO: (6) /api/v1/namespaces/proxy-4838/services/https:proxy-service-rnsxp:tlsportname2/proxy/: tls qux (200; 3.380262ms)
    Jan 30 12:02:20.089: INFO: (6) /api/v1/namespaces/proxy-4838/pods/https:proxy-service-rnsxp-gwgn8:462/proxy/: tls qux (200; 3.362462ms)
    Jan 30 12:02:20.089: INFO: (6) /api/v1/namespaces/proxy-4838/pods/https:proxy-service-rnsxp-gwgn8:443/proxy/: <a href="/api/v1/namespaces/proxy-4838/pods/https:proxy-service-rnsxp-gwgn8:443/proxy/tlsrewritem... (200; 3.344217ms)
    Jan 30 12:02:20.089: INFO: (6) /api/v1/namespaces/proxy-4838/pods/http:proxy-service-rnsxp-gwgn8:162/proxy/: bar (200; 3.382122ms)
    Jan 30 12:02:20.089: INFO: (6) /api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8:162/proxy/: bar (200; 3.379498ms)
    Jan 30 12:02:20.089: INFO: (6) /api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8:1080/proxy/: <a href="/api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8:1080/proxy/rewriteme">test<... (200; 3.573069ms)
    Jan 30 12:02:20.089: INFO: (6) /api/v1/namespaces/proxy-4838/services/proxy-service-rnsxp:portname2/proxy/: bar (200; 3.598828ms)
    Jan 30 12:02:20.089: INFO: (6) /api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8:160/proxy/: foo (200; 3.54442ms)
    Jan 30 12:02:20.091: INFO: (7) /api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8:1080/proxy/: <a href="/api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8:1080/proxy/rewriteme">test<... (200; 1.949196ms)
    Jan 30 12:02:20.091: INFO: (7) /api/v1/namespaces/proxy-4838/pods/https:proxy-service-rnsxp-gwgn8:460/proxy/: tls baz (200; 2.231916ms)
    Jan 30 12:02:20.091: INFO: (7) /api/v1/namespaces/proxy-4838/pods/http:proxy-service-rnsxp-gwgn8:160/proxy/: foo (200; 2.236161ms)
    Jan 30 12:02:20.091: INFO: (7) /api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8:162/proxy/: bar (200; 2.394591ms)
    Jan 30 12:02:20.092: INFO: (7) /api/v1/namespaces/proxy-4838/pods/https:proxy-service-rnsxp-gwgn8:462/proxy/: tls qux (200; 2.537342ms)
    Jan 30 12:02:20.092: INFO: (7) /api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8/proxy/: <a href="/api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8/proxy/rewriteme">test</a> (200; 2.517086ms)
    Jan 30 12:02:20.092: INFO: (7) /api/v1/namespaces/proxy-4838/services/https:proxy-service-rnsxp:tlsportname2/proxy/: tls qux (200; 3.119242ms)
    Jan 30 12:02:20.092: INFO: (7) /api/v1/namespaces/proxy-4838/services/http:proxy-service-rnsxp:portname2/proxy/: bar (200; 3.124934ms)
    Jan 30 12:02:20.092: INFO: (7) /api/v1/namespaces/proxy-4838/services/proxy-service-rnsxp:portname1/proxy/: foo (200; 3.114517ms)
    Jan 30 12:02:20.092: INFO: (7) /api/v1/namespaces/proxy-4838/pods/https:proxy-service-rnsxp-gwgn8:443/proxy/: <a href="/api/v1/namespaces/proxy-4838/pods/https:proxy-service-rnsxp-gwgn8:443/proxy/tlsrewritem... (200; 3.19861ms)
    Jan 30 12:02:20.092: INFO: (7) /api/v1/namespaces/proxy-4838/services/https:proxy-service-rnsxp:tlsportname1/proxy/: tls baz (200; 3.182814ms)
    Jan 30 12:02:20.092: INFO: (7) /api/v1/namespaces/proxy-4838/services/proxy-service-rnsxp:portname2/proxy/: bar (200; 3.163732ms)
    Jan 30 12:02:20.092: INFO: (7) /api/v1/namespaces/proxy-4838/pods/http:proxy-service-rnsxp-gwgn8:1080/proxy/: <a href="/api/v1/namespaces/proxy-4838/pods/http:proxy-service-rnsxp-gwgn8:1080/proxy/rewriteme">... (200; 3.137984ms)
    Jan 30 12:02:20.092: INFO: (7) /api/v1/namespaces/proxy-4838/services/http:proxy-service-rnsxp:portname1/proxy/: foo (200; 3.257544ms)
    Jan 30 12:02:20.092: INFO: (7) /api/v1/namespaces/proxy-4838/pods/http:proxy-service-rnsxp-gwgn8:162/proxy/: bar (200; 3.321681ms)
    Jan 30 12:02:20.092: INFO: (7) /api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8:160/proxy/: foo (200; 3.327967ms)
    Jan 30 12:02:20.095: INFO: (8) /api/v1/namespaces/proxy-4838/pods/https:proxy-service-rnsxp-gwgn8:443/proxy/: <a href="/api/v1/namespaces/proxy-4838/pods/https:proxy-service-rnsxp-gwgn8:443/proxy/tlsrewritem... (200; 2.528524ms)
    Jan 30 12:02:20.095: INFO: (8) /api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8:1080/proxy/: <a href="/api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8:1080/proxy/rewriteme">test<... (200; 2.533801ms)
    Jan 30 12:02:20.095: INFO: (8) /api/v1/namespaces/proxy-4838/services/https:proxy-service-rnsxp:tlsportname1/proxy/: tls baz (200; 2.622508ms)
    Jan 30 12:02:20.095: INFO: (8) /api/v1/namespaces/proxy-4838/pods/https:proxy-service-rnsxp-gwgn8:462/proxy/: tls qux (200; 2.768118ms)
    Jan 30 12:02:20.095: INFO: (8) /api/v1/namespaces/proxy-4838/services/proxy-service-rnsxp:portname2/proxy/: bar (200; 2.778002ms)
    Jan 30 12:02:20.095: INFO: (8) /api/v1/namespaces/proxy-4838/services/http:proxy-service-rnsxp:portname2/proxy/: bar (200; 2.874662ms)
    Jan 30 12:02:20.095: INFO: (8) /api/v1/namespaces/proxy-4838/services/http:proxy-service-rnsxp:portname1/proxy/: foo (200; 2.926312ms)
    Jan 30 12:02:20.095: INFO: (8) /api/v1/namespaces/proxy-4838/pods/http:proxy-service-rnsxp-gwgn8:1080/proxy/: <a href="/api/v1/namespaces/proxy-4838/pods/http:proxy-service-rnsxp-gwgn8:1080/proxy/rewriteme">... (200; 2.974176ms)
    Jan 30 12:02:20.096: INFO: (8) /api/v1/namespaces/proxy-4838/services/proxy-service-rnsxp:portname1/proxy/: foo (200; 3.044079ms)
    Jan 30 12:02:20.096: INFO: (8) /api/v1/namespaces/proxy-4838/services/https:proxy-service-rnsxp:tlsportname2/proxy/: tls qux (200; 3.265421ms)
    Jan 30 12:02:20.096: INFO: (8) /api/v1/namespaces/proxy-4838/pods/http:proxy-service-rnsxp-gwgn8:162/proxy/: bar (200; 3.266081ms)
    Jan 30 12:02:20.096: INFO: (8) /api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8:162/proxy/: bar (200; 3.29945ms)
    Jan 30 12:02:20.096: INFO: (8) /api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8:160/proxy/: foo (200; 3.316995ms)
    Jan 30 12:02:20.096: INFO: (8) /api/v1/namespaces/proxy-4838/pods/http:proxy-service-rnsxp-gwgn8:160/proxy/: foo (200; 3.382556ms)
    Jan 30 12:02:20.096: INFO: (8) /api/v1/namespaces/proxy-4838/pods/https:proxy-service-rnsxp-gwgn8:460/proxy/: tls baz (200; 3.392283ms)
    Jan 30 12:02:20.096: INFO: (8) /api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8/proxy/: <a href="/api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8/proxy/rewriteme">test</a> (200; 3.418686ms)
    Jan 30 12:02:20.098: INFO: (9) /api/v1/namespaces/proxy-4838/pods/http:proxy-service-rnsxp-gwgn8:1080/proxy/: <a href="/api/v1/namespaces/proxy-4838/pods/http:proxy-service-rnsxp-gwgn8:1080/proxy/rewriteme">... (200; 2.375635ms)
    Jan 30 12:02:20.099: INFO: (9) /api/v1/namespaces/proxy-4838/services/http:proxy-service-rnsxp:portname2/proxy/: bar (200; 2.961634ms)
    Jan 30 12:02:20.099: INFO: (9) /api/v1/namespaces/proxy-4838/services/proxy-service-rnsxp:portname1/proxy/: foo (200; 3.055455ms)
    Jan 30 12:02:20.099: INFO: (9) /api/v1/namespaces/proxy-4838/pods/https:proxy-service-rnsxp-gwgn8:443/proxy/: <a href="/api/v1/namespaces/proxy-4838/pods/https:proxy-service-rnsxp-gwgn8:443/proxy/tlsrewritem... (200; 3.08042ms)
    Jan 30 12:02:20.099: INFO: (9) /api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8:162/proxy/: bar (200; 3.116227ms)
    Jan 30 12:02:20.099: INFO: (9) /api/v1/namespaces/proxy-4838/services/https:proxy-service-rnsxp:tlsportname1/proxy/: tls baz (200; 3.195809ms)
    Jan 30 12:02:20.099: INFO: (9) /api/v1/namespaces/proxy-4838/services/https:proxy-service-rnsxp:tlsportname2/proxy/: tls qux (200; 3.22908ms)
    Jan 30 12:02:20.099: INFO: (9) /api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8:1080/proxy/: <a href="/api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8:1080/proxy/rewriteme">test<... (200; 3.274335ms)
    Jan 30 12:02:20.099: INFO: (9) /api/v1/namespaces/proxy-4838/pods/http:proxy-service-rnsxp-gwgn8:162/proxy/: bar (200; 3.429355ms)
    Jan 30 12:02:20.099: INFO: (9) /api/v1/namespaces/proxy-4838/services/http:proxy-service-rnsxp:portname1/proxy/: foo (200; 3.39343ms)
    Jan 30 12:02:20.099: INFO: (9) /api/v1/namespaces/proxy-4838/pods/https:proxy-service-rnsxp-gwgn8:462/proxy/: tls qux (200; 3.404537ms)
    Jan 30 12:02:20.099: INFO: (9) /api/v1/namespaces/proxy-4838/pods/https:proxy-service-rnsxp-gwgn8:460/proxy/: tls baz (200; 3.416615ms)
    Jan 30 12:02:20.099: INFO: (9) /api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8/proxy/: <a href="/api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8/proxy/rewriteme">test</a> (200; 3.443969ms)
    Jan 30 12:02:20.099: INFO: (9) /api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8:160/proxy/: foo (200; 3.484985ms)
    Jan 30 12:02:20.099: INFO: (9) /api/v1/namespaces/proxy-4838/services/proxy-service-rnsxp:portname2/proxy/: bar (200; 3.420189ms)
    Jan 30 12:02:20.099: INFO: (9) /api/v1/namespaces/proxy-4838/pods/http:proxy-service-rnsxp-gwgn8:160/proxy/: foo (200; 3.551046ms)
    Jan 30 12:02:20.102: INFO: (10) /api/v1/namespaces/proxy-4838/pods/http:proxy-service-rnsxp-gwgn8:1080/proxy/: <a href="/api/v1/namespaces/proxy-4838/pods/http:proxy-service-rnsxp-gwgn8:1080/proxy/rewriteme">... (200; 2.319864ms)
    Jan 30 12:02:20.102: INFO: (10) /api/v1/namespaces/proxy-4838/pods/https:proxy-service-rnsxp-gwgn8:460/proxy/: tls baz (200; 2.344883ms)
    Jan 30 12:02:20.102: INFO: (10) /api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8:1080/proxy/: <a href="/api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8:1080/proxy/rewriteme">test<... (200; 2.469396ms)
    Jan 30 12:02:20.102: INFO: (10) /api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8:162/proxy/: bar (200; 2.503132ms)
    Jan 30 12:02:20.102: INFO: (10) /api/v1/namespaces/proxy-4838/pods/https:proxy-service-rnsxp-gwgn8:462/proxy/: tls qux (200; 2.519266ms)
    Jan 30 12:02:20.102: INFO: (10) /api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8:160/proxy/: foo (200; 2.707564ms)
    Jan 30 12:02:20.102: INFO: (10) /api/v1/namespaces/proxy-4838/services/proxy-service-rnsxp:portname1/proxy/: foo (200; 2.756384ms)
    Jan 30 12:02:20.102: INFO: (10) /api/v1/namespaces/proxy-4838/services/http:proxy-service-rnsxp:portname2/proxy/: bar (200; 2.868168ms)
    Jan 30 12:02:20.103: INFO: (10) /api/v1/namespaces/proxy-4838/services/https:proxy-service-rnsxp:tlsportname2/proxy/: tls qux (200; 2.97779ms)
    Jan 30 12:02:20.103: INFO: (10) /api/v1/namespaces/proxy-4838/services/http:proxy-service-rnsxp:portname1/proxy/: foo (200; 2.970948ms)
    Jan 30 12:02:20.103: INFO: (10) /api/v1/namespaces/proxy-4838/services/proxy-service-rnsxp:portname2/proxy/: bar (200; 2.968969ms)
    Jan 30 12:02:20.103: INFO: (10) /api/v1/namespaces/proxy-4838/pods/http:proxy-service-rnsxp-gwgn8:160/proxy/: foo (200; 3.193274ms)
    Jan 30 12:02:20.103: INFO: (10) /api/v1/namespaces/proxy-4838/pods/https:proxy-service-rnsxp-gwgn8:443/proxy/: <a href="/api/v1/namespaces/proxy-4838/pods/https:proxy-service-rnsxp-gwgn8:443/proxy/tlsrewritem... (200; 3.213854ms)
    Jan 30 12:02:20.103: INFO: (10) /api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8/proxy/: <a href="/api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8/proxy/rewriteme">test</a> (200; 3.185103ms)
    Jan 30 12:02:20.103: INFO: (10) /api/v1/namespaces/proxy-4838/services/https:proxy-service-rnsxp:tlsportname1/proxy/: tls baz (200; 3.149393ms)
    Jan 30 12:02:20.103: INFO: (10) /api/v1/namespaces/proxy-4838/pods/http:proxy-service-rnsxp-gwgn8:162/proxy/: bar (200; 3.208731ms)
    Jan 30 12:02:20.105: INFO: (11) /api/v1/namespaces/proxy-4838/services/proxy-service-rnsxp:portname1/proxy/: foo (200; 2.451398ms)
    Jan 30 12:02:20.105: INFO: (11) /api/v1/namespaces/proxy-4838/services/http:proxy-service-rnsxp:portname1/proxy/: foo (200; 2.540359ms)
    Jan 30 12:02:20.106: INFO: (11) /api/v1/namespaces/proxy-4838/pods/http:proxy-service-rnsxp-gwgn8:160/proxy/: foo (200; 2.731012ms)
    Jan 30 12:02:20.106: INFO: (11) /api/v1/namespaces/proxy-4838/pods/https:proxy-service-rnsxp-gwgn8:462/proxy/: tls qux (200; 2.747784ms)
    Jan 30 12:02:20.106: INFO: (11) /api/v1/namespaces/proxy-4838/services/https:proxy-service-rnsxp:tlsportname2/proxy/: tls qux (200; 2.811256ms)
    Jan 30 12:02:20.106: INFO: (11) /api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8/proxy/: <a href="/api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8/proxy/rewriteme">test</a> (200; 2.805841ms)
    Jan 30 12:02:20.106: INFO: (11) /api/v1/namespaces/proxy-4838/services/proxy-service-rnsxp:portname2/proxy/: bar (200; 2.869944ms)
    Jan 30 12:02:20.106: INFO: (11) /api/v1/namespaces/proxy-4838/services/https:proxy-service-rnsxp:tlsportname1/proxy/: tls baz (200; 2.814383ms)
    Jan 30 12:02:20.106: INFO: (11) /api/v1/namespaces/proxy-4838/services/http:proxy-service-rnsxp:portname2/proxy/: bar (200; 2.815395ms)
    Jan 30 12:02:20.106: INFO: (11) /api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8:1080/proxy/: <a href="/api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8:1080/proxy/rewriteme">test<... (200; 2.852989ms)
    Jan 30 12:02:20.106: INFO: (11) /api/v1/namespaces/proxy-4838/pods/http:proxy-service-rnsxp-gwgn8:1080/proxy/: <a href="/api/v1/namespaces/proxy-4838/pods/http:proxy-service-rnsxp-gwgn8:1080/proxy/rewriteme">... (200; 2.846392ms)
    Jan 30 12:02:20.106: INFO: (11) /api/v1/namespaces/proxy-4838/pods/https:proxy-service-rnsxp-gwgn8:443/proxy/: <a href="/api/v1/namespaces/proxy-4838/pods/https:proxy-service-rnsxp-gwgn8:443/proxy/tlsrewritem... (200; 2.881455ms)
    Jan 30 12:02:20.106: INFO: (11) /api/v1/namespaces/proxy-4838/pods/https:proxy-service-rnsxp-gwgn8:460/proxy/: tls baz (200; 2.894165ms)
    Jan 30 12:02:20.106: INFO: (11) /api/v1/namespaces/proxy-4838/pods/http:proxy-service-rnsxp-gwgn8:162/proxy/: bar (200; 2.945399ms)
    Jan 30 12:02:20.106: INFO: (11) /api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8:160/proxy/: foo (200; 3.0454ms)
    Jan 30 12:02:20.106: INFO: (11) /api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8:162/proxy/: bar (200; 3.054808ms)
    Jan 30 12:02:20.108: INFO: (12) /api/v1/namespaces/proxy-4838/pods/http:proxy-service-rnsxp-gwgn8:162/proxy/: bar (200; 1.920575ms)
    Jan 30 12:02:20.108: INFO: (12) /api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8:160/proxy/: foo (200; 1.910892ms)
    Jan 30 12:02:20.108: INFO: (12) /api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8:162/proxy/: bar (200; 2.417206ms)
    Jan 30 12:02:20.108: INFO: (12) /api/v1/namespaces/proxy-4838/services/https:proxy-service-rnsxp:tlsportname1/proxy/: tls baz (200; 2.546941ms)
    Jan 30 12:02:20.108: INFO: (12) /api/v1/namespaces/proxy-4838/pods/https:proxy-service-rnsxp-gwgn8:443/proxy/: <a href="/api/v1/namespaces/proxy-4838/pods/https:proxy-service-rnsxp-gwgn8:443/proxy/tlsrewritem... (200; 2.563678ms)
    Jan 30 12:02:20.108: INFO: (12) /api/v1/namespaces/proxy-4838/pods/http:proxy-service-rnsxp-gwgn8:160/proxy/: foo (200; 2.574865ms)
    Jan 30 12:02:20.109: INFO: (12) /api/v1/namespaces/proxy-4838/services/proxy-service-rnsxp:portname2/proxy/: bar (200; 2.651071ms)
    Jan 30 12:02:20.109: INFO: (12) /api/v1/namespaces/proxy-4838/pods/https:proxy-service-rnsxp-gwgn8:460/proxy/: tls baz (200; 2.746888ms)
    Jan 30 12:02:20.109: INFO: (12) /api/v1/namespaces/proxy-4838/pods/https:proxy-service-rnsxp-gwgn8:462/proxy/: tls qux (200; 2.895378ms)
    Jan 30 12:02:20.109: INFO: (12) /api/v1/namespaces/proxy-4838/services/http:proxy-service-rnsxp:portname2/proxy/: bar (200; 2.899626ms)
    Jan 30 12:02:20.109: INFO: (12) /api/v1/namespaces/proxy-4838/services/http:proxy-service-rnsxp:portname1/proxy/: foo (200; 2.919446ms)
    Jan 30 12:02:20.109: INFO: (12) /api/v1/namespaces/proxy-4838/services/proxy-service-rnsxp:portname1/proxy/: foo (200; 2.890155ms)
    Jan 30 12:02:20.109: INFO: (12) /api/v1/namespaces/proxy-4838/pods/http:proxy-service-rnsxp-gwgn8:1080/proxy/: <a href="/api/v1/namespaces/proxy-4838/pods/http:proxy-service-rnsxp-gwgn8:1080/proxy/rewriteme">... (200; 3.022503ms)
    Jan 30 12:02:20.109: INFO: (12) /api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8/proxy/: <a href="/api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8/proxy/rewriteme">test</a> (200; 3.044131ms)
    Jan 30 12:02:20.109: INFO: (12) /api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8:1080/proxy/: <a href="/api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8:1080/proxy/rewriteme">test<... (200; 3.057814ms)
    Jan 30 12:02:20.109: INFO: (12) /api/v1/namespaces/proxy-4838/services/https:proxy-service-rnsxp:tlsportname2/proxy/: tls qux (200; 3.060944ms)
    Jan 30 12:02:20.111: INFO: (13) /api/v1/namespaces/proxy-4838/pods/http:proxy-service-rnsxp-gwgn8:160/proxy/: foo (200; 2.167102ms)
    Jan 30 12:02:20.111: INFO: (13) /api/v1/namespaces/proxy-4838/pods/https:proxy-service-rnsxp-gwgn8:462/proxy/: tls qux (200; 2.182345ms)
    Jan 30 12:02:20.111: INFO: (13) /api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8:160/proxy/: foo (200; 2.207303ms)
    Jan 30 12:02:20.111: INFO: (13) /api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8:1080/proxy/: <a href="/api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8:1080/proxy/rewriteme">test<... (200; 2.327398ms)
    Jan 30 12:02:20.112: INFO: (13) /api/v1/namespaces/proxy-4838/services/http:proxy-service-rnsxp:portname1/proxy/: foo (200; 2.583423ms)
    Jan 30 12:02:20.112: INFO: (13) /api/v1/namespaces/proxy-4838/services/proxy-service-rnsxp:portname1/proxy/: foo (200; 2.707679ms)
    Jan 30 12:02:20.112: INFO: (13) /api/v1/namespaces/proxy-4838/pods/https:proxy-service-rnsxp-gwgn8:460/proxy/: tls baz (200; 2.783102ms)
    Jan 30 12:02:20.112: INFO: (13) /api/v1/namespaces/proxy-4838/services/proxy-service-rnsxp:portname2/proxy/: bar (200; 2.796663ms)
    Jan 30 12:02:20.112: INFO: (13) /api/v1/namespaces/proxy-4838/services/https:proxy-service-rnsxp:tlsportname2/proxy/: tls qux (200; 2.834007ms)
    Jan 30 12:02:20.112: INFO: (13) /api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8/proxy/: <a href="/api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8/proxy/rewriteme">test</a> (200; 2.852818ms)
    Jan 30 12:02:20.112: INFO: (13) /api/v1/namespaces/proxy-4838/services/https:proxy-service-rnsxp:tlsportname1/proxy/: tls baz (200; 2.884749ms)
    Jan 30 12:02:20.112: INFO: (13) /api/v1/namespaces/proxy-4838/pods/http:proxy-service-rnsxp-gwgn8:1080/proxy/: <a href="/api/v1/namespaces/proxy-4838/pods/http:proxy-service-rnsxp-gwgn8:1080/proxy/rewriteme">... (200; 2.919679ms)
    Jan 30 12:02:20.112: INFO: (13) /api/v1/namespaces/proxy-4838/pods/https:proxy-service-rnsxp-gwgn8:443/proxy/: <a href="/api/v1/namespaces/proxy-4838/pods/https:proxy-service-rnsxp-gwgn8:443/proxy/tlsrewritem... (200; 3.148687ms)
    Jan 30 12:02:20.112: INFO: (13) /api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8:162/proxy/: bar (200; 3.164164ms)
    Jan 30 12:02:20.112: INFO: (13) /api/v1/namespaces/proxy-4838/services/http:proxy-service-rnsxp:portname2/proxy/: bar (200; 3.077785ms)
    Jan 30 12:02:20.112: INFO: (13) /api/v1/namespaces/proxy-4838/pods/http:proxy-service-rnsxp-gwgn8:162/proxy/: bar (200; 3.236506ms)
    Jan 30 12:02:20.115: INFO: (14) /api/v1/namespaces/proxy-4838/pods/http:proxy-service-rnsxp-gwgn8:162/proxy/: bar (200; 2.262429ms)
    Jan 30 12:02:20.115: INFO: (14) /api/v1/namespaces/proxy-4838/pods/http:proxy-service-rnsxp-gwgn8:160/proxy/: foo (200; 2.294105ms)
    Jan 30 12:02:20.115: INFO: (14) /api/v1/namespaces/proxy-4838/pods/https:proxy-service-rnsxp-gwgn8:462/proxy/: tls qux (200; 2.350172ms)
    Jan 30 12:02:20.115: INFO: (14) /api/v1/namespaces/proxy-4838/services/http:proxy-service-rnsxp:portname2/proxy/: bar (200; 2.477061ms)
    Jan 30 12:02:20.115: INFO: (14) /api/v1/namespaces/proxy-4838/services/proxy-service-rnsxp:portname1/proxy/: foo (200; 2.464079ms)
    Jan 30 12:02:20.115: INFO: (14) /api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8:160/proxy/: foo (200; 2.54147ms)
    Jan 30 12:02:20.115: INFO: (14) /api/v1/namespaces/proxy-4838/pods/https:proxy-service-rnsxp-gwgn8:460/proxy/: tls baz (200; 2.665656ms)
    Jan 30 12:02:20.115: INFO: (14) /api/v1/namespaces/proxy-4838/services/proxy-service-rnsxp:portname2/proxy/: bar (200; 3.021495ms)
    Jan 30 12:02:20.115: INFO: (14) /api/v1/namespaces/proxy-4838/services/http:proxy-service-rnsxp:portname1/proxy/: foo (200; 2.985603ms)
    Jan 30 12:02:20.115: INFO: (14) /api/v1/namespaces/proxy-4838/services/https:proxy-service-rnsxp:tlsportname2/proxy/: tls qux (200; 2.999605ms)
    Jan 30 12:02:20.115: INFO: (14) /api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8:162/proxy/: bar (200; 3.067221ms)
    Jan 30 12:02:20.115: INFO: (14) /api/v1/namespaces/proxy-4838/services/https:proxy-service-rnsxp:tlsportname1/proxy/: tls baz (200; 3.013504ms)
    Jan 30 12:02:20.115: INFO: (14) /api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8/proxy/: <a href="/api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8/proxy/rewriteme">test</a> (200; 3.030115ms)
    Jan 30 12:02:20.115: INFO: (14) /api/v1/namespaces/proxy-4838/pods/https:proxy-service-rnsxp-gwgn8:443/proxy/: <a href="/api/v1/namespaces/proxy-4838/pods/https:proxy-service-rnsxp-gwgn8:443/proxy/tlsrewritem... (200; 3.07162ms)
    Jan 30 12:02:20.115: INFO: (14) /api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8:1080/proxy/: <a href="/api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8:1080/proxy/rewriteme">test<... (200; 3.034137ms)
    Jan 30 12:02:20.115: INFO: (14) /api/v1/namespaces/proxy-4838/pods/http:proxy-service-rnsxp-gwgn8:1080/proxy/: <a href="/api/v1/namespaces/proxy-4838/pods/http:proxy-service-rnsxp-gwgn8:1080/proxy/rewriteme">... (200; 3.112411ms)
    Jan 30 12:02:20.118: INFO: (15) /api/v1/namespaces/proxy-4838/services/proxy-service-rnsxp:portname1/proxy/: foo (200; 2.42285ms)
    Jan 30 12:02:20.118: INFO: (15) /api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8/proxy/: <a href="/api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8/proxy/rewriteme">test</a> (200; 2.485299ms)
    Jan 30 12:02:20.118: INFO: (15) /api/v1/namespaces/proxy-4838/pods/https:proxy-service-rnsxp-gwgn8:462/proxy/: tls qux (200; 2.537644ms)
    Jan 30 12:02:20.118: INFO: (15) /api/v1/namespaces/proxy-4838/pods/https:proxy-service-rnsxp-gwgn8:443/proxy/: <a href="/api/v1/namespaces/proxy-4838/pods/https:proxy-service-rnsxp-gwgn8:443/proxy/tlsrewritem... (200; 2.536486ms)
    Jan 30 12:02:20.118: INFO: (15) /api/v1/namespaces/proxy-4838/services/http:proxy-service-rnsxp:portname2/proxy/: bar (200; 2.75827ms)
    Jan 30 12:02:20.118: INFO: (15) /api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8:160/proxy/: foo (200; 2.903032ms)
    Jan 30 12:02:20.118: INFO: (15) /api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8:162/proxy/: bar (200; 2.854792ms)
    Jan 30 12:02:20.118: INFO: (15) /api/v1/namespaces/proxy-4838/pods/https:proxy-service-rnsxp-gwgn8:460/proxy/: tls baz (200; 2.907533ms)
    Jan 30 12:02:20.118: INFO: (15) /api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8:1080/proxy/: <a href="/api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8:1080/proxy/rewriteme">test<... (200; 2.931726ms)
    Jan 30 12:02:20.119: INFO: (15) /api/v1/namespaces/proxy-4838/pods/http:proxy-service-rnsxp-gwgn8:1080/proxy/: <a href="/api/v1/namespaces/proxy-4838/pods/http:proxy-service-rnsxp-gwgn8:1080/proxy/rewriteme">... (200; 3.003026ms)
    Jan 30 12:02:20.119: INFO: (15) /api/v1/namespaces/proxy-4838/pods/http:proxy-service-rnsxp-gwgn8:162/proxy/: bar (200; 2.977494ms)
    Jan 30 12:02:20.119: INFO: (15) /api/v1/namespaces/proxy-4838/services/https:proxy-service-rnsxp:tlsportname2/proxy/: tls qux (200; 3.083681ms)
    Jan 30 12:02:20.119: INFO: (15) /api/v1/namespaces/proxy-4838/services/http:proxy-service-rnsxp:portname1/proxy/: foo (200; 3.098538ms)
    Jan 30 12:02:20.119: INFO: (15) /api/v1/namespaces/proxy-4838/services/proxy-service-rnsxp:portname2/proxy/: bar (200; 3.133549ms)
    Jan 30 12:02:20.119: INFO: (15) /api/v1/namespaces/proxy-4838/services/https:proxy-service-rnsxp:tlsportname1/proxy/: tls baz (200; 3.138649ms)
    Jan 30 12:02:20.119: INFO: (15) /api/v1/namespaces/proxy-4838/pods/http:proxy-service-rnsxp-gwgn8:160/proxy/: foo (200; 3.18582ms)
    Jan 30 12:02:20.120: INFO: (16) /api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8:162/proxy/: bar (200; 1.746966ms)
    Jan 30 12:02:20.121: INFO: (16) /api/v1/namespaces/proxy-4838/pods/http:proxy-service-rnsxp-gwgn8:1080/proxy/: <a href="/api/v1/namespaces/proxy-4838/pods/http:proxy-service-rnsxp-gwgn8:1080/proxy/rewriteme">... (200; 2.104598ms)
    Jan 30 12:02:20.122: INFO: (16) /api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8/proxy/: <a href="/api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8/proxy/rewriteme">test</a> (200; 3.047971ms)
    Jan 30 12:02:20.122: INFO: (16) /api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8:160/proxy/: foo (200; 3.079301ms)
    Jan 30 12:02:20.122: INFO: (16) /api/v1/namespaces/proxy-4838/services/proxy-service-rnsxp:portname2/proxy/: bar (200; 3.098015ms)
    Jan 30 12:02:20.122: INFO: (16) /api/v1/namespaces/proxy-4838/services/https:proxy-service-rnsxp:tlsportname1/proxy/: tls baz (200; 3.150929ms)
    Jan 30 12:02:20.122: INFO: (16) /api/v1/namespaces/proxy-4838/services/http:proxy-service-rnsxp:portname2/proxy/: bar (200; 3.119641ms)
    Jan 30 12:02:20.122: INFO: (16) /api/v1/namespaces/proxy-4838/services/proxy-service-rnsxp:portname1/proxy/: foo (200; 3.167865ms)
    Jan 30 12:02:20.122: INFO: (16) /api/v1/namespaces/proxy-4838/pods/http:proxy-service-rnsxp-gwgn8:162/proxy/: bar (200; 3.36007ms)
    Jan 30 12:02:20.122: INFO: (16) /api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8:1080/proxy/: <a href="/api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8:1080/proxy/rewriteme">test<... (200; 3.397543ms)
    Jan 30 12:02:20.122: INFO: (16) /api/v1/namespaces/proxy-4838/services/https:proxy-service-rnsxp:tlsportname2/proxy/: tls qux (200; 3.389924ms)
    Jan 30 12:02:20.122: INFO: (16) /api/v1/namespaces/proxy-4838/pods/https:proxy-service-rnsxp-gwgn8:460/proxy/: tls baz (200; 3.425363ms)
    Jan 30 12:02:20.122: INFO: (16) /api/v1/namespaces/proxy-4838/pods/http:proxy-service-rnsxp-gwgn8:160/proxy/: foo (200; 3.466403ms)
    Jan 30 12:02:20.122: INFO: (16) /api/v1/namespaces/proxy-4838/pods/https:proxy-service-rnsxp-gwgn8:462/proxy/: tls qux (200; 3.446995ms)
    Jan 30 12:02:20.122: INFO: (16) /api/v1/namespaces/proxy-4838/services/http:proxy-service-rnsxp:portname1/proxy/: foo (200; 3.508791ms)
    Jan 30 12:02:20.122: INFO: (16) /api/v1/namespaces/proxy-4838/pods/https:proxy-service-rnsxp-gwgn8:443/proxy/: <a href="/api/v1/namespaces/proxy-4838/pods/https:proxy-service-rnsxp-gwgn8:443/proxy/tlsrewritem... (200; 3.50431ms)
    Jan 30 12:02:20.125: INFO: (17) /api/v1/namespaces/proxy-4838/services/proxy-service-rnsxp:portname2/proxy/: bar (200; 2.551689ms)
    Jan 30 12:02:20.125: INFO: (17) /api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8:1080/proxy/: <a href="/api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8:1080/proxy/rewriteme">test<... (200; 2.530783ms)
    Jan 30 12:02:20.125: INFO: (17) /api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8:162/proxy/: bar (200; 2.513421ms)
    Jan 30 12:02:20.125: INFO: (17) /api/v1/namespaces/proxy-4838/services/https:proxy-service-rnsxp:tlsportname1/proxy/: tls baz (200; 2.844104ms)
    Jan 30 12:02:20.125: INFO: (17) /api/v1/namespaces/proxy-4838/services/http:proxy-service-rnsxp:portname2/proxy/: bar (200; 3.055474ms)
    Jan 30 12:02:20.125: INFO: (17) /api/v1/namespaces/proxy-4838/pods/https:proxy-service-rnsxp-gwgn8:460/proxy/: tls baz (200; 3.0855ms)
    Jan 30 12:02:20.125: INFO: (17) /api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8/proxy/: <a href="/api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8/proxy/rewriteme">test</a> (200; 3.114705ms)
    Jan 30 12:02:20.125: INFO: (17) /api/v1/namespaces/proxy-4838/services/proxy-service-rnsxp:portname1/proxy/: foo (200; 3.0846ms)
    Jan 30 12:02:20.125: INFO: (17) /api/v1/namespaces/proxy-4838/services/https:proxy-service-rnsxp:tlsportname2/proxy/: tls qux (200; 3.129383ms)
    Jan 30 12:02:20.125: INFO: (17) /api/v1/namespaces/proxy-4838/pods/http:proxy-service-rnsxp-gwgn8:162/proxy/: bar (200; 3.17722ms)
    Jan 30 12:02:20.126: INFO: (17) /api/v1/namespaces/proxy-4838/services/http:proxy-service-rnsxp:portname1/proxy/: foo (200; 3.136285ms)
    Jan 30 12:02:20.126: INFO: (17) /api/v1/namespaces/proxy-4838/pods/https:proxy-service-rnsxp-gwgn8:443/proxy/: <a href="/api/v1/namespaces/proxy-4838/pods/https:proxy-service-rnsxp-gwgn8:443/proxy/tlsrewritem... (200; 3.185053ms)
    Jan 30 12:02:20.126: INFO: (17) /api/v1/namespaces/proxy-4838/pods/http:proxy-service-rnsxp-gwgn8:1080/proxy/: <a href="/api/v1/namespaces/proxy-4838/pods/http:proxy-service-rnsxp-gwgn8:1080/proxy/rewriteme">... (200; 3.248766ms)
    Jan 30 12:02:20.126: INFO: (17) /api/v1/namespaces/proxy-4838/pods/http:proxy-service-rnsxp-gwgn8:160/proxy/: foo (200; 3.265316ms)
    Jan 30 12:02:20.126: INFO: (17) /api/v1/namespaces/proxy-4838/pods/https:proxy-service-rnsxp-gwgn8:462/proxy/: tls qux (200; 3.338356ms)
    Jan 30 12:02:20.126: INFO: (17) /api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8:160/proxy/: foo (200; 3.324456ms)
    Jan 30 12:02:20.128: INFO: (18) /api/v1/namespaces/proxy-4838/services/http:proxy-service-rnsxp:portname1/proxy/: foo (200; 2.673017ms)
    Jan 30 12:02:20.128: INFO: (18) /api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8/proxy/: <a href="/api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8/proxy/rewriteme">test</a> (200; 2.71233ms)
    Jan 30 12:02:20.128: INFO: (18) /api/v1/namespaces/proxy-4838/pods/http:proxy-service-rnsxp-gwgn8:162/proxy/: bar (200; 2.693873ms)
    Jan 30 12:02:20.129: INFO: (18) /api/v1/namespaces/proxy-4838/pods/https:proxy-service-rnsxp-gwgn8:460/proxy/: tls baz (200; 3.109412ms)
    Jan 30 12:02:20.129: INFO: (18) /api/v1/namespaces/proxy-4838/services/proxy-service-rnsxp:portname1/proxy/: foo (200; 3.169009ms)
    Jan 30 12:02:20.129: INFO: (18) /api/v1/namespaces/proxy-4838/pods/http:proxy-service-rnsxp-gwgn8:1080/proxy/: <a href="/api/v1/namespaces/proxy-4838/pods/http:proxy-service-rnsxp-gwgn8:1080/proxy/rewriteme">... (200; 3.182784ms)
    Jan 30 12:02:20.129: INFO: (18) /api/v1/namespaces/proxy-4838/services/https:proxy-service-rnsxp:tlsportname1/proxy/: tls baz (200; 3.217193ms)
    Jan 30 12:02:20.129: INFO: (18) /api/v1/namespaces/proxy-4838/services/proxy-service-rnsxp:portname2/proxy/: bar (200; 3.17416ms)
    Jan 30 12:02:20.129: INFO: (18) /api/v1/namespaces/proxy-4838/pods/http:proxy-service-rnsxp-gwgn8:160/proxy/: foo (200; 3.243941ms)
    Jan 30 12:02:20.129: INFO: (18) /api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8:162/proxy/: bar (200; 3.266699ms)
    Jan 30 12:02:20.129: INFO: (18) /api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8:160/proxy/: foo (200; 3.275446ms)
    Jan 30 12:02:20.129: INFO: (18) /api/v1/namespaces/proxy-4838/services/http:proxy-service-rnsxp:portname2/proxy/: bar (200; 3.383051ms)
    Jan 30 12:02:20.129: INFO: (18) /api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8:1080/proxy/: <a href="/api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8:1080/proxy/rewriteme">test<... (200; 3.383268ms)
    Jan 30 12:02:20.129: INFO: (18) /api/v1/namespaces/proxy-4838/pods/https:proxy-service-rnsxp-gwgn8:462/proxy/: tls qux (200; 3.363689ms)
    Jan 30 12:02:20.129: INFO: (18) /api/v1/namespaces/proxy-4838/pods/https:proxy-service-rnsxp-gwgn8:443/proxy/: <a href="/api/v1/namespaces/proxy-4838/pods/https:proxy-service-rnsxp-gwgn8:443/proxy/tlsrewritem... (200; 3.353435ms)
    Jan 30 12:02:20.129: INFO: (18) /api/v1/namespaces/proxy-4838/services/https:proxy-service-rnsxp:tlsportname2/proxy/: tls qux (200; 3.423481ms)
    Jan 30 12:02:20.131: INFO: (19) /api/v1/namespaces/proxy-4838/pods/http:proxy-service-rnsxp-gwgn8:1080/proxy/: <a href="/api/v1/namespaces/proxy-4838/pods/http:proxy-service-rnsxp-gwgn8:1080/proxy/rewriteme">... (200; 1.933778ms)
    Jan 30 12:02:20.131: INFO: (19) /api/v1/namespaces/proxy-4838/pods/https:proxy-service-rnsxp-gwgn8:460/proxy/: tls baz (200; 2.074361ms)
    Jan 30 12:02:20.131: INFO: (19) /api/v1/namespaces/proxy-4838/pods/http:proxy-service-rnsxp-gwgn8:160/proxy/: foo (200; 2.068491ms)
    Jan 30 12:02:20.131: INFO: (19) /api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8:162/proxy/: bar (200; 2.126517ms)
    Jan 30 12:02:20.131: INFO: (19) /api/v1/namespaces/proxy-4838/pods/https:proxy-service-rnsxp-gwgn8:443/proxy/: <a href="/api/v1/namespaces/proxy-4838/pods/https:proxy-service-rnsxp-gwgn8:443/proxy/tlsrewritem... (200; 2.167476ms)
    Jan 30 12:02:20.132: INFO: (19) /api/v1/namespaces/proxy-4838/services/proxy-service-rnsxp:portname2/proxy/: bar (200; 2.859624ms)
    Jan 30 12:02:20.132: INFO: (19) /api/v1/namespaces/proxy-4838/services/proxy-service-rnsxp:portname1/proxy/: foo (200; 2.882393ms)
    Jan 30 12:02:20.132: INFO: (19) /api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8/proxy/: <a href="/api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8/proxy/rewriteme">test</a> (200; 2.850396ms)
    Jan 30 12:02:20.132: INFO: (19) /api/v1/namespaces/proxy-4838/services/https:proxy-service-rnsxp:tlsportname2/proxy/: tls qux (200; 2.883224ms)
    Jan 30 12:02:20.132: INFO: (19) /api/v1/namespaces/proxy-4838/services/http:proxy-service-rnsxp:portname1/proxy/: foo (200; 2.954026ms)
    Jan 30 12:02:20.132: INFO: (19) /api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8:1080/proxy/: <a href="/api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8:1080/proxy/rewriteme">test<... (200; 2.969008ms)
    Jan 30 12:02:20.132: INFO: (19) /api/v1/namespaces/proxy-4838/services/http:proxy-service-rnsxp:portname2/proxy/: bar (200; 3.054947ms)
    Jan 30 12:02:20.132: INFO: (19) /api/v1/namespaces/proxy-4838/services/https:proxy-service-rnsxp:tlsportname1/proxy/: tls baz (200; 3.067134ms)
    Jan 30 12:02:20.132: INFO: (19) /api/v1/namespaces/proxy-4838/pods/http:proxy-service-rnsxp-gwgn8:162/proxy/: bar (200; 3.119736ms)
    Jan 30 12:02:20.132: INFO: (19) /api/v1/namespaces/proxy-4838/pods/proxy-service-rnsxp-gwgn8:160/proxy/: foo (200; 3.125445ms)
    Jan 30 12:02:20.133: INFO: (19) /api/v1/namespaces/proxy-4838/pods/https:proxy-service-rnsxp-gwgn8:462/proxy/: tls qux (200; 3.344738ms)
    STEP: deleting ReplicationController proxy-service-rnsxp in namespace proxy-4838, will wait for the garbage collector to delete the pods 01/30/23 12:02:20.133
    Jan 30 12:02:20.187: INFO: Deleting ReplicationController proxy-service-rnsxp took: 2.316883ms
    Jan 30 12:02:20.288: INFO: Terminating ReplicationController proxy-service-rnsxp pods took: 100.777029ms
    [AfterEach] version v1
      test/e2e/framework/node/init/init.go:32
    Jan 30 12:02:22.788: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] version v1
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] version v1
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] version v1
      tear down framework | framework.go:193
    STEP: Destroying namespace "proxy-4838" for this suite. 01/30/23 12:02:22.791
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-network] Services
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2250
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 12:02:22.793
Jan 30 12:02:22.793: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename services 01/30/23 12:02:22.794
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:02:22.8
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:02:22.802
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2250
STEP: creating service in namespace services-4609 01/30/23 12:02:22.804
STEP: creating service affinity-nodeport-transition in namespace services-4609 01/30/23 12:02:22.804
STEP: creating replication controller affinity-nodeport-transition in namespace services-4609 01/30/23 12:02:22.809
I0130 12:02:22.811664      23 runners.go:193] Created replication controller with name: affinity-nodeport-transition, namespace: services-4609, replica count: 3
I0130 12:02:25.862444      23 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 30 12:02:25.868: INFO: Creating new exec pod
Jan 30 12:02:25.870: INFO: Waiting up to 5m0s for pod "execpod-affinityfhps2" in namespace "services-4609" to be "running"
Jan 30 12:02:25.872: INFO: Pod "execpod-affinityfhps2": Phase="Pending", Reason="", readiness=false. Elapsed: 1.501342ms
Jan 30 12:02:27.874: INFO: Pod "execpod-affinityfhps2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003567136s
Jan 30 12:02:29.875: INFO: Pod "execpod-affinityfhps2": Phase="Running", Reason="", readiness=true. Elapsed: 4.004631514s
Jan 30 12:02:29.875: INFO: Pod "execpod-affinityfhps2" satisfied condition "running"
Jan 30 12:02:30.878: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=services-4609 exec execpod-affinityfhps2 -- /bin/sh -x -c nc -v -z -w 2 affinity-nodeport-transition 80'
Jan 30 12:02:31.037: INFO: stderr: "+ nc -v -z -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
Jan 30 12:02:31.038: INFO: stdout: ""
Jan 30 12:02:31.038: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=services-4609 exec execpod-affinityfhps2 -- /bin/sh -x -c nc -v -z -w 2 10.178.73.15 80'
Jan 30 12:02:31.168: INFO: stderr: "+ nc -v -z -w 2 10.178.73.15 80\nConnection to 10.178.73.15 80 port [tcp/http] succeeded!\n"
Jan 30 12:02:31.168: INFO: stdout: ""
Jan 30 12:02:31.168: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=services-4609 exec execpod-affinityfhps2 -- /bin/sh -x -c nc -v -z -w 2 10.182.0.82 32089'
Jan 30 12:02:31.291: INFO: stderr: "+ nc -v -z -w 2 10.182.0.82 32089\nConnection to 10.182.0.82 32089 port [tcp/*] succeeded!\n"
Jan 30 12:02:31.292: INFO: stdout: ""
Jan 30 12:02:31.292: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=services-4609 exec execpod-affinityfhps2 -- /bin/sh -x -c nc -v -z -w 2 10.182.0.84 32089'
Jan 30 12:02:31.555: INFO: stderr: "+ nc -v -z -w 2 10.182.0.84 32089\nConnection to 10.182.0.84 32089 port [tcp/*] succeeded!\n"
Jan 30 12:02:31.555: INFO: stdout: ""
Jan 30 12:02:31.560: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=services-4609 exec execpod-affinityfhps2 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.182.0.82:32089/ ; done'
Jan 30 12:02:31.735: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.182.0.82:32089/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.182.0.82:32089/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.182.0.82:32089/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.182.0.82:32089/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.182.0.82:32089/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.182.0.82:32089/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.182.0.82:32089/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.182.0.82:32089/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.182.0.82:32089/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.182.0.82:32089/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.182.0.82:32089/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.182.0.82:32089/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.182.0.82:32089/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.182.0.82:32089/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.182.0.82:32089/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.182.0.82:32089/\n"
Jan 30 12:02:31.735: INFO: stdout: "\naffinity-nodeport-transition-bkrf4\naffinity-nodeport-transition-bkrf4\naffinity-nodeport-transition-bkrf4\naffinity-nodeport-transition-bkrf4\naffinity-nodeport-transition-bkrf4\naffinity-nodeport-transition-bkrf4\naffinity-nodeport-transition-bkrf4\naffinity-nodeport-transition-bkrf4\naffinity-nodeport-transition-bkrf4\naffinity-nodeport-transition-bkrf4\naffinity-nodeport-transition-bkrf4\naffinity-nodeport-transition-bkrf4\naffinity-nodeport-transition-bkrf4\naffinity-nodeport-transition-bkrf4\naffinity-nodeport-transition-bkrf4\naffinity-nodeport-transition-bkrf4"
Jan 30 12:02:31.735: INFO: Received response from host: affinity-nodeport-transition-bkrf4
Jan 30 12:02:31.735: INFO: Received response from host: affinity-nodeport-transition-bkrf4
Jan 30 12:02:31.735: INFO: Received response from host: affinity-nodeport-transition-bkrf4
Jan 30 12:02:31.735: INFO: Received response from host: affinity-nodeport-transition-bkrf4
Jan 30 12:02:31.735: INFO: Received response from host: affinity-nodeport-transition-bkrf4
Jan 30 12:02:31.735: INFO: Received response from host: affinity-nodeport-transition-bkrf4
Jan 30 12:02:31.735: INFO: Received response from host: affinity-nodeport-transition-bkrf4
Jan 30 12:02:31.735: INFO: Received response from host: affinity-nodeport-transition-bkrf4
Jan 30 12:02:31.735: INFO: Received response from host: affinity-nodeport-transition-bkrf4
Jan 30 12:02:31.735: INFO: Received response from host: affinity-nodeport-transition-bkrf4
Jan 30 12:02:31.735: INFO: Received response from host: affinity-nodeport-transition-bkrf4
Jan 30 12:02:31.735: INFO: Received response from host: affinity-nodeport-transition-bkrf4
Jan 30 12:02:31.735: INFO: Received response from host: affinity-nodeport-transition-bkrf4
Jan 30 12:02:31.735: INFO: Received response from host: affinity-nodeport-transition-bkrf4
Jan 30 12:02:31.735: INFO: Received response from host: affinity-nodeport-transition-bkrf4
Jan 30 12:02:31.735: INFO: Received response from host: affinity-nodeport-transition-bkrf4
Jan 30 12:03:01.735: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=services-4609 exec execpod-affinityfhps2 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.182.0.82:32089/ ; done'
Jan 30 12:03:01.936: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.182.0.82:32089/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.182.0.82:32089/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.182.0.82:32089/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.182.0.82:32089/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.182.0.82:32089/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.182.0.82:32089/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.182.0.82:32089/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.182.0.82:32089/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.182.0.82:32089/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.182.0.82:32089/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.182.0.82:32089/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.182.0.82:32089/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.182.0.82:32089/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.182.0.82:32089/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.182.0.82:32089/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.182.0.82:32089/\n"
Jan 30 12:03:01.936: INFO: stdout: "\naffinity-nodeport-transition-qfp5g\naffinity-nodeport-transition-qfp5g\naffinity-nodeport-transition-qfp5g\naffinity-nodeport-transition-dzl57\naffinity-nodeport-transition-qfp5g\naffinity-nodeport-transition-bkrf4\naffinity-nodeport-transition-dzl57\naffinity-nodeport-transition-dzl57\naffinity-nodeport-transition-bkrf4\naffinity-nodeport-transition-qfp5g\naffinity-nodeport-transition-bkrf4\naffinity-nodeport-transition-bkrf4\naffinity-nodeport-transition-dzl57\naffinity-nodeport-transition-qfp5g\naffinity-nodeport-transition-bkrf4\naffinity-nodeport-transition-qfp5g"
Jan 30 12:03:01.936: INFO: Received response from host: affinity-nodeport-transition-qfp5g
Jan 30 12:03:01.936: INFO: Received response from host: affinity-nodeport-transition-qfp5g
Jan 30 12:03:01.936: INFO: Received response from host: affinity-nodeport-transition-qfp5g
Jan 30 12:03:01.936: INFO: Received response from host: affinity-nodeport-transition-dzl57
Jan 30 12:03:01.936: INFO: Received response from host: affinity-nodeport-transition-qfp5g
Jan 30 12:03:01.936: INFO: Received response from host: affinity-nodeport-transition-bkrf4
Jan 30 12:03:01.936: INFO: Received response from host: affinity-nodeport-transition-dzl57
Jan 30 12:03:01.936: INFO: Received response from host: affinity-nodeport-transition-dzl57
Jan 30 12:03:01.936: INFO: Received response from host: affinity-nodeport-transition-bkrf4
Jan 30 12:03:01.936: INFO: Received response from host: affinity-nodeport-transition-qfp5g
Jan 30 12:03:01.936: INFO: Received response from host: affinity-nodeport-transition-bkrf4
Jan 30 12:03:01.936: INFO: Received response from host: affinity-nodeport-transition-bkrf4
Jan 30 12:03:01.936: INFO: Received response from host: affinity-nodeport-transition-dzl57
Jan 30 12:03:01.936: INFO: Received response from host: affinity-nodeport-transition-qfp5g
Jan 30 12:03:01.936: INFO: Received response from host: affinity-nodeport-transition-bkrf4
Jan 30 12:03:01.936: INFO: Received response from host: affinity-nodeport-transition-qfp5g
Jan 30 12:03:01.942: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=services-4609 exec execpod-affinityfhps2 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.182.0.82:32089/ ; done'
Jan 30 12:03:02.132: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.182.0.82:32089/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.182.0.82:32089/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.182.0.82:32089/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.182.0.82:32089/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.182.0.82:32089/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.182.0.82:32089/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.182.0.82:32089/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.182.0.82:32089/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.182.0.82:32089/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.182.0.82:32089/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.182.0.82:32089/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.182.0.82:32089/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.182.0.82:32089/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.182.0.82:32089/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.182.0.82:32089/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.182.0.82:32089/\n"
Jan 30 12:03:02.132: INFO: stdout: "\naffinity-nodeport-transition-dzl57\naffinity-nodeport-transition-dzl57\naffinity-nodeport-transition-dzl57\naffinity-nodeport-transition-dzl57\naffinity-nodeport-transition-dzl57\naffinity-nodeport-transition-dzl57\naffinity-nodeport-transition-dzl57\naffinity-nodeport-transition-dzl57\naffinity-nodeport-transition-dzl57\naffinity-nodeport-transition-dzl57\naffinity-nodeport-transition-dzl57\naffinity-nodeport-transition-dzl57\naffinity-nodeport-transition-dzl57\naffinity-nodeport-transition-dzl57\naffinity-nodeport-transition-dzl57\naffinity-nodeport-transition-dzl57"
Jan 30 12:03:02.132: INFO: Received response from host: affinity-nodeport-transition-dzl57
Jan 30 12:03:02.132: INFO: Received response from host: affinity-nodeport-transition-dzl57
Jan 30 12:03:02.132: INFO: Received response from host: affinity-nodeport-transition-dzl57
Jan 30 12:03:02.132: INFO: Received response from host: affinity-nodeport-transition-dzl57
Jan 30 12:03:02.132: INFO: Received response from host: affinity-nodeport-transition-dzl57
Jan 30 12:03:02.132: INFO: Received response from host: affinity-nodeport-transition-dzl57
Jan 30 12:03:02.132: INFO: Received response from host: affinity-nodeport-transition-dzl57
Jan 30 12:03:02.132: INFO: Received response from host: affinity-nodeport-transition-dzl57
Jan 30 12:03:02.132: INFO: Received response from host: affinity-nodeport-transition-dzl57
Jan 30 12:03:02.132: INFO: Received response from host: affinity-nodeport-transition-dzl57
Jan 30 12:03:02.132: INFO: Received response from host: affinity-nodeport-transition-dzl57
Jan 30 12:03:02.132: INFO: Received response from host: affinity-nodeport-transition-dzl57
Jan 30 12:03:02.132: INFO: Received response from host: affinity-nodeport-transition-dzl57
Jan 30 12:03:02.132: INFO: Received response from host: affinity-nodeport-transition-dzl57
Jan 30 12:03:02.132: INFO: Received response from host: affinity-nodeport-transition-dzl57
Jan 30 12:03:02.132: INFO: Received response from host: affinity-nodeport-transition-dzl57
Jan 30 12:03:02.132: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-4609, will wait for the garbage collector to delete the pods 01/30/23 12:03:02.137
Jan 30 12:03:02.192: INFO: Deleting ReplicationController affinity-nodeport-transition took: 2.270674ms
Jan 30 12:03:02.292: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 100.435368ms
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jan 30 12:03:04.800: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-4609" for this suite. 01/30/23 12:03:04.802
------------------------------
• [SLOW TEST] [42.011 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2250

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 12:02:22.793
    Jan 30 12:02:22.793: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename services 01/30/23 12:02:22.794
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:02:22.8
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:02:22.802
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2250
    STEP: creating service in namespace services-4609 01/30/23 12:02:22.804
    STEP: creating service affinity-nodeport-transition in namespace services-4609 01/30/23 12:02:22.804
    STEP: creating replication controller affinity-nodeport-transition in namespace services-4609 01/30/23 12:02:22.809
    I0130 12:02:22.811664      23 runners.go:193] Created replication controller with name: affinity-nodeport-transition, namespace: services-4609, replica count: 3
    I0130 12:02:25.862444      23 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan 30 12:02:25.868: INFO: Creating new exec pod
    Jan 30 12:02:25.870: INFO: Waiting up to 5m0s for pod "execpod-affinityfhps2" in namespace "services-4609" to be "running"
    Jan 30 12:02:25.872: INFO: Pod "execpod-affinityfhps2": Phase="Pending", Reason="", readiness=false. Elapsed: 1.501342ms
    Jan 30 12:02:27.874: INFO: Pod "execpod-affinityfhps2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003567136s
    Jan 30 12:02:29.875: INFO: Pod "execpod-affinityfhps2": Phase="Running", Reason="", readiness=true. Elapsed: 4.004631514s
    Jan 30 12:02:29.875: INFO: Pod "execpod-affinityfhps2" satisfied condition "running"
    Jan 30 12:02:30.878: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=services-4609 exec execpod-affinityfhps2 -- /bin/sh -x -c nc -v -z -w 2 affinity-nodeport-transition 80'
    Jan 30 12:02:31.037: INFO: stderr: "+ nc -v -z -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
    Jan 30 12:02:31.038: INFO: stdout: ""
    Jan 30 12:02:31.038: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=services-4609 exec execpod-affinityfhps2 -- /bin/sh -x -c nc -v -z -w 2 10.178.73.15 80'
    Jan 30 12:02:31.168: INFO: stderr: "+ nc -v -z -w 2 10.178.73.15 80\nConnection to 10.178.73.15 80 port [tcp/http] succeeded!\n"
    Jan 30 12:02:31.168: INFO: stdout: ""
    Jan 30 12:02:31.168: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=services-4609 exec execpod-affinityfhps2 -- /bin/sh -x -c nc -v -z -w 2 10.182.0.82 32089'
    Jan 30 12:02:31.291: INFO: stderr: "+ nc -v -z -w 2 10.182.0.82 32089\nConnection to 10.182.0.82 32089 port [tcp/*] succeeded!\n"
    Jan 30 12:02:31.292: INFO: stdout: ""
    Jan 30 12:02:31.292: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=services-4609 exec execpod-affinityfhps2 -- /bin/sh -x -c nc -v -z -w 2 10.182.0.84 32089'
    Jan 30 12:02:31.555: INFO: stderr: "+ nc -v -z -w 2 10.182.0.84 32089\nConnection to 10.182.0.84 32089 port [tcp/*] succeeded!\n"
    Jan 30 12:02:31.555: INFO: stdout: ""
    Jan 30 12:02:31.560: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=services-4609 exec execpod-affinityfhps2 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.182.0.82:32089/ ; done'
    Jan 30 12:02:31.735: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.182.0.82:32089/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.182.0.82:32089/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.182.0.82:32089/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.182.0.82:32089/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.182.0.82:32089/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.182.0.82:32089/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.182.0.82:32089/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.182.0.82:32089/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.182.0.82:32089/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.182.0.82:32089/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.182.0.82:32089/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.182.0.82:32089/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.182.0.82:32089/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.182.0.82:32089/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.182.0.82:32089/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.182.0.82:32089/\n"
    Jan 30 12:02:31.735: INFO: stdout: "\naffinity-nodeport-transition-bkrf4\naffinity-nodeport-transition-bkrf4\naffinity-nodeport-transition-bkrf4\naffinity-nodeport-transition-bkrf4\naffinity-nodeport-transition-bkrf4\naffinity-nodeport-transition-bkrf4\naffinity-nodeport-transition-bkrf4\naffinity-nodeport-transition-bkrf4\naffinity-nodeport-transition-bkrf4\naffinity-nodeport-transition-bkrf4\naffinity-nodeport-transition-bkrf4\naffinity-nodeport-transition-bkrf4\naffinity-nodeport-transition-bkrf4\naffinity-nodeport-transition-bkrf4\naffinity-nodeport-transition-bkrf4\naffinity-nodeport-transition-bkrf4"
    Jan 30 12:02:31.735: INFO: Received response from host: affinity-nodeport-transition-bkrf4
    Jan 30 12:02:31.735: INFO: Received response from host: affinity-nodeport-transition-bkrf4
    Jan 30 12:02:31.735: INFO: Received response from host: affinity-nodeport-transition-bkrf4
    Jan 30 12:02:31.735: INFO: Received response from host: affinity-nodeport-transition-bkrf4
    Jan 30 12:02:31.735: INFO: Received response from host: affinity-nodeport-transition-bkrf4
    Jan 30 12:02:31.735: INFO: Received response from host: affinity-nodeport-transition-bkrf4
    Jan 30 12:02:31.735: INFO: Received response from host: affinity-nodeport-transition-bkrf4
    Jan 30 12:02:31.735: INFO: Received response from host: affinity-nodeport-transition-bkrf4
    Jan 30 12:02:31.735: INFO: Received response from host: affinity-nodeport-transition-bkrf4
    Jan 30 12:02:31.735: INFO: Received response from host: affinity-nodeport-transition-bkrf4
    Jan 30 12:02:31.735: INFO: Received response from host: affinity-nodeport-transition-bkrf4
    Jan 30 12:02:31.735: INFO: Received response from host: affinity-nodeport-transition-bkrf4
    Jan 30 12:02:31.735: INFO: Received response from host: affinity-nodeport-transition-bkrf4
    Jan 30 12:02:31.735: INFO: Received response from host: affinity-nodeport-transition-bkrf4
    Jan 30 12:02:31.735: INFO: Received response from host: affinity-nodeport-transition-bkrf4
    Jan 30 12:02:31.735: INFO: Received response from host: affinity-nodeport-transition-bkrf4
    Jan 30 12:03:01.735: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=services-4609 exec execpod-affinityfhps2 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.182.0.82:32089/ ; done'
    Jan 30 12:03:01.936: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.182.0.82:32089/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.182.0.82:32089/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.182.0.82:32089/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.182.0.82:32089/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.182.0.82:32089/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.182.0.82:32089/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.182.0.82:32089/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.182.0.82:32089/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.182.0.82:32089/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.182.0.82:32089/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.182.0.82:32089/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.182.0.82:32089/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.182.0.82:32089/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.182.0.82:32089/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.182.0.82:32089/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.182.0.82:32089/\n"
    Jan 30 12:03:01.936: INFO: stdout: "\naffinity-nodeport-transition-qfp5g\naffinity-nodeport-transition-qfp5g\naffinity-nodeport-transition-qfp5g\naffinity-nodeport-transition-dzl57\naffinity-nodeport-transition-qfp5g\naffinity-nodeport-transition-bkrf4\naffinity-nodeport-transition-dzl57\naffinity-nodeport-transition-dzl57\naffinity-nodeport-transition-bkrf4\naffinity-nodeport-transition-qfp5g\naffinity-nodeport-transition-bkrf4\naffinity-nodeport-transition-bkrf4\naffinity-nodeport-transition-dzl57\naffinity-nodeport-transition-qfp5g\naffinity-nodeport-transition-bkrf4\naffinity-nodeport-transition-qfp5g"
    Jan 30 12:03:01.936: INFO: Received response from host: affinity-nodeport-transition-qfp5g
    Jan 30 12:03:01.936: INFO: Received response from host: affinity-nodeport-transition-qfp5g
    Jan 30 12:03:01.936: INFO: Received response from host: affinity-nodeport-transition-qfp5g
    Jan 30 12:03:01.936: INFO: Received response from host: affinity-nodeport-transition-dzl57
    Jan 30 12:03:01.936: INFO: Received response from host: affinity-nodeport-transition-qfp5g
    Jan 30 12:03:01.936: INFO: Received response from host: affinity-nodeport-transition-bkrf4
    Jan 30 12:03:01.936: INFO: Received response from host: affinity-nodeport-transition-dzl57
    Jan 30 12:03:01.936: INFO: Received response from host: affinity-nodeport-transition-dzl57
    Jan 30 12:03:01.936: INFO: Received response from host: affinity-nodeport-transition-bkrf4
    Jan 30 12:03:01.936: INFO: Received response from host: affinity-nodeport-transition-qfp5g
    Jan 30 12:03:01.936: INFO: Received response from host: affinity-nodeport-transition-bkrf4
    Jan 30 12:03:01.936: INFO: Received response from host: affinity-nodeport-transition-bkrf4
    Jan 30 12:03:01.936: INFO: Received response from host: affinity-nodeport-transition-dzl57
    Jan 30 12:03:01.936: INFO: Received response from host: affinity-nodeport-transition-qfp5g
    Jan 30 12:03:01.936: INFO: Received response from host: affinity-nodeport-transition-bkrf4
    Jan 30 12:03:01.936: INFO: Received response from host: affinity-nodeport-transition-qfp5g
    Jan 30 12:03:01.942: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=services-4609 exec execpod-affinityfhps2 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.182.0.82:32089/ ; done'
    Jan 30 12:03:02.132: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.182.0.82:32089/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.182.0.82:32089/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.182.0.82:32089/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.182.0.82:32089/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.182.0.82:32089/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.182.0.82:32089/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.182.0.82:32089/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.182.0.82:32089/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.182.0.82:32089/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.182.0.82:32089/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.182.0.82:32089/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.182.0.82:32089/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.182.0.82:32089/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.182.0.82:32089/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.182.0.82:32089/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.182.0.82:32089/\n"
    Jan 30 12:03:02.132: INFO: stdout: "\naffinity-nodeport-transition-dzl57\naffinity-nodeport-transition-dzl57\naffinity-nodeport-transition-dzl57\naffinity-nodeport-transition-dzl57\naffinity-nodeport-transition-dzl57\naffinity-nodeport-transition-dzl57\naffinity-nodeport-transition-dzl57\naffinity-nodeport-transition-dzl57\naffinity-nodeport-transition-dzl57\naffinity-nodeport-transition-dzl57\naffinity-nodeport-transition-dzl57\naffinity-nodeport-transition-dzl57\naffinity-nodeport-transition-dzl57\naffinity-nodeport-transition-dzl57\naffinity-nodeport-transition-dzl57\naffinity-nodeport-transition-dzl57"
    Jan 30 12:03:02.132: INFO: Received response from host: affinity-nodeport-transition-dzl57
    Jan 30 12:03:02.132: INFO: Received response from host: affinity-nodeport-transition-dzl57
    Jan 30 12:03:02.132: INFO: Received response from host: affinity-nodeport-transition-dzl57
    Jan 30 12:03:02.132: INFO: Received response from host: affinity-nodeport-transition-dzl57
    Jan 30 12:03:02.132: INFO: Received response from host: affinity-nodeport-transition-dzl57
    Jan 30 12:03:02.132: INFO: Received response from host: affinity-nodeport-transition-dzl57
    Jan 30 12:03:02.132: INFO: Received response from host: affinity-nodeport-transition-dzl57
    Jan 30 12:03:02.132: INFO: Received response from host: affinity-nodeport-transition-dzl57
    Jan 30 12:03:02.132: INFO: Received response from host: affinity-nodeport-transition-dzl57
    Jan 30 12:03:02.132: INFO: Received response from host: affinity-nodeport-transition-dzl57
    Jan 30 12:03:02.132: INFO: Received response from host: affinity-nodeport-transition-dzl57
    Jan 30 12:03:02.132: INFO: Received response from host: affinity-nodeport-transition-dzl57
    Jan 30 12:03:02.132: INFO: Received response from host: affinity-nodeport-transition-dzl57
    Jan 30 12:03:02.132: INFO: Received response from host: affinity-nodeport-transition-dzl57
    Jan 30 12:03:02.132: INFO: Received response from host: affinity-nodeport-transition-dzl57
    Jan 30 12:03:02.132: INFO: Received response from host: affinity-nodeport-transition-dzl57
    Jan 30 12:03:02.132: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-4609, will wait for the garbage collector to delete the pods 01/30/23 12:03:02.137
    Jan 30 12:03:02.192: INFO: Deleting ReplicationController affinity-nodeport-transition took: 2.270674ms
    Jan 30 12:03:02.292: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 100.435368ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jan 30 12:03:04.800: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-4609" for this suite. 01/30/23 12:03:04.802
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207
[BeforeEach] [sig-instrumentation] Events API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 12:03:04.805
Jan 30 12:03:04.805: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename events 01/30/23 12:03:04.806
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:03:04.812
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:03:04.814
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/instrumentation/events.go:84
[It] should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207
STEP: Create set of events 01/30/23 12:03:04.816
STEP: get a list of Events with a label in the current namespace 01/30/23 12:03:04.822
STEP: delete a list of events 01/30/23 12:03:04.824
Jan 30 12:03:04.824: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity 01/30/23 12:03:04.831
[AfterEach] [sig-instrumentation] Events API
  test/e2e/framework/node/init/init.go:32
Jan 30 12:03:04.832: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-instrumentation] Events API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-instrumentation] Events API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-instrumentation] Events API
  tear down framework | framework.go:193
STEP: Destroying namespace "events-4110" for this suite. 01/30/23 12:03:04.834
------------------------------
• [0.031 seconds]
[sig-instrumentation] Events API
test/e2e/instrumentation/common/framework.go:23
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 12:03:04.805
    Jan 30 12:03:04.805: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename events 01/30/23 12:03:04.806
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:03:04.812
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:03:04.814
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/instrumentation/events.go:84
    [It] should delete a collection of events [Conformance]
      test/e2e/instrumentation/events.go:207
    STEP: Create set of events 01/30/23 12:03:04.816
    STEP: get a list of Events with a label in the current namespace 01/30/23 12:03:04.822
    STEP: delete a list of events 01/30/23 12:03:04.824
    Jan 30 12:03:04.824: INFO: requesting DeleteCollection of events
    STEP: check that the list of events matches the requested quantity 01/30/23 12:03:04.831
    [AfterEach] [sig-instrumentation] Events API
      test/e2e/framework/node/init/init.go:32
    Jan 30 12:03:04.832: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      tear down framework | framework.go:193
    STEP: Destroying namespace "events-4110" for this suite. 01/30/23 12:03:04.834
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 12:03:04.837
Jan 30 12:03:04.837: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename deployment 01/30/23 12:03:04.838
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:03:04.843
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:03:04.845
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122
Jan 30 12:03:04.851: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Jan 30 12:03:09.854: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 01/30/23 12:03:09.854
Jan 30 12:03:09.854: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up 01/30/23 12:03:09.86
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan 30 12:03:09.864: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-4130  075b2cbf-bdbd-4e78-84bd-83a9d903d038 28216 1 2023-01-30 12:03:09 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] [{e2e.test Update apps/v1 2023-01-30 12:03:09 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] []} {[] [] [{agnhost harbor.cloud.netease.com/qzprod-k8s/e2e/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc000a2d8f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

Jan 30 12:03:09.866: INFO: New ReplicaSet of Deployment "test-cleanup-deployment" is nil.
Jan 30 12:03:09.866: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
Jan 30 12:03:09.866: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-4130  fb6b9b9d-3c94-4110-838e-9993cb8acc28 28221 1 2023-01-30 12:03:04 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment 075b2cbf-bdbd-4e78-84bd-83a9d903d038 0xc006cd30a7 0xc006cd30a8}] [] [{e2e.test Update apps/v1 2023-01-30 12:03:04 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-30 12:03:06 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-01-30 12:03:09 +0000 UTC FieldsV1 {"f:metadata":{"f:ownerReferences":{".":{},"k:{\"uid\":\"075b2cbf-bdbd-4e78-84bd-83a9d903d038\"}":{}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] [] []} {[] [] [{httpd harbor.cloud.netease.com/qzprod-k8s/e2e/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc006cd3178 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jan 30 12:03:09.868: INFO: Pod "test-cleanup-controller-2hgkg" is available:
&Pod{ObjectMeta:{test-cleanup-controller-2hgkg test-cleanup-controller- deployment-4130  46173773-a576-40b4-a9d6-57f250a0e175 28186 0 2023-01-30 12:03:04 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[cni.projectcalico.org/podIP:10.178.151.54/32 cni.projectcalico.org/podIPs:10.178.151.54/32] [{apps/v1 ReplicaSet test-cleanup-controller fb6b9b9d-3c94-4110-838e-9993cb8acc28 0xc004c79457 0xc004c79458}] [] [{kube-controller-manager Update v1 2023-01-30 12:03:04 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fb6b9b9d-3c94-4110-838e-9993cb8acc28\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-30 12:03:05 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-30 12:03:06 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.178.151.54\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-t8kpb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:harbor.cloud.netease.com/qzprod-k8s/e2e/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-t8kpb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pubt2-nks-for-dev1.dg.163.org,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 12:03:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 12:03:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 12:03:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 12:03:04 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.182.0.82,PodIP:10.178.151.54,StartTime:2023-01-30 12:03:04 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-30 12:03:05 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:harbor.cloud.netease.com/qzprod-k8s/e2e/httpd:2.4.38-4,ImageID:docker-pullable://harbor.cloud.netease.com/qzprod-k8s/e2e/httpd@sha256:8b745b6e9cb6a6184353110aabe28d8d6f6145d3fe7c0e1899aa70c8228c0fd3,ContainerID:docker://1c27bc19dc6de0a08777dc18262f07a71e00af8ebdaaaa6a30f255fad61ba78e,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.178.151.54,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Jan 30 12:03:09.868: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-4130" for this suite. 01/30/23 12:03:09.871
------------------------------
• [SLOW TEST] [5.036 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 12:03:04.837
    Jan 30 12:03:04.837: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename deployment 01/30/23 12:03:04.838
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:03:04.843
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:03:04.845
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should delete old replica sets [Conformance]
      test/e2e/apps/deployment.go:122
    Jan 30 12:03:04.851: INFO: Pod name cleanup-pod: Found 0 pods out of 1
    Jan 30 12:03:09.854: INFO: Pod name cleanup-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 01/30/23 12:03:09.854
    Jan 30 12:03:09.854: INFO: Creating deployment test-cleanup-deployment
    STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up 01/30/23 12:03:09.86
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jan 30 12:03:09.864: INFO: Deployment "test-cleanup-deployment":
    &Deployment{ObjectMeta:{test-cleanup-deployment  deployment-4130  075b2cbf-bdbd-4e78-84bd-83a9d903d038 28216 1 2023-01-30 12:03:09 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] [{e2e.test Update apps/v1 2023-01-30 12:03:09 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] []} {[] [] [{agnhost harbor.cloud.netease.com/qzprod-k8s/e2e/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc000a2d8f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

    Jan 30 12:03:09.866: INFO: New ReplicaSet of Deployment "test-cleanup-deployment" is nil.
    Jan 30 12:03:09.866: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
    Jan 30 12:03:09.866: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-4130  fb6b9b9d-3c94-4110-838e-9993cb8acc28 28221 1 2023-01-30 12:03:04 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment 075b2cbf-bdbd-4e78-84bd-83a9d903d038 0xc006cd30a7 0xc006cd30a8}] [] [{e2e.test Update apps/v1 2023-01-30 12:03:04 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-30 12:03:06 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-01-30 12:03:09 +0000 UTC FieldsV1 {"f:metadata":{"f:ownerReferences":{".":{},"k:{\"uid\":\"075b2cbf-bdbd-4e78-84bd-83a9d903d038\"}":{}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] [] []} {[] [] [{httpd harbor.cloud.netease.com/qzprod-k8s/e2e/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc006cd3178 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Jan 30 12:03:09.868: INFO: Pod "test-cleanup-controller-2hgkg" is available:
    &Pod{ObjectMeta:{test-cleanup-controller-2hgkg test-cleanup-controller- deployment-4130  46173773-a576-40b4-a9d6-57f250a0e175 28186 0 2023-01-30 12:03:04 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[cni.projectcalico.org/podIP:10.178.151.54/32 cni.projectcalico.org/podIPs:10.178.151.54/32] [{apps/v1 ReplicaSet test-cleanup-controller fb6b9b9d-3c94-4110-838e-9993cb8acc28 0xc004c79457 0xc004c79458}] [] [{kube-controller-manager Update v1 2023-01-30 12:03:04 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fb6b9b9d-3c94-4110-838e-9993cb8acc28\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-30 12:03:05 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-30 12:03:06 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.178.151.54\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-t8kpb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:harbor.cloud.netease.com/qzprod-k8s/e2e/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-t8kpb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pubt2-nks-for-dev1.dg.163.org,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 12:03:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 12:03:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 12:03:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 12:03:04 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.182.0.82,PodIP:10.178.151.54,StartTime:2023-01-30 12:03:04 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-30 12:03:05 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:harbor.cloud.netease.com/qzprod-k8s/e2e/httpd:2.4.38-4,ImageID:docker-pullable://harbor.cloud.netease.com/qzprod-k8s/e2e/httpd@sha256:8b745b6e9cb6a6184353110aabe28d8d6f6145d3fe7c0e1899aa70c8228c0fd3,ContainerID:docker://1c27bc19dc6de0a08777dc18262f07a71e00af8ebdaaaa6a30f255fad61ba78e,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.178.151.54,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Jan 30 12:03:09.868: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-4130" for this suite. 01/30/23 12:03:09.871
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces
  should list and delete a collection of PodDisruptionBudgets [Conformance]
  test/e2e/apps/disruption.go:87
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 12:03:09.875
Jan 30 12:03:09.875: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename disruption 01/30/23 12:03:09.875
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:03:09.881
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:03:09.884
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[BeforeEach] Listing PodDisruptionBudgets for all namespaces
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 12:03:09.886
Jan 30 12:03:09.886: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename disruption-2 01/30/23 12:03:09.886
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:03:09.892
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:03:09.895
[BeforeEach] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/metrics/init/init.go:31
[It] should list and delete a collection of PodDisruptionBudgets [Conformance]
  test/e2e/apps/disruption.go:87
STEP: Waiting for the pdb to be processed 01/30/23 12:03:09.899
STEP: Waiting for the pdb to be processed 01/30/23 12:03:11.906
STEP: Waiting for the pdb to be processed 01/30/23 12:03:13.913
STEP: listing a collection of PDBs across all namespaces 01/30/23 12:03:15.918
STEP: listing a collection of PDBs in namespace disruption-6580 01/30/23 12:03:15.92
STEP: deleting a collection of PDBs 01/30/23 12:03:15.921
STEP: Waiting for the PDB collection to be deleted 01/30/23 12:03:15.925
[AfterEach] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/node/init/init.go:32
Jan 30 12:03:15.927: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Jan 30 12:03:15.929: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
  dump namespaces | framework.go:196
[DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-2-4987" for this suite. 01/30/23 12:03:15.931
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-6580" for this suite. 01/30/23 12:03:15.933
------------------------------
• [SLOW TEST] [6.061 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  Listing PodDisruptionBudgets for all namespaces
  test/e2e/apps/disruption.go:78
    should list and delete a collection of PodDisruptionBudgets [Conformance]
    test/e2e/apps/disruption.go:87

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 12:03:09.875
    Jan 30 12:03:09.875: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename disruption 01/30/23 12:03:09.875
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:03:09.881
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:03:09.884
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [BeforeEach] Listing PodDisruptionBudgets for all namespaces
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 12:03:09.886
    Jan 30 12:03:09.886: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename disruption-2 01/30/23 12:03:09.886
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:03:09.892
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:03:09.895
    [BeforeEach] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/metrics/init/init.go:31
    [It] should list and delete a collection of PodDisruptionBudgets [Conformance]
      test/e2e/apps/disruption.go:87
    STEP: Waiting for the pdb to be processed 01/30/23 12:03:09.899
    STEP: Waiting for the pdb to be processed 01/30/23 12:03:11.906
    STEP: Waiting for the pdb to be processed 01/30/23 12:03:13.913
    STEP: listing a collection of PDBs across all namespaces 01/30/23 12:03:15.918
    STEP: listing a collection of PDBs in namespace disruption-6580 01/30/23 12:03:15.92
    STEP: deleting a collection of PDBs 01/30/23 12:03:15.921
    STEP: Waiting for the PDB collection to be deleted 01/30/23 12:03:15.925
    [AfterEach] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/node/init/init.go:32
    Jan 30 12:03:15.927: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Jan 30 12:03:15.929: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-2-4987" for this suite. 01/30/23 12:03:15.931
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-6580" for this suite. 01/30/23 12:03:15.933
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] Pods
  should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:845
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 12:03:15.936
Jan 30 12:03:15.936: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename pods 01/30/23 12:03:15.936
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:03:15.943
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:03:15.945
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:845
STEP: Create set of pods 01/30/23 12:03:15.947
Jan 30 12:03:15.951: INFO: created test-pod-1
Jan 30 12:03:15.954: INFO: created test-pod-2
Jan 30 12:03:15.956: INFO: created test-pod-3
STEP: waiting for all 3 pods to be running 01/30/23 12:03:15.956
Jan 30 12:03:15.956: INFO: Waiting up to 5m0s for all pods (need at least 3) in namespace 'pods-7675' to be running and ready
Jan 30 12:03:15.961: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Jan 30 12:03:15.961: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Jan 30 12:03:15.961: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Jan 30 12:03:15.961: INFO: 0 / 3 pods in namespace 'pods-7675' are running and ready (0 seconds elapsed)
Jan 30 12:03:15.961: INFO: expected 0 pod replicas in namespace 'pods-7675', 0 are Running and Ready.
Jan 30 12:03:15.961: INFO: POD         NODE                           PHASE    GRACE  CONDITIONS
Jan 30 12:03:15.961: INFO: test-pod-1  pubt2-nks-for-dev1.dg.163.org  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-30 12:03:15 +0000 UTC  }]
Jan 30 12:03:15.961: INFO: test-pod-2  pubt2-nks-for-dev1.dg.163.org  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-30 12:03:15 +0000 UTC  }]
Jan 30 12:03:15.961: INFO: test-pod-3  pubt2-nks-for-dev1.dg.163.org  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-30 12:03:15 +0000 UTC  }]
Jan 30 12:03:15.961: INFO: 
Jan 30 12:03:17.967: INFO: 3 / 3 pods in namespace 'pods-7675' are running and ready (2 seconds elapsed)
Jan 30 12:03:17.967: INFO: expected 0 pod replicas in namespace 'pods-7675', 0 are Running and Ready.
STEP: waiting for all pods to be deleted 01/30/23 12:03:17.973
Jan 30 12:03:17.976: INFO: Pod quantity 3 is different from expected quantity 0
Jan 30 12:03:18.980: INFO: Pod quantity 3 is different from expected quantity 0
Jan 30 12:03:19.978: INFO: Pod quantity 3 is different from expected quantity 0
Jan 30 12:03:20.978: INFO: Pod quantity 3 is different from expected quantity 0
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Jan 30 12:03:21.978: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-7675" for this suite. 01/30/23 12:03:21.981
------------------------------
• [SLOW TEST] [6.047 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:845

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 12:03:15.936
    Jan 30 12:03:15.936: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename pods 01/30/23 12:03:15.936
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:03:15.943
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:03:15.945
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should delete a collection of pods [Conformance]
      test/e2e/common/node/pods.go:845
    STEP: Create set of pods 01/30/23 12:03:15.947
    Jan 30 12:03:15.951: INFO: created test-pod-1
    Jan 30 12:03:15.954: INFO: created test-pod-2
    Jan 30 12:03:15.956: INFO: created test-pod-3
    STEP: waiting for all 3 pods to be running 01/30/23 12:03:15.956
    Jan 30 12:03:15.956: INFO: Waiting up to 5m0s for all pods (need at least 3) in namespace 'pods-7675' to be running and ready
    Jan 30 12:03:15.961: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Jan 30 12:03:15.961: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Jan 30 12:03:15.961: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Jan 30 12:03:15.961: INFO: 0 / 3 pods in namespace 'pods-7675' are running and ready (0 seconds elapsed)
    Jan 30 12:03:15.961: INFO: expected 0 pod replicas in namespace 'pods-7675', 0 are Running and Ready.
    Jan 30 12:03:15.961: INFO: POD         NODE                           PHASE    GRACE  CONDITIONS
    Jan 30 12:03:15.961: INFO: test-pod-1  pubt2-nks-for-dev1.dg.163.org  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-30 12:03:15 +0000 UTC  }]
    Jan 30 12:03:15.961: INFO: test-pod-2  pubt2-nks-for-dev1.dg.163.org  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-30 12:03:15 +0000 UTC  }]
    Jan 30 12:03:15.961: INFO: test-pod-3  pubt2-nks-for-dev1.dg.163.org  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-30 12:03:15 +0000 UTC  }]
    Jan 30 12:03:15.961: INFO: 
    Jan 30 12:03:17.967: INFO: 3 / 3 pods in namespace 'pods-7675' are running and ready (2 seconds elapsed)
    Jan 30 12:03:17.967: INFO: expected 0 pod replicas in namespace 'pods-7675', 0 are Running and Ready.
    STEP: waiting for all pods to be deleted 01/30/23 12:03:17.973
    Jan 30 12:03:17.976: INFO: Pod quantity 3 is different from expected quantity 0
    Jan 30 12:03:18.980: INFO: Pod quantity 3 is different from expected quantity 0
    Jan 30 12:03:19.978: INFO: Pod quantity 3 is different from expected quantity 0
    Jan 30 12:03:20.978: INFO: Pod quantity 3 is different from expected quantity 0
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Jan 30 12:03:21.978: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-7675" for this suite. 01/30/23 12:03:21.981
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:216
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 12:03:21.983
Jan 30 12:03:21.983: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename container-runtime 01/30/23 12:03:21.984
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:03:21.99
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:03:21.992
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:216
STEP: create the container 01/30/23 12:03:21.994
STEP: wait for the container to reach Failed 01/30/23 12:03:21.999
STEP: get the container status 01/30/23 12:03:27.012
STEP: the container should be terminated 01/30/23 12:03:27.014
STEP: the termination message should be set 01/30/23 12:03:27.014
Jan 30 12:03:27.014: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container 01/30/23 12:03:27.014
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Jan 30 12:03:27.019: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-8589" for this suite. 01/30/23 12:03:27.022
------------------------------
• [SLOW TEST] [5.041 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    on terminated container
    test/e2e/common/node/runtime.go:137
      should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:216

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 12:03:21.983
    Jan 30 12:03:21.983: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename container-runtime 01/30/23 12:03:21.984
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:03:21.99
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:03:21.992
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:216
    STEP: create the container 01/30/23 12:03:21.994
    STEP: wait for the container to reach Failed 01/30/23 12:03:21.999
    STEP: get the container status 01/30/23 12:03:27.012
    STEP: the container should be terminated 01/30/23 12:03:27.014
    STEP: the termination message should be set 01/30/23 12:03:27.014
    Jan 30 12:03:27.014: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
    STEP: delete the container 01/30/23 12:03:27.014
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Jan 30 12:03:27.019: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-8589" for this suite. 01/30/23 12:03:27.022
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:207
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 12:03:27.025
Jan 30 12:03:27.026: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename projected 01/30/23 12:03:27.026
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:03:27.032
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:03:27.034
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:207
STEP: Creating a pod to test downward API volume plugin 01/30/23 12:03:27.036
Jan 30 12:03:27.041: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2fd82a37-922e-4ff8-b675-929cbbc3e3e4" in namespace "projected-9823" to be "Succeeded or Failed"
Jan 30 12:03:27.042: INFO: Pod "downwardapi-volume-2fd82a37-922e-4ff8-b675-929cbbc3e3e4": Phase="Pending", Reason="", readiness=false. Elapsed: 1.573983ms
Jan 30 12:03:29.045: INFO: Pod "downwardapi-volume-2fd82a37-922e-4ff8-b675-929cbbc3e3e4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00428284s
Jan 30 12:03:31.046: INFO: Pod "downwardapi-volume-2fd82a37-922e-4ff8-b675-929cbbc3e3e4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005494476s
STEP: Saw pod success 01/30/23 12:03:31.046
Jan 30 12:03:31.046: INFO: Pod "downwardapi-volume-2fd82a37-922e-4ff8-b675-929cbbc3e3e4" satisfied condition "Succeeded or Failed"
Jan 30 12:03:31.048: INFO: Trying to get logs from node pubt2-nks-for-dev1.dg.163.org pod downwardapi-volume-2fd82a37-922e-4ff8-b675-929cbbc3e3e4 container client-container: <nil>
STEP: delete the pod 01/30/23 12:03:31.054
Jan 30 12:03:31.059: INFO: Waiting for pod downwardapi-volume-2fd82a37-922e-4ff8-b675-929cbbc3e3e4 to disappear
Jan 30 12:03:31.060: INFO: Pod downwardapi-volume-2fd82a37-922e-4ff8-b675-929cbbc3e3e4 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Jan 30 12:03:31.060: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-9823" for this suite. 01/30/23 12:03:31.062
------------------------------
• [4.039 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 12:03:27.025
    Jan 30 12:03:27.026: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename projected 01/30/23 12:03:27.026
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:03:27.032
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:03:27.034
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide container's memory limit [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:207
    STEP: Creating a pod to test downward API volume plugin 01/30/23 12:03:27.036
    Jan 30 12:03:27.041: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2fd82a37-922e-4ff8-b675-929cbbc3e3e4" in namespace "projected-9823" to be "Succeeded or Failed"
    Jan 30 12:03:27.042: INFO: Pod "downwardapi-volume-2fd82a37-922e-4ff8-b675-929cbbc3e3e4": Phase="Pending", Reason="", readiness=false. Elapsed: 1.573983ms
    Jan 30 12:03:29.045: INFO: Pod "downwardapi-volume-2fd82a37-922e-4ff8-b675-929cbbc3e3e4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00428284s
    Jan 30 12:03:31.046: INFO: Pod "downwardapi-volume-2fd82a37-922e-4ff8-b675-929cbbc3e3e4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005494476s
    STEP: Saw pod success 01/30/23 12:03:31.046
    Jan 30 12:03:31.046: INFO: Pod "downwardapi-volume-2fd82a37-922e-4ff8-b675-929cbbc3e3e4" satisfied condition "Succeeded or Failed"
    Jan 30 12:03:31.048: INFO: Trying to get logs from node pubt2-nks-for-dev1.dg.163.org pod downwardapi-volume-2fd82a37-922e-4ff8-b675-929cbbc3e3e4 container client-container: <nil>
    STEP: delete the pod 01/30/23 12:03:31.054
    Jan 30 12:03:31.059: INFO: Waiting for pod downwardapi-volume-2fd82a37-922e-4ff8-b675-929cbbc3e3e4 to disappear
    Jan 30 12:03:31.060: INFO: Pod downwardapi-volume-2fd82a37-922e-4ff8-b675-929cbbc3e3e4 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Jan 30 12:03:31.060: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-9823" for this suite. 01/30/23 12:03:31.062
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:109
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 12:03:31.067
Jan 30 12:03:31.067: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename projected 01/30/23 12:03:31.067
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:03:31.073
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:03:31.075
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:109
STEP: Creating configMap with name projected-configmap-test-volume-map-864596b2-dcd7-4ebe-8996-a06b0ab05c06 01/30/23 12:03:31.077
STEP: Creating a pod to test consume configMaps 01/30/23 12:03:31.079
Jan 30 12:03:31.083: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-ec30f490-d8a4-427f-b164-0ba8df4ae2fd" in namespace "projected-706" to be "Succeeded or Failed"
Jan 30 12:03:31.085: INFO: Pod "pod-projected-configmaps-ec30f490-d8a4-427f-b164-0ba8df4ae2fd": Phase="Pending", Reason="", readiness=false. Elapsed: 1.4735ms
Jan 30 12:03:33.087: INFO: Pod "pod-projected-configmaps-ec30f490-d8a4-427f-b164-0ba8df4ae2fd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004266372s
Jan 30 12:03:35.088: INFO: Pod "pod-projected-configmaps-ec30f490-d8a4-427f-b164-0ba8df4ae2fd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00514244s
STEP: Saw pod success 01/30/23 12:03:35.088
Jan 30 12:03:35.088: INFO: Pod "pod-projected-configmaps-ec30f490-d8a4-427f-b164-0ba8df4ae2fd" satisfied condition "Succeeded or Failed"
Jan 30 12:03:35.090: INFO: Trying to get logs from node pubt2-nks-for-dev1.dg.163.org pod pod-projected-configmaps-ec30f490-d8a4-427f-b164-0ba8df4ae2fd container agnhost-container: <nil>
STEP: delete the pod 01/30/23 12:03:35.095
Jan 30 12:03:35.099: INFO: Waiting for pod pod-projected-configmaps-ec30f490-d8a4-427f-b164-0ba8df4ae2fd to disappear
Jan 30 12:03:35.101: INFO: Pod pod-projected-configmaps-ec30f490-d8a4-427f-b164-0ba8df4ae2fd no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Jan 30 12:03:35.101: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-706" for this suite. 01/30/23 12:03:35.103
------------------------------
• [4.039 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:109

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 12:03:31.067
    Jan 30 12:03:31.067: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename projected 01/30/23 12:03:31.067
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:03:31.073
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:03:31.075
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:109
    STEP: Creating configMap with name projected-configmap-test-volume-map-864596b2-dcd7-4ebe-8996-a06b0ab05c06 01/30/23 12:03:31.077
    STEP: Creating a pod to test consume configMaps 01/30/23 12:03:31.079
    Jan 30 12:03:31.083: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-ec30f490-d8a4-427f-b164-0ba8df4ae2fd" in namespace "projected-706" to be "Succeeded or Failed"
    Jan 30 12:03:31.085: INFO: Pod "pod-projected-configmaps-ec30f490-d8a4-427f-b164-0ba8df4ae2fd": Phase="Pending", Reason="", readiness=false. Elapsed: 1.4735ms
    Jan 30 12:03:33.087: INFO: Pod "pod-projected-configmaps-ec30f490-d8a4-427f-b164-0ba8df4ae2fd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004266372s
    Jan 30 12:03:35.088: INFO: Pod "pod-projected-configmaps-ec30f490-d8a4-427f-b164-0ba8df4ae2fd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00514244s
    STEP: Saw pod success 01/30/23 12:03:35.088
    Jan 30 12:03:35.088: INFO: Pod "pod-projected-configmaps-ec30f490-d8a4-427f-b164-0ba8df4ae2fd" satisfied condition "Succeeded or Failed"
    Jan 30 12:03:35.090: INFO: Trying to get logs from node pubt2-nks-for-dev1.dg.163.org pod pod-projected-configmaps-ec30f490-d8a4-427f-b164-0ba8df4ae2fd container agnhost-container: <nil>
    STEP: delete the pod 01/30/23 12:03:35.095
    Jan 30 12:03:35.099: INFO: Waiting for pod pod-projected-configmaps-ec30f490-d8a4-427f-b164-0ba8df4ae2fd to disappear
    Jan 30 12:03:35.101: INFO: Pod pod-projected-configmaps-ec30f490-d8a4-427f-b164-0ba8df4ae2fd no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Jan 30 12:03:35.101: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-706" for this suite. 01/30/23 12:03:35.103
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 12:03:35.105
Jan 30 12:03:35.105: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename crd-webhook 01/30/23 12:03:35.106
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:03:35.112
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:03:35.114
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:128
STEP: Setting up server cert 01/30/23 12:03:35.116
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 01/30/23 12:03:35.498
STEP: Deploying the custom resource conversion webhook pod 01/30/23 12:03:35.501
STEP: Wait for the deployment to be ready 01/30/23 12:03:35.507
Jan 30 12:03:35.510: INFO: new replicaset for deployment "sample-crd-conversion-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 01/30/23 12:03:37.515
STEP: Verifying the service has paired with the endpoint 01/30/23 12:03:37.519
Jan 30 12:03:38.520: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149
Jan 30 12:03:38.522: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Creating a v1 custom resource 01/30/23 12:03:41.112
STEP: v2 custom resource should be converted 01/30/23 12:03:41.115
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 30 12:03:41.625: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:139
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-webhook-3294" for this suite. 01/30/23 12:03:41.64
------------------------------
• [SLOW TEST] [6.537 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 12:03:35.105
    Jan 30 12:03:35.105: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename crd-webhook 01/30/23 12:03:35.106
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:03:35.112
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:03:35.114
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:128
    STEP: Setting up server cert 01/30/23 12:03:35.116
    STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 01/30/23 12:03:35.498
    STEP: Deploying the custom resource conversion webhook pod 01/30/23 12:03:35.501
    STEP: Wait for the deployment to be ready 01/30/23 12:03:35.507
    Jan 30 12:03:35.510: INFO: new replicaset for deployment "sample-crd-conversion-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 01/30/23 12:03:37.515
    STEP: Verifying the service has paired with the endpoint 01/30/23 12:03:37.519
    Jan 30 12:03:38.520: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
    [It] should be able to convert from CR v1 to CR v2 [Conformance]
      test/e2e/apimachinery/crd_conversion_webhook.go:149
    Jan 30 12:03:38.522: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Creating a v1 custom resource 01/30/23 12:03:41.112
    STEP: v2 custom resource should be converted 01/30/23 12:03:41.115
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 30 12:03:41.625: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:139
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-webhook-3294" for this suite. 01/30/23 12:03:41.64
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-apps] Job
  should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:507
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 12:03:41.642
Jan 30 12:03:41.643: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename job 01/30/23 12:03:41.643
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:03:41.65
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:03:41.652
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:507
STEP: Creating a job 01/30/23 12:03:41.653
STEP: Ensuring active pods == parallelism 01/30/23 12:03:41.656
STEP: Orphaning one of the Job's Pods 01/30/23 12:03:43.659
Jan 30 12:03:44.170: INFO: Successfully updated pod "adopt-release-5kdgg"
STEP: Checking that the Job readopts the Pod 01/30/23 12:03:44.17
Jan 30 12:03:44.170: INFO: Waiting up to 15m0s for pod "adopt-release-5kdgg" in namespace "job-5541" to be "adopted"
Jan 30 12:03:44.172: INFO: Pod "adopt-release-5kdgg": Phase="Running", Reason="", readiness=true. Elapsed: 1.66188ms
Jan 30 12:03:46.174: INFO: Pod "adopt-release-5kdgg": Phase="Running", Reason="", readiness=true. Elapsed: 2.003680676s
Jan 30 12:03:46.174: INFO: Pod "adopt-release-5kdgg" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod 01/30/23 12:03:46.174
Jan 30 12:03:46.682: INFO: Successfully updated pod "adopt-release-5kdgg"
STEP: Checking that the Job releases the Pod 01/30/23 12:03:46.682
Jan 30 12:03:46.682: INFO: Waiting up to 15m0s for pod "adopt-release-5kdgg" in namespace "job-5541" to be "released"
Jan 30 12:03:46.684: INFO: Pod "adopt-release-5kdgg": Phase="Running", Reason="", readiness=true. Elapsed: 1.607771ms
Jan 30 12:03:48.686: INFO: Pod "adopt-release-5kdgg": Phase="Running", Reason="", readiness=true. Elapsed: 2.004296173s
Jan 30 12:03:48.686: INFO: Pod "adopt-release-5kdgg" satisfied condition "released"
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Jan 30 12:03:48.686: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-5541" for this suite. 01/30/23 12:03:48.689
------------------------------
• [SLOW TEST] [7.049 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:507

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 12:03:41.642
    Jan 30 12:03:41.643: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename job 01/30/23 12:03:41.643
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:03:41.65
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:03:41.652
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should adopt matching orphans and release non-matching pods [Conformance]
      test/e2e/apps/job.go:507
    STEP: Creating a job 01/30/23 12:03:41.653
    STEP: Ensuring active pods == parallelism 01/30/23 12:03:41.656
    STEP: Orphaning one of the Job's Pods 01/30/23 12:03:43.659
    Jan 30 12:03:44.170: INFO: Successfully updated pod "adopt-release-5kdgg"
    STEP: Checking that the Job readopts the Pod 01/30/23 12:03:44.17
    Jan 30 12:03:44.170: INFO: Waiting up to 15m0s for pod "adopt-release-5kdgg" in namespace "job-5541" to be "adopted"
    Jan 30 12:03:44.172: INFO: Pod "adopt-release-5kdgg": Phase="Running", Reason="", readiness=true. Elapsed: 1.66188ms
    Jan 30 12:03:46.174: INFO: Pod "adopt-release-5kdgg": Phase="Running", Reason="", readiness=true. Elapsed: 2.003680676s
    Jan 30 12:03:46.174: INFO: Pod "adopt-release-5kdgg" satisfied condition "adopted"
    STEP: Removing the labels from the Job's Pod 01/30/23 12:03:46.174
    Jan 30 12:03:46.682: INFO: Successfully updated pod "adopt-release-5kdgg"
    STEP: Checking that the Job releases the Pod 01/30/23 12:03:46.682
    Jan 30 12:03:46.682: INFO: Waiting up to 15m0s for pod "adopt-release-5kdgg" in namespace "job-5541" to be "released"
    Jan 30 12:03:46.684: INFO: Pod "adopt-release-5kdgg": Phase="Running", Reason="", readiness=true. Elapsed: 1.607771ms
    Jan 30 12:03:48.686: INFO: Pod "adopt-release-5kdgg": Phase="Running", Reason="", readiness=true. Elapsed: 2.004296173s
    Jan 30 12:03:48.686: INFO: Pod "adopt-release-5kdgg" satisfied condition "released"
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Jan 30 12:03:48.686: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-5541" for this suite. 01/30/23 12:03:48.689
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 12:03:48.692
Jan 30 12:03:48.692: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename replicaset 01/30/23 12:03:48.693
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:03:48.698
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:03:48.7
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154
Jan 30 12:03:48.710: INFO: Pod name sample-pod: Found 0 pods out of 1
Jan 30 12:03:53.713: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 01/30/23 12:03:53.713
STEP: Scaling up "test-rs" replicaset  01/30/23 12:03:53.713
Jan 30 12:03:53.718: INFO: Updating replica set "test-rs"
STEP: patching the ReplicaSet 01/30/23 12:03:53.718
W0130 12:03:53.722274      23 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Jan 30 12:03:53.723: INFO: observed ReplicaSet test-rs in namespace replicaset-6706 with ReadyReplicas 1, AvailableReplicas 1
Jan 30 12:03:53.729: INFO: observed ReplicaSet test-rs in namespace replicaset-6706 with ReadyReplicas 1, AvailableReplicas 1
Jan 30 12:03:53.736: INFO: observed ReplicaSet test-rs in namespace replicaset-6706 with ReadyReplicas 1, AvailableReplicas 1
Jan 30 12:03:53.740: INFO: observed ReplicaSet test-rs in namespace replicaset-6706 with ReadyReplicas 1, AvailableReplicas 1
Jan 30 12:03:55.857: INFO: observed ReplicaSet test-rs in namespace replicaset-6706 with ReadyReplicas 2, AvailableReplicas 2
Jan 30 12:03:55.913: INFO: observed Replicaset test-rs in namespace replicaset-6706 with ReadyReplicas 3 found true
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Jan 30 12:03:55.913: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-6706" for this suite. 01/30/23 12:03:55.915
------------------------------
• [SLOW TEST] [7.225 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 12:03:48.692
    Jan 30 12:03:48.692: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename replicaset 01/30/23 12:03:48.693
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:03:48.698
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:03:48.7
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] Replace and Patch tests [Conformance]
      test/e2e/apps/replica_set.go:154
    Jan 30 12:03:48.710: INFO: Pod name sample-pod: Found 0 pods out of 1
    Jan 30 12:03:53.713: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 01/30/23 12:03:53.713
    STEP: Scaling up "test-rs" replicaset  01/30/23 12:03:53.713
    Jan 30 12:03:53.718: INFO: Updating replica set "test-rs"
    STEP: patching the ReplicaSet 01/30/23 12:03:53.718
    W0130 12:03:53.722274      23 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Jan 30 12:03:53.723: INFO: observed ReplicaSet test-rs in namespace replicaset-6706 with ReadyReplicas 1, AvailableReplicas 1
    Jan 30 12:03:53.729: INFO: observed ReplicaSet test-rs in namespace replicaset-6706 with ReadyReplicas 1, AvailableReplicas 1
    Jan 30 12:03:53.736: INFO: observed ReplicaSet test-rs in namespace replicaset-6706 with ReadyReplicas 1, AvailableReplicas 1
    Jan 30 12:03:53.740: INFO: observed ReplicaSet test-rs in namespace replicaset-6706 with ReadyReplicas 1, AvailableReplicas 1
    Jan 30 12:03:55.857: INFO: observed ReplicaSet test-rs in namespace replicaset-6706 with ReadyReplicas 2, AvailableReplicas 2
    Jan 30 12:03:55.913: INFO: observed Replicaset test-rs in namespace replicaset-6706 with ReadyReplicas 3 found true
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Jan 30 12:03:55.913: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-6706" for this suite. 01/30/23 12:03:55.915
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute poststart http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:167
[BeforeEach] [sig-node] Container Lifecycle Hook
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 12:03:55.918
Jan 30 12:03:55.918: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename container-lifecycle-hook 01/30/23 12:03:55.919
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:03:55.925
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:03:55.927
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:77
STEP: create the container to handle the HTTPGet hook request. 01/30/23 12:03:55.931
Jan 30 12:03:55.935: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-2917" to be "running and ready"
Jan 30 12:03:55.937: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 1.521769ms
Jan 30 12:03:55.937: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jan 30 12:03:57.939: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.004402756s
Jan 30 12:03:57.939: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Jan 30 12:03:57.939: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:167
STEP: create the pod with lifecycle hook 01/30/23 12:03:57.941
Jan 30 12:03:57.944: INFO: Waiting up to 5m0s for pod "pod-with-poststart-http-hook" in namespace "container-lifecycle-hook-2917" to be "running and ready"
Jan 30 12:03:57.946: INFO: Pod "pod-with-poststart-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 1.564571ms
Jan 30 12:03:57.946: INFO: The phase of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
Jan 30 12:03:59.949: INFO: Pod "pod-with-poststart-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005262341s
Jan 30 12:03:59.949: INFO: The phase of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
Jan 30 12:04:01.949: INFO: Pod "pod-with-poststart-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 4.005472375s
Jan 30 12:04:01.950: INFO: The phase of Pod pod-with-poststart-http-hook is Running (Ready = true)
Jan 30 12:04:01.950: INFO: Pod "pod-with-poststart-http-hook" satisfied condition "running and ready"
STEP: check poststart hook 01/30/23 12:04:01.951
STEP: delete the pod with lifecycle hook 01/30/23 12:04:01.966
Jan 30 12:04:01.969: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jan 30 12:04:01.970: INFO: Pod pod-with-poststart-http-hook still exists
Jan 30 12:04:03.971: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jan 30 12:04:03.973: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/node/init/init.go:32
Jan 30 12:04:03.973: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  tear down framework | framework.go:193
STEP: Destroying namespace "container-lifecycle-hook-2917" for this suite. 01/30/23 12:04:03.978
------------------------------
• [SLOW TEST] [8.063 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute poststart http hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:167

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 12:03:55.918
    Jan 30 12:03:55.918: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename container-lifecycle-hook 01/30/23 12:03:55.919
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:03:55.925
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:03:55.927
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:77
    STEP: create the container to handle the HTTPGet hook request. 01/30/23 12:03:55.931
    Jan 30 12:03:55.935: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-2917" to be "running and ready"
    Jan 30 12:03:55.937: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 1.521769ms
    Jan 30 12:03:55.937: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Jan 30 12:03:57.939: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.004402756s
    Jan 30 12:03:57.939: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Jan 30 12:03:57.939: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute poststart http hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:167
    STEP: create the pod with lifecycle hook 01/30/23 12:03:57.941
    Jan 30 12:03:57.944: INFO: Waiting up to 5m0s for pod "pod-with-poststart-http-hook" in namespace "container-lifecycle-hook-2917" to be "running and ready"
    Jan 30 12:03:57.946: INFO: Pod "pod-with-poststart-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 1.564571ms
    Jan 30 12:03:57.946: INFO: The phase of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
    Jan 30 12:03:59.949: INFO: Pod "pod-with-poststart-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005262341s
    Jan 30 12:03:59.949: INFO: The phase of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
    Jan 30 12:04:01.949: INFO: Pod "pod-with-poststart-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 4.005472375s
    Jan 30 12:04:01.950: INFO: The phase of Pod pod-with-poststart-http-hook is Running (Ready = true)
    Jan 30 12:04:01.950: INFO: Pod "pod-with-poststart-http-hook" satisfied condition "running and ready"
    STEP: check poststart hook 01/30/23 12:04:01.951
    STEP: delete the pod with lifecycle hook 01/30/23 12:04:01.966
    Jan 30 12:04:01.969: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Jan 30 12:04:01.970: INFO: Pod pod-with-poststart-http-hook still exists
    Jan 30 12:04:03.971: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Jan 30 12:04:03.973: INFO: Pod pod-with-poststart-http-hook no longer exists
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/node/init/init.go:32
    Jan 30 12:04:03.973: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-lifecycle-hook-2917" for this suite. 01/30/23 12:04:03.978
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2191
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 12:04:03.982
Jan 30 12:04:03.982: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename services 01/30/23 12:04:03.982
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:04:03.989
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:04:03.991
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2191
STEP: creating service in namespace services-3809 01/30/23 12:04:03.993
STEP: creating service affinity-clusterip in namespace services-3809 01/30/23 12:04:03.993
STEP: creating replication controller affinity-clusterip in namespace services-3809 01/30/23 12:04:03.997
I0130 12:04:04.000231      23 runners.go:193] Created replication controller with name: affinity-clusterip, namespace: services-3809, replica count: 3
I0130 12:04:07.051500      23 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 30 12:04:07.055: INFO: Creating new exec pod
Jan 30 12:04:07.057: INFO: Waiting up to 5m0s for pod "execpod-affinityjrd9p" in namespace "services-3809" to be "running"
Jan 30 12:04:07.059: INFO: Pod "execpod-affinityjrd9p": Phase="Pending", Reason="", readiness=false. Elapsed: 1.664512ms
Jan 30 12:04:09.061: INFO: Pod "execpod-affinityjrd9p": Phase="Running", Reason="", readiness=true. Elapsed: 2.003779623s
Jan 30 12:04:09.061: INFO: Pod "execpod-affinityjrd9p" satisfied condition "running"
Jan 30 12:04:10.062: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=services-3809 exec execpod-affinityjrd9p -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip 80'
Jan 30 12:04:11.222: INFO: stderr: "+ nc -v -z -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
Jan 30 12:04:11.222: INFO: stdout: ""
Jan 30 12:04:11.222: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=services-3809 exec execpod-affinityjrd9p -- /bin/sh -x -c nc -v -z -w 2 10.178.79.201 80'
Jan 30 12:04:11.384: INFO: stderr: "+ nc -v -z -w 2 10.178.79.201 80\nConnection to 10.178.79.201 80 port [tcp/http] succeeded!\n"
Jan 30 12:04:11.384: INFO: stdout: ""
Jan 30 12:04:11.384: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=services-3809 exec execpod-affinityjrd9p -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.178.79.201:80/ ; done'
Jan 30 12:04:11.574: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.178.79.201:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.178.79.201:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.178.79.201:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.178.79.201:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.178.79.201:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.178.79.201:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.178.79.201:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.178.79.201:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.178.79.201:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.178.79.201:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.178.79.201:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.178.79.201:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.178.79.201:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.178.79.201:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.178.79.201:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.178.79.201:80/\n"
Jan 30 12:04:11.574: INFO: stdout: "\naffinity-clusterip-q8p86\naffinity-clusterip-q8p86\naffinity-clusterip-q8p86\naffinity-clusterip-q8p86\naffinity-clusterip-q8p86\naffinity-clusterip-q8p86\naffinity-clusterip-q8p86\naffinity-clusterip-q8p86\naffinity-clusterip-q8p86\naffinity-clusterip-q8p86\naffinity-clusterip-q8p86\naffinity-clusterip-q8p86\naffinity-clusterip-q8p86\naffinity-clusterip-q8p86\naffinity-clusterip-q8p86\naffinity-clusterip-q8p86"
Jan 30 12:04:11.574: INFO: Received response from host: affinity-clusterip-q8p86
Jan 30 12:04:11.574: INFO: Received response from host: affinity-clusterip-q8p86
Jan 30 12:04:11.574: INFO: Received response from host: affinity-clusterip-q8p86
Jan 30 12:04:11.574: INFO: Received response from host: affinity-clusterip-q8p86
Jan 30 12:04:11.574: INFO: Received response from host: affinity-clusterip-q8p86
Jan 30 12:04:11.574: INFO: Received response from host: affinity-clusterip-q8p86
Jan 30 12:04:11.574: INFO: Received response from host: affinity-clusterip-q8p86
Jan 30 12:04:11.574: INFO: Received response from host: affinity-clusterip-q8p86
Jan 30 12:04:11.574: INFO: Received response from host: affinity-clusterip-q8p86
Jan 30 12:04:11.574: INFO: Received response from host: affinity-clusterip-q8p86
Jan 30 12:04:11.574: INFO: Received response from host: affinity-clusterip-q8p86
Jan 30 12:04:11.574: INFO: Received response from host: affinity-clusterip-q8p86
Jan 30 12:04:11.574: INFO: Received response from host: affinity-clusterip-q8p86
Jan 30 12:04:11.574: INFO: Received response from host: affinity-clusterip-q8p86
Jan 30 12:04:11.574: INFO: Received response from host: affinity-clusterip-q8p86
Jan 30 12:04:11.574: INFO: Received response from host: affinity-clusterip-q8p86
Jan 30 12:04:11.574: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip in namespace services-3809, will wait for the garbage collector to delete the pods 01/30/23 12:04:11.579
Jan 30 12:04:11.633: INFO: Deleting ReplicationController affinity-clusterip took: 2.200383ms
Jan 30 12:04:11.733: INFO: Terminating ReplicationController affinity-clusterip pods took: 100.287926ms
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jan 30 12:04:14.439: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-3809" for this suite. 01/30/23 12:04:14.441
------------------------------
• [SLOW TEST] [10.462 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2191

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 12:04:03.982
    Jan 30 12:04:03.982: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename services 01/30/23 12:04:03.982
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:04:03.989
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:04:03.991
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2191
    STEP: creating service in namespace services-3809 01/30/23 12:04:03.993
    STEP: creating service affinity-clusterip in namespace services-3809 01/30/23 12:04:03.993
    STEP: creating replication controller affinity-clusterip in namespace services-3809 01/30/23 12:04:03.997
    I0130 12:04:04.000231      23 runners.go:193] Created replication controller with name: affinity-clusterip, namespace: services-3809, replica count: 3
    I0130 12:04:07.051500      23 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan 30 12:04:07.055: INFO: Creating new exec pod
    Jan 30 12:04:07.057: INFO: Waiting up to 5m0s for pod "execpod-affinityjrd9p" in namespace "services-3809" to be "running"
    Jan 30 12:04:07.059: INFO: Pod "execpod-affinityjrd9p": Phase="Pending", Reason="", readiness=false. Elapsed: 1.664512ms
    Jan 30 12:04:09.061: INFO: Pod "execpod-affinityjrd9p": Phase="Running", Reason="", readiness=true. Elapsed: 2.003779623s
    Jan 30 12:04:09.061: INFO: Pod "execpod-affinityjrd9p" satisfied condition "running"
    Jan 30 12:04:10.062: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=services-3809 exec execpod-affinityjrd9p -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip 80'
    Jan 30 12:04:11.222: INFO: stderr: "+ nc -v -z -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
    Jan 30 12:04:11.222: INFO: stdout: ""
    Jan 30 12:04:11.222: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=services-3809 exec execpod-affinityjrd9p -- /bin/sh -x -c nc -v -z -w 2 10.178.79.201 80'
    Jan 30 12:04:11.384: INFO: stderr: "+ nc -v -z -w 2 10.178.79.201 80\nConnection to 10.178.79.201 80 port [tcp/http] succeeded!\n"
    Jan 30 12:04:11.384: INFO: stdout: ""
    Jan 30 12:04:11.384: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=services-3809 exec execpod-affinityjrd9p -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.178.79.201:80/ ; done'
    Jan 30 12:04:11.574: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.178.79.201:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.178.79.201:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.178.79.201:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.178.79.201:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.178.79.201:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.178.79.201:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.178.79.201:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.178.79.201:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.178.79.201:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.178.79.201:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.178.79.201:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.178.79.201:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.178.79.201:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.178.79.201:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.178.79.201:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.178.79.201:80/\n"
    Jan 30 12:04:11.574: INFO: stdout: "\naffinity-clusterip-q8p86\naffinity-clusterip-q8p86\naffinity-clusterip-q8p86\naffinity-clusterip-q8p86\naffinity-clusterip-q8p86\naffinity-clusterip-q8p86\naffinity-clusterip-q8p86\naffinity-clusterip-q8p86\naffinity-clusterip-q8p86\naffinity-clusterip-q8p86\naffinity-clusterip-q8p86\naffinity-clusterip-q8p86\naffinity-clusterip-q8p86\naffinity-clusterip-q8p86\naffinity-clusterip-q8p86\naffinity-clusterip-q8p86"
    Jan 30 12:04:11.574: INFO: Received response from host: affinity-clusterip-q8p86
    Jan 30 12:04:11.574: INFO: Received response from host: affinity-clusterip-q8p86
    Jan 30 12:04:11.574: INFO: Received response from host: affinity-clusterip-q8p86
    Jan 30 12:04:11.574: INFO: Received response from host: affinity-clusterip-q8p86
    Jan 30 12:04:11.574: INFO: Received response from host: affinity-clusterip-q8p86
    Jan 30 12:04:11.574: INFO: Received response from host: affinity-clusterip-q8p86
    Jan 30 12:04:11.574: INFO: Received response from host: affinity-clusterip-q8p86
    Jan 30 12:04:11.574: INFO: Received response from host: affinity-clusterip-q8p86
    Jan 30 12:04:11.574: INFO: Received response from host: affinity-clusterip-q8p86
    Jan 30 12:04:11.574: INFO: Received response from host: affinity-clusterip-q8p86
    Jan 30 12:04:11.574: INFO: Received response from host: affinity-clusterip-q8p86
    Jan 30 12:04:11.574: INFO: Received response from host: affinity-clusterip-q8p86
    Jan 30 12:04:11.574: INFO: Received response from host: affinity-clusterip-q8p86
    Jan 30 12:04:11.574: INFO: Received response from host: affinity-clusterip-q8p86
    Jan 30 12:04:11.574: INFO: Received response from host: affinity-clusterip-q8p86
    Jan 30 12:04:11.574: INFO: Received response from host: affinity-clusterip-q8p86
    Jan 30 12:04:11.574: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-clusterip in namespace services-3809, will wait for the garbage collector to delete the pods 01/30/23 12:04:11.579
    Jan 30 12:04:11.633: INFO: Deleting ReplicationController affinity-clusterip took: 2.200383ms
    Jan 30 12:04:11.733: INFO: Terminating ReplicationController affinity-clusterip pods took: 100.287926ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jan 30 12:04:14.439: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-3809" for this suite. 01/30/23 12:04:14.441
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-apps] DisruptionController
  should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:164
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 12:04:14.444
Jan 30 12:04:14.444: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename disruption 01/30/23 12:04:14.444
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:04:14.451
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:04:14.452
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[It] should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:164
STEP: Waiting for the pdb to be processed 01/30/23 12:04:14.456
STEP: Updating PodDisruptionBudget status 01/30/23 12:04:16.461
STEP: Waiting for all pods to be running 01/30/23 12:04:16.465
Jan 30 12:04:16.467: INFO: running pods: 0 < 1
Jan 30 12:04:18.471: INFO: running pods: 0 < 1
STEP: locating a running pod 01/30/23 12:04:20.471
STEP: Waiting for the pdb to be processed 01/30/23 12:04:20.477
STEP: Patching PodDisruptionBudget status 01/30/23 12:04:20.48
STEP: Waiting for the pdb to be processed 01/30/23 12:04:20.485
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Jan 30 12:04:20.487: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-8357" for this suite. 01/30/23 12:04:20.489
------------------------------
• [SLOW TEST] [6.047 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:164

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 12:04:14.444
    Jan 30 12:04:14.444: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename disruption 01/30/23 12:04:14.444
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:04:14.451
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:04:14.452
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [It] should update/patch PodDisruptionBudget status [Conformance]
      test/e2e/apps/disruption.go:164
    STEP: Waiting for the pdb to be processed 01/30/23 12:04:14.456
    STEP: Updating PodDisruptionBudget status 01/30/23 12:04:16.461
    STEP: Waiting for all pods to be running 01/30/23 12:04:16.465
    Jan 30 12:04:16.467: INFO: running pods: 0 < 1
    Jan 30 12:04:18.471: INFO: running pods: 0 < 1
    STEP: locating a running pod 01/30/23 12:04:20.471
    STEP: Waiting for the pdb to be processed 01/30/23 12:04:20.477
    STEP: Patching PodDisruptionBudget status 01/30/23 12:04:20.48
    STEP: Waiting for the pdb to be processed 01/30/23 12:04:20.485
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Jan 30 12:04:20.487: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-8357" for this suite. 01/30/23 12:04:20.489
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client Kubectl diff
  should check if kubectl diff finds a difference for Deployments [Conformance]
  test/e2e/kubectl/kubectl.go:931
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 12:04:20.491
Jan 30 12:04:20.491: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename kubectl 01/30/23 12:04:20.492
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:04:20.498
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:04:20.5
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if kubectl diff finds a difference for Deployments [Conformance]
  test/e2e/kubectl/kubectl.go:931
STEP: create deployment with httpd image 01/30/23 12:04:20.502
Jan 30 12:04:20.502: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=kubectl-7155 create -f -'
Jan 30 12:04:22.050: INFO: stderr: ""
Jan 30 12:04:22.050: INFO: stdout: "deployment.apps/httpd-deployment created\n"
STEP: verify diff finds difference between live and declared image 01/30/23 12:04:22.05
Jan 30 12:04:22.050: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=kubectl-7155 diff -f -'
Jan 30 12:04:22.233: INFO: rc: 1
Jan 30 12:04:22.233: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=kubectl-7155 delete -f -'
Jan 30 12:04:22.291: INFO: stderr: ""
Jan 30 12:04:22.291: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jan 30 12:04:22.291: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-7155" for this suite. 01/30/23 12:04:22.293
------------------------------
• [1.804 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl diff
  test/e2e/kubectl/kubectl.go:925
    should check if kubectl diff finds a difference for Deployments [Conformance]
    test/e2e/kubectl/kubectl.go:931

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 12:04:20.491
    Jan 30 12:04:20.491: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename kubectl 01/30/23 12:04:20.492
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:04:20.498
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:04:20.5
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if kubectl diff finds a difference for Deployments [Conformance]
      test/e2e/kubectl/kubectl.go:931
    STEP: create deployment with httpd image 01/30/23 12:04:20.502
    Jan 30 12:04:20.502: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=kubectl-7155 create -f -'
    Jan 30 12:04:22.050: INFO: stderr: ""
    Jan 30 12:04:22.050: INFO: stdout: "deployment.apps/httpd-deployment created\n"
    STEP: verify diff finds difference between live and declared image 01/30/23 12:04:22.05
    Jan 30 12:04:22.050: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=kubectl-7155 diff -f -'
    Jan 30 12:04:22.233: INFO: rc: 1
    Jan 30 12:04:22.233: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=kubectl-7155 delete -f -'
    Jan 30 12:04:22.291: INFO: stderr: ""
    Jan 30 12:04:22.291: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jan 30 12:04:22.291: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-7155" for this suite. 01/30/23 12:04:22.293
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:47
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 12:04:22.297
Jan 30 12:04:22.298: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename secrets 01/30/23 12:04:22.298
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:04:22.304
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:04:22.306
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:47
STEP: Creating secret with name secret-test-56ac3a0d-aba3-4bf2-a77a-cb30e7d48ed1 01/30/23 12:04:22.308
STEP: Creating a pod to test consume secrets 01/30/23 12:04:22.31
Jan 30 12:04:22.314: INFO: Waiting up to 5m0s for pod "pod-secrets-695c7ddf-3dfb-40a7-b1a6-b565cb472768" in namespace "secrets-5825" to be "Succeeded or Failed"
Jan 30 12:04:22.315: INFO: Pod "pod-secrets-695c7ddf-3dfb-40a7-b1a6-b565cb472768": Phase="Pending", Reason="", readiness=false. Elapsed: 1.494043ms
Jan 30 12:04:24.318: INFO: Pod "pod-secrets-695c7ddf-3dfb-40a7-b1a6-b565cb472768": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00390413s
Jan 30 12:04:26.319: INFO: Pod "pod-secrets-695c7ddf-3dfb-40a7-b1a6-b565cb472768": Phase="Pending", Reason="", readiness=false. Elapsed: 4.004730332s
Jan 30 12:04:28.318: INFO: Pod "pod-secrets-695c7ddf-3dfb-40a7-b1a6-b565cb472768": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.004067018s
STEP: Saw pod success 01/30/23 12:04:28.318
Jan 30 12:04:28.318: INFO: Pod "pod-secrets-695c7ddf-3dfb-40a7-b1a6-b565cb472768" satisfied condition "Succeeded or Failed"
Jan 30 12:04:28.320: INFO: Trying to get logs from node pubt2-nks-for-dev1.dg.163.org pod pod-secrets-695c7ddf-3dfb-40a7-b1a6-b565cb472768 container secret-volume-test: <nil>
STEP: delete the pod 01/30/23 12:04:28.326
Jan 30 12:04:28.330: INFO: Waiting for pod pod-secrets-695c7ddf-3dfb-40a7-b1a6-b565cb472768 to disappear
Jan 30 12:04:28.332: INFO: Pod pod-secrets-695c7ddf-3dfb-40a7-b1a6-b565cb472768 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Jan 30 12:04:28.332: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-5825" for this suite. 01/30/23 12:04:28.334
------------------------------
• [SLOW TEST] [6.038 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:47

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 12:04:22.297
    Jan 30 12:04:22.298: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename secrets 01/30/23 12:04:22.298
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:04:22.304
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:04:22.306
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:47
    STEP: Creating secret with name secret-test-56ac3a0d-aba3-4bf2-a77a-cb30e7d48ed1 01/30/23 12:04:22.308
    STEP: Creating a pod to test consume secrets 01/30/23 12:04:22.31
    Jan 30 12:04:22.314: INFO: Waiting up to 5m0s for pod "pod-secrets-695c7ddf-3dfb-40a7-b1a6-b565cb472768" in namespace "secrets-5825" to be "Succeeded or Failed"
    Jan 30 12:04:22.315: INFO: Pod "pod-secrets-695c7ddf-3dfb-40a7-b1a6-b565cb472768": Phase="Pending", Reason="", readiness=false. Elapsed: 1.494043ms
    Jan 30 12:04:24.318: INFO: Pod "pod-secrets-695c7ddf-3dfb-40a7-b1a6-b565cb472768": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00390413s
    Jan 30 12:04:26.319: INFO: Pod "pod-secrets-695c7ddf-3dfb-40a7-b1a6-b565cb472768": Phase="Pending", Reason="", readiness=false. Elapsed: 4.004730332s
    Jan 30 12:04:28.318: INFO: Pod "pod-secrets-695c7ddf-3dfb-40a7-b1a6-b565cb472768": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.004067018s
    STEP: Saw pod success 01/30/23 12:04:28.318
    Jan 30 12:04:28.318: INFO: Pod "pod-secrets-695c7ddf-3dfb-40a7-b1a6-b565cb472768" satisfied condition "Succeeded or Failed"
    Jan 30 12:04:28.320: INFO: Trying to get logs from node pubt2-nks-for-dev1.dg.163.org pod pod-secrets-695c7ddf-3dfb-40a7-b1a6-b565cb472768 container secret-volume-test: <nil>
    STEP: delete the pod 01/30/23 12:04:28.326
    Jan 30 12:04:28.330: INFO: Waiting for pod pod-secrets-695c7ddf-3dfb-40a7-b1a6-b565cb472768 to disappear
    Jan 30 12:04:28.332: INFO: Pod pod-secrets-695c7ddf-3dfb-40a7-b1a6-b565cb472768 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Jan 30 12:04:28.332: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-5825" for this suite. 01/30/23 12:04:28.334
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 12:04:28.336
Jan 30 12:04:28.336: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename crd-webhook 01/30/23 12:04:28.337
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:04:28.343
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:04:28.345
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:128
STEP: Setting up server cert 01/30/23 12:04:28.347
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 01/30/23 12:04:28.855
STEP: Deploying the custom resource conversion webhook pod 01/30/23 12:04:28.859
STEP: Wait for the deployment to be ready 01/30/23 12:04:28.864
Jan 30 12:04:28.867: INFO: new replicaset for deployment "sample-crd-conversion-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 01/30/23 12:04:30.873
STEP: Verifying the service has paired with the endpoint 01/30/23 12:04:30.877
Jan 30 12:04:31.877: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184
Jan 30 12:04:31.879: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Creating a v1 custom resource 01/30/23 12:04:34.468
STEP: Create a v2 custom resource 01/30/23 12:04:34.479
STEP: List CRs in v1 01/30/23 12:04:34.551
STEP: List CRs in v2 01/30/23 12:04:34.554
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 30 12:04:35.063: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:139
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-webhook-4347" for this suite. 01/30/23 12:04:35.077
------------------------------
• [SLOW TEST] [6.743 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 12:04:28.336
    Jan 30 12:04:28.336: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename crd-webhook 01/30/23 12:04:28.337
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:04:28.343
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:04:28.345
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:128
    STEP: Setting up server cert 01/30/23 12:04:28.347
    STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 01/30/23 12:04:28.855
    STEP: Deploying the custom resource conversion webhook pod 01/30/23 12:04:28.859
    STEP: Wait for the deployment to be ready 01/30/23 12:04:28.864
    Jan 30 12:04:28.867: INFO: new replicaset for deployment "sample-crd-conversion-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 01/30/23 12:04:30.873
    STEP: Verifying the service has paired with the endpoint 01/30/23 12:04:30.877
    Jan 30 12:04:31.877: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
    [It] should be able to convert a non homogeneous list of CRs [Conformance]
      test/e2e/apimachinery/crd_conversion_webhook.go:184
    Jan 30 12:04:31.879: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Creating a v1 custom resource 01/30/23 12:04:34.468
    STEP: Create a v2 custom resource 01/30/23 12:04:34.479
    STEP: List CRs in v1 01/30/23 12:04:34.551
    STEP: List CRs in v2 01/30/23 12:04:34.554
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 30 12:04:35.063: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:139
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-webhook-4347" for this suite. 01/30/23 12:04:35.077
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates
  should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122
[BeforeEach] [sig-node] PodTemplates
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 12:04:35.082
Jan 30 12:04:35.082: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename podtemplate 01/30/23 12:04:35.082
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:04:35.088
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:04:35.09
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:31
[It] should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122
STEP: Create set of pod templates 01/30/23 12:04:35.092
Jan 30 12:04:35.094: INFO: created test-podtemplate-1
Jan 30 12:04:35.096: INFO: created test-podtemplate-2
Jan 30 12:04:35.099: INFO: created test-podtemplate-3
STEP: get a list of pod templates with a label in the current namespace 01/30/23 12:04:35.099
STEP: delete collection of pod templates 01/30/23 12:04:35.1
Jan 30 12:04:35.100: INFO: requesting DeleteCollection of pod templates
STEP: check that the list of pod templates matches the requested quantity 01/30/23 12:04:35.109
Jan 30 12:04:35.109: INFO: requesting list of pod templates to confirm quantity
[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/node/init/init.go:32
Jan 30 12:04:35.110: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] PodTemplates
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] PodTemplates
  tear down framework | framework.go:193
STEP: Destroying namespace "podtemplate-181" for this suite. 01/30/23 12:04:35.112
------------------------------
• [0.033 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 12:04:35.082
    Jan 30 12:04:35.082: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename podtemplate 01/30/23 12:04:35.082
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:04:35.088
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:04:35.09
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete a collection of pod templates [Conformance]
      test/e2e/common/node/podtemplates.go:122
    STEP: Create set of pod templates 01/30/23 12:04:35.092
    Jan 30 12:04:35.094: INFO: created test-podtemplate-1
    Jan 30 12:04:35.096: INFO: created test-podtemplate-2
    Jan 30 12:04:35.099: INFO: created test-podtemplate-3
    STEP: get a list of pod templates with a label in the current namespace 01/30/23 12:04:35.099
    STEP: delete collection of pod templates 01/30/23 12:04:35.1
    Jan 30 12:04:35.100: INFO: requesting DeleteCollection of pod templates
    STEP: check that the list of pod templates matches the requested quantity 01/30/23 12:04:35.109
    Jan 30 12:04:35.109: INFO: requesting list of pod templates to confirm quantity
    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/node/init/init.go:32
    Jan 30 12:04:35.110: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] PodTemplates
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] PodTemplates
      tear down framework | framework.go:193
    STEP: Destroying namespace "podtemplate-181" for this suite. 01/30/23 12:04:35.112
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:347
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 12:04:35.117
Jan 30 12:04:35.117: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename disruption 01/30/23 12:04:35.117
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:04:35.123
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:04:35.125
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[It] should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:347
STEP: Creating a pdb that targets all three pods in a test replica set 01/30/23 12:04:35.127
STEP: Waiting for the pdb to be processed 01/30/23 12:04:35.129
STEP: First trying to evict a pod which shouldn't be evictable 01/30/23 12:04:37.134
STEP: Waiting for all pods to be running 01/30/23 12:04:37.135
Jan 30 12:04:37.136: INFO: pods: 0 < 3
Jan 30 12:04:39.139: INFO: running pods: 2 < 3
STEP: locating a running pod 01/30/23 12:04:41.141
STEP: Updating the pdb to allow a pod to be evicted 01/30/23 12:04:41.147
STEP: Waiting for the pdb to be processed 01/30/23 12:04:41.151
STEP: Trying to evict the same pod we tried earlier which should now be evictable 01/30/23 12:04:43.154
STEP: Waiting for all pods to be running 01/30/23 12:04:43.154
STEP: Waiting for the pdb to observed all healthy pods 01/30/23 12:04:43.156
STEP: Patching the pdb to disallow a pod to be evicted 01/30/23 12:04:43.167
STEP: Waiting for the pdb to be processed 01/30/23 12:04:43.177
STEP: Waiting for all pods to be running 01/30/23 12:04:45.181
STEP: locating a running pod 01/30/23 12:04:45.183
STEP: Deleting the pdb to allow a pod to be evicted 01/30/23 12:04:45.188
STEP: Waiting for the pdb to be deleted 01/30/23 12:04:45.19
STEP: Trying to evict the same pod we tried earlier which should now be evictable 01/30/23 12:04:45.192
STEP: Waiting for all pods to be running 01/30/23 12:04:45.192
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Jan 30 12:04:45.200: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-9418" for this suite. 01/30/23 12:04:45.202
------------------------------
• [SLOW TEST] [10.088 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:347

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 12:04:35.117
    Jan 30 12:04:35.117: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename disruption 01/30/23 12:04:35.117
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:04:35.123
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:04:35.125
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [It] should block an eviction until the PDB is updated to allow it [Conformance]
      test/e2e/apps/disruption.go:347
    STEP: Creating a pdb that targets all three pods in a test replica set 01/30/23 12:04:35.127
    STEP: Waiting for the pdb to be processed 01/30/23 12:04:35.129
    STEP: First trying to evict a pod which shouldn't be evictable 01/30/23 12:04:37.134
    STEP: Waiting for all pods to be running 01/30/23 12:04:37.135
    Jan 30 12:04:37.136: INFO: pods: 0 < 3
    Jan 30 12:04:39.139: INFO: running pods: 2 < 3
    STEP: locating a running pod 01/30/23 12:04:41.141
    STEP: Updating the pdb to allow a pod to be evicted 01/30/23 12:04:41.147
    STEP: Waiting for the pdb to be processed 01/30/23 12:04:41.151
    STEP: Trying to evict the same pod we tried earlier which should now be evictable 01/30/23 12:04:43.154
    STEP: Waiting for all pods to be running 01/30/23 12:04:43.154
    STEP: Waiting for the pdb to observed all healthy pods 01/30/23 12:04:43.156
    STEP: Patching the pdb to disallow a pod to be evicted 01/30/23 12:04:43.167
    STEP: Waiting for the pdb to be processed 01/30/23 12:04:43.177
    STEP: Waiting for all pods to be running 01/30/23 12:04:45.181
    STEP: locating a running pod 01/30/23 12:04:45.183
    STEP: Deleting the pdb to allow a pod to be evicted 01/30/23 12:04:45.188
    STEP: Waiting for the pdb to be deleted 01/30/23 12:04:45.19
    STEP: Trying to evict the same pod we tried earlier which should now be evictable 01/30/23 12:04:45.192
    STEP: Waiting for all pods to be running 01/30/23 12:04:45.192
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Jan 30 12:04:45.200: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-9418" for this suite. 01/30/23 12:04:45.202
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:68
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 12:04:45.205
Jan 30 12:04:45.205: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename secrets 01/30/23 12:04:45.206
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:04:45.212
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:04:45.214
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:68
STEP: Creating secret with name secret-test-08d11a36-9281-4e21-bc73-131ea08eaa58 01/30/23 12:04:45.216
STEP: Creating a pod to test consume secrets 01/30/23 12:04:45.217
Jan 30 12:04:45.221: INFO: Waiting up to 5m0s for pod "pod-secrets-9b6bb768-9e48-40ff-af4c-4baa213f594b" in namespace "secrets-2536" to be "Succeeded or Failed"
Jan 30 12:04:45.223: INFO: Pod "pod-secrets-9b6bb768-9e48-40ff-af4c-4baa213f594b": Phase="Pending", Reason="", readiness=false. Elapsed: 1.484732ms
Jan 30 12:04:47.225: INFO: Pod "pod-secrets-9b6bb768-9e48-40ff-af4c-4baa213f594b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004308819s
Jan 30 12:04:49.225: INFO: Pod "pod-secrets-9b6bb768-9e48-40ff-af4c-4baa213f594b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004143869s
STEP: Saw pod success 01/30/23 12:04:49.225
Jan 30 12:04:49.225: INFO: Pod "pod-secrets-9b6bb768-9e48-40ff-af4c-4baa213f594b" satisfied condition "Succeeded or Failed"
Jan 30 12:04:49.227: INFO: Trying to get logs from node pubt2-nks-for-dev1.dg.163.org pod pod-secrets-9b6bb768-9e48-40ff-af4c-4baa213f594b container secret-volume-test: <nil>
STEP: delete the pod 01/30/23 12:04:49.232
Jan 30 12:04:49.236: INFO: Waiting for pod pod-secrets-9b6bb768-9e48-40ff-af4c-4baa213f594b to disappear
Jan 30 12:04:49.238: INFO: Pod pod-secrets-9b6bb768-9e48-40ff-af4c-4baa213f594b no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Jan 30 12:04:49.238: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-2536" for this suite. 01/30/23 12:04:49.24
------------------------------
• [4.037 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 12:04:45.205
    Jan 30 12:04:45.205: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename secrets 01/30/23 12:04:45.206
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:04:45.212
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:04:45.214
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:68
    STEP: Creating secret with name secret-test-08d11a36-9281-4e21-bc73-131ea08eaa58 01/30/23 12:04:45.216
    STEP: Creating a pod to test consume secrets 01/30/23 12:04:45.217
    Jan 30 12:04:45.221: INFO: Waiting up to 5m0s for pod "pod-secrets-9b6bb768-9e48-40ff-af4c-4baa213f594b" in namespace "secrets-2536" to be "Succeeded or Failed"
    Jan 30 12:04:45.223: INFO: Pod "pod-secrets-9b6bb768-9e48-40ff-af4c-4baa213f594b": Phase="Pending", Reason="", readiness=false. Elapsed: 1.484732ms
    Jan 30 12:04:47.225: INFO: Pod "pod-secrets-9b6bb768-9e48-40ff-af4c-4baa213f594b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004308819s
    Jan 30 12:04:49.225: INFO: Pod "pod-secrets-9b6bb768-9e48-40ff-af4c-4baa213f594b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004143869s
    STEP: Saw pod success 01/30/23 12:04:49.225
    Jan 30 12:04:49.225: INFO: Pod "pod-secrets-9b6bb768-9e48-40ff-af4c-4baa213f594b" satisfied condition "Succeeded or Failed"
    Jan 30 12:04:49.227: INFO: Trying to get logs from node pubt2-nks-for-dev1.dg.163.org pod pod-secrets-9b6bb768-9e48-40ff-af4c-4baa213f594b container secret-volume-test: <nil>
    STEP: delete the pod 01/30/23 12:04:49.232
    Jan 30 12:04:49.236: INFO: Waiting for pod pod-secrets-9b6bb768-9e48-40ff-af4c-4baa213f594b to disappear
    Jan 30 12:04:49.238: INFO: Pod pod-secrets-9b6bb768-9e48-40ff-af4c-4baa213f594b no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Jan 30 12:04:49.238: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-2536" for this suite. 01/30/23 12:04:49.24
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:92
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 12:04:49.243
Jan 30 12:04:49.243: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename var-expansion 01/30/23 12:04:49.244
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:04:49.25
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:04:49.251
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:92
STEP: Creating a pod to test substitution in container's args 01/30/23 12:04:49.253
Jan 30 12:04:49.257: INFO: Waiting up to 5m0s for pod "var-expansion-e8c7fd87-8835-4294-a568-86cdc57aac10" in namespace "var-expansion-4127" to be "Succeeded or Failed"
Jan 30 12:04:49.259: INFO: Pod "var-expansion-e8c7fd87-8835-4294-a568-86cdc57aac10": Phase="Pending", Reason="", readiness=false. Elapsed: 1.520076ms
Jan 30 12:04:51.262: INFO: Pod "var-expansion-e8c7fd87-8835-4294-a568-86cdc57aac10": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004924216s
Jan 30 12:04:53.261: INFO: Pod "var-expansion-e8c7fd87-8835-4294-a568-86cdc57aac10": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003804974s
STEP: Saw pod success 01/30/23 12:04:53.261
Jan 30 12:04:53.261: INFO: Pod "var-expansion-e8c7fd87-8835-4294-a568-86cdc57aac10" satisfied condition "Succeeded or Failed"
Jan 30 12:04:53.263: INFO: Trying to get logs from node pubt2-nks-for-dev1.dg.163.org pod var-expansion-e8c7fd87-8835-4294-a568-86cdc57aac10 container dapi-container: <nil>
STEP: delete the pod 01/30/23 12:04:53.267
Jan 30 12:04:53.271: INFO: Waiting for pod var-expansion-e8c7fd87-8835-4294-a568-86cdc57aac10 to disappear
Jan 30 12:04:53.273: INFO: Pod var-expansion-e8c7fd87-8835-4294-a568-86cdc57aac10 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Jan 30 12:04:53.273: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-4127" for this suite. 01/30/23 12:04:53.275
------------------------------
• [4.034 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 12:04:49.243
    Jan 30 12:04:49.243: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename var-expansion 01/30/23 12:04:49.244
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:04:49.25
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:04:49.251
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow substituting values in a container's args [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:92
    STEP: Creating a pod to test substitution in container's args 01/30/23 12:04:49.253
    Jan 30 12:04:49.257: INFO: Waiting up to 5m0s for pod "var-expansion-e8c7fd87-8835-4294-a568-86cdc57aac10" in namespace "var-expansion-4127" to be "Succeeded or Failed"
    Jan 30 12:04:49.259: INFO: Pod "var-expansion-e8c7fd87-8835-4294-a568-86cdc57aac10": Phase="Pending", Reason="", readiness=false. Elapsed: 1.520076ms
    Jan 30 12:04:51.262: INFO: Pod "var-expansion-e8c7fd87-8835-4294-a568-86cdc57aac10": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004924216s
    Jan 30 12:04:53.261: INFO: Pod "var-expansion-e8c7fd87-8835-4294-a568-86cdc57aac10": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003804974s
    STEP: Saw pod success 01/30/23 12:04:53.261
    Jan 30 12:04:53.261: INFO: Pod "var-expansion-e8c7fd87-8835-4294-a568-86cdc57aac10" satisfied condition "Succeeded or Failed"
    Jan 30 12:04:53.263: INFO: Trying to get logs from node pubt2-nks-for-dev1.dg.163.org pod var-expansion-e8c7fd87-8835-4294-a568-86cdc57aac10 container dapi-container: <nil>
    STEP: delete the pod 01/30/23 12:04:53.267
    Jan 30 12:04:53.271: INFO: Waiting for pod var-expansion-e8c7fd87-8835-4294-a568-86cdc57aac10 to disappear
    Jan 30 12:04:53.273: INFO: Pod var-expansion-e8c7fd87-8835-4294-a568-86cdc57aac10 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Jan 30 12:04:53.273: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-4127" for this suite. 01/30/23 12:04:53.275
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:78
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 12:04:53.278
Jan 30 12:04:53.278: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename svcaccounts 01/30/23 12:04:53.279
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:04:53.284
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:04:53.286
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:78
Jan 30 12:04:53.294: INFO: Waiting up to 5m0s for pod "pod-service-account-3ab50130-efc5-4cc5-bf4a-051c476c726a" in namespace "svcaccounts-3810" to be "running"
Jan 30 12:04:53.295: INFO: Pod "pod-service-account-3ab50130-efc5-4cc5-bf4a-051c476c726a": Phase="Pending", Reason="", readiness=false. Elapsed: 1.518714ms
Jan 30 12:04:55.298: INFO: Pod "pod-service-account-3ab50130-efc5-4cc5-bf4a-051c476c726a": Phase="Running", Reason="", readiness=true. Elapsed: 2.004097995s
Jan 30 12:04:55.298: INFO: Pod "pod-service-account-3ab50130-efc5-4cc5-bf4a-051c476c726a" satisfied condition "running"
STEP: reading a file in the container 01/30/23 12:04:55.298
Jan 30 12:04:55.298: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-3810 pod-service-account-3ab50130-efc5-4cc5-bf4a-051c476c726a -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container 01/30/23 12:04:55.437
Jan 30 12:04:55.437: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-3810 pod-service-account-3ab50130-efc5-4cc5-bf4a-051c476c726a -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container 01/30/23 12:04:55.569
Jan 30 12:04:55.569: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-3810 pod-service-account-3ab50130-efc5-4cc5-bf4a-051c476c726a -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
Jan 30 12:04:55.713: INFO: Got root ca configmap in namespace "svcaccounts-3810"
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Jan 30 12:04:55.714: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-3810" for this suite. 01/30/23 12:04:55.717
------------------------------
• [2.441 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:78

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 12:04:53.278
    Jan 30 12:04:53.278: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename svcaccounts 01/30/23 12:04:53.279
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:04:53.284
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:04:53.286
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should mount an API token into pods  [Conformance]
      test/e2e/auth/service_accounts.go:78
    Jan 30 12:04:53.294: INFO: Waiting up to 5m0s for pod "pod-service-account-3ab50130-efc5-4cc5-bf4a-051c476c726a" in namespace "svcaccounts-3810" to be "running"
    Jan 30 12:04:53.295: INFO: Pod "pod-service-account-3ab50130-efc5-4cc5-bf4a-051c476c726a": Phase="Pending", Reason="", readiness=false. Elapsed: 1.518714ms
    Jan 30 12:04:55.298: INFO: Pod "pod-service-account-3ab50130-efc5-4cc5-bf4a-051c476c726a": Phase="Running", Reason="", readiness=true. Elapsed: 2.004097995s
    Jan 30 12:04:55.298: INFO: Pod "pod-service-account-3ab50130-efc5-4cc5-bf4a-051c476c726a" satisfied condition "running"
    STEP: reading a file in the container 01/30/23 12:04:55.298
    Jan 30 12:04:55.298: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-3810 pod-service-account-3ab50130-efc5-4cc5-bf4a-051c476c726a -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
    STEP: reading a file in the container 01/30/23 12:04:55.437
    Jan 30 12:04:55.437: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-3810 pod-service-account-3ab50130-efc5-4cc5-bf4a-051c476c726a -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
    STEP: reading a file in the container 01/30/23 12:04:55.569
    Jan 30 12:04:55.569: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-3810 pod-service-account-3ab50130-efc5-4cc5-bf4a-051c476c726a -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
    Jan 30 12:04:55.713: INFO: Got root ca configmap in namespace "svcaccounts-3810"
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Jan 30 12:04:55.714: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-3810" for this suite. 01/30/23 12:04:55.717
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:215
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 12:04:55.719
Jan 30 12:04:55.719: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename container-probe 01/30/23 12:04:55.72
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:04:55.726
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:04:55.728
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:215
STEP: Creating pod test-webserver-e4c210af-f78b-46ac-994f-6d051cf574bc in namespace container-probe-6372 01/30/23 12:04:55.73
Jan 30 12:04:55.735: INFO: Waiting up to 5m0s for pod "test-webserver-e4c210af-f78b-46ac-994f-6d051cf574bc" in namespace "container-probe-6372" to be "not pending"
Jan 30 12:04:55.736: INFO: Pod "test-webserver-e4c210af-f78b-46ac-994f-6d051cf574bc": Phase="Pending", Reason="", readiness=false. Elapsed: 1.46179ms
Jan 30 12:04:57.739: INFO: Pod "test-webserver-e4c210af-f78b-46ac-994f-6d051cf574bc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004259026s
Jan 30 12:04:59.739: INFO: Pod "test-webserver-e4c210af-f78b-46ac-994f-6d051cf574bc": Phase="Running", Reason="", readiness=true. Elapsed: 4.003712083s
Jan 30 12:04:59.739: INFO: Pod "test-webserver-e4c210af-f78b-46ac-994f-6d051cf574bc" satisfied condition "not pending"
Jan 30 12:04:59.739: INFO: Started pod test-webserver-e4c210af-f78b-46ac-994f-6d051cf574bc in namespace container-probe-6372
STEP: checking the pod's current state and verifying that restartCount is present 01/30/23 12:04:59.739
Jan 30 12:04:59.740: INFO: Initial restart count of pod test-webserver-e4c210af-f78b-46ac-994f-6d051cf574bc is 0
STEP: deleting the pod 01/30/23 12:09:00.105
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Jan 30 12:09:00.111: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-6372" for this suite. 01/30/23 12:09:00.113
------------------------------
• [SLOW TEST] [244.397 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:215

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 12:04:55.719
    Jan 30 12:04:55.719: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename container-probe 01/30/23 12:04:55.72
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:04:55.726
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:04:55.728
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:215
    STEP: Creating pod test-webserver-e4c210af-f78b-46ac-994f-6d051cf574bc in namespace container-probe-6372 01/30/23 12:04:55.73
    Jan 30 12:04:55.735: INFO: Waiting up to 5m0s for pod "test-webserver-e4c210af-f78b-46ac-994f-6d051cf574bc" in namespace "container-probe-6372" to be "not pending"
    Jan 30 12:04:55.736: INFO: Pod "test-webserver-e4c210af-f78b-46ac-994f-6d051cf574bc": Phase="Pending", Reason="", readiness=false. Elapsed: 1.46179ms
    Jan 30 12:04:57.739: INFO: Pod "test-webserver-e4c210af-f78b-46ac-994f-6d051cf574bc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004259026s
    Jan 30 12:04:59.739: INFO: Pod "test-webserver-e4c210af-f78b-46ac-994f-6d051cf574bc": Phase="Running", Reason="", readiness=true. Elapsed: 4.003712083s
    Jan 30 12:04:59.739: INFO: Pod "test-webserver-e4c210af-f78b-46ac-994f-6d051cf574bc" satisfied condition "not pending"
    Jan 30 12:04:59.739: INFO: Started pod test-webserver-e4c210af-f78b-46ac-994f-6d051cf574bc in namespace container-probe-6372
    STEP: checking the pod's current state and verifying that restartCount is present 01/30/23 12:04:59.739
    Jan 30 12:04:59.740: INFO: Initial restart count of pod test-webserver-e4c210af-f78b-46ac-994f-6d051cf574bc is 0
    STEP: deleting the pod 01/30/23 12:09:00.105
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Jan 30 12:09:00.111: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-6372" for this suite. 01/30/23 12:09:00.113
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:656
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 12:09:00.116
Jan 30 12:09:00.116: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename webhook 01/30/23 12:09:00.117
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:09:00.131
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:09:00.133
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 01/30/23 12:09:00.141
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/30/23 12:09:00.393
STEP: Deploying the webhook pod 01/30/23 12:09:00.397
STEP: Wait for the deployment to be ready 01/30/23 12:09:00.403
Jan 30 12:09:00.407: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 01/30/23 12:09:02.412
STEP: Verifying the service has paired with the endpoint 01/30/23 12:09:02.416
Jan 30 12:09:03.416: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:656
STEP: Listing all of the created validation webhooks 01/30/23 12:09:03.441
STEP: Creating a configMap that should be mutated 01/30/23 12:09:03.45
STEP: Deleting the collection of validation webhooks 01/30/23 12:09:03.468
STEP: Creating a configMap that should not be mutated 01/30/23 12:09:03.483
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 30 12:09:03.487: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-3936" for this suite. 01/30/23 12:09:03.5
STEP: Destroying namespace "webhook-3936-markers" for this suite. 01/30/23 12:09:03.502
------------------------------
• [3.388 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:656

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 12:09:00.116
    Jan 30 12:09:00.116: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename webhook 01/30/23 12:09:00.117
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:09:00.131
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:09:00.133
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 01/30/23 12:09:00.141
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/30/23 12:09:00.393
    STEP: Deploying the webhook pod 01/30/23 12:09:00.397
    STEP: Wait for the deployment to be ready 01/30/23 12:09:00.403
    Jan 30 12:09:00.407: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 01/30/23 12:09:02.412
    STEP: Verifying the service has paired with the endpoint 01/30/23 12:09:02.416
    Jan 30 12:09:03.416: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] listing mutating webhooks should work [Conformance]
      test/e2e/apimachinery/webhook.go:656
    STEP: Listing all of the created validation webhooks 01/30/23 12:09:03.441
    STEP: Creating a configMap that should be mutated 01/30/23 12:09:03.45
    STEP: Deleting the collection of validation webhooks 01/30/23 12:09:03.468
    STEP: Creating a configMap that should not be mutated 01/30/23 12:09:03.483
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 30 12:09:03.487: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-3936" for this suite. 01/30/23 12:09:03.5
    STEP: Destroying namespace "webhook-3936-markers" for this suite. 01/30/23 12:09:03.502
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:232
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 12:09:03.505
Jan 30 12:09:03.505: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename container-runtime 01/30/23 12:09:03.506
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:09:03.512
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:09:03.514
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:232
STEP: create the container 01/30/23 12:09:03.516
STEP: wait for the container to reach Succeeded 01/30/23 12:09:03.52
STEP: get the container status 01/30/23 12:09:07.53
STEP: the container should be terminated 01/30/23 12:09:07.532
STEP: the termination message should be set 01/30/23 12:09:07.532
Jan 30 12:09:07.532: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container 01/30/23 12:09:07.532
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Jan 30 12:09:07.537: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-5266" for this suite. 01/30/23 12:09:07.539
------------------------------
• [4.037 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    on terminated container
    test/e2e/common/node/runtime.go:137
      should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:232

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 12:09:03.505
    Jan 30 12:09:03.505: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename container-runtime 01/30/23 12:09:03.506
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:09:03.512
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:09:03.514
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:232
    STEP: create the container 01/30/23 12:09:03.516
    STEP: wait for the container to reach Succeeded 01/30/23 12:09:03.52
    STEP: get the container status 01/30/23 12:09:07.53
    STEP: the container should be terminated 01/30/23 12:09:07.532
    STEP: the termination message should be set 01/30/23 12:09:07.532
    Jan 30 12:09:07.532: INFO: Expected: &{} to match Container's Termination Message:  --
    STEP: delete the container 01/30/23 12:09:07.532
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Jan 30 12:09:07.537: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-5266" for this suite. 01/30/23 12:09:07.539
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 12:09:07.542
Jan 30 12:09:07.542: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename replicaset 01/30/23 12:09:07.543
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:09:07.549
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:09:07.551
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111
Jan 30 12:09:07.553: INFO: Creating ReplicaSet my-hostname-basic-57885328-0e68-45ea-a0d5-da1a6652c12c
Jan 30 12:09:07.557: INFO: Pod name my-hostname-basic-57885328-0e68-45ea-a0d5-da1a6652c12c: Found 0 pods out of 1
Jan 30 12:09:12.560: INFO: Pod name my-hostname-basic-57885328-0e68-45ea-a0d5-da1a6652c12c: Found 1 pods out of 1
Jan 30 12:09:12.560: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-57885328-0e68-45ea-a0d5-da1a6652c12c" is running
Jan 30 12:09:12.560: INFO: Waiting up to 5m0s for pod "my-hostname-basic-57885328-0e68-45ea-a0d5-da1a6652c12c-dw55q" in namespace "replicaset-7566" to be "running"
Jan 30 12:09:12.561: INFO: Pod "my-hostname-basic-57885328-0e68-45ea-a0d5-da1a6652c12c-dw55q": Phase="Running", Reason="", readiness=true. Elapsed: 1.639088ms
Jan 30 12:09:12.561: INFO: Pod "my-hostname-basic-57885328-0e68-45ea-a0d5-da1a6652c12c-dw55q" satisfied condition "running"
Jan 30 12:09:12.561: INFO: Pod "my-hostname-basic-57885328-0e68-45ea-a0d5-da1a6652c12c-dw55q" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-30 12:09:07 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-30 12:09:09 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-30 12:09:09 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-30 12:09:07 +0000 UTC Reason: Message:}])
Jan 30 12:09:12.561: INFO: Trying to dial the pod
Jan 30 12:09:17.568: INFO: Controller my-hostname-basic-57885328-0e68-45ea-a0d5-da1a6652c12c: Got expected result from replica 1 [my-hostname-basic-57885328-0e68-45ea-a0d5-da1a6652c12c-dw55q]: "my-hostname-basic-57885328-0e68-45ea-a0d5-da1a6652c12c-dw55q", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Jan 30 12:09:17.568: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-7566" for this suite. 01/30/23 12:09:17.57
------------------------------
• [SLOW TEST] [10.031 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 12:09:07.542
    Jan 30 12:09:07.542: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename replicaset 01/30/23 12:09:07.543
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:09:07.549
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:09:07.551
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] should serve a basic image on each replica with a public image  [Conformance]
      test/e2e/apps/replica_set.go:111
    Jan 30 12:09:07.553: INFO: Creating ReplicaSet my-hostname-basic-57885328-0e68-45ea-a0d5-da1a6652c12c
    Jan 30 12:09:07.557: INFO: Pod name my-hostname-basic-57885328-0e68-45ea-a0d5-da1a6652c12c: Found 0 pods out of 1
    Jan 30 12:09:12.560: INFO: Pod name my-hostname-basic-57885328-0e68-45ea-a0d5-da1a6652c12c: Found 1 pods out of 1
    Jan 30 12:09:12.560: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-57885328-0e68-45ea-a0d5-da1a6652c12c" is running
    Jan 30 12:09:12.560: INFO: Waiting up to 5m0s for pod "my-hostname-basic-57885328-0e68-45ea-a0d5-da1a6652c12c-dw55q" in namespace "replicaset-7566" to be "running"
    Jan 30 12:09:12.561: INFO: Pod "my-hostname-basic-57885328-0e68-45ea-a0d5-da1a6652c12c-dw55q": Phase="Running", Reason="", readiness=true. Elapsed: 1.639088ms
    Jan 30 12:09:12.561: INFO: Pod "my-hostname-basic-57885328-0e68-45ea-a0d5-da1a6652c12c-dw55q" satisfied condition "running"
    Jan 30 12:09:12.561: INFO: Pod "my-hostname-basic-57885328-0e68-45ea-a0d5-da1a6652c12c-dw55q" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-30 12:09:07 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-30 12:09:09 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-30 12:09:09 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-30 12:09:07 +0000 UTC Reason: Message:}])
    Jan 30 12:09:12.561: INFO: Trying to dial the pod
    Jan 30 12:09:17.568: INFO: Controller my-hostname-basic-57885328-0e68-45ea-a0d5-da1a6652c12c: Got expected result from replica 1 [my-hostname-basic-57885328-0e68-45ea-a0d5-da1a6652c12c-dw55q]: "my-hostname-basic-57885328-0e68-45ea-a0d5-da1a6652c12c-dw55q", 1 of 1 required successes so far
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Jan 30 12:09:17.568: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-7566" for this suite. 01/30/23 12:09:17.57
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:209
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 12:09:17.573
Jan 30 12:09:17.573: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename webhook 01/30/23 12:09:17.574
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:09:17.58
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:09:17.582
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 01/30/23 12:09:17.59
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/30/23 12:09:18.002
STEP: Deploying the webhook pod 01/30/23 12:09:18.005
STEP: Wait for the deployment to be ready 01/30/23 12:09:18.01
Jan 30 12:09:18.013: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 01/30/23 12:09:20.019
STEP: Verifying the service has paired with the endpoint 01/30/23 12:09:20.023
Jan 30 12:09:21.023: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:209
STEP: Registering the webhook via the AdmissionRegistration API 01/30/23 12:09:21.025
STEP: create a pod 01/30/23 12:09:21.034
Jan 30 12:09:21.038: INFO: Waiting up to 5m0s for pod "to-be-attached-pod" in namespace "webhook-1984" to be "running"
Jan 30 12:09:21.041: INFO: Pod "to-be-attached-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.498941ms
Jan 30 12:09:23.043: INFO: Pod "to-be-attached-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.004836835s
Jan 30 12:09:23.043: INFO: Pod "to-be-attached-pod" satisfied condition "running"
STEP: 'kubectl attach' the pod, should be denied by the webhook 01/30/23 12:09:23.043
Jan 30 12:09:23.043: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=webhook-1984 attach --namespace=webhook-1984 to-be-attached-pod -i -c=container1'
Jan 30 12:09:23.112: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 30 12:09:23.115: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-1984" for this suite. 01/30/23 12:09:23.129
STEP: Destroying namespace "webhook-1984-markers" for this suite. 01/30/23 12:09:23.131
------------------------------
• [SLOW TEST] [5.560 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:209

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 12:09:17.573
    Jan 30 12:09:17.573: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename webhook 01/30/23 12:09:17.574
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:09:17.58
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:09:17.582
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 01/30/23 12:09:17.59
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/30/23 12:09:18.002
    STEP: Deploying the webhook pod 01/30/23 12:09:18.005
    STEP: Wait for the deployment to be ready 01/30/23 12:09:18.01
    Jan 30 12:09:18.013: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 01/30/23 12:09:20.019
    STEP: Verifying the service has paired with the endpoint 01/30/23 12:09:20.023
    Jan 30 12:09:21.023: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny attaching pod [Conformance]
      test/e2e/apimachinery/webhook.go:209
    STEP: Registering the webhook via the AdmissionRegistration API 01/30/23 12:09:21.025
    STEP: create a pod 01/30/23 12:09:21.034
    Jan 30 12:09:21.038: INFO: Waiting up to 5m0s for pod "to-be-attached-pod" in namespace "webhook-1984" to be "running"
    Jan 30 12:09:21.041: INFO: Pod "to-be-attached-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.498941ms
    Jan 30 12:09:23.043: INFO: Pod "to-be-attached-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.004836835s
    Jan 30 12:09:23.043: INFO: Pod "to-be-attached-pod" satisfied condition "running"
    STEP: 'kubectl attach' the pod, should be denied by the webhook 01/30/23 12:09:23.043
    Jan 30 12:09:23.043: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=webhook-1984 attach --namespace=webhook-1984 to-be-attached-pod -i -c=container1'
    Jan 30 12:09:23.112: INFO: rc: 1
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 30 12:09:23.115: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-1984" for this suite. 01/30/23 12:09:23.129
    STEP: Destroying namespace "webhook-1984-markers" for this suite. 01/30/23 12:09:23.131
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] EmptyDir volumes
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:87
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 12:09:23.134
Jan 30 12:09:23.134: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename emptydir 01/30/23 12:09:23.134
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:09:23.14
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:09:23.142
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:87
STEP: Creating a pod to test emptydir volume type on tmpfs 01/30/23 12:09:23.144
Jan 30 12:09:23.148: INFO: Waiting up to 5m0s for pod "pod-1d133758-39aa-4d1b-92a6-e3c02ad967f8" in namespace "emptydir-2789" to be "Succeeded or Failed"
Jan 30 12:09:23.150: INFO: Pod "pod-1d133758-39aa-4d1b-92a6-e3c02ad967f8": Phase="Pending", Reason="", readiness=false. Elapsed: 1.645245ms
Jan 30 12:09:25.153: INFO: Pod "pod-1d133758-39aa-4d1b-92a6-e3c02ad967f8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005041414s
Jan 30 12:09:27.153: INFO: Pod "pod-1d133758-39aa-4d1b-92a6-e3c02ad967f8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.004863622s
Jan 30 12:09:29.153: INFO: Pod "pod-1d133758-39aa-4d1b-92a6-e3c02ad967f8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.005001692s
STEP: Saw pod success 01/30/23 12:09:29.153
Jan 30 12:09:29.153: INFO: Pod "pod-1d133758-39aa-4d1b-92a6-e3c02ad967f8" satisfied condition "Succeeded or Failed"
Jan 30 12:09:29.155: INFO: Trying to get logs from node pubt2-nks-for-dev1.dg.163.org pod pod-1d133758-39aa-4d1b-92a6-e3c02ad967f8 container test-container: <nil>
STEP: delete the pod 01/30/23 12:09:29.168
Jan 30 12:09:29.173: INFO: Waiting for pod pod-1d133758-39aa-4d1b-92a6-e3c02ad967f8 to disappear
Jan 30 12:09:29.174: INFO: Pod pod-1d133758-39aa-4d1b-92a6-e3c02ad967f8 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jan 30 12:09:29.174: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-2789" for this suite. 01/30/23 12:09:29.176
------------------------------
• [SLOW TEST] [6.045 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:87

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 12:09:23.134
    Jan 30 12:09:23.134: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename emptydir 01/30/23 12:09:23.134
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:09:23.14
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:09:23.142
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:87
    STEP: Creating a pod to test emptydir volume type on tmpfs 01/30/23 12:09:23.144
    Jan 30 12:09:23.148: INFO: Waiting up to 5m0s for pod "pod-1d133758-39aa-4d1b-92a6-e3c02ad967f8" in namespace "emptydir-2789" to be "Succeeded or Failed"
    Jan 30 12:09:23.150: INFO: Pod "pod-1d133758-39aa-4d1b-92a6-e3c02ad967f8": Phase="Pending", Reason="", readiness=false. Elapsed: 1.645245ms
    Jan 30 12:09:25.153: INFO: Pod "pod-1d133758-39aa-4d1b-92a6-e3c02ad967f8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005041414s
    Jan 30 12:09:27.153: INFO: Pod "pod-1d133758-39aa-4d1b-92a6-e3c02ad967f8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.004863622s
    Jan 30 12:09:29.153: INFO: Pod "pod-1d133758-39aa-4d1b-92a6-e3c02ad967f8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.005001692s
    STEP: Saw pod success 01/30/23 12:09:29.153
    Jan 30 12:09:29.153: INFO: Pod "pod-1d133758-39aa-4d1b-92a6-e3c02ad967f8" satisfied condition "Succeeded or Failed"
    Jan 30 12:09:29.155: INFO: Trying to get logs from node pubt2-nks-for-dev1.dg.163.org pod pod-1d133758-39aa-4d1b-92a6-e3c02ad967f8 container test-container: <nil>
    STEP: delete the pod 01/30/23 12:09:29.168
    Jan 30 12:09:29.173: INFO: Waiting for pod pod-1d133758-39aa-4d1b-92a6-e3c02ad967f8 to disappear
    Jan 30 12:09:29.174: INFO: Pod pod-1d133758-39aa-4d1b-92a6-e3c02ad967f8 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jan 30 12:09:29.174: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-2789" for this suite. 01/30/23 12:09:29.176
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:67
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 12:09:29.179
Jan 30 12:09:29.179: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename replication-controller 01/30/23 12:09:29.18
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:09:29.186
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:09:29.188
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:67
STEP: Creating replication controller my-hostname-basic-8c90aead-7cad-4f22-b2b0-2078543020d7 01/30/23 12:09:29.19
Jan 30 12:09:29.194: INFO: Pod name my-hostname-basic-8c90aead-7cad-4f22-b2b0-2078543020d7: Found 0 pods out of 1
Jan 30 12:09:34.197: INFO: Pod name my-hostname-basic-8c90aead-7cad-4f22-b2b0-2078543020d7: Found 1 pods out of 1
Jan 30 12:09:34.197: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-8c90aead-7cad-4f22-b2b0-2078543020d7" are running
Jan 30 12:09:34.197: INFO: Waiting up to 5m0s for pod "my-hostname-basic-8c90aead-7cad-4f22-b2b0-2078543020d7-fnlgv" in namespace "replication-controller-4923" to be "running"
Jan 30 12:09:34.199: INFO: Pod "my-hostname-basic-8c90aead-7cad-4f22-b2b0-2078543020d7-fnlgv": Phase="Running", Reason="", readiness=true. Elapsed: 1.945176ms
Jan 30 12:09:34.199: INFO: Pod "my-hostname-basic-8c90aead-7cad-4f22-b2b0-2078543020d7-fnlgv" satisfied condition "running"
Jan 30 12:09:34.199: INFO: Pod "my-hostname-basic-8c90aead-7cad-4f22-b2b0-2078543020d7-fnlgv" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-30 12:09:29 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-30 12:09:30 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-30 12:09:30 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-30 12:09:29 +0000 UTC Reason: Message:}])
Jan 30 12:09:34.199: INFO: Trying to dial the pod
Jan 30 12:09:39.207: INFO: Controller my-hostname-basic-8c90aead-7cad-4f22-b2b0-2078543020d7: Got expected result from replica 1 [my-hostname-basic-8c90aead-7cad-4f22-b2b0-2078543020d7-fnlgv]: "my-hostname-basic-8c90aead-7cad-4f22-b2b0-2078543020d7-fnlgv", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Jan 30 12:09:39.207: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-4923" for this suite. 01/30/23 12:09:39.209
------------------------------
• [SLOW TEST] [10.032 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 12:09:29.179
    Jan 30 12:09:29.179: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename replication-controller 01/30/23 12:09:29.18
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:09:29.186
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:09:29.188
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should serve a basic image on each replica with a public image  [Conformance]
      test/e2e/apps/rc.go:67
    STEP: Creating replication controller my-hostname-basic-8c90aead-7cad-4f22-b2b0-2078543020d7 01/30/23 12:09:29.19
    Jan 30 12:09:29.194: INFO: Pod name my-hostname-basic-8c90aead-7cad-4f22-b2b0-2078543020d7: Found 0 pods out of 1
    Jan 30 12:09:34.197: INFO: Pod name my-hostname-basic-8c90aead-7cad-4f22-b2b0-2078543020d7: Found 1 pods out of 1
    Jan 30 12:09:34.197: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-8c90aead-7cad-4f22-b2b0-2078543020d7" are running
    Jan 30 12:09:34.197: INFO: Waiting up to 5m0s for pod "my-hostname-basic-8c90aead-7cad-4f22-b2b0-2078543020d7-fnlgv" in namespace "replication-controller-4923" to be "running"
    Jan 30 12:09:34.199: INFO: Pod "my-hostname-basic-8c90aead-7cad-4f22-b2b0-2078543020d7-fnlgv": Phase="Running", Reason="", readiness=true. Elapsed: 1.945176ms
    Jan 30 12:09:34.199: INFO: Pod "my-hostname-basic-8c90aead-7cad-4f22-b2b0-2078543020d7-fnlgv" satisfied condition "running"
    Jan 30 12:09:34.199: INFO: Pod "my-hostname-basic-8c90aead-7cad-4f22-b2b0-2078543020d7-fnlgv" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-30 12:09:29 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-30 12:09:30 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-30 12:09:30 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-30 12:09:29 +0000 UTC Reason: Message:}])
    Jan 30 12:09:34.199: INFO: Trying to dial the pod
    Jan 30 12:09:39.207: INFO: Controller my-hostname-basic-8c90aead-7cad-4f22-b2b0-2078543020d7: Got expected result from replica 1 [my-hostname-basic-8c90aead-7cad-4f22-b2b0-2078543020d7-fnlgv]: "my-hostname-basic-8c90aead-7cad-4f22-b2b0-2078543020d7-fnlgv", 1 of 1 required successes so far
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Jan 30 12:09:39.207: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-4923" for this suite. 01/30/23 12:09:39.209
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:341
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 12:09:39.212
Jan 30 12:09:39.212: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename webhook 01/30/23 12:09:39.213
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:09:39.219
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:09:39.221
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 01/30/23 12:09:39.229
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/30/23 12:09:39.611
STEP: Deploying the webhook pod 01/30/23 12:09:39.614
STEP: Wait for the deployment to be ready 01/30/23 12:09:39.62
Jan 30 12:09:39.623: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 01/30/23 12:09:41.628
STEP: Verifying the service has paired with the endpoint 01/30/23 12:09:41.632
Jan 30 12:09:42.632: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:341
Jan 30 12:09:42.634: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-5038-crds.webhook.example.com via the AdmissionRegistration API 01/30/23 12:09:43.141
STEP: Creating a custom resource that should be mutated by the webhook 01/30/23 12:09:43.152
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 30 12:09:45.737: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-9015" for this suite. 01/30/23 12:09:45.751
STEP: Destroying namespace "webhook-9015-markers" for this suite. 01/30/23 12:09:45.753
------------------------------
• [SLOW TEST] [6.544 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:341

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 12:09:39.212
    Jan 30 12:09:39.212: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename webhook 01/30/23 12:09:39.213
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:09:39.219
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:09:39.221
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 01/30/23 12:09:39.229
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/30/23 12:09:39.611
    STEP: Deploying the webhook pod 01/30/23 12:09:39.614
    STEP: Wait for the deployment to be ready 01/30/23 12:09:39.62
    Jan 30 12:09:39.623: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 01/30/23 12:09:41.628
    STEP: Verifying the service has paired with the endpoint 01/30/23 12:09:41.632
    Jan 30 12:09:42.632: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource with pruning [Conformance]
      test/e2e/apimachinery/webhook.go:341
    Jan 30 12:09:42.634: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-5038-crds.webhook.example.com via the AdmissionRegistration API 01/30/23 12:09:43.141
    STEP: Creating a custom resource that should be mutated by the webhook 01/30/23 12:09:43.152
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 30 12:09:45.737: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-9015" for this suite. 01/30/23 12:09:45.751
    STEP: Destroying namespace "webhook-9015-markers" for this suite. 01/30/23 12:09:45.753
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3428
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 12:09:45.757
Jan 30 12:09:45.757: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename services 01/30/23 12:09:45.757
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:09:45.763
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:09:45.765
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3428
STEP: creating a Service 01/30/23 12:09:45.769
STEP: watching for the Service to be added 01/30/23 12:09:45.772
Jan 30 12:09:45.773: INFO: Found Service test-service-764xh in namespace services-3299 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
Jan 30 12:09:45.773: INFO: Service test-service-764xh created
STEP: Getting /status 01/30/23 12:09:45.773
Jan 30 12:09:45.775: INFO: Service test-service-764xh has LoadBalancer: {[]}
STEP: patching the ServiceStatus 01/30/23 12:09:45.775
STEP: watching for the Service to be patched 01/30/23 12:09:45.779
Jan 30 12:09:45.780: INFO: observed Service test-service-764xh in namespace services-3299 with annotations: map[] & LoadBalancer: {[]}
Jan 30 12:09:45.780: INFO: Found Service test-service-764xh in namespace services-3299 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
Jan 30 12:09:45.780: INFO: Service test-service-764xh has service status patched
STEP: updating the ServiceStatus 01/30/23 12:09:45.78
Jan 30 12:09:45.784: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Service to be updated 01/30/23 12:09:45.784
Jan 30 12:09:45.785: INFO: Observed Service test-service-764xh in namespace services-3299 with annotations: map[] & Conditions: {[]}
Jan 30 12:09:45.785: INFO: Observed event: &Service{ObjectMeta:{test-service-764xh  services-3299  9aa3807b-4c00-4434-980e-a43e6641f942 30480 0 2023-01-30 12:09:45 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2023-01-30 12:09:45 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2023-01-30 12:09:45 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:10.178.72.230,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[10.178.72.230],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
Jan 30 12:09:45.785: INFO: Found Service test-service-764xh in namespace services-3299 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Jan 30 12:09:45.785: INFO: Service test-service-764xh has service status updated
STEP: patching the service 01/30/23 12:09:45.785
STEP: watching for the Service to be patched 01/30/23 12:09:45.797
Jan 30 12:09:45.798: INFO: observed Service test-service-764xh in namespace services-3299 with labels: map[test-service-static:true]
Jan 30 12:09:45.798: INFO: observed Service test-service-764xh in namespace services-3299 with labels: map[test-service-static:true]
Jan 30 12:09:45.798: INFO: observed Service test-service-764xh in namespace services-3299 with labels: map[test-service-static:true]
Jan 30 12:09:45.798: INFO: Found Service test-service-764xh in namespace services-3299 with labels: map[test-service:patched test-service-static:true]
Jan 30 12:09:45.798: INFO: Service test-service-764xh patched
STEP: deleting the service 01/30/23 12:09:45.798
STEP: watching for the Service to be deleted 01/30/23 12:09:45.802
Jan 30 12:09:45.803: INFO: Observed event: ADDED
Jan 30 12:09:45.803: INFO: Observed event: MODIFIED
Jan 30 12:09:45.803: INFO: Observed event: MODIFIED
Jan 30 12:09:45.803: INFO: Observed event: MODIFIED
Jan 30 12:09:45.803: INFO: Found Service test-service-764xh in namespace services-3299 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
Jan 30 12:09:45.803: INFO: Service test-service-764xh deleted
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jan 30 12:09:45.803: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-3299" for this suite. 01/30/23 12:09:45.805
------------------------------
• [0.051 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3428

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 12:09:45.757
    Jan 30 12:09:45.757: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename services 01/30/23 12:09:45.757
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:09:45.763
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:09:45.765
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should complete a service status lifecycle [Conformance]
      test/e2e/network/service.go:3428
    STEP: creating a Service 01/30/23 12:09:45.769
    STEP: watching for the Service to be added 01/30/23 12:09:45.772
    Jan 30 12:09:45.773: INFO: Found Service test-service-764xh in namespace services-3299 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
    Jan 30 12:09:45.773: INFO: Service test-service-764xh created
    STEP: Getting /status 01/30/23 12:09:45.773
    Jan 30 12:09:45.775: INFO: Service test-service-764xh has LoadBalancer: {[]}
    STEP: patching the ServiceStatus 01/30/23 12:09:45.775
    STEP: watching for the Service to be patched 01/30/23 12:09:45.779
    Jan 30 12:09:45.780: INFO: observed Service test-service-764xh in namespace services-3299 with annotations: map[] & LoadBalancer: {[]}
    Jan 30 12:09:45.780: INFO: Found Service test-service-764xh in namespace services-3299 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
    Jan 30 12:09:45.780: INFO: Service test-service-764xh has service status patched
    STEP: updating the ServiceStatus 01/30/23 12:09:45.78
    Jan 30 12:09:45.784: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the Service to be updated 01/30/23 12:09:45.784
    Jan 30 12:09:45.785: INFO: Observed Service test-service-764xh in namespace services-3299 with annotations: map[] & Conditions: {[]}
    Jan 30 12:09:45.785: INFO: Observed event: &Service{ObjectMeta:{test-service-764xh  services-3299  9aa3807b-4c00-4434-980e-a43e6641f942 30480 0 2023-01-30 12:09:45 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2023-01-30 12:09:45 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2023-01-30 12:09:45 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:10.178.72.230,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[10.178.72.230],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
    Jan 30 12:09:45.785: INFO: Found Service test-service-764xh in namespace services-3299 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Jan 30 12:09:45.785: INFO: Service test-service-764xh has service status updated
    STEP: patching the service 01/30/23 12:09:45.785
    STEP: watching for the Service to be patched 01/30/23 12:09:45.797
    Jan 30 12:09:45.798: INFO: observed Service test-service-764xh in namespace services-3299 with labels: map[test-service-static:true]
    Jan 30 12:09:45.798: INFO: observed Service test-service-764xh in namespace services-3299 with labels: map[test-service-static:true]
    Jan 30 12:09:45.798: INFO: observed Service test-service-764xh in namespace services-3299 with labels: map[test-service-static:true]
    Jan 30 12:09:45.798: INFO: Found Service test-service-764xh in namespace services-3299 with labels: map[test-service:patched test-service-static:true]
    Jan 30 12:09:45.798: INFO: Service test-service-764xh patched
    STEP: deleting the service 01/30/23 12:09:45.798
    STEP: watching for the Service to be deleted 01/30/23 12:09:45.802
    Jan 30 12:09:45.803: INFO: Observed event: ADDED
    Jan 30 12:09:45.803: INFO: Observed event: MODIFIED
    Jan 30 12:09:45.803: INFO: Observed event: MODIFIED
    Jan 30 12:09:45.803: INFO: Observed event: MODIFIED
    Jan 30 12:09:45.803: INFO: Found Service test-service-764xh in namespace services-3299 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
    Jan 30 12:09:45.803: INFO: Service test-service-764xh deleted
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jan 30 12:09:45.803: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-3299" for this suite. 01/30/23 12:09:45.805
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should update a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:810
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 12:09:45.808
Jan 30 12:09:45.808: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename svcaccounts 01/30/23 12:09:45.809
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:09:45.814
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:09:45.816
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should update a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:810
STEP: Creating ServiceAccount "e2e-sa-nj9qp"  01/30/23 12:09:45.818
Jan 30 12:09:45.819: INFO: AutomountServiceAccountToken: false
STEP: Updating ServiceAccount "e2e-sa-nj9qp"  01/30/23 12:09:45.819
Jan 30 12:09:45.823: INFO: AutomountServiceAccountToken: true
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Jan 30 12:09:45.823: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-638" for this suite. 01/30/23 12:09:45.825
------------------------------
• [0.019 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should update a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:810

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 12:09:45.808
    Jan 30 12:09:45.808: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename svcaccounts 01/30/23 12:09:45.809
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:09:45.814
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:09:45.816
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should update a ServiceAccount [Conformance]
      test/e2e/auth/service_accounts.go:810
    STEP: Creating ServiceAccount "e2e-sa-nj9qp"  01/30/23 12:09:45.818
    Jan 30 12:09:45.819: INFO: AutomountServiceAccountToken: false
    STEP: Updating ServiceAccount "e2e-sa-nj9qp"  01/30/23 12:09:45.819
    Jan 30 12:09:45.823: INFO: AutomountServiceAccountToken: true
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Jan 30 12:09:45.823: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-638" for this suite. 01/30/23 12:09:45.825
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:344
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 12:09:45.828
Jan 30 12:09:45.828: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename pods 01/30/23 12:09:45.829
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:09:45.834
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:09:45.836
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:344
STEP: creating the pod 01/30/23 12:09:45.838
STEP: submitting the pod to kubernetes 01/30/23 12:09:45.838
Jan 30 12:09:45.842: INFO: Waiting up to 5m0s for pod "pod-update-218af5d9-bdf8-470d-abc8-d09a2d3e3a48" in namespace "pods-815" to be "running and ready"
Jan 30 12:09:45.844: INFO: Pod "pod-update-218af5d9-bdf8-470d-abc8-d09a2d3e3a48": Phase="Pending", Reason="", readiness=false. Elapsed: 1.52003ms
Jan 30 12:09:45.844: INFO: The phase of Pod pod-update-218af5d9-bdf8-470d-abc8-d09a2d3e3a48 is Pending, waiting for it to be Running (with Ready = true)
Jan 30 12:09:47.847: INFO: Pod "pod-update-218af5d9-bdf8-470d-abc8-d09a2d3e3a48": Phase="Running", Reason="", readiness=true. Elapsed: 2.004367513s
Jan 30 12:09:47.847: INFO: The phase of Pod pod-update-218af5d9-bdf8-470d-abc8-d09a2d3e3a48 is Running (Ready = true)
Jan 30 12:09:47.847: INFO: Pod "pod-update-218af5d9-bdf8-470d-abc8-d09a2d3e3a48" satisfied condition "running and ready"
STEP: verifying the pod is in kubernetes 01/30/23 12:09:47.848
STEP: updating the pod 01/30/23 12:09:47.85
Jan 30 12:09:48.358: INFO: Successfully updated pod "pod-update-218af5d9-bdf8-470d-abc8-d09a2d3e3a48"
Jan 30 12:09:48.358: INFO: Waiting up to 5m0s for pod "pod-update-218af5d9-bdf8-470d-abc8-d09a2d3e3a48" in namespace "pods-815" to be "running"
Jan 30 12:09:48.360: INFO: Pod "pod-update-218af5d9-bdf8-470d-abc8-d09a2d3e3a48": Phase="Running", Reason="", readiness=true. Elapsed: 2.030592ms
Jan 30 12:09:48.360: INFO: Pod "pod-update-218af5d9-bdf8-470d-abc8-d09a2d3e3a48" satisfied condition "running"
STEP: verifying the updated pod is in kubernetes 01/30/23 12:09:48.36
Jan 30 12:09:48.362: INFO: Pod update OK
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Jan 30 12:09:48.362: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-815" for this suite. 01/30/23 12:09:48.364
------------------------------
• [2.538 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:344

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 12:09:45.828
    Jan 30 12:09:45.828: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename pods 01/30/23 12:09:45.829
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:09:45.834
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:09:45.836
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should be updated [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:344
    STEP: creating the pod 01/30/23 12:09:45.838
    STEP: submitting the pod to kubernetes 01/30/23 12:09:45.838
    Jan 30 12:09:45.842: INFO: Waiting up to 5m0s for pod "pod-update-218af5d9-bdf8-470d-abc8-d09a2d3e3a48" in namespace "pods-815" to be "running and ready"
    Jan 30 12:09:45.844: INFO: Pod "pod-update-218af5d9-bdf8-470d-abc8-d09a2d3e3a48": Phase="Pending", Reason="", readiness=false. Elapsed: 1.52003ms
    Jan 30 12:09:45.844: INFO: The phase of Pod pod-update-218af5d9-bdf8-470d-abc8-d09a2d3e3a48 is Pending, waiting for it to be Running (with Ready = true)
    Jan 30 12:09:47.847: INFO: Pod "pod-update-218af5d9-bdf8-470d-abc8-d09a2d3e3a48": Phase="Running", Reason="", readiness=true. Elapsed: 2.004367513s
    Jan 30 12:09:47.847: INFO: The phase of Pod pod-update-218af5d9-bdf8-470d-abc8-d09a2d3e3a48 is Running (Ready = true)
    Jan 30 12:09:47.847: INFO: Pod "pod-update-218af5d9-bdf8-470d-abc8-d09a2d3e3a48" satisfied condition "running and ready"
    STEP: verifying the pod is in kubernetes 01/30/23 12:09:47.848
    STEP: updating the pod 01/30/23 12:09:47.85
    Jan 30 12:09:48.358: INFO: Successfully updated pod "pod-update-218af5d9-bdf8-470d-abc8-d09a2d3e3a48"
    Jan 30 12:09:48.358: INFO: Waiting up to 5m0s for pod "pod-update-218af5d9-bdf8-470d-abc8-d09a2d3e3a48" in namespace "pods-815" to be "running"
    Jan 30 12:09:48.360: INFO: Pod "pod-update-218af5d9-bdf8-470d-abc8-d09a2d3e3a48": Phase="Running", Reason="", readiness=true. Elapsed: 2.030592ms
    Jan 30 12:09:48.360: INFO: Pod "pod-update-218af5d9-bdf8-470d-abc8-d09a2d3e3a48" satisfied condition "running"
    STEP: verifying the updated pod is in kubernetes 01/30/23 12:09:48.36
    Jan 30 12:09:48.362: INFO: Pod update OK
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Jan 30 12:09:48.362: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-815" for this suite. 01/30/23 12:09:48.364
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] Downward API
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:44
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 12:09:48.367
Jan 30 12:09:48.367: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename downward-api 01/30/23 12:09:48.368
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:09:48.373
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:09:48.375
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:44
STEP: Creating a pod to test downward api env vars 01/30/23 12:09:48.377
Jan 30 12:09:48.382: INFO: Waiting up to 5m0s for pod "downward-api-9cc6b8c1-dcac-4923-8788-66a259b945f0" in namespace "downward-api-9214" to be "Succeeded or Failed"
Jan 30 12:09:48.383: INFO: Pod "downward-api-9cc6b8c1-dcac-4923-8788-66a259b945f0": Phase="Pending", Reason="", readiness=false. Elapsed: 1.599701ms
Jan 30 12:09:50.387: INFO: Pod "downward-api-9cc6b8c1-dcac-4923-8788-66a259b945f0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005536453s
Jan 30 12:09:52.387: INFO: Pod "downward-api-9cc6b8c1-dcac-4923-8788-66a259b945f0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.005246764s
Jan 30 12:09:54.386: INFO: Pod "downward-api-9cc6b8c1-dcac-4923-8788-66a259b945f0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.004254635s
STEP: Saw pod success 01/30/23 12:09:54.386
Jan 30 12:09:54.386: INFO: Pod "downward-api-9cc6b8c1-dcac-4923-8788-66a259b945f0" satisfied condition "Succeeded or Failed"
Jan 30 12:09:54.388: INFO: Trying to get logs from node pubt2-nks-for-dev1.dg.163.org pod downward-api-9cc6b8c1-dcac-4923-8788-66a259b945f0 container dapi-container: <nil>
STEP: delete the pod 01/30/23 12:09:54.393
Jan 30 12:09:54.397: INFO: Waiting for pod downward-api-9cc6b8c1-dcac-4923-8788-66a259b945f0 to disappear
Jan 30 12:09:54.398: INFO: Pod downward-api-9cc6b8c1-dcac-4923-8788-66a259b945f0 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Jan 30 12:09:54.398: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-9214" for this suite. 01/30/23 12:09:54.4
------------------------------
• [SLOW TEST] [6.036 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:44

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 12:09:48.367
    Jan 30 12:09:48.367: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename downward-api 01/30/23 12:09:48.368
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:09:48.373
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:09:48.375
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:44
    STEP: Creating a pod to test downward api env vars 01/30/23 12:09:48.377
    Jan 30 12:09:48.382: INFO: Waiting up to 5m0s for pod "downward-api-9cc6b8c1-dcac-4923-8788-66a259b945f0" in namespace "downward-api-9214" to be "Succeeded or Failed"
    Jan 30 12:09:48.383: INFO: Pod "downward-api-9cc6b8c1-dcac-4923-8788-66a259b945f0": Phase="Pending", Reason="", readiness=false. Elapsed: 1.599701ms
    Jan 30 12:09:50.387: INFO: Pod "downward-api-9cc6b8c1-dcac-4923-8788-66a259b945f0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005536453s
    Jan 30 12:09:52.387: INFO: Pod "downward-api-9cc6b8c1-dcac-4923-8788-66a259b945f0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.005246764s
    Jan 30 12:09:54.386: INFO: Pod "downward-api-9cc6b8c1-dcac-4923-8788-66a259b945f0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.004254635s
    STEP: Saw pod success 01/30/23 12:09:54.386
    Jan 30 12:09:54.386: INFO: Pod "downward-api-9cc6b8c1-dcac-4923-8788-66a259b945f0" satisfied condition "Succeeded or Failed"
    Jan 30 12:09:54.388: INFO: Trying to get logs from node pubt2-nks-for-dev1.dg.163.org pod downward-api-9cc6b8c1-dcac-4923-8788-66a259b945f0 container dapi-container: <nil>
    STEP: delete the pod 01/30/23 12:09:54.393
    Jan 30 12:09:54.397: INFO: Waiting for pod downward-api-9cc6b8c1-dcac-4923-8788-66a259b945f0 to disappear
    Jan 30 12:09:54.398: INFO: Pod downward-api-9cc6b8c1-dcac-4923-8788-66a259b945f0 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Jan 30 12:09:54.398: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-9214" for this suite. 01/30/23 12:09:54.4
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:444
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 12:09:54.404
Jan 30 12:09:54.404: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename pods 01/30/23 12:09:54.404
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:09:54.41
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:09:54.413
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:444
Jan 30 12:09:54.419: INFO: Waiting up to 5m0s for pod "server-envvars-d021eecc-63d2-40da-9e33-3a365ca8ace7" in namespace "pods-1312" to be "running and ready"
Jan 30 12:09:54.420: INFO: Pod "server-envvars-d021eecc-63d2-40da-9e33-3a365ca8ace7": Phase="Pending", Reason="", readiness=false. Elapsed: 1.537631ms
Jan 30 12:09:54.420: INFO: The phase of Pod server-envvars-d021eecc-63d2-40da-9e33-3a365ca8ace7 is Pending, waiting for it to be Running (with Ready = true)
Jan 30 12:09:56.423: INFO: Pod "server-envvars-d021eecc-63d2-40da-9e33-3a365ca8ace7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003970284s
Jan 30 12:09:56.423: INFO: The phase of Pod server-envvars-d021eecc-63d2-40da-9e33-3a365ca8ace7 is Pending, waiting for it to be Running (with Ready = true)
Jan 30 12:09:58.423: INFO: Pod "server-envvars-d021eecc-63d2-40da-9e33-3a365ca8ace7": Phase="Running", Reason="", readiness=true. Elapsed: 4.004832625s
Jan 30 12:09:58.423: INFO: The phase of Pod server-envvars-d021eecc-63d2-40da-9e33-3a365ca8ace7 is Running (Ready = true)
Jan 30 12:09:58.423: INFO: Pod "server-envvars-d021eecc-63d2-40da-9e33-3a365ca8ace7" satisfied condition "running and ready"
Jan 30 12:09:58.432: INFO: Waiting up to 5m0s for pod "client-envvars-8c992d7a-a7d1-4873-aef3-7ca25b591843" in namespace "pods-1312" to be "Succeeded or Failed"
Jan 30 12:09:58.433: INFO: Pod "client-envvars-8c992d7a-a7d1-4873-aef3-7ca25b591843": Phase="Pending", Reason="", readiness=false. Elapsed: 1.530054ms
Jan 30 12:10:00.437: INFO: Pod "client-envvars-8c992d7a-a7d1-4873-aef3-7ca25b591843": Phase="Running", Reason="", readiness=true. Elapsed: 2.00513596s
Jan 30 12:10:02.436: INFO: Pod "client-envvars-8c992d7a-a7d1-4873-aef3-7ca25b591843": Phase="Running", Reason="", readiness=false. Elapsed: 4.004370678s
Jan 30 12:10:04.436: INFO: Pod "client-envvars-8c992d7a-a7d1-4873-aef3-7ca25b591843": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.004851807s
STEP: Saw pod success 01/30/23 12:10:04.436
Jan 30 12:10:04.437: INFO: Pod "client-envvars-8c992d7a-a7d1-4873-aef3-7ca25b591843" satisfied condition "Succeeded or Failed"
Jan 30 12:10:04.438: INFO: Trying to get logs from node pubt2-nks-for-dev1.dg.163.org pod client-envvars-8c992d7a-a7d1-4873-aef3-7ca25b591843 container env3cont: <nil>
STEP: delete the pod 01/30/23 12:10:04.444
Jan 30 12:10:04.448: INFO: Waiting for pod client-envvars-8c992d7a-a7d1-4873-aef3-7ca25b591843 to disappear
Jan 30 12:10:04.451: INFO: Pod client-envvars-8c992d7a-a7d1-4873-aef3-7ca25b591843 no longer exists
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Jan 30 12:10:04.451: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-1312" for this suite. 01/30/23 12:10:04.453
------------------------------
• [SLOW TEST] [10.052 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:444

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 12:09:54.404
    Jan 30 12:09:54.404: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename pods 01/30/23 12:09:54.404
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:09:54.41
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:09:54.413
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should contain environment variables for services [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:444
    Jan 30 12:09:54.419: INFO: Waiting up to 5m0s for pod "server-envvars-d021eecc-63d2-40da-9e33-3a365ca8ace7" in namespace "pods-1312" to be "running and ready"
    Jan 30 12:09:54.420: INFO: Pod "server-envvars-d021eecc-63d2-40da-9e33-3a365ca8ace7": Phase="Pending", Reason="", readiness=false. Elapsed: 1.537631ms
    Jan 30 12:09:54.420: INFO: The phase of Pod server-envvars-d021eecc-63d2-40da-9e33-3a365ca8ace7 is Pending, waiting for it to be Running (with Ready = true)
    Jan 30 12:09:56.423: INFO: Pod "server-envvars-d021eecc-63d2-40da-9e33-3a365ca8ace7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003970284s
    Jan 30 12:09:56.423: INFO: The phase of Pod server-envvars-d021eecc-63d2-40da-9e33-3a365ca8ace7 is Pending, waiting for it to be Running (with Ready = true)
    Jan 30 12:09:58.423: INFO: Pod "server-envvars-d021eecc-63d2-40da-9e33-3a365ca8ace7": Phase="Running", Reason="", readiness=true. Elapsed: 4.004832625s
    Jan 30 12:09:58.423: INFO: The phase of Pod server-envvars-d021eecc-63d2-40da-9e33-3a365ca8ace7 is Running (Ready = true)
    Jan 30 12:09:58.423: INFO: Pod "server-envvars-d021eecc-63d2-40da-9e33-3a365ca8ace7" satisfied condition "running and ready"
    Jan 30 12:09:58.432: INFO: Waiting up to 5m0s for pod "client-envvars-8c992d7a-a7d1-4873-aef3-7ca25b591843" in namespace "pods-1312" to be "Succeeded or Failed"
    Jan 30 12:09:58.433: INFO: Pod "client-envvars-8c992d7a-a7d1-4873-aef3-7ca25b591843": Phase="Pending", Reason="", readiness=false. Elapsed: 1.530054ms
    Jan 30 12:10:00.437: INFO: Pod "client-envvars-8c992d7a-a7d1-4873-aef3-7ca25b591843": Phase="Running", Reason="", readiness=true. Elapsed: 2.00513596s
    Jan 30 12:10:02.436: INFO: Pod "client-envvars-8c992d7a-a7d1-4873-aef3-7ca25b591843": Phase="Running", Reason="", readiness=false. Elapsed: 4.004370678s
    Jan 30 12:10:04.436: INFO: Pod "client-envvars-8c992d7a-a7d1-4873-aef3-7ca25b591843": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.004851807s
    STEP: Saw pod success 01/30/23 12:10:04.436
    Jan 30 12:10:04.437: INFO: Pod "client-envvars-8c992d7a-a7d1-4873-aef3-7ca25b591843" satisfied condition "Succeeded or Failed"
    Jan 30 12:10:04.438: INFO: Trying to get logs from node pubt2-nks-for-dev1.dg.163.org pod client-envvars-8c992d7a-a7d1-4873-aef3-7ca25b591843 container env3cont: <nil>
    STEP: delete the pod 01/30/23 12:10:04.444
    Jan 30 12:10:04.448: INFO: Waiting for pod client-envvars-8c992d7a-a7d1-4873-aef3-7ca25b591843 to disappear
    Jan 30 12:10:04.451: INFO: Pod client-envvars-8c992d7a-a7d1-4873-aef3-7ca25b591843 no longer exists
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Jan 30 12:10:04.451: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-1312" for this suite. 01/30/23 12:10:04.453
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should apply a finalizer to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:394
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 12:10:04.456
Jan 30 12:10:04.456: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename namespaces 01/30/23 12:10:04.457
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:10:04.463
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:10:04.465
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should apply a finalizer to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:394
STEP: Creating namespace "e2e-ns-qbbvs" 01/30/23 12:10:04.467
Jan 30 12:10:04.473: INFO: Namespace "e2e-ns-qbbvs-8437" has []v1.FinalizerName{"kubernetes"}
STEP: Adding e2e finalizer to namespace "e2e-ns-qbbvs-8437" 01/30/23 12:10:04.473
Jan 30 12:10:04.476: INFO: Namespace "e2e-ns-qbbvs-8437" has []v1.FinalizerName{"kubernetes", "e2e.example.com/fakeFinalizer"}
STEP: Removing e2e finalizer from namespace "e2e-ns-qbbvs-8437" 01/30/23 12:10:04.476
Jan 30 12:10:04.480: INFO: Namespace "e2e-ns-qbbvs-8437" has []v1.FinalizerName{"kubernetes"}
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Jan 30 12:10:04.480: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-3371" for this suite. 01/30/23 12:10:04.482
STEP: Destroying namespace "e2e-ns-qbbvs-8437" for this suite. 01/30/23 12:10:04.484
------------------------------
• [0.031 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should apply a finalizer to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:394

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 12:10:04.456
    Jan 30 12:10:04.456: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename namespaces 01/30/23 12:10:04.457
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:10:04.463
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:10:04.465
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply a finalizer to a Namespace [Conformance]
      test/e2e/apimachinery/namespace.go:394
    STEP: Creating namespace "e2e-ns-qbbvs" 01/30/23 12:10:04.467
    Jan 30 12:10:04.473: INFO: Namespace "e2e-ns-qbbvs-8437" has []v1.FinalizerName{"kubernetes"}
    STEP: Adding e2e finalizer to namespace "e2e-ns-qbbvs-8437" 01/30/23 12:10:04.473
    Jan 30 12:10:04.476: INFO: Namespace "e2e-ns-qbbvs-8437" has []v1.FinalizerName{"kubernetes", "e2e.example.com/fakeFinalizer"}
    STEP: Removing e2e finalizer from namespace "e2e-ns-qbbvs-8437" 01/30/23 12:10:04.476
    Jan 30 12:10:04.480: INFO: Namespace "e2e-ns-qbbvs-8437" has []v1.FinalizerName{"kubernetes"}
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan 30 12:10:04.480: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-3371" for this suite. 01/30/23 12:10:04.482
    STEP: Destroying namespace "e2e-ns-qbbvs-8437" for this suite. 01/30/23 12:10:04.484
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod
  should have an terminated reason [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:110
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 12:10:04.487
Jan 30 12:10:04.487: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename kubelet-test 01/30/23 12:10:04.487
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:10:04.493
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:10:04.494
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[BeforeEach] when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:85
[It] should have an terminated reason [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:110
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Jan 30 12:10:08.505: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-8763" for this suite. 01/30/23 12:10:08.508
------------------------------
• [4.024 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:82
    should have an terminated reason [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:110

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 12:10:04.487
    Jan 30 12:10:04.487: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename kubelet-test 01/30/23 12:10:04.487
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:10:04.493
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:10:04.494
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [BeforeEach] when scheduling a busybox command that always fails in a pod
      test/e2e/common/node/kubelet.go:85
    [It] should have an terminated reason [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:110
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Jan 30 12:10:08.505: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-8763" for this suite. 01/30/23 12:10:08.508
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-apps] Deployment
  should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 12:10:08.511
Jan 30 12:10:08.511: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename deployment 01/30/23 12:10:08.512
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:10:08.518
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:10:08.52
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185
STEP: creating a Deployment 01/30/23 12:10:08.524
STEP: waiting for Deployment to be created 01/30/23 12:10:08.527
STEP: waiting for all Replicas to be Ready 01/30/23 12:10:08.528
Jan 30 12:10:08.529: INFO: observed Deployment test-deployment in namespace deployment-851 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan 30 12:10:08.529: INFO: observed Deployment test-deployment in namespace deployment-851 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan 30 12:10:08.534: INFO: observed Deployment test-deployment in namespace deployment-851 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan 30 12:10:08.534: INFO: observed Deployment test-deployment in namespace deployment-851 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan 30 12:10:08.542: INFO: observed Deployment test-deployment in namespace deployment-851 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan 30 12:10:08.542: INFO: observed Deployment test-deployment in namespace deployment-851 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan 30 12:10:08.548: INFO: observed Deployment test-deployment in namespace deployment-851 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan 30 12:10:08.548: INFO: observed Deployment test-deployment in namespace deployment-851 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan 30 12:10:09.955: INFO: observed Deployment test-deployment in namespace deployment-851 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Jan 30 12:10:09.955: INFO: observed Deployment test-deployment in namespace deployment-851 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Jan 30 12:10:10.280: INFO: observed Deployment test-deployment in namespace deployment-851 with ReadyReplicas 2 and labels map[test-deployment-static:true]
STEP: patching the Deployment 01/30/23 12:10:10.28
W0130 12:10:10.286071      23 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Jan 30 12:10:10.287: INFO: observed event type ADDED
STEP: waiting for Replicas to scale 01/30/23 12:10:10.287
Jan 30 12:10:10.288: INFO: observed Deployment test-deployment in namespace deployment-851 with ReadyReplicas 0
Jan 30 12:10:10.288: INFO: observed Deployment test-deployment in namespace deployment-851 with ReadyReplicas 0
Jan 30 12:10:10.288: INFO: observed Deployment test-deployment in namespace deployment-851 with ReadyReplicas 0
Jan 30 12:10:10.288: INFO: observed Deployment test-deployment in namespace deployment-851 with ReadyReplicas 0
Jan 30 12:10:10.288: INFO: observed Deployment test-deployment in namespace deployment-851 with ReadyReplicas 0
Jan 30 12:10:10.288: INFO: observed Deployment test-deployment in namespace deployment-851 with ReadyReplicas 0
Jan 30 12:10:10.288: INFO: observed Deployment test-deployment in namespace deployment-851 with ReadyReplicas 0
Jan 30 12:10:10.288: INFO: observed Deployment test-deployment in namespace deployment-851 with ReadyReplicas 0
Jan 30 12:10:10.288: INFO: observed Deployment test-deployment in namespace deployment-851 with ReadyReplicas 1
Jan 30 12:10:10.288: INFO: observed Deployment test-deployment in namespace deployment-851 with ReadyReplicas 1
Jan 30 12:10:10.288: INFO: observed Deployment test-deployment in namespace deployment-851 with ReadyReplicas 2
Jan 30 12:10:10.288: INFO: observed Deployment test-deployment in namespace deployment-851 with ReadyReplicas 2
Jan 30 12:10:10.288: INFO: observed Deployment test-deployment in namespace deployment-851 with ReadyReplicas 2
Jan 30 12:10:10.288: INFO: observed Deployment test-deployment in namespace deployment-851 with ReadyReplicas 2
Jan 30 12:10:10.293: INFO: observed Deployment test-deployment in namespace deployment-851 with ReadyReplicas 2
Jan 30 12:10:10.293: INFO: observed Deployment test-deployment in namespace deployment-851 with ReadyReplicas 2
Jan 30 12:10:10.300: INFO: observed Deployment test-deployment in namespace deployment-851 with ReadyReplicas 2
Jan 30 12:10:10.300: INFO: observed Deployment test-deployment in namespace deployment-851 with ReadyReplicas 2
Jan 30 12:10:10.305: INFO: observed Deployment test-deployment in namespace deployment-851 with ReadyReplicas 1
Jan 30 12:10:10.305: INFO: observed Deployment test-deployment in namespace deployment-851 with ReadyReplicas 1
Jan 30 12:10:10.309: INFO: observed Deployment test-deployment in namespace deployment-851 with ReadyReplicas 1
Jan 30 12:10:10.309: INFO: observed Deployment test-deployment in namespace deployment-851 with ReadyReplicas 1
Jan 30 12:10:12.016: INFO: observed Deployment test-deployment in namespace deployment-851 with ReadyReplicas 2
Jan 30 12:10:12.016: INFO: observed Deployment test-deployment in namespace deployment-851 with ReadyReplicas 2
Jan 30 12:10:12.026: INFO: observed Deployment test-deployment in namespace deployment-851 with ReadyReplicas 1
STEP: listing Deployments 01/30/23 12:10:12.026
Jan 30 12:10:12.029: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
STEP: updating the Deployment 01/30/23 12:10:12.029
Jan 30 12:10:12.038: INFO: observed Deployment test-deployment in namespace deployment-851 with ReadyReplicas 1
STEP: fetching the DeploymentStatus 01/30/23 12:10:12.038
Jan 30 12:10:12.042: INFO: observed Deployment test-deployment in namespace deployment-851 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jan 30 12:10:12.044: INFO: observed Deployment test-deployment in namespace deployment-851 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jan 30 12:10:12.051: INFO: observed Deployment test-deployment in namespace deployment-851 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jan 30 12:10:12.060: INFO: observed Deployment test-deployment in namespace deployment-851 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jan 30 12:10:12.064: INFO: observed Deployment test-deployment in namespace deployment-851 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jan 30 12:10:13.317: INFO: observed Deployment test-deployment in namespace deployment-851 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Jan 30 12:10:14.174: INFO: observed Deployment test-deployment in namespace deployment-851 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
Jan 30 12:10:14.185: INFO: observed Deployment test-deployment in namespace deployment-851 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Jan 30 12:10:14.192: INFO: observed Deployment test-deployment in namespace deployment-851 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Jan 30 12:10:15.355: INFO: observed Deployment test-deployment in namespace deployment-851 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
STEP: patching the DeploymentStatus 01/30/23 12:10:15.367
STEP: fetching the DeploymentStatus 01/30/23 12:10:15.373
Jan 30 12:10:15.376: INFO: observed Deployment test-deployment in namespace deployment-851 with ReadyReplicas 1
Jan 30 12:10:15.376: INFO: observed Deployment test-deployment in namespace deployment-851 with ReadyReplicas 1
Jan 30 12:10:15.376: INFO: observed Deployment test-deployment in namespace deployment-851 with ReadyReplicas 1
Jan 30 12:10:15.376: INFO: observed Deployment test-deployment in namespace deployment-851 with ReadyReplicas 1
Jan 30 12:10:15.376: INFO: observed Deployment test-deployment in namespace deployment-851 with ReadyReplicas 1
Jan 30 12:10:15.376: INFO: observed Deployment test-deployment in namespace deployment-851 with ReadyReplicas 2
Jan 30 12:10:15.376: INFO: observed Deployment test-deployment in namespace deployment-851 with ReadyReplicas 3
Jan 30 12:10:15.376: INFO: observed Deployment test-deployment in namespace deployment-851 with ReadyReplicas 2
Jan 30 12:10:15.376: INFO: observed Deployment test-deployment in namespace deployment-851 with ReadyReplicas 2
Jan 30 12:10:15.376: INFO: observed Deployment test-deployment in namespace deployment-851 with ReadyReplicas 3
STEP: deleting the Deployment 01/30/23 12:10:15.376
Jan 30 12:10:15.381: INFO: observed event type MODIFIED
Jan 30 12:10:15.381: INFO: observed event type MODIFIED
Jan 30 12:10:15.381: INFO: observed event type MODIFIED
Jan 30 12:10:15.381: INFO: observed event type MODIFIED
Jan 30 12:10:15.381: INFO: observed event type MODIFIED
Jan 30 12:10:15.381: INFO: observed event type MODIFIED
Jan 30 12:10:15.381: INFO: observed event type MODIFIED
Jan 30 12:10:15.381: INFO: observed event type MODIFIED
Jan 30 12:10:15.381: INFO: observed event type MODIFIED
Jan 30 12:10:15.381: INFO: observed event type MODIFIED
Jan 30 12:10:15.381: INFO: observed event type MODIFIED
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan 30 12:10:15.383: INFO: Log out all the ReplicaSets if there is no deployment created
Jan 30 12:10:15.385: INFO: ReplicaSet "test-deployment-67f5fc87cd":
&ReplicaSet{ObjectMeta:{test-deployment-67f5fc87cd  deployment-851  91e3ca3a-ed9f-4183-9e94-2f7d3c63725b 30905 2 2023-01-30 12:10:12 +0000 UTC <nil> <nil> map[pod-template-hash:67f5fc87cd test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment 9264794e-7d7f-43e8-9007-0fbb76d54a90 0xc00708cf67 0xc00708cf68}] [] [{kube-controller-manager Update apps/v1 2023-01-30 12:10:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9264794e-7d7f-43e8-9007-0fbb76d54a90\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-30 12:10:15 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 67f5fc87cd,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:67f5fc87cd test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment harbor.cloud.netease.com/qzprod-k8s/e2e/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00708cff0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:2,AvailableReplicas:2,Conditions:[]ReplicaSetCondition{},},}

Jan 30 12:10:15.387: INFO: pod: "test-deployment-67f5fc87cd-7txv7":
&Pod{ObjectMeta:{test-deployment-67f5fc87cd-7txv7 test-deployment-67f5fc87cd- deployment-851  ff2a3063-20bf-4580-bb98-f6ea57ca849f 30904 0 2023-01-30 12:10:14 +0000 UTC <nil> <nil> map[pod-template-hash:67f5fc87cd test-deployment-static:true] map[cni.projectcalico.org/podIP:10.178.197.171/32 cni.projectcalico.org/podIPs:10.178.197.171/32] [{apps/v1 ReplicaSet test-deployment-67f5fc87cd 91e3ca3a-ed9f-4183-9e94-2f7d3c63725b 0xc00062d3a7 0xc00062d3a8}] [] [{kube-controller-manager Update v1 2023-01-30 12:10:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"91e3ca3a-ed9f-4183-9e94-2f7d3c63725b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-30 12:10:15 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-30 12:10:15 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.178.197.171\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zqw76,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:harbor.cloud.netease.com/qzprod-k8s/e2e/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zqw76,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pubt2-nks-for-dev3.dg.163.org,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 12:10:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 12:10:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 12:10:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 12:10:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.182.0.84,PodIP:10.178.197.171,StartTime:2023-01-30 12:10:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-30 12:10:15 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:harbor.cloud.netease.com/qzprod-k8s/e2e/httpd:2.4.38-4,ImageID:docker-pullable://harbor.cloud.netease.com/qzprod-k8s/e2e/httpd@sha256:8b745b6e9cb6a6184353110aabe28d8d6f6145d3fe7c0e1899aa70c8228c0fd3,ContainerID:docker://6b58cb5f59aea904ec789767023a13bdae165345c416ced10308c933825c270b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.178.197.171,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Jan 30 12:10:15.388: INFO: pod: "test-deployment-67f5fc87cd-f7q7m":
&Pod{ObjectMeta:{test-deployment-67f5fc87cd-f7q7m test-deployment-67f5fc87cd- deployment-851  4ec4f0ef-41be-4a0f-aca9-4fca84588521 30868 0 2023-01-30 12:10:12 +0000 UTC <nil> <nil> map[pod-template-hash:67f5fc87cd test-deployment-static:true] map[cni.projectcalico.org/podIP:10.178.151.106/32 cni.projectcalico.org/podIPs:10.178.151.106/32] [{apps/v1 ReplicaSet test-deployment-67f5fc87cd 91e3ca3a-ed9f-4183-9e94-2f7d3c63725b 0xc00062d5b7 0xc00062d5b8}] [] [{calico Update v1 2023-01-30 12:10:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-01-30 12:10:12 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"91e3ca3a-ed9f-4183-9e94-2f7d3c63725b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-30 12:10:14 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.178.151.106\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rwbhq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:harbor.cloud.netease.com/qzprod-k8s/e2e/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rwbhq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pubt2-nks-for-dev1.dg.163.org,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 12:10:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 12:10:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 12:10:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 12:10:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.182.0.82,PodIP:10.178.151.106,StartTime:2023-01-30 12:10:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-30 12:10:13 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:harbor.cloud.netease.com/qzprod-k8s/e2e/httpd:2.4.38-4,ImageID:docker-pullable://harbor.cloud.netease.com/qzprod-k8s/e2e/httpd@sha256:8b745b6e9cb6a6184353110aabe28d8d6f6145d3fe7c0e1899aa70c8228c0fd3,ContainerID:docker://af63bbea96d5d6bbf04358c862a571ade2a0f877be9a4b41e5eb3489a35fb407,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.178.151.106,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Jan 30 12:10:15.388: INFO: ReplicaSet "test-deployment-6bd78669b6":
&ReplicaSet{ObjectMeta:{test-deployment-6bd78669b6  deployment-851  f6abcfd6-61f1-4d26-8f42-f6d9e2fb3e9c 30913 4 2023-01-30 12:10:10 +0000 UTC <nil> <nil> map[pod-template-hash:6bd78669b6 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment 9264794e-7d7f-43e8-9007-0fbb76d54a90 0xc00708d057 0xc00708d058}] [] [{kube-controller-manager Update apps/v1 2023-01-30 12:10:15 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9264794e-7d7f-43e8-9007-0fbb76d54a90\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-30 12:10:15 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 6bd78669b6,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:6bd78669b6 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment harbor.cloud.netease.com/qzprod-k8s/e2e/pause:3.9 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00708d0e0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

Jan 30 12:10:15.390: INFO: pod: "test-deployment-6bd78669b6-8tz9m":
&Pod{ObjectMeta:{test-deployment-6bd78669b6-8tz9m test-deployment-6bd78669b6- deployment-851  14ce0cf6-b99c-4d7d-9455-0c6e82e9ec7c 30908 0 2023-01-30 12:10:12 +0000 UTC 2023-01-30 12:10:16 +0000 UTC 0xc009691108 map[pod-template-hash:6bd78669b6 test-deployment-static:true] map[cni.projectcalico.org/podIP:10.178.197.170/32 cni.projectcalico.org/podIPs:10.178.197.170/32] [{apps/v1 ReplicaSet test-deployment-6bd78669b6 f6abcfd6-61f1-4d26-8f42-f6d9e2fb3e9c 0xc009691137 0xc009691138}] [] [{calico Update v1 2023-01-30 12:10:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-01-30 12:10:12 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f6abcfd6-61f1-4d26-8f42-f6d9e2fb3e9c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-30 12:10:13 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.178.197.170\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hlq7p,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:harbor.cloud.netease.com/qzprod-k8s/e2e/pause:3.9,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hlq7p,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pubt2-nks-for-dev3.dg.163.org,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 12:10:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 12:10:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 12:10:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 12:10:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.182.0.84,PodIP:10.178.197.170,StartTime:2023-01-30 12:10:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-30 12:10:13 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:harbor.cloud.netease.com/qzprod-k8s/e2e/pause:3.9,ImageID:docker-pullable://harbor.cloud.netease.com/qzprod-k8s/e2e/pause@sha256:0fc1f3b764be56f7c881a69cbd553ae25a2b5523c6901fbacb8270307c29d0c4,ContainerID:docker://61c7f3b1bc2701476876dc5fa4fcb0c926216f82ab392dfb00cf8580532fc846,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.178.197.170,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Jan 30 12:10:15.390: INFO: ReplicaSet "test-deployment-d44579459":
&ReplicaSet{ObjectMeta:{test-deployment-d44579459  deployment-851  af92794a-841f-4e9a-b23b-ea972a186481 30800 3 2023-01-30 12:10:08 +0000 UTC <nil> <nil> map[pod-template-hash:d44579459 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment 9264794e-7d7f-43e8-9007-0fbb76d54a90 0xc00708d147 0xc00708d148}] [] [{kube-controller-manager Update apps/v1 2023-01-30 12:10:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9264794e-7d7f-43e8-9007-0fbb76d54a90\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-30 12:10:12 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: d44579459,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:d44579459 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment harbor.cloud.netease.com/qzprod-k8s/e2e/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00708d1d0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Jan 30 12:10:15.392: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-851" for this suite. 01/30/23 12:10:15.394
------------------------------
• [SLOW TEST] [6.885 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 12:10:08.511
    Jan 30 12:10:08.511: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename deployment 01/30/23 12:10:08.512
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:10:08.518
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:10:08.52
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] should run the lifecycle of a Deployment [Conformance]
      test/e2e/apps/deployment.go:185
    STEP: creating a Deployment 01/30/23 12:10:08.524
    STEP: waiting for Deployment to be created 01/30/23 12:10:08.527
    STEP: waiting for all Replicas to be Ready 01/30/23 12:10:08.528
    Jan 30 12:10:08.529: INFO: observed Deployment test-deployment in namespace deployment-851 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jan 30 12:10:08.529: INFO: observed Deployment test-deployment in namespace deployment-851 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jan 30 12:10:08.534: INFO: observed Deployment test-deployment in namespace deployment-851 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jan 30 12:10:08.534: INFO: observed Deployment test-deployment in namespace deployment-851 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jan 30 12:10:08.542: INFO: observed Deployment test-deployment in namespace deployment-851 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jan 30 12:10:08.542: INFO: observed Deployment test-deployment in namespace deployment-851 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jan 30 12:10:08.548: INFO: observed Deployment test-deployment in namespace deployment-851 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jan 30 12:10:08.548: INFO: observed Deployment test-deployment in namespace deployment-851 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jan 30 12:10:09.955: INFO: observed Deployment test-deployment in namespace deployment-851 with ReadyReplicas 1 and labels map[test-deployment-static:true]
    Jan 30 12:10:09.955: INFO: observed Deployment test-deployment in namespace deployment-851 with ReadyReplicas 1 and labels map[test-deployment-static:true]
    Jan 30 12:10:10.280: INFO: observed Deployment test-deployment in namespace deployment-851 with ReadyReplicas 2 and labels map[test-deployment-static:true]
    STEP: patching the Deployment 01/30/23 12:10:10.28
    W0130 12:10:10.286071      23 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Jan 30 12:10:10.287: INFO: observed event type ADDED
    STEP: waiting for Replicas to scale 01/30/23 12:10:10.287
    Jan 30 12:10:10.288: INFO: observed Deployment test-deployment in namespace deployment-851 with ReadyReplicas 0
    Jan 30 12:10:10.288: INFO: observed Deployment test-deployment in namespace deployment-851 with ReadyReplicas 0
    Jan 30 12:10:10.288: INFO: observed Deployment test-deployment in namespace deployment-851 with ReadyReplicas 0
    Jan 30 12:10:10.288: INFO: observed Deployment test-deployment in namespace deployment-851 with ReadyReplicas 0
    Jan 30 12:10:10.288: INFO: observed Deployment test-deployment in namespace deployment-851 with ReadyReplicas 0
    Jan 30 12:10:10.288: INFO: observed Deployment test-deployment in namespace deployment-851 with ReadyReplicas 0
    Jan 30 12:10:10.288: INFO: observed Deployment test-deployment in namespace deployment-851 with ReadyReplicas 0
    Jan 30 12:10:10.288: INFO: observed Deployment test-deployment in namespace deployment-851 with ReadyReplicas 0
    Jan 30 12:10:10.288: INFO: observed Deployment test-deployment in namespace deployment-851 with ReadyReplicas 1
    Jan 30 12:10:10.288: INFO: observed Deployment test-deployment in namespace deployment-851 with ReadyReplicas 1
    Jan 30 12:10:10.288: INFO: observed Deployment test-deployment in namespace deployment-851 with ReadyReplicas 2
    Jan 30 12:10:10.288: INFO: observed Deployment test-deployment in namespace deployment-851 with ReadyReplicas 2
    Jan 30 12:10:10.288: INFO: observed Deployment test-deployment in namespace deployment-851 with ReadyReplicas 2
    Jan 30 12:10:10.288: INFO: observed Deployment test-deployment in namespace deployment-851 with ReadyReplicas 2
    Jan 30 12:10:10.293: INFO: observed Deployment test-deployment in namespace deployment-851 with ReadyReplicas 2
    Jan 30 12:10:10.293: INFO: observed Deployment test-deployment in namespace deployment-851 with ReadyReplicas 2
    Jan 30 12:10:10.300: INFO: observed Deployment test-deployment in namespace deployment-851 with ReadyReplicas 2
    Jan 30 12:10:10.300: INFO: observed Deployment test-deployment in namespace deployment-851 with ReadyReplicas 2
    Jan 30 12:10:10.305: INFO: observed Deployment test-deployment in namespace deployment-851 with ReadyReplicas 1
    Jan 30 12:10:10.305: INFO: observed Deployment test-deployment in namespace deployment-851 with ReadyReplicas 1
    Jan 30 12:10:10.309: INFO: observed Deployment test-deployment in namespace deployment-851 with ReadyReplicas 1
    Jan 30 12:10:10.309: INFO: observed Deployment test-deployment in namespace deployment-851 with ReadyReplicas 1
    Jan 30 12:10:12.016: INFO: observed Deployment test-deployment in namespace deployment-851 with ReadyReplicas 2
    Jan 30 12:10:12.016: INFO: observed Deployment test-deployment in namespace deployment-851 with ReadyReplicas 2
    Jan 30 12:10:12.026: INFO: observed Deployment test-deployment in namespace deployment-851 with ReadyReplicas 1
    STEP: listing Deployments 01/30/23 12:10:12.026
    Jan 30 12:10:12.029: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
    STEP: updating the Deployment 01/30/23 12:10:12.029
    Jan 30 12:10:12.038: INFO: observed Deployment test-deployment in namespace deployment-851 with ReadyReplicas 1
    STEP: fetching the DeploymentStatus 01/30/23 12:10:12.038
    Jan 30 12:10:12.042: INFO: observed Deployment test-deployment in namespace deployment-851 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Jan 30 12:10:12.044: INFO: observed Deployment test-deployment in namespace deployment-851 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Jan 30 12:10:12.051: INFO: observed Deployment test-deployment in namespace deployment-851 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Jan 30 12:10:12.060: INFO: observed Deployment test-deployment in namespace deployment-851 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Jan 30 12:10:12.064: INFO: observed Deployment test-deployment in namespace deployment-851 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Jan 30 12:10:13.317: INFO: observed Deployment test-deployment in namespace deployment-851 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Jan 30 12:10:14.174: INFO: observed Deployment test-deployment in namespace deployment-851 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
    Jan 30 12:10:14.185: INFO: observed Deployment test-deployment in namespace deployment-851 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Jan 30 12:10:14.192: INFO: observed Deployment test-deployment in namespace deployment-851 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Jan 30 12:10:15.355: INFO: observed Deployment test-deployment in namespace deployment-851 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
    STEP: patching the DeploymentStatus 01/30/23 12:10:15.367
    STEP: fetching the DeploymentStatus 01/30/23 12:10:15.373
    Jan 30 12:10:15.376: INFO: observed Deployment test-deployment in namespace deployment-851 with ReadyReplicas 1
    Jan 30 12:10:15.376: INFO: observed Deployment test-deployment in namespace deployment-851 with ReadyReplicas 1
    Jan 30 12:10:15.376: INFO: observed Deployment test-deployment in namespace deployment-851 with ReadyReplicas 1
    Jan 30 12:10:15.376: INFO: observed Deployment test-deployment in namespace deployment-851 with ReadyReplicas 1
    Jan 30 12:10:15.376: INFO: observed Deployment test-deployment in namespace deployment-851 with ReadyReplicas 1
    Jan 30 12:10:15.376: INFO: observed Deployment test-deployment in namespace deployment-851 with ReadyReplicas 2
    Jan 30 12:10:15.376: INFO: observed Deployment test-deployment in namespace deployment-851 with ReadyReplicas 3
    Jan 30 12:10:15.376: INFO: observed Deployment test-deployment in namespace deployment-851 with ReadyReplicas 2
    Jan 30 12:10:15.376: INFO: observed Deployment test-deployment in namespace deployment-851 with ReadyReplicas 2
    Jan 30 12:10:15.376: INFO: observed Deployment test-deployment in namespace deployment-851 with ReadyReplicas 3
    STEP: deleting the Deployment 01/30/23 12:10:15.376
    Jan 30 12:10:15.381: INFO: observed event type MODIFIED
    Jan 30 12:10:15.381: INFO: observed event type MODIFIED
    Jan 30 12:10:15.381: INFO: observed event type MODIFIED
    Jan 30 12:10:15.381: INFO: observed event type MODIFIED
    Jan 30 12:10:15.381: INFO: observed event type MODIFIED
    Jan 30 12:10:15.381: INFO: observed event type MODIFIED
    Jan 30 12:10:15.381: INFO: observed event type MODIFIED
    Jan 30 12:10:15.381: INFO: observed event type MODIFIED
    Jan 30 12:10:15.381: INFO: observed event type MODIFIED
    Jan 30 12:10:15.381: INFO: observed event type MODIFIED
    Jan 30 12:10:15.381: INFO: observed event type MODIFIED
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jan 30 12:10:15.383: INFO: Log out all the ReplicaSets if there is no deployment created
    Jan 30 12:10:15.385: INFO: ReplicaSet "test-deployment-67f5fc87cd":
    &ReplicaSet{ObjectMeta:{test-deployment-67f5fc87cd  deployment-851  91e3ca3a-ed9f-4183-9e94-2f7d3c63725b 30905 2 2023-01-30 12:10:12 +0000 UTC <nil> <nil> map[pod-template-hash:67f5fc87cd test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment 9264794e-7d7f-43e8-9007-0fbb76d54a90 0xc00708cf67 0xc00708cf68}] [] [{kube-controller-manager Update apps/v1 2023-01-30 12:10:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9264794e-7d7f-43e8-9007-0fbb76d54a90\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-30 12:10:15 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 67f5fc87cd,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:67f5fc87cd test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment harbor.cloud.netease.com/qzprod-k8s/e2e/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00708cff0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:2,AvailableReplicas:2,Conditions:[]ReplicaSetCondition{},},}

    Jan 30 12:10:15.387: INFO: pod: "test-deployment-67f5fc87cd-7txv7":
    &Pod{ObjectMeta:{test-deployment-67f5fc87cd-7txv7 test-deployment-67f5fc87cd- deployment-851  ff2a3063-20bf-4580-bb98-f6ea57ca849f 30904 0 2023-01-30 12:10:14 +0000 UTC <nil> <nil> map[pod-template-hash:67f5fc87cd test-deployment-static:true] map[cni.projectcalico.org/podIP:10.178.197.171/32 cni.projectcalico.org/podIPs:10.178.197.171/32] [{apps/v1 ReplicaSet test-deployment-67f5fc87cd 91e3ca3a-ed9f-4183-9e94-2f7d3c63725b 0xc00062d3a7 0xc00062d3a8}] [] [{kube-controller-manager Update v1 2023-01-30 12:10:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"91e3ca3a-ed9f-4183-9e94-2f7d3c63725b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-30 12:10:15 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-30 12:10:15 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.178.197.171\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zqw76,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:harbor.cloud.netease.com/qzprod-k8s/e2e/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zqw76,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pubt2-nks-for-dev3.dg.163.org,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 12:10:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 12:10:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 12:10:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 12:10:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.182.0.84,PodIP:10.178.197.171,StartTime:2023-01-30 12:10:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-30 12:10:15 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:harbor.cloud.netease.com/qzprod-k8s/e2e/httpd:2.4.38-4,ImageID:docker-pullable://harbor.cloud.netease.com/qzprod-k8s/e2e/httpd@sha256:8b745b6e9cb6a6184353110aabe28d8d6f6145d3fe7c0e1899aa70c8228c0fd3,ContainerID:docker://6b58cb5f59aea904ec789767023a13bdae165345c416ced10308c933825c270b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.178.197.171,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    Jan 30 12:10:15.388: INFO: pod: "test-deployment-67f5fc87cd-f7q7m":
    &Pod{ObjectMeta:{test-deployment-67f5fc87cd-f7q7m test-deployment-67f5fc87cd- deployment-851  4ec4f0ef-41be-4a0f-aca9-4fca84588521 30868 0 2023-01-30 12:10:12 +0000 UTC <nil> <nil> map[pod-template-hash:67f5fc87cd test-deployment-static:true] map[cni.projectcalico.org/podIP:10.178.151.106/32 cni.projectcalico.org/podIPs:10.178.151.106/32] [{apps/v1 ReplicaSet test-deployment-67f5fc87cd 91e3ca3a-ed9f-4183-9e94-2f7d3c63725b 0xc00062d5b7 0xc00062d5b8}] [] [{calico Update v1 2023-01-30 12:10:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-01-30 12:10:12 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"91e3ca3a-ed9f-4183-9e94-2f7d3c63725b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-30 12:10:14 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.178.151.106\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rwbhq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:harbor.cloud.netease.com/qzprod-k8s/e2e/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rwbhq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pubt2-nks-for-dev1.dg.163.org,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 12:10:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 12:10:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 12:10:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 12:10:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.182.0.82,PodIP:10.178.151.106,StartTime:2023-01-30 12:10:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-30 12:10:13 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:harbor.cloud.netease.com/qzprod-k8s/e2e/httpd:2.4.38-4,ImageID:docker-pullable://harbor.cloud.netease.com/qzprod-k8s/e2e/httpd@sha256:8b745b6e9cb6a6184353110aabe28d8d6f6145d3fe7c0e1899aa70c8228c0fd3,ContainerID:docker://af63bbea96d5d6bbf04358c862a571ade2a0f877be9a4b41e5eb3489a35fb407,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.178.151.106,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    Jan 30 12:10:15.388: INFO: ReplicaSet "test-deployment-6bd78669b6":
    &ReplicaSet{ObjectMeta:{test-deployment-6bd78669b6  deployment-851  f6abcfd6-61f1-4d26-8f42-f6d9e2fb3e9c 30913 4 2023-01-30 12:10:10 +0000 UTC <nil> <nil> map[pod-template-hash:6bd78669b6 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment 9264794e-7d7f-43e8-9007-0fbb76d54a90 0xc00708d057 0xc00708d058}] [] [{kube-controller-manager Update apps/v1 2023-01-30 12:10:15 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9264794e-7d7f-43e8-9007-0fbb76d54a90\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-30 12:10:15 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 6bd78669b6,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:6bd78669b6 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment harbor.cloud.netease.com/qzprod-k8s/e2e/pause:3.9 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00708d0e0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

    Jan 30 12:10:15.390: INFO: pod: "test-deployment-6bd78669b6-8tz9m":
    &Pod{ObjectMeta:{test-deployment-6bd78669b6-8tz9m test-deployment-6bd78669b6- deployment-851  14ce0cf6-b99c-4d7d-9455-0c6e82e9ec7c 30908 0 2023-01-30 12:10:12 +0000 UTC 2023-01-30 12:10:16 +0000 UTC 0xc009691108 map[pod-template-hash:6bd78669b6 test-deployment-static:true] map[cni.projectcalico.org/podIP:10.178.197.170/32 cni.projectcalico.org/podIPs:10.178.197.170/32] [{apps/v1 ReplicaSet test-deployment-6bd78669b6 f6abcfd6-61f1-4d26-8f42-f6d9e2fb3e9c 0xc009691137 0xc009691138}] [] [{calico Update v1 2023-01-30 12:10:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-01-30 12:10:12 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f6abcfd6-61f1-4d26-8f42-f6d9e2fb3e9c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-30 12:10:13 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.178.197.170\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hlq7p,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:harbor.cloud.netease.com/qzprod-k8s/e2e/pause:3.9,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hlq7p,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pubt2-nks-for-dev3.dg.163.org,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 12:10:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 12:10:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 12:10:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 12:10:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.182.0.84,PodIP:10.178.197.170,StartTime:2023-01-30 12:10:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-30 12:10:13 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:harbor.cloud.netease.com/qzprod-k8s/e2e/pause:3.9,ImageID:docker-pullable://harbor.cloud.netease.com/qzprod-k8s/e2e/pause@sha256:0fc1f3b764be56f7c881a69cbd553ae25a2b5523c6901fbacb8270307c29d0c4,ContainerID:docker://61c7f3b1bc2701476876dc5fa4fcb0c926216f82ab392dfb00cf8580532fc846,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.178.197.170,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    Jan 30 12:10:15.390: INFO: ReplicaSet "test-deployment-d44579459":
    &ReplicaSet{ObjectMeta:{test-deployment-d44579459  deployment-851  af92794a-841f-4e9a-b23b-ea972a186481 30800 3 2023-01-30 12:10:08 +0000 UTC <nil> <nil> map[pod-template-hash:d44579459 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment 9264794e-7d7f-43e8-9007-0fbb76d54a90 0xc00708d147 0xc00708d148}] [] [{kube-controller-manager Update apps/v1 2023-01-30 12:10:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9264794e-7d7f-43e8-9007-0fbb76d54a90\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-30 12:10:12 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: d44579459,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:d44579459 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment harbor.cloud.netease.com/qzprod-k8s/e2e/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00708d1d0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Jan 30 12:10:15.392: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-851" for this suite. 01/30/23 12:10:15.394
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:74
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 12:10:15.406
Jan 30 12:10:15.406: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename projected 01/30/23 12:10:15.408
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:10:15.416
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:10:15.418
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:74
STEP: Creating configMap with name projected-configmap-test-volume-ae07dfac-1b68-4e17-bb37-c90131cc8994 01/30/23 12:10:15.42
STEP: Creating a pod to test consume configMaps 01/30/23 12:10:15.422
Jan 30 12:10:15.426: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-29430be4-4751-4224-a636-b06deab8349b" in namespace "projected-9526" to be "Succeeded or Failed"
Jan 30 12:10:15.428: INFO: Pod "pod-projected-configmaps-29430be4-4751-4224-a636-b06deab8349b": Phase="Pending", Reason="", readiness=false. Elapsed: 1.612093ms
Jan 30 12:10:17.431: INFO: Pod "pod-projected-configmaps-29430be4-4751-4224-a636-b06deab8349b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004964392s
Jan 30 12:10:19.431: INFO: Pod "pod-projected-configmaps-29430be4-4751-4224-a636-b06deab8349b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005040449s
STEP: Saw pod success 01/30/23 12:10:19.431
Jan 30 12:10:19.431: INFO: Pod "pod-projected-configmaps-29430be4-4751-4224-a636-b06deab8349b" satisfied condition "Succeeded or Failed"
Jan 30 12:10:19.434: INFO: Trying to get logs from node pubt2-nks-for-dev1.dg.163.org pod pod-projected-configmaps-29430be4-4751-4224-a636-b06deab8349b container agnhost-container: <nil>
STEP: delete the pod 01/30/23 12:10:19.438
Jan 30 12:10:19.442: INFO: Waiting for pod pod-projected-configmaps-29430be4-4751-4224-a636-b06deab8349b to disappear
Jan 30 12:10:19.443: INFO: Pod pod-projected-configmaps-29430be4-4751-4224-a636-b06deab8349b no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Jan 30 12:10:19.443: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-9526" for this suite. 01/30/23 12:10:19.446
------------------------------
• [4.042 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:74

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 12:10:15.406
    Jan 30 12:10:15.406: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename projected 01/30/23 12:10:15.408
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:10:15.416
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:10:15.418
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:74
    STEP: Creating configMap with name projected-configmap-test-volume-ae07dfac-1b68-4e17-bb37-c90131cc8994 01/30/23 12:10:15.42
    STEP: Creating a pod to test consume configMaps 01/30/23 12:10:15.422
    Jan 30 12:10:15.426: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-29430be4-4751-4224-a636-b06deab8349b" in namespace "projected-9526" to be "Succeeded or Failed"
    Jan 30 12:10:15.428: INFO: Pod "pod-projected-configmaps-29430be4-4751-4224-a636-b06deab8349b": Phase="Pending", Reason="", readiness=false. Elapsed: 1.612093ms
    Jan 30 12:10:17.431: INFO: Pod "pod-projected-configmaps-29430be4-4751-4224-a636-b06deab8349b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004964392s
    Jan 30 12:10:19.431: INFO: Pod "pod-projected-configmaps-29430be4-4751-4224-a636-b06deab8349b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005040449s
    STEP: Saw pod success 01/30/23 12:10:19.431
    Jan 30 12:10:19.431: INFO: Pod "pod-projected-configmaps-29430be4-4751-4224-a636-b06deab8349b" satisfied condition "Succeeded or Failed"
    Jan 30 12:10:19.434: INFO: Trying to get logs from node pubt2-nks-for-dev1.dg.163.org pod pod-projected-configmaps-29430be4-4751-4224-a636-b06deab8349b container agnhost-container: <nil>
    STEP: delete the pod 01/30/23 12:10:19.438
    Jan 30 12:10:19.442: INFO: Waiting for pod pod-projected-configmaps-29430be4-4751-4224-a636-b06deab8349b to disappear
    Jan 30 12:10:19.443: INFO: Pod pod-projected-configmaps-29430be4-4751-4224-a636-b06deab8349b no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Jan 30 12:10:19.443: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-9526" for this suite. 01/30/23 12:10:19.446
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should perform rolling updates and roll backs of template modifications [Conformance]
  test/e2e/apps/statefulset.go:306
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 12:10:19.448
Jan 30 12:10:19.448: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename statefulset 01/30/23 12:10:19.449
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:10:19.454
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:10:19.456
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-3849 01/30/23 12:10:19.458
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  test/e2e/apps/statefulset.go:306
STEP: Creating a new StatefulSet 01/30/23 12:10:19.46
Jan 30 12:10:19.464: INFO: Found 0 stateful pods, waiting for 3
Jan 30 12:10:29.468: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jan 30 12:10:29.468: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jan 30 12:10:29.468: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Jan 30 12:10:29.473: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=statefulset-3849 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 30 12:10:29.627: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 30 12:10:29.627: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 30 12:10:29.627: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from harbor.cloud.netease.com/qzprod-k8s/e2e/httpd:2.4.38-4 to harbor.cloud.netease.com/qzprod-k8s/e2e/httpd:2.4.39-4 01/30/23 12:10:39.636
Jan 30 12:10:39.651: INFO: Updating stateful set ss2
STEP: Creating a new revision 01/30/23 12:10:39.651
STEP: Updating Pods in reverse ordinal order 01/30/23 12:10:49.66
Jan 30 12:10:49.662: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=statefulset-3849 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 30 12:10:49.810: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan 30 12:10:49.810: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 30 12:10:49.810: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 30 12:10:59.823: INFO: Waiting for StatefulSet statefulset-3849/ss2 to complete update
STEP: Rolling back to a previous revision 01/30/23 12:11:09.827
Jan 30 12:11:09.827: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=statefulset-3849 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 30 12:11:09.999: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 30 12:11:09.999: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 30 12:11:09.999: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 30 12:11:20.024: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order 01/30/23 12:11:30.032
Jan 30 12:11:30.034: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=statefulset-3849 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 30 12:11:30.194: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan 30 12:11:30.194: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 30 12:11:30.194: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 30 12:11:40.208: INFO: Waiting for StatefulSet statefulset-3849/ss2 to complete update
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Jan 30 12:11:50.213: INFO: Deleting all statefulset in ns statefulset-3849
Jan 30 12:11:50.214: INFO: Scaling statefulset ss2 to 0
Jan 30 12:12:00.225: INFO: Waiting for statefulset status.replicas updated to 0
Jan 30 12:12:00.227: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Jan 30 12:12:00.232: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-3849" for this suite. 01/30/23 12:12:00.234
------------------------------
• [SLOW TEST] [100.789 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should perform rolling updates and roll backs of template modifications [Conformance]
    test/e2e/apps/statefulset.go:306

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 12:10:19.448
    Jan 30 12:10:19.448: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename statefulset 01/30/23 12:10:19.449
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:10:19.454
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:10:19.456
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-3849 01/30/23 12:10:19.458
    [It] should perform rolling updates and roll backs of template modifications [Conformance]
      test/e2e/apps/statefulset.go:306
    STEP: Creating a new StatefulSet 01/30/23 12:10:19.46
    Jan 30 12:10:19.464: INFO: Found 0 stateful pods, waiting for 3
    Jan 30 12:10:29.468: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Jan 30 12:10:29.468: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Jan 30 12:10:29.468: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    Jan 30 12:10:29.473: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=statefulset-3849 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan 30 12:10:29.627: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan 30 12:10:29.627: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan 30 12:10:29.627: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    STEP: Updating StatefulSet template: update image from harbor.cloud.netease.com/qzprod-k8s/e2e/httpd:2.4.38-4 to harbor.cloud.netease.com/qzprod-k8s/e2e/httpd:2.4.39-4 01/30/23 12:10:39.636
    Jan 30 12:10:39.651: INFO: Updating stateful set ss2
    STEP: Creating a new revision 01/30/23 12:10:39.651
    STEP: Updating Pods in reverse ordinal order 01/30/23 12:10:49.66
    Jan 30 12:10:49.662: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=statefulset-3849 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan 30 12:10:49.810: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Jan 30 12:10:49.810: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jan 30 12:10:49.810: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jan 30 12:10:59.823: INFO: Waiting for StatefulSet statefulset-3849/ss2 to complete update
    STEP: Rolling back to a previous revision 01/30/23 12:11:09.827
    Jan 30 12:11:09.827: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=statefulset-3849 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan 30 12:11:09.999: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan 30 12:11:09.999: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan 30 12:11:09.999: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jan 30 12:11:20.024: INFO: Updating stateful set ss2
    STEP: Rolling back update in reverse ordinal order 01/30/23 12:11:30.032
    Jan 30 12:11:30.034: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=statefulset-3849 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan 30 12:11:30.194: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Jan 30 12:11:30.194: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jan 30 12:11:30.194: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jan 30 12:11:40.208: INFO: Waiting for StatefulSet statefulset-3849/ss2 to complete update
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Jan 30 12:11:50.213: INFO: Deleting all statefulset in ns statefulset-3849
    Jan 30 12:11:50.214: INFO: Scaling statefulset ss2 to 0
    Jan 30 12:12:00.225: INFO: Waiting for statefulset status.replicas updated to 0
    Jan 30 12:12:00.227: INFO: Deleting statefulset ss2
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Jan 30 12:12:00.232: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-3849" for this suite. 01/30/23 12:12:00.234
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-network] DNS
  should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 12:12:00.237
Jan 30 12:12:00.237: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename dns 01/30/23 12:12:00.238
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:12:00.244
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:12:00.246
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137
STEP: Creating a test headless service 01/30/23 12:12:00.248
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7138.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-7138.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7138.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-7138.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-7138.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-7138.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-7138.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-7138.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-7138.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-7138.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-7138.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-7138.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 132.83.178.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.178.83.132_udp@PTR;check="$$(dig +tcp +noall +answer +search 132.83.178.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.178.83.132_tcp@PTR;sleep 1; done
 01/30/23 12:12:00.254
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7138.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-7138.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7138.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-7138.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-7138.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-7138.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-7138.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-7138.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-7138.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-7138.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-7138.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-7138.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 132.83.178.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.178.83.132_udp@PTR;check="$$(dig +tcp +noall +answer +search 132.83.178.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.178.83.132_tcp@PTR;sleep 1; done
 01/30/23 12:12:00.254
STEP: creating a pod to probe DNS 01/30/23 12:12:00.254
STEP: submitting the pod to kubernetes 01/30/23 12:12:00.254
Jan 30 12:12:00.259: INFO: Waiting up to 15m0s for pod "dns-test-2fc20672-96e0-4436-a7f6-48839f57e64b" in namespace "dns-7138" to be "running"
Jan 30 12:12:00.260: INFO: Pod "dns-test-2fc20672-96e0-4436-a7f6-48839f57e64b": Phase="Pending", Reason="", readiness=false. Elapsed: 1.660317ms
Jan 30 12:12:02.263: INFO: Pod "dns-test-2fc20672-96e0-4436-a7f6-48839f57e64b": Phase="Running", Reason="", readiness=true. Elapsed: 2.004326761s
Jan 30 12:12:02.263: INFO: Pod "dns-test-2fc20672-96e0-4436-a7f6-48839f57e64b" satisfied condition "running"
STEP: retrieving the pod 01/30/23 12:12:02.263
STEP: looking for the results for each expected name from probers 01/30/23 12:12:02.265
Jan 30 12:12:02.267: INFO: Unable to read wheezy_udp@dns-test-service.dns-7138.svc.cluster.local from pod dns-7138/dns-test-2fc20672-96e0-4436-a7f6-48839f57e64b: the server could not find the requested resource (get pods dns-test-2fc20672-96e0-4436-a7f6-48839f57e64b)
Jan 30 12:12:02.269: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7138.svc.cluster.local from pod dns-7138/dns-test-2fc20672-96e0-4436-a7f6-48839f57e64b: the server could not find the requested resource (get pods dns-test-2fc20672-96e0-4436-a7f6-48839f57e64b)
Jan 30 12:12:02.271: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-7138.svc.cluster.local from pod dns-7138/dns-test-2fc20672-96e0-4436-a7f6-48839f57e64b: the server could not find the requested resource (get pods dns-test-2fc20672-96e0-4436-a7f6-48839f57e64b)
Jan 30 12:12:02.273: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-7138.svc.cluster.local from pod dns-7138/dns-test-2fc20672-96e0-4436-a7f6-48839f57e64b: the server could not find the requested resource (get pods dns-test-2fc20672-96e0-4436-a7f6-48839f57e64b)
Jan 30 12:12:02.282: INFO: Unable to read jessie_udp@dns-test-service.dns-7138.svc.cluster.local from pod dns-7138/dns-test-2fc20672-96e0-4436-a7f6-48839f57e64b: the server could not find the requested resource (get pods dns-test-2fc20672-96e0-4436-a7f6-48839f57e64b)
Jan 30 12:12:02.283: INFO: Unable to read jessie_tcp@dns-test-service.dns-7138.svc.cluster.local from pod dns-7138/dns-test-2fc20672-96e0-4436-a7f6-48839f57e64b: the server could not find the requested resource (get pods dns-test-2fc20672-96e0-4436-a7f6-48839f57e64b)
Jan 30 12:12:02.285: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-7138.svc.cluster.local from pod dns-7138/dns-test-2fc20672-96e0-4436-a7f6-48839f57e64b: the server could not find the requested resource (get pods dns-test-2fc20672-96e0-4436-a7f6-48839f57e64b)
Jan 30 12:12:02.287: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-7138.svc.cluster.local from pod dns-7138/dns-test-2fc20672-96e0-4436-a7f6-48839f57e64b: the server could not find the requested resource (get pods dns-test-2fc20672-96e0-4436-a7f6-48839f57e64b)
Jan 30 12:12:02.293: INFO: Lookups using dns-7138/dns-test-2fc20672-96e0-4436-a7f6-48839f57e64b failed for: [wheezy_udp@dns-test-service.dns-7138.svc.cluster.local wheezy_tcp@dns-test-service.dns-7138.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-7138.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-7138.svc.cluster.local jessie_udp@dns-test-service.dns-7138.svc.cluster.local jessie_tcp@dns-test-service.dns-7138.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-7138.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-7138.svc.cluster.local]

Jan 30 12:12:07.324: INFO: DNS probes using dns-7138/dns-test-2fc20672-96e0-4436-a7f6-48839f57e64b succeeded

STEP: deleting the pod 01/30/23 12:12:07.324
STEP: deleting the test service 01/30/23 12:12:07.329
STEP: deleting the test headless service 01/30/23 12:12:07.334
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Jan 30 12:12:07.338: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-7138" for this suite. 01/30/23 12:12:07.34
------------------------------
• [SLOW TEST] [7.105 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 12:12:00.237
    Jan 30 12:12:00.237: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename dns 01/30/23 12:12:00.238
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:12:00.244
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:12:00.246
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for services  [Conformance]
      test/e2e/network/dns.go:137
    STEP: Creating a test headless service 01/30/23 12:12:00.248
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7138.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-7138.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7138.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-7138.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-7138.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-7138.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-7138.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-7138.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-7138.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-7138.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-7138.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-7138.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 132.83.178.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.178.83.132_udp@PTR;check="$$(dig +tcp +noall +answer +search 132.83.178.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.178.83.132_tcp@PTR;sleep 1; done
     01/30/23 12:12:00.254
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7138.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-7138.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7138.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-7138.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-7138.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-7138.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-7138.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-7138.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-7138.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-7138.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-7138.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-7138.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 132.83.178.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.178.83.132_udp@PTR;check="$$(dig +tcp +noall +answer +search 132.83.178.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.178.83.132_tcp@PTR;sleep 1; done
     01/30/23 12:12:00.254
    STEP: creating a pod to probe DNS 01/30/23 12:12:00.254
    STEP: submitting the pod to kubernetes 01/30/23 12:12:00.254
    Jan 30 12:12:00.259: INFO: Waiting up to 15m0s for pod "dns-test-2fc20672-96e0-4436-a7f6-48839f57e64b" in namespace "dns-7138" to be "running"
    Jan 30 12:12:00.260: INFO: Pod "dns-test-2fc20672-96e0-4436-a7f6-48839f57e64b": Phase="Pending", Reason="", readiness=false. Elapsed: 1.660317ms
    Jan 30 12:12:02.263: INFO: Pod "dns-test-2fc20672-96e0-4436-a7f6-48839f57e64b": Phase="Running", Reason="", readiness=true. Elapsed: 2.004326761s
    Jan 30 12:12:02.263: INFO: Pod "dns-test-2fc20672-96e0-4436-a7f6-48839f57e64b" satisfied condition "running"
    STEP: retrieving the pod 01/30/23 12:12:02.263
    STEP: looking for the results for each expected name from probers 01/30/23 12:12:02.265
    Jan 30 12:12:02.267: INFO: Unable to read wheezy_udp@dns-test-service.dns-7138.svc.cluster.local from pod dns-7138/dns-test-2fc20672-96e0-4436-a7f6-48839f57e64b: the server could not find the requested resource (get pods dns-test-2fc20672-96e0-4436-a7f6-48839f57e64b)
    Jan 30 12:12:02.269: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7138.svc.cluster.local from pod dns-7138/dns-test-2fc20672-96e0-4436-a7f6-48839f57e64b: the server could not find the requested resource (get pods dns-test-2fc20672-96e0-4436-a7f6-48839f57e64b)
    Jan 30 12:12:02.271: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-7138.svc.cluster.local from pod dns-7138/dns-test-2fc20672-96e0-4436-a7f6-48839f57e64b: the server could not find the requested resource (get pods dns-test-2fc20672-96e0-4436-a7f6-48839f57e64b)
    Jan 30 12:12:02.273: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-7138.svc.cluster.local from pod dns-7138/dns-test-2fc20672-96e0-4436-a7f6-48839f57e64b: the server could not find the requested resource (get pods dns-test-2fc20672-96e0-4436-a7f6-48839f57e64b)
    Jan 30 12:12:02.282: INFO: Unable to read jessie_udp@dns-test-service.dns-7138.svc.cluster.local from pod dns-7138/dns-test-2fc20672-96e0-4436-a7f6-48839f57e64b: the server could not find the requested resource (get pods dns-test-2fc20672-96e0-4436-a7f6-48839f57e64b)
    Jan 30 12:12:02.283: INFO: Unable to read jessie_tcp@dns-test-service.dns-7138.svc.cluster.local from pod dns-7138/dns-test-2fc20672-96e0-4436-a7f6-48839f57e64b: the server could not find the requested resource (get pods dns-test-2fc20672-96e0-4436-a7f6-48839f57e64b)
    Jan 30 12:12:02.285: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-7138.svc.cluster.local from pod dns-7138/dns-test-2fc20672-96e0-4436-a7f6-48839f57e64b: the server could not find the requested resource (get pods dns-test-2fc20672-96e0-4436-a7f6-48839f57e64b)
    Jan 30 12:12:02.287: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-7138.svc.cluster.local from pod dns-7138/dns-test-2fc20672-96e0-4436-a7f6-48839f57e64b: the server could not find the requested resource (get pods dns-test-2fc20672-96e0-4436-a7f6-48839f57e64b)
    Jan 30 12:12:02.293: INFO: Lookups using dns-7138/dns-test-2fc20672-96e0-4436-a7f6-48839f57e64b failed for: [wheezy_udp@dns-test-service.dns-7138.svc.cluster.local wheezy_tcp@dns-test-service.dns-7138.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-7138.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-7138.svc.cluster.local jessie_udp@dns-test-service.dns-7138.svc.cluster.local jessie_tcp@dns-test-service.dns-7138.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-7138.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-7138.svc.cluster.local]

    Jan 30 12:12:07.324: INFO: DNS probes using dns-7138/dns-test-2fc20672-96e0-4436-a7f6-48839f57e64b succeeded

    STEP: deleting the pod 01/30/23 12:12:07.324
    STEP: deleting the test service 01/30/23 12:12:07.329
    STEP: deleting the test headless service 01/30/23 12:12:07.334
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Jan 30 12:12:07.338: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-7138" for this suite. 01/30/23 12:12:07.34
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-scheduling] LimitRange
  should list, patch and delete a LimitRange by collection [Conformance]
  test/e2e/scheduling/limit_range.go:239
[BeforeEach] [sig-scheduling] LimitRange
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 12:12:07.343
Jan 30 12:12:07.343: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename limitrange 01/30/23 12:12:07.343
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:12:07.349
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:12:07.351
[BeforeEach] [sig-scheduling] LimitRange
  test/e2e/framework/metrics/init/init.go:31
[It] should list, patch and delete a LimitRange by collection [Conformance]
  test/e2e/scheduling/limit_range.go:239
STEP: Creating LimitRange "e2e-limitrange-c5222" in namespace "limitrange-1670" 01/30/23 12:12:07.353
STEP: Creating another limitRange in another namespace 01/30/23 12:12:07.355
Jan 30 12:12:07.361: INFO: Namespace "e2e-limitrange-c5222-8008" created
Jan 30 12:12:07.361: INFO: Creating LimitRange "e2e-limitrange-c5222" in namespace "e2e-limitrange-c5222-8008"
STEP: Listing all LimitRanges with label "e2e-test=e2e-limitrange-c5222" 01/30/23 12:12:07.363
Jan 30 12:12:07.364: INFO: Found 2 limitRanges
STEP: Patching LimitRange "e2e-limitrange-c5222" in "limitrange-1670" namespace 01/30/23 12:12:07.364
Jan 30 12:12:07.369: INFO: LimitRange "e2e-limitrange-c5222" has been patched
STEP: Delete LimitRange "e2e-limitrange-c5222" by Collection with labelSelector: "e2e-limitrange-c5222=patched" 01/30/23 12:12:07.369
STEP: Confirm that the limitRange "e2e-limitrange-c5222" has been deleted 01/30/23 12:12:07.372
Jan 30 12:12:07.372: INFO: Requesting list of LimitRange to confirm quantity
Jan 30 12:12:07.375: INFO: Found 0 LimitRange with label "e2e-limitrange-c5222=patched"
Jan 30 12:12:07.375: INFO: LimitRange "e2e-limitrange-c5222" has been deleted.
STEP: Confirm that a single LimitRange still exists with label "e2e-test=e2e-limitrange-c5222" 01/30/23 12:12:07.375
Jan 30 12:12:07.378: INFO: Found 1 limitRange
[AfterEach] [sig-scheduling] LimitRange
  test/e2e/framework/node/init/init.go:32
Jan 30 12:12:07.378: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  tear down framework | framework.go:193
STEP: Destroying namespace "limitrange-1670" for this suite. 01/30/23 12:12:07.38
STEP: Destroying namespace "e2e-limitrange-c5222-8008" for this suite. 01/30/23 12:12:07.383
------------------------------
• [0.042 seconds]
[sig-scheduling] LimitRange
test/e2e/scheduling/framework.go:40
  should list, patch and delete a LimitRange by collection [Conformance]
  test/e2e/scheduling/limit_range.go:239

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] LimitRange
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 12:12:07.343
    Jan 30 12:12:07.343: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename limitrange 01/30/23 12:12:07.343
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:12:07.349
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:12:07.351
    [BeforeEach] [sig-scheduling] LimitRange
      test/e2e/framework/metrics/init/init.go:31
    [It] should list, patch and delete a LimitRange by collection [Conformance]
      test/e2e/scheduling/limit_range.go:239
    STEP: Creating LimitRange "e2e-limitrange-c5222" in namespace "limitrange-1670" 01/30/23 12:12:07.353
    STEP: Creating another limitRange in another namespace 01/30/23 12:12:07.355
    Jan 30 12:12:07.361: INFO: Namespace "e2e-limitrange-c5222-8008" created
    Jan 30 12:12:07.361: INFO: Creating LimitRange "e2e-limitrange-c5222" in namespace "e2e-limitrange-c5222-8008"
    STEP: Listing all LimitRanges with label "e2e-test=e2e-limitrange-c5222" 01/30/23 12:12:07.363
    Jan 30 12:12:07.364: INFO: Found 2 limitRanges
    STEP: Patching LimitRange "e2e-limitrange-c5222" in "limitrange-1670" namespace 01/30/23 12:12:07.364
    Jan 30 12:12:07.369: INFO: LimitRange "e2e-limitrange-c5222" has been patched
    STEP: Delete LimitRange "e2e-limitrange-c5222" by Collection with labelSelector: "e2e-limitrange-c5222=patched" 01/30/23 12:12:07.369
    STEP: Confirm that the limitRange "e2e-limitrange-c5222" has been deleted 01/30/23 12:12:07.372
    Jan 30 12:12:07.372: INFO: Requesting list of LimitRange to confirm quantity
    Jan 30 12:12:07.375: INFO: Found 0 LimitRange with label "e2e-limitrange-c5222=patched"
    Jan 30 12:12:07.375: INFO: LimitRange "e2e-limitrange-c5222" has been deleted.
    STEP: Confirm that a single LimitRange still exists with label "e2e-test=e2e-limitrange-c5222" 01/30/23 12:12:07.375
    Jan 30 12:12:07.378: INFO: Found 1 limitRange
    [AfterEach] [sig-scheduling] LimitRange
      test/e2e/framework/node/init/init.go:32
    Jan 30 12:12:07.378: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      tear down framework | framework.go:193
    STEP: Destroying namespace "limitrange-1670" for this suite. 01/30/23 12:12:07.38
    STEP: Destroying namespace "e2e-limitrange-c5222-8008" for this suite. 01/30/23 12:12:07.383
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 12:12:07.385
Jan 30 12:12:07.386: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename cronjob 01/30/23 12:12:07.386
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:12:07.392
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:12:07.394
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96
STEP: Creating a suspended cronjob 01/30/23 12:12:07.396
STEP: Ensuring no jobs are scheduled 01/30/23 12:12:07.399
STEP: Ensuring no job exists by listing jobs explicitly 01/30/23 12:17:07.403
STEP: Removing cronjob 01/30/23 12:17:07.405
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Jan 30 12:17:07.408: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-6989" for this suite. 01/30/23 12:17:07.41
------------------------------
• [SLOW TEST] [300.027 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 12:12:07.385
    Jan 30 12:12:07.386: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename cronjob 01/30/23 12:12:07.386
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:12:07.392
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:12:07.394
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should not schedule jobs when suspended [Slow] [Conformance]
      test/e2e/apps/cronjob.go:96
    STEP: Creating a suspended cronjob 01/30/23 12:12:07.396
    STEP: Ensuring no jobs are scheduled 01/30/23 12:12:07.399
    STEP: Ensuring no job exists by listing jobs explicitly 01/30/23 12:17:07.403
    STEP: Removing cronjob 01/30/23 12:17:07.405
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Jan 30 12:17:07.408: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-6989" for this suite. 01/30/23 12:17:07.41
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:89
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 12:17:07.413
Jan 30 12:17:07.413: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename projected 01/30/23 12:17:07.414
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:17:07.42
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:17:07.422
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:89
STEP: Creating configMap with name projected-configmap-test-volume-map-0cfe0102-3b7b-4295-9867-fe990c7792f3 01/30/23 12:17:07.424
STEP: Creating a pod to test consume configMaps 01/30/23 12:17:07.427
Jan 30 12:17:07.431: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-75600286-9fca-4698-a522-1c68a8946c74" in namespace "projected-3815" to be "Succeeded or Failed"
Jan 30 12:17:07.432: INFO: Pod "pod-projected-configmaps-75600286-9fca-4698-a522-1c68a8946c74": Phase="Pending", Reason="", readiness=false. Elapsed: 1.500703ms
Jan 30 12:17:09.434: INFO: Pod "pod-projected-configmaps-75600286-9fca-4698-a522-1c68a8946c74": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003620493s
Jan 30 12:17:11.435: INFO: Pod "pod-projected-configmaps-75600286-9fca-4698-a522-1c68a8946c74": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004158185s
STEP: Saw pod success 01/30/23 12:17:11.435
Jan 30 12:17:11.435: INFO: Pod "pod-projected-configmaps-75600286-9fca-4698-a522-1c68a8946c74" satisfied condition "Succeeded or Failed"
Jan 30 12:17:11.437: INFO: Trying to get logs from node pubt2-nks-for-dev1.dg.163.org pod pod-projected-configmaps-75600286-9fca-4698-a522-1c68a8946c74 container agnhost-container: <nil>
STEP: delete the pod 01/30/23 12:17:11.45
Jan 30 12:17:11.455: INFO: Waiting for pod pod-projected-configmaps-75600286-9fca-4698-a522-1c68a8946c74 to disappear
Jan 30 12:17:11.456: INFO: Pod pod-projected-configmaps-75600286-9fca-4698-a522-1c68a8946c74 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Jan 30 12:17:11.456: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-3815" for this suite. 01/30/23 12:17:11.459
------------------------------
• [4.048 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:89

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 12:17:07.413
    Jan 30 12:17:07.413: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename projected 01/30/23 12:17:07.414
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:17:07.42
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:17:07.422
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:89
    STEP: Creating configMap with name projected-configmap-test-volume-map-0cfe0102-3b7b-4295-9867-fe990c7792f3 01/30/23 12:17:07.424
    STEP: Creating a pod to test consume configMaps 01/30/23 12:17:07.427
    Jan 30 12:17:07.431: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-75600286-9fca-4698-a522-1c68a8946c74" in namespace "projected-3815" to be "Succeeded or Failed"
    Jan 30 12:17:07.432: INFO: Pod "pod-projected-configmaps-75600286-9fca-4698-a522-1c68a8946c74": Phase="Pending", Reason="", readiness=false. Elapsed: 1.500703ms
    Jan 30 12:17:09.434: INFO: Pod "pod-projected-configmaps-75600286-9fca-4698-a522-1c68a8946c74": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003620493s
    Jan 30 12:17:11.435: INFO: Pod "pod-projected-configmaps-75600286-9fca-4698-a522-1c68a8946c74": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004158185s
    STEP: Saw pod success 01/30/23 12:17:11.435
    Jan 30 12:17:11.435: INFO: Pod "pod-projected-configmaps-75600286-9fca-4698-a522-1c68a8946c74" satisfied condition "Succeeded or Failed"
    Jan 30 12:17:11.437: INFO: Trying to get logs from node pubt2-nks-for-dev1.dg.163.org pod pod-projected-configmaps-75600286-9fca-4698-a522-1c68a8946c74 container agnhost-container: <nil>
    STEP: delete the pod 01/30/23 12:17:11.45
    Jan 30 12:17:11.455: INFO: Waiting for pod pod-projected-configmaps-75600286-9fca-4698-a522-1c68a8946c74 to disappear
    Jan 30 12:17:11.456: INFO: Pod pod-projected-configmaps-75600286-9fca-4698-a522-1c68a8946c74 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Jan 30 12:17:11.456: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-3815" for this suite. 01/30/23 12:17:11.459
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:164
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 12:17:11.462
Jan 30 12:17:11.462: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename security-context 01/30/23 12:17:11.463
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:17:11.468
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:17:11.47
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:164
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 01/30/23 12:17:11.472
Jan 30 12:17:11.476: INFO: Waiting up to 5m0s for pod "security-context-324c3a68-48e0-4046-98a6-39b8b5dd961b" in namespace "security-context-1992" to be "Succeeded or Failed"
Jan 30 12:17:11.478: INFO: Pod "security-context-324c3a68-48e0-4046-98a6-39b8b5dd961b": Phase="Pending", Reason="", readiness=false. Elapsed: 1.541348ms
Jan 30 12:17:13.480: INFO: Pod "security-context-324c3a68-48e0-4046-98a6-39b8b5dd961b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003847753s
Jan 30 12:17:15.480: INFO: Pod "security-context-324c3a68-48e0-4046-98a6-39b8b5dd961b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004007613s
STEP: Saw pod success 01/30/23 12:17:15.48
Jan 30 12:17:15.481: INFO: Pod "security-context-324c3a68-48e0-4046-98a6-39b8b5dd961b" satisfied condition "Succeeded or Failed"
Jan 30 12:17:15.482: INFO: Trying to get logs from node pubt2-nks-for-dev1.dg.163.org pod security-context-324c3a68-48e0-4046-98a6-39b8b5dd961b container test-container: <nil>
STEP: delete the pod 01/30/23 12:17:15.486
Jan 30 12:17:15.491: INFO: Waiting for pod security-context-324c3a68-48e0-4046-98a6-39b8b5dd961b to disappear
Jan 30 12:17:15.492: INFO: Pod security-context-324c3a68-48e0-4046-98a6-39b8b5dd961b no longer exists
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Jan 30 12:17:15.492: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-1992" for this suite. 01/30/23 12:17:15.494
------------------------------
• [4.034 seconds]
[sig-node] Security Context
test/e2e/node/framework.go:23
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:164

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 12:17:11.462
    Jan 30 12:17:11.462: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename security-context 01/30/23 12:17:11.463
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:17:11.468
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:17:11.47
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
      test/e2e/node/security_context.go:164
    STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 01/30/23 12:17:11.472
    Jan 30 12:17:11.476: INFO: Waiting up to 5m0s for pod "security-context-324c3a68-48e0-4046-98a6-39b8b5dd961b" in namespace "security-context-1992" to be "Succeeded or Failed"
    Jan 30 12:17:11.478: INFO: Pod "security-context-324c3a68-48e0-4046-98a6-39b8b5dd961b": Phase="Pending", Reason="", readiness=false. Elapsed: 1.541348ms
    Jan 30 12:17:13.480: INFO: Pod "security-context-324c3a68-48e0-4046-98a6-39b8b5dd961b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003847753s
    Jan 30 12:17:15.480: INFO: Pod "security-context-324c3a68-48e0-4046-98a6-39b8b5dd961b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004007613s
    STEP: Saw pod success 01/30/23 12:17:15.48
    Jan 30 12:17:15.481: INFO: Pod "security-context-324c3a68-48e0-4046-98a6-39b8b5dd961b" satisfied condition "Succeeded or Failed"
    Jan 30 12:17:15.482: INFO: Trying to get logs from node pubt2-nks-for-dev1.dg.163.org pod security-context-324c3a68-48e0-4046-98a6-39b8b5dd961b container test-container: <nil>
    STEP: delete the pod 01/30/23 12:17:15.486
    Jan 30 12:17:15.491: INFO: Waiting for pod security-context-324c3a68-48e0-4046-98a6-39b8b5dd961b to disappear
    Jan 30 12:17:15.492: INFO: Pod security-context-324c3a68-48e0-4046-98a6-39b8b5dd961b no longer exists
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Jan 30 12:17:15.492: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-1992" for this suite. 01/30/23 12:17:15.494
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server
  should support proxy with --port 0  [Conformance]
  test/e2e/kubectl/kubectl.go:1787
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 12:17:15.497
Jan 30 12:17:15.497: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename kubectl 01/30/23 12:17:15.498
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:17:15.503
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:17:15.505
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should support proxy with --port 0  [Conformance]
  test/e2e/kubectl/kubectl.go:1787
STEP: starting the proxy server 01/30/23 12:17:15.507
Jan 30 12:17:15.507: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=kubectl-3209 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output 01/30/23 12:17:15.552
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jan 30 12:17:15.558: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-3209" for this suite. 01/30/23 12:17:15.561
------------------------------
• [0.066 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Proxy server
  test/e2e/kubectl/kubectl.go:1780
    should support proxy with --port 0  [Conformance]
    test/e2e/kubectl/kubectl.go:1787

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 12:17:15.497
    Jan 30 12:17:15.497: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename kubectl 01/30/23 12:17:15.498
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:17:15.503
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:17:15.505
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should support proxy with --port 0  [Conformance]
      test/e2e/kubectl/kubectl.go:1787
    STEP: starting the proxy server 01/30/23 12:17:15.507
    Jan 30 12:17:15.507: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=kubectl-3209 proxy -p 0 --disable-filter'
    STEP: curling proxy /api/ output 01/30/23 12:17:15.552
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jan 30 12:17:15.558: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-3209" for this suite. 01/30/23 12:17:15.561
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:37
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 12:17:15.564
Jan 30 12:17:15.564: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename sysctl 01/30/23 12:17:15.564
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:17:15.57
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:17:15.571
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:67
[It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77
STEP: Creating a pod with the kernel.shm_rmid_forced sysctl 01/30/23 12:17:15.574
STEP: Watching for error events or started pod 01/30/23 12:17:15.578
STEP: Waiting for pod completion 01/30/23 12:17:17.581
Jan 30 12:17:17.581: INFO: Waiting up to 3m0s for pod "sysctl-efd47259-a9a9-41fd-a284-8e4e9673f74e" in namespace "sysctl-4785" to be "completed"
Jan 30 12:17:17.582: INFO: Pod "sysctl-efd47259-a9a9-41fd-a284-8e4e9673f74e": Phase="Pending", Reason="", readiness=false. Elapsed: 1.557333ms
Jan 30 12:17:19.585: INFO: Pod "sysctl-efd47259-a9a9-41fd-a284-8e4e9673f74e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003893856s
Jan 30 12:17:21.585: INFO: Pod "sysctl-efd47259-a9a9-41fd-a284-8e4e9673f74e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004101073s
Jan 30 12:17:21.585: INFO: Pod "sysctl-efd47259-a9a9-41fd-a284-8e4e9673f74e" satisfied condition "completed"
STEP: Checking that the pod succeeded 01/30/23 12:17:21.586
STEP: Getting logs from the pod 01/30/23 12:17:21.587
STEP: Checking that the sysctl is actually updated 01/30/23 12:17:21.591
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Jan 30 12:17:21.591: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "sysctl-4785" for this suite. 01/30/23 12:17:21.594
------------------------------
• [SLOW TEST] [6.032 seconds]
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
test/e2e/common/node/framework.go:23
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:37
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 12:17:15.564
    Jan 30 12:17:15.564: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename sysctl 01/30/23 12:17:15.564
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:17:15.57
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:17:15.571
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:67
    [It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
      test/e2e/common/node/sysctl.go:77
    STEP: Creating a pod with the kernel.shm_rmid_forced sysctl 01/30/23 12:17:15.574
    STEP: Watching for error events or started pod 01/30/23 12:17:15.578
    STEP: Waiting for pod completion 01/30/23 12:17:17.581
    Jan 30 12:17:17.581: INFO: Waiting up to 3m0s for pod "sysctl-efd47259-a9a9-41fd-a284-8e4e9673f74e" in namespace "sysctl-4785" to be "completed"
    Jan 30 12:17:17.582: INFO: Pod "sysctl-efd47259-a9a9-41fd-a284-8e4e9673f74e": Phase="Pending", Reason="", readiness=false. Elapsed: 1.557333ms
    Jan 30 12:17:19.585: INFO: Pod "sysctl-efd47259-a9a9-41fd-a284-8e4e9673f74e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003893856s
    Jan 30 12:17:21.585: INFO: Pod "sysctl-efd47259-a9a9-41fd-a284-8e4e9673f74e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004101073s
    Jan 30 12:17:21.585: INFO: Pod "sysctl-efd47259-a9a9-41fd-a284-8e4e9673f74e" satisfied condition "completed"
    STEP: Checking that the pod succeeded 01/30/23 12:17:21.586
    STEP: Getting logs from the pod 01/30/23 12:17:21.587
    STEP: Checking that the sysctl is actually updated 01/30/23 12:17:21.591
    [AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Jan 30 12:17:21.591: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sysctl-4785" for this suite. 01/30/23 12:17:21.594
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98
[BeforeEach] [sig-instrumentation] Events API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 12:17:21.596
Jan 30 12:17:21.596: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename events 01/30/23 12:17:21.597
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:17:21.603
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:17:21.605
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/instrumentation/events.go:84
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98
STEP: creating a test event 01/30/23 12:17:21.607
STEP: listing events in all namespaces 01/30/23 12:17:21.609
STEP: listing events in test namespace 01/30/23 12:17:21.616
STEP: listing events with field selection filtering on source 01/30/23 12:17:21.617
STEP: listing events with field selection filtering on reportingController 01/30/23 12:17:21.619
STEP: getting the test event 01/30/23 12:17:21.621
STEP: patching the test event 01/30/23 12:17:21.622
STEP: getting the test event 01/30/23 12:17:21.625
STEP: updating the test event 01/30/23 12:17:21.627
STEP: getting the test event 01/30/23 12:17:21.63
STEP: deleting the test event 01/30/23 12:17:21.631
STEP: listing events in all namespaces 01/30/23 12:17:21.633
STEP: listing events in test namespace 01/30/23 12:17:21.656
[AfterEach] [sig-instrumentation] Events API
  test/e2e/framework/node/init/init.go:32
Jan 30 12:17:21.660: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-instrumentation] Events API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-instrumentation] Events API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-instrumentation] Events API
  tear down framework | framework.go:193
STEP: Destroying namespace "events-6277" for this suite. 01/30/23 12:17:21.662
------------------------------
• [0.068 seconds]
[sig-instrumentation] Events API
test/e2e/instrumentation/common/framework.go:23
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 12:17:21.596
    Jan 30 12:17:21.596: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename events 01/30/23 12:17:21.597
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:17:21.603
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:17:21.605
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/instrumentation/events.go:84
    [It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
      test/e2e/instrumentation/events.go:98
    STEP: creating a test event 01/30/23 12:17:21.607
    STEP: listing events in all namespaces 01/30/23 12:17:21.609
    STEP: listing events in test namespace 01/30/23 12:17:21.616
    STEP: listing events with field selection filtering on source 01/30/23 12:17:21.617
    STEP: listing events with field selection filtering on reportingController 01/30/23 12:17:21.619
    STEP: getting the test event 01/30/23 12:17:21.621
    STEP: patching the test event 01/30/23 12:17:21.622
    STEP: getting the test event 01/30/23 12:17:21.625
    STEP: updating the test event 01/30/23 12:17:21.627
    STEP: getting the test event 01/30/23 12:17:21.63
    STEP: deleting the test event 01/30/23 12:17:21.631
    STEP: listing events in all namespaces 01/30/23 12:17:21.633
    STEP: listing events in test namespace 01/30/23 12:17:21.656
    [AfterEach] [sig-instrumentation] Events API
      test/e2e/framework/node/init/init.go:32
    Jan 30 12:17:21.660: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      tear down framework | framework.go:193
    STEP: Destroying namespace "events-6277" for this suite. 01/30/23 12:17:21.662
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:309
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 12:17:21.665
Jan 30 12:17:21.665: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename crd-publish-openapi 01/30/23 12:17:21.666
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:17:21.672
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:17:21.674
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:309
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation 01/30/23 12:17:21.675
Jan 30 12:17:21.676: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation 01/30/23 12:17:29.443
Jan 30 12:17:29.444: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
Jan 30 12:17:31.762: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 30 12:17:39.252: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-3372" for this suite. 01/30/23 12:17:39.259
------------------------------
• [SLOW TEST] [17.597 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:309

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 12:17:21.665
    Jan 30 12:17:21.665: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename crd-publish-openapi 01/30/23 12:17:21.666
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:17:21.672
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:17:21.674
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for multiple CRDs of same group but different versions [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:309
    STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation 01/30/23 12:17:21.675
    Jan 30 12:17:21.676: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation 01/30/23 12:17:29.443
    Jan 30 12:17:29.444: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    Jan 30 12:17:31.762: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 30 12:17:39.252: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-3372" for this suite. 01/30/23 12:17:39.259
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 12:17:39.263
Jan 30 12:17:39.263: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename replicaset 01/30/23 12:17:39.264
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:17:39.271
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:17:39.272
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176
STEP: Create a Replicaset 01/30/23 12:17:39.277
STEP: Verify that the required pods have come up. 01/30/23 12:17:39.28
Jan 30 12:17:39.282: INFO: Pod name sample-pod: Found 0 pods out of 1
Jan 30 12:17:44.285: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 01/30/23 12:17:44.285
STEP: Getting /status 01/30/23 12:17:44.285
Jan 30 12:17:44.288: INFO: Replicaset test-rs has Conditions: []
STEP: updating the Replicaset Status 01/30/23 12:17:44.288
Jan 30 12:17:44.293: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the ReplicaSet status to be updated 01/30/23 12:17:44.293
Jan 30 12:17:44.295: INFO: Observed &ReplicaSet event: ADDED
Jan 30 12:17:44.295: INFO: Observed &ReplicaSet event: MODIFIED
Jan 30 12:17:44.295: INFO: Observed &ReplicaSet event: MODIFIED
Jan 30 12:17:44.295: INFO: Observed &ReplicaSet event: MODIFIED
Jan 30 12:17:44.295: INFO: Found replicaset test-rs in namespace replicaset-6652 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Jan 30 12:17:44.295: INFO: Replicaset test-rs has an updated status
STEP: patching the Replicaset Status 01/30/23 12:17:44.295
Jan 30 12:17:44.295: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Jan 30 12:17:44.300: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Replicaset status to be patched 01/30/23 12:17:44.3
Jan 30 12:17:44.302: INFO: Observed &ReplicaSet event: ADDED
Jan 30 12:17:44.302: INFO: Observed &ReplicaSet event: MODIFIED
Jan 30 12:17:44.302: INFO: Observed &ReplicaSet event: MODIFIED
Jan 30 12:17:44.302: INFO: Observed &ReplicaSet event: MODIFIED
Jan 30 12:17:44.302: INFO: Observed replicaset test-rs in namespace replicaset-6652 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Jan 30 12:17:44.302: INFO: Observed &ReplicaSet event: MODIFIED
Jan 30 12:17:44.302: INFO: Found replicaset test-rs in namespace replicaset-6652 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
Jan 30 12:17:44.302: INFO: Replicaset test-rs has a patched status
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Jan 30 12:17:44.302: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-6652" for this suite. 01/30/23 12:17:44.304
------------------------------
• [SLOW TEST] [5.043 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 12:17:39.263
    Jan 30 12:17:39.263: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename replicaset 01/30/23 12:17:39.264
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:17:39.271
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:17:39.272
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] should validate Replicaset Status endpoints [Conformance]
      test/e2e/apps/replica_set.go:176
    STEP: Create a Replicaset 01/30/23 12:17:39.277
    STEP: Verify that the required pods have come up. 01/30/23 12:17:39.28
    Jan 30 12:17:39.282: INFO: Pod name sample-pod: Found 0 pods out of 1
    Jan 30 12:17:44.285: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 01/30/23 12:17:44.285
    STEP: Getting /status 01/30/23 12:17:44.285
    Jan 30 12:17:44.288: INFO: Replicaset test-rs has Conditions: []
    STEP: updating the Replicaset Status 01/30/23 12:17:44.288
    Jan 30 12:17:44.293: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the ReplicaSet status to be updated 01/30/23 12:17:44.293
    Jan 30 12:17:44.295: INFO: Observed &ReplicaSet event: ADDED
    Jan 30 12:17:44.295: INFO: Observed &ReplicaSet event: MODIFIED
    Jan 30 12:17:44.295: INFO: Observed &ReplicaSet event: MODIFIED
    Jan 30 12:17:44.295: INFO: Observed &ReplicaSet event: MODIFIED
    Jan 30 12:17:44.295: INFO: Found replicaset test-rs in namespace replicaset-6652 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Jan 30 12:17:44.295: INFO: Replicaset test-rs has an updated status
    STEP: patching the Replicaset Status 01/30/23 12:17:44.295
    Jan 30 12:17:44.295: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Jan 30 12:17:44.300: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Replicaset status to be patched 01/30/23 12:17:44.3
    Jan 30 12:17:44.302: INFO: Observed &ReplicaSet event: ADDED
    Jan 30 12:17:44.302: INFO: Observed &ReplicaSet event: MODIFIED
    Jan 30 12:17:44.302: INFO: Observed &ReplicaSet event: MODIFIED
    Jan 30 12:17:44.302: INFO: Observed &ReplicaSet event: MODIFIED
    Jan 30 12:17:44.302: INFO: Observed replicaset test-rs in namespace replicaset-6652 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Jan 30 12:17:44.302: INFO: Observed &ReplicaSet event: MODIFIED
    Jan 30 12:17:44.302: INFO: Found replicaset test-rs in namespace replicaset-6652 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
    Jan 30 12:17:44.302: INFO: Replicaset test-rs has a patched status
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Jan 30 12:17:44.302: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-6652" for this suite. 01/30/23 12:17:44.304
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:249
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 12:17:44.307
Jan 30 12:17:44.307: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename projected 01/30/23 12:17:44.307
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:17:44.313
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:17:44.315
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:249
STEP: Creating a pod to test downward API volume plugin 01/30/23 12:17:44.317
Jan 30 12:17:44.321: INFO: Waiting up to 5m0s for pod "downwardapi-volume-75b4a9b6-bb3a-402e-baed-92232a6d8c21" in namespace "projected-2087" to be "Succeeded or Failed"
Jan 30 12:17:44.323: INFO: Pod "downwardapi-volume-75b4a9b6-bb3a-402e-baed-92232a6d8c21": Phase="Pending", Reason="", readiness=false. Elapsed: 1.629195ms
Jan 30 12:17:46.326: INFO: Pod "downwardapi-volume-75b4a9b6-bb3a-402e-baed-92232a6d8c21": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004605325s
Jan 30 12:17:48.327: INFO: Pod "downwardapi-volume-75b4a9b6-bb3a-402e-baed-92232a6d8c21": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005246376s
STEP: Saw pod success 01/30/23 12:17:48.327
Jan 30 12:17:48.327: INFO: Pod "downwardapi-volume-75b4a9b6-bb3a-402e-baed-92232a6d8c21" satisfied condition "Succeeded or Failed"
Jan 30 12:17:48.329: INFO: Trying to get logs from node pubt2-nks-for-dev1.dg.163.org pod downwardapi-volume-75b4a9b6-bb3a-402e-baed-92232a6d8c21 container client-container: <nil>
STEP: delete the pod 01/30/23 12:17:48.333
Jan 30 12:17:48.338: INFO: Waiting for pod downwardapi-volume-75b4a9b6-bb3a-402e-baed-92232a6d8c21 to disappear
Jan 30 12:17:48.339: INFO: Pod downwardapi-volume-75b4a9b6-bb3a-402e-baed-92232a6d8c21 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Jan 30 12:17:48.339: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-2087" for this suite. 01/30/23 12:17:48.341
------------------------------
• [4.037 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:249

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 12:17:44.307
    Jan 30 12:17:44.307: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename projected 01/30/23 12:17:44.307
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:17:44.313
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:17:44.315
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:249
    STEP: Creating a pod to test downward API volume plugin 01/30/23 12:17:44.317
    Jan 30 12:17:44.321: INFO: Waiting up to 5m0s for pod "downwardapi-volume-75b4a9b6-bb3a-402e-baed-92232a6d8c21" in namespace "projected-2087" to be "Succeeded or Failed"
    Jan 30 12:17:44.323: INFO: Pod "downwardapi-volume-75b4a9b6-bb3a-402e-baed-92232a6d8c21": Phase="Pending", Reason="", readiness=false. Elapsed: 1.629195ms
    Jan 30 12:17:46.326: INFO: Pod "downwardapi-volume-75b4a9b6-bb3a-402e-baed-92232a6d8c21": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004605325s
    Jan 30 12:17:48.327: INFO: Pod "downwardapi-volume-75b4a9b6-bb3a-402e-baed-92232a6d8c21": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005246376s
    STEP: Saw pod success 01/30/23 12:17:48.327
    Jan 30 12:17:48.327: INFO: Pod "downwardapi-volume-75b4a9b6-bb3a-402e-baed-92232a6d8c21" satisfied condition "Succeeded or Failed"
    Jan 30 12:17:48.329: INFO: Trying to get logs from node pubt2-nks-for-dev1.dg.163.org pod downwardapi-volume-75b4a9b6-bb3a-402e-baed-92232a6d8c21 container client-container: <nil>
    STEP: delete the pod 01/30/23 12:17:48.333
    Jan 30 12:17:48.338: INFO: Waiting for pod downwardapi-volume-75b4a9b6-bb3a-402e-baed-92232a6d8c21 to disappear
    Jan 30 12:17:48.339: INFO: Pod downwardapi-volume-75b4a9b6-bb3a-402e-baed-92232a6d8c21 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Jan 30 12:17:48.339: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-2087" for this suite. 01/30/23 12:17:48.341
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Downward API
  should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:267
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 12:17:48.344
Jan 30 12:17:48.344: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename downward-api 01/30/23 12:17:48.345
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:17:48.352
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:17:48.354
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:267
STEP: Creating a pod to test downward api env vars 01/30/23 12:17:48.356
Jan 30 12:17:48.360: INFO: Waiting up to 5m0s for pod "downward-api-9e5a33ad-742b-4664-95bd-f11643bc8e49" in namespace "downward-api-5885" to be "Succeeded or Failed"
Jan 30 12:17:48.362: INFO: Pod "downward-api-9e5a33ad-742b-4664-95bd-f11643bc8e49": Phase="Pending", Reason="", readiness=false. Elapsed: 1.612587ms
Jan 30 12:17:50.364: INFO: Pod "downward-api-9e5a33ad-742b-4664-95bd-f11643bc8e49": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003827678s
Jan 30 12:17:52.365: INFO: Pod "downward-api-9e5a33ad-742b-4664-95bd-f11643bc8e49": Phase="Pending", Reason="", readiness=false. Elapsed: 4.005142693s
Jan 30 12:17:54.367: INFO: Pod "downward-api-9e5a33ad-742b-4664-95bd-f11643bc8e49": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.006486928s
STEP: Saw pod success 01/30/23 12:17:54.367
Jan 30 12:17:54.367: INFO: Pod "downward-api-9e5a33ad-742b-4664-95bd-f11643bc8e49" satisfied condition "Succeeded or Failed"
Jan 30 12:17:54.369: INFO: Trying to get logs from node pubt2-nks-for-dev1.dg.163.org pod downward-api-9e5a33ad-742b-4664-95bd-f11643bc8e49 container dapi-container: <nil>
STEP: delete the pod 01/30/23 12:17:54.375
Jan 30 12:17:54.379: INFO: Waiting for pod downward-api-9e5a33ad-742b-4664-95bd-f11643bc8e49 to disappear
Jan 30 12:17:54.381: INFO: Pod downward-api-9e5a33ad-742b-4664-95bd-f11643bc8e49 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Jan 30 12:17:54.381: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-5885" for this suite. 01/30/23 12:17:54.383
------------------------------
• [SLOW TEST] [6.041 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:267

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 12:17:48.344
    Jan 30 12:17:48.344: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename downward-api 01/30/23 12:17:48.345
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:17:48.352
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:17:48.354
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide pod UID as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:267
    STEP: Creating a pod to test downward api env vars 01/30/23 12:17:48.356
    Jan 30 12:17:48.360: INFO: Waiting up to 5m0s for pod "downward-api-9e5a33ad-742b-4664-95bd-f11643bc8e49" in namespace "downward-api-5885" to be "Succeeded or Failed"
    Jan 30 12:17:48.362: INFO: Pod "downward-api-9e5a33ad-742b-4664-95bd-f11643bc8e49": Phase="Pending", Reason="", readiness=false. Elapsed: 1.612587ms
    Jan 30 12:17:50.364: INFO: Pod "downward-api-9e5a33ad-742b-4664-95bd-f11643bc8e49": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003827678s
    Jan 30 12:17:52.365: INFO: Pod "downward-api-9e5a33ad-742b-4664-95bd-f11643bc8e49": Phase="Pending", Reason="", readiness=false. Elapsed: 4.005142693s
    Jan 30 12:17:54.367: INFO: Pod "downward-api-9e5a33ad-742b-4664-95bd-f11643bc8e49": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.006486928s
    STEP: Saw pod success 01/30/23 12:17:54.367
    Jan 30 12:17:54.367: INFO: Pod "downward-api-9e5a33ad-742b-4664-95bd-f11643bc8e49" satisfied condition "Succeeded or Failed"
    Jan 30 12:17:54.369: INFO: Trying to get logs from node pubt2-nks-for-dev1.dg.163.org pod downward-api-9e5a33ad-742b-4664-95bd-f11643bc8e49 container dapi-container: <nil>
    STEP: delete the pod 01/30/23 12:17:54.375
    Jan 30 12:17:54.379: INFO: Waiting for pod downward-api-9e5a33ad-742b-4664-95bd-f11643bc8e49 to disappear
    Jan 30 12:17:54.381: INFO: Pod downward-api-9e5a33ad-742b-4664-95bd-f11643bc8e49 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Jan 30 12:17:54.381: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-5885" for this suite. 01/30/23 12:17:54.383
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-scheduling] LimitRange
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:61
[BeforeEach] [sig-scheduling] LimitRange
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 12:17:54.386
Jan 30 12:17:54.386: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename limitrange 01/30/23 12:17:54.386
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:17:54.393
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:17:54.395
[BeforeEach] [sig-scheduling] LimitRange
  test/e2e/framework/metrics/init/init.go:31
[It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:61
STEP: Creating a LimitRange 01/30/23 12:17:54.397
STEP: Setting up watch 01/30/23 12:17:54.397
STEP: Submitting a LimitRange 01/30/23 12:17:54.498
STEP: Verifying LimitRange creation was observed 01/30/23 12:17:54.501
STEP: Fetching the LimitRange to ensure it has proper values 01/30/23 12:17:54.501
Jan 30 12:17:54.502: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Jan 30 12:17:54.502: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with no resource requirements 01/30/23 12:17:54.502
STEP: Ensuring Pod has resource requirements applied from LimitRange 01/30/23 12:17:54.505
Jan 30 12:17:54.507: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Jan 30 12:17:54.507: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with partial resource requirements 01/30/23 12:17:54.507
STEP: Ensuring Pod has merged resource requirements applied from LimitRange 01/30/23 12:17:54.509
Jan 30 12:17:54.511: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
Jan 30 12:17:54.511: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Failing to create a Pod with less than min resources 01/30/23 12:17:54.511
STEP: Failing to create a Pod with more than max resources 01/30/23 12:17:54.513
STEP: Updating a LimitRange 01/30/23 12:17:54.515
STEP: Verifying LimitRange updating is effective 01/30/23 12:17:54.517
STEP: Creating a Pod with less than former min resources 01/30/23 12:17:56.52
STEP: Failing to create a Pod with more than max resources 01/30/23 12:17:56.523
STEP: Deleting a LimitRange 01/30/23 12:17:56.525
STEP: Verifying the LimitRange was deleted 01/30/23 12:17:56.527
Jan 30 12:18:01.531: INFO: limitRange is already deleted
STEP: Creating a Pod with more than former max resources 01/30/23 12:18:01.531
[AfterEach] [sig-scheduling] LimitRange
  test/e2e/framework/node/init/init.go:32
Jan 30 12:18:01.536: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  tear down framework | framework.go:193
STEP: Destroying namespace "limitrange-7925" for this suite. 01/30/23 12:18:01.539
------------------------------
• [SLOW TEST] [7.155 seconds]
[sig-scheduling] LimitRange
test/e2e/scheduling/framework.go:40
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:61

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] LimitRange
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 12:17:54.386
    Jan 30 12:17:54.386: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename limitrange 01/30/23 12:17:54.386
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:17:54.393
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:17:54.395
    [BeforeEach] [sig-scheduling] LimitRange
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
      test/e2e/scheduling/limit_range.go:61
    STEP: Creating a LimitRange 01/30/23 12:17:54.397
    STEP: Setting up watch 01/30/23 12:17:54.397
    STEP: Submitting a LimitRange 01/30/23 12:17:54.498
    STEP: Verifying LimitRange creation was observed 01/30/23 12:17:54.501
    STEP: Fetching the LimitRange to ensure it has proper values 01/30/23 12:17:54.501
    Jan 30 12:17:54.502: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
    Jan 30 12:17:54.502: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Creating a Pod with no resource requirements 01/30/23 12:17:54.502
    STEP: Ensuring Pod has resource requirements applied from LimitRange 01/30/23 12:17:54.505
    Jan 30 12:17:54.507: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
    Jan 30 12:17:54.507: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Creating a Pod with partial resource requirements 01/30/23 12:17:54.507
    STEP: Ensuring Pod has merged resource requirements applied from LimitRange 01/30/23 12:17:54.509
    Jan 30 12:17:54.511: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
    Jan 30 12:17:54.511: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Failing to create a Pod with less than min resources 01/30/23 12:17:54.511
    STEP: Failing to create a Pod with more than max resources 01/30/23 12:17:54.513
    STEP: Updating a LimitRange 01/30/23 12:17:54.515
    STEP: Verifying LimitRange updating is effective 01/30/23 12:17:54.517
    STEP: Creating a Pod with less than former min resources 01/30/23 12:17:56.52
    STEP: Failing to create a Pod with more than max resources 01/30/23 12:17:56.523
    STEP: Deleting a LimitRange 01/30/23 12:17:56.525
    STEP: Verifying the LimitRange was deleted 01/30/23 12:17:56.527
    Jan 30 12:18:01.531: INFO: limitRange is already deleted
    STEP: Creating a Pod with more than former max resources 01/30/23 12:18:01.531
    [AfterEach] [sig-scheduling] LimitRange
      test/e2e/framework/node/init/init.go:32
    Jan 30 12:18:01.536: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      tear down framework | framework.go:193
    STEP: Destroying namespace "limitrange-7925" for this suite. 01/30/23 12:18:01.539
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:197
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 12:18:01.542
Jan 30 12:18:01.542: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename webhook 01/30/23 12:18:01.543
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:18:01.559
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:18:01.561
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 01/30/23 12:18:01.569
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/30/23 12:18:01.921
STEP: Deploying the webhook pod 01/30/23 12:18:01.926
STEP: Wait for the deployment to be ready 01/30/23 12:18:01.933
Jan 30 12:18:01.936: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 01/30/23 12:18:03.943
STEP: Verifying the service has paired with the endpoint 01/30/23 12:18:03.947
Jan 30 12:18:04.948: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:197
STEP: Registering the webhook via the AdmissionRegistration API 01/30/23 12:18:04.951
STEP: create a pod that should be denied by the webhook 01/30/23 12:18:04.96
STEP: create a pod that causes the webhook to hang 01/30/23 12:18:04.968
STEP: create a configmap that should be denied by the webhook 01/30/23 12:18:14.974
STEP: create a configmap that should be admitted by the webhook 01/30/23 12:18:14.981
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook 01/30/23 12:18:14.986
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook 01/30/23 12:18:14.991
STEP: create a namespace that bypass the webhook 01/30/23 12:18:14.994
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace 01/30/23 12:18:14.998
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 30 12:18:15.009: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-3796" for this suite. 01/30/23 12:18:15.023
STEP: Destroying namespace "webhook-3796-markers" for this suite. 01/30/23 12:18:15.025
------------------------------
• [SLOW TEST] [13.486 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:197

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 12:18:01.542
    Jan 30 12:18:01.542: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename webhook 01/30/23 12:18:01.543
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:18:01.559
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:18:01.561
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 01/30/23 12:18:01.569
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/30/23 12:18:01.921
    STEP: Deploying the webhook pod 01/30/23 12:18:01.926
    STEP: Wait for the deployment to be ready 01/30/23 12:18:01.933
    Jan 30 12:18:01.936: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 01/30/23 12:18:03.943
    STEP: Verifying the service has paired with the endpoint 01/30/23 12:18:03.947
    Jan 30 12:18:04.948: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny pod and configmap creation [Conformance]
      test/e2e/apimachinery/webhook.go:197
    STEP: Registering the webhook via the AdmissionRegistration API 01/30/23 12:18:04.951
    STEP: create a pod that should be denied by the webhook 01/30/23 12:18:04.96
    STEP: create a pod that causes the webhook to hang 01/30/23 12:18:04.968
    STEP: create a configmap that should be denied by the webhook 01/30/23 12:18:14.974
    STEP: create a configmap that should be admitted by the webhook 01/30/23 12:18:14.981
    STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook 01/30/23 12:18:14.986
    STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook 01/30/23 12:18:14.991
    STEP: create a namespace that bypass the webhook 01/30/23 12:18:14.994
    STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace 01/30/23 12:18:14.998
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 30 12:18:15.009: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-3796" for this suite. 01/30/23 12:18:15.023
    STEP: Destroying namespace "webhook-3796-markers" for this suite. 01/30/23 12:18:15.025
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates
  should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53
[BeforeEach] [sig-node] PodTemplates
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 12:18:15.03
Jan 30 12:18:15.030: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename podtemplate 01/30/23 12:18:15.031
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:18:15.037
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:18:15.039
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:31
[It] should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53
[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/node/init/init.go:32
Jan 30 12:18:15.055: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] PodTemplates
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] PodTemplates
  tear down framework | framework.go:193
STEP: Destroying namespace "podtemplate-9039" for this suite. 01/30/23 12:18:15.057
------------------------------
• [0.030 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 12:18:15.03
    Jan 30 12:18:15.030: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename podtemplate 01/30/23 12:18:15.031
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:18:15.037
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:18:15.039
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:31
    [It] should run the lifecycle of PodTemplates [Conformance]
      test/e2e/common/node/podtemplates.go:53
    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/node/init/init.go:32
    Jan 30 12:18:15.055: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] PodTemplates
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] PodTemplates
      tear down framework | framework.go:193
    STEP: Destroying namespace "podtemplate-9039" for this suite. 01/30/23 12:18:15.057
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-instrumentation] Events
  should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57
[BeforeEach] [sig-instrumentation] Events
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 12:18:15.06
Jan 30 12:18:15.060: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename events 01/30/23 12:18:15.06
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:18:15.066
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:18:15.067
[BeforeEach] [sig-instrumentation] Events
  test/e2e/framework/metrics/init/init.go:31
[It] should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57
STEP: creating a test event 01/30/23 12:18:15.069
STEP: listing all events in all namespaces 01/30/23 12:18:15.072
STEP: patching the test event 01/30/23 12:18:15.079
STEP: fetching the test event 01/30/23 12:18:15.082
STEP: updating the test event 01/30/23 12:18:15.083
STEP: getting the test event 01/30/23 12:18:15.087
STEP: deleting the test event 01/30/23 12:18:15.089
STEP: listing all events in all namespaces 01/30/23 12:18:15.091
[AfterEach] [sig-instrumentation] Events
  test/e2e/framework/node/init/init.go:32
Jan 30 12:18:15.098: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-instrumentation] Events
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-instrumentation] Events
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-instrumentation] Events
  tear down framework | framework.go:193
STEP: Destroying namespace "events-7509" for this suite. 01/30/23 12:18:15.1
------------------------------
• [0.043 seconds]
[sig-instrumentation] Events
test/e2e/instrumentation/common/framework.go:23
  should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 12:18:15.06
    Jan 30 12:18:15.060: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename events 01/30/23 12:18:15.06
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:18:15.066
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:18:15.067
    [BeforeEach] [sig-instrumentation] Events
      test/e2e/framework/metrics/init/init.go:31
    [It] should manage the lifecycle of an event [Conformance]
      test/e2e/instrumentation/core_events.go:57
    STEP: creating a test event 01/30/23 12:18:15.069
    STEP: listing all events in all namespaces 01/30/23 12:18:15.072
    STEP: patching the test event 01/30/23 12:18:15.079
    STEP: fetching the test event 01/30/23 12:18:15.082
    STEP: updating the test event 01/30/23 12:18:15.083
    STEP: getting the test event 01/30/23 12:18:15.087
    STEP: deleting the test event 01/30/23 12:18:15.089
    STEP: listing all events in all namespaces 01/30/23 12:18:15.091
    [AfterEach] [sig-instrumentation] Events
      test/e2e/framework/node/init/init.go:32
    Jan 30 12:18:15.098: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-instrumentation] Events
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-instrumentation] Events
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-instrumentation] Events
      tear down framework | framework.go:193
    STEP: Destroying namespace "events-7509" for this suite. 01/30/23 12:18:15.1
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers
  should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:73
[BeforeEach] [sig-node] Containers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 12:18:15.103
Jan 30 12:18:15.103: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename containers 01/30/23 12:18:15.103
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:18:15.109
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:18:15.111
[BeforeEach] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:73
STEP: Creating a pod to test override command 01/30/23 12:18:15.112
Jan 30 12:18:15.116: INFO: Waiting up to 5m0s for pod "client-containers-0618ebaa-9153-496b-994a-0d52286a3023" in namespace "containers-7694" to be "Succeeded or Failed"
Jan 30 12:18:15.118: INFO: Pod "client-containers-0618ebaa-9153-496b-994a-0d52286a3023": Phase="Pending", Reason="", readiness=false. Elapsed: 1.558388ms
Jan 30 12:18:17.120: INFO: Pod "client-containers-0618ebaa-9153-496b-994a-0d52286a3023": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003663766s
Jan 30 12:18:19.121: INFO: Pod "client-containers-0618ebaa-9153-496b-994a-0d52286a3023": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00478567s
STEP: Saw pod success 01/30/23 12:18:19.121
Jan 30 12:18:19.121: INFO: Pod "client-containers-0618ebaa-9153-496b-994a-0d52286a3023" satisfied condition "Succeeded or Failed"
Jan 30 12:18:19.123: INFO: Trying to get logs from node pubt2-nks-for-dev1.dg.163.org pod client-containers-0618ebaa-9153-496b-994a-0d52286a3023 container agnhost-container: <nil>
STEP: delete the pod 01/30/23 12:18:19.127
Jan 30 12:18:19.132: INFO: Waiting for pod client-containers-0618ebaa-9153-496b-994a-0d52286a3023 to disappear
Jan 30 12:18:19.133: INFO: Pod client-containers-0618ebaa-9153-496b-994a-0d52286a3023 no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/node/init/init.go:32
Jan 30 12:18:19.134: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Containers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Containers
  tear down framework | framework.go:193
STEP: Destroying namespace "containers-7694" for this suite. 01/30/23 12:18:19.136
------------------------------
• [4.035 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:73

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 12:18:15.103
    Jan 30 12:18:15.103: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename containers 01/30/23 12:18:15.103
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:18:15.109
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:18:15.111
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:73
    STEP: Creating a pod to test override command 01/30/23 12:18:15.112
    Jan 30 12:18:15.116: INFO: Waiting up to 5m0s for pod "client-containers-0618ebaa-9153-496b-994a-0d52286a3023" in namespace "containers-7694" to be "Succeeded or Failed"
    Jan 30 12:18:15.118: INFO: Pod "client-containers-0618ebaa-9153-496b-994a-0d52286a3023": Phase="Pending", Reason="", readiness=false. Elapsed: 1.558388ms
    Jan 30 12:18:17.120: INFO: Pod "client-containers-0618ebaa-9153-496b-994a-0d52286a3023": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003663766s
    Jan 30 12:18:19.121: INFO: Pod "client-containers-0618ebaa-9153-496b-994a-0d52286a3023": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00478567s
    STEP: Saw pod success 01/30/23 12:18:19.121
    Jan 30 12:18:19.121: INFO: Pod "client-containers-0618ebaa-9153-496b-994a-0d52286a3023" satisfied condition "Succeeded or Failed"
    Jan 30 12:18:19.123: INFO: Trying to get logs from node pubt2-nks-for-dev1.dg.163.org pod client-containers-0618ebaa-9153-496b-994a-0d52286a3023 container agnhost-container: <nil>
    STEP: delete the pod 01/30/23 12:18:19.127
    Jan 30 12:18:19.132: INFO: Waiting for pod client-containers-0618ebaa-9153-496b-994a-0d52286a3023 to disappear
    Jan 30 12:18:19.133: INFO: Pod client-containers-0618ebaa-9153-496b-994a-0d52286a3023 no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/node/init/init.go:32
    Jan 30 12:18:19.134: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Containers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Containers
      tear down framework | framework.go:193
    STEP: Destroying namespace "containers-7694" for this suite. 01/30/23 12:18:19.136
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:53
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 12:18:19.138
Jan 30 12:18:19.138: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename projected 01/30/23 12:18:19.139
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:18:19.145
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:18:19.147
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:53
STEP: Creating a pod to test downward API volume plugin 01/30/23 12:18:19.149
Jan 30 12:18:19.153: INFO: Waiting up to 5m0s for pod "downwardapi-volume-45c30163-49cd-474f-814c-15d14a562c63" in namespace "projected-6018" to be "Succeeded or Failed"
Jan 30 12:18:19.154: INFO: Pod "downwardapi-volume-45c30163-49cd-474f-814c-15d14a562c63": Phase="Pending", Reason="", readiness=false. Elapsed: 1.637947ms
Jan 30 12:18:21.157: INFO: Pod "downwardapi-volume-45c30163-49cd-474f-814c-15d14a562c63": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00466799s
Jan 30 12:18:23.162: INFO: Pod "downwardapi-volume-45c30163-49cd-474f-814c-15d14a562c63": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009632998s
STEP: Saw pod success 01/30/23 12:18:23.162
Jan 30 12:18:23.162: INFO: Pod "downwardapi-volume-45c30163-49cd-474f-814c-15d14a562c63" satisfied condition "Succeeded or Failed"
Jan 30 12:18:23.164: INFO: Trying to get logs from node pubt2-nks-for-dev1.dg.163.org pod downwardapi-volume-45c30163-49cd-474f-814c-15d14a562c63 container client-container: <nil>
STEP: delete the pod 01/30/23 12:18:23.169
Jan 30 12:18:23.173: INFO: Waiting for pod downwardapi-volume-45c30163-49cd-474f-814c-15d14a562c63 to disappear
Jan 30 12:18:23.174: INFO: Pod downwardapi-volume-45c30163-49cd-474f-814c-15d14a562c63 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Jan 30 12:18:23.175: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-6018" for this suite. 01/30/23 12:18:23.177
------------------------------
• [4.040 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 12:18:19.138
    Jan 30 12:18:19.138: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename projected 01/30/23 12:18:19.139
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:18:19.145
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:18:19.147
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide podname only [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:53
    STEP: Creating a pod to test downward API volume plugin 01/30/23 12:18:19.149
    Jan 30 12:18:19.153: INFO: Waiting up to 5m0s for pod "downwardapi-volume-45c30163-49cd-474f-814c-15d14a562c63" in namespace "projected-6018" to be "Succeeded or Failed"
    Jan 30 12:18:19.154: INFO: Pod "downwardapi-volume-45c30163-49cd-474f-814c-15d14a562c63": Phase="Pending", Reason="", readiness=false. Elapsed: 1.637947ms
    Jan 30 12:18:21.157: INFO: Pod "downwardapi-volume-45c30163-49cd-474f-814c-15d14a562c63": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00466799s
    Jan 30 12:18:23.162: INFO: Pod "downwardapi-volume-45c30163-49cd-474f-814c-15d14a562c63": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009632998s
    STEP: Saw pod success 01/30/23 12:18:23.162
    Jan 30 12:18:23.162: INFO: Pod "downwardapi-volume-45c30163-49cd-474f-814c-15d14a562c63" satisfied condition "Succeeded or Failed"
    Jan 30 12:18:23.164: INFO: Trying to get logs from node pubt2-nks-for-dev1.dg.163.org pod downwardapi-volume-45c30163-49cd-474f-814c-15d14a562c63 container client-container: <nil>
    STEP: delete the pod 01/30/23 12:18:23.169
    Jan 30 12:18:23.173: INFO: Waiting for pod downwardapi-volume-45c30163-49cd-474f-814c-15d14a562c63 to disappear
    Jan 30 12:18:23.174: INFO: Pod downwardapi-volume-45c30163-49cd-474f-814c-15d14a562c63 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Jan 30 12:18:23.175: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-6018" for this suite. 01/30/23 12:18:23.177
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:78
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 12:18:23.179
Jan 30 12:18:23.180: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename projected 01/30/23 12:18:23.18
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:18:23.186
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:18:23.188
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:78
STEP: Creating projection with secret that has name projected-secret-test-map-99dcb478-82bd-4ea9-b9e7-e4a80649d61d 01/30/23 12:18:23.19
STEP: Creating a pod to test consume secrets 01/30/23 12:18:23.192
Jan 30 12:18:23.196: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-e9964a7e-e23b-4b85-afbe-4a6b6c8f2e25" in namespace "projected-7599" to be "Succeeded or Failed"
Jan 30 12:18:23.198: INFO: Pod "pod-projected-secrets-e9964a7e-e23b-4b85-afbe-4a6b6c8f2e25": Phase="Pending", Reason="", readiness=false. Elapsed: 1.499805ms
Jan 30 12:18:25.200: INFO: Pod "pod-projected-secrets-e9964a7e-e23b-4b85-afbe-4a6b6c8f2e25": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004139351s
Jan 30 12:18:27.202: INFO: Pod "pod-projected-secrets-e9964a7e-e23b-4b85-afbe-4a6b6c8f2e25": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005439842s
STEP: Saw pod success 01/30/23 12:18:27.202
Jan 30 12:18:27.202: INFO: Pod "pod-projected-secrets-e9964a7e-e23b-4b85-afbe-4a6b6c8f2e25" satisfied condition "Succeeded or Failed"
Jan 30 12:18:27.203: INFO: Trying to get logs from node pubt2-nks-for-dev1.dg.163.org pod pod-projected-secrets-e9964a7e-e23b-4b85-afbe-4a6b6c8f2e25 container projected-secret-volume-test: <nil>
STEP: delete the pod 01/30/23 12:18:27.208
Jan 30 12:18:27.212: INFO: Waiting for pod pod-projected-secrets-e9964a7e-e23b-4b85-afbe-4a6b6c8f2e25 to disappear
Jan 30 12:18:27.213: INFO: Pod pod-projected-secrets-e9964a7e-e23b-4b85-afbe-4a6b6c8f2e25 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Jan 30 12:18:27.213: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-7599" for this suite. 01/30/23 12:18:27.215
------------------------------
• [4.038 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:78

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 12:18:23.179
    Jan 30 12:18:23.180: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename projected 01/30/23 12:18:23.18
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:18:23.186
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:18:23.188
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:78
    STEP: Creating projection with secret that has name projected-secret-test-map-99dcb478-82bd-4ea9-b9e7-e4a80649d61d 01/30/23 12:18:23.19
    STEP: Creating a pod to test consume secrets 01/30/23 12:18:23.192
    Jan 30 12:18:23.196: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-e9964a7e-e23b-4b85-afbe-4a6b6c8f2e25" in namespace "projected-7599" to be "Succeeded or Failed"
    Jan 30 12:18:23.198: INFO: Pod "pod-projected-secrets-e9964a7e-e23b-4b85-afbe-4a6b6c8f2e25": Phase="Pending", Reason="", readiness=false. Elapsed: 1.499805ms
    Jan 30 12:18:25.200: INFO: Pod "pod-projected-secrets-e9964a7e-e23b-4b85-afbe-4a6b6c8f2e25": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004139351s
    Jan 30 12:18:27.202: INFO: Pod "pod-projected-secrets-e9964a7e-e23b-4b85-afbe-4a6b6c8f2e25": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005439842s
    STEP: Saw pod success 01/30/23 12:18:27.202
    Jan 30 12:18:27.202: INFO: Pod "pod-projected-secrets-e9964a7e-e23b-4b85-afbe-4a6b6c8f2e25" satisfied condition "Succeeded or Failed"
    Jan 30 12:18:27.203: INFO: Trying to get logs from node pubt2-nks-for-dev1.dg.163.org pod pod-projected-secrets-e9964a7e-e23b-4b85-afbe-4a6b6c8f2e25 container projected-secret-volume-test: <nil>
    STEP: delete the pod 01/30/23 12:18:27.208
    Jan 30 12:18:27.212: INFO: Waiting for pod pod-projected-secrets-e9964a7e-e23b-4b85-afbe-4a6b6c8f2e25 to disappear
    Jan 30 12:18:27.213: INFO: Pod pod-projected-secrets-e9964a7e-e23b-4b85-afbe-4a6b6c8f2e25 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Jan 30 12:18:27.213: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-7599" for this suite. 01/30/23 12:18:27.215
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:235
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 12:18:27.218
Jan 30 12:18:27.218: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename projected 01/30/23 12:18:27.219
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:18:27.224
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:18:27.226
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:235
STEP: Creating a pod to test downward API volume plugin 01/30/23 12:18:27.228
Jan 30 12:18:27.233: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3f22f44b-aa5a-4900-a91e-ac3c49f5e470" in namespace "projected-1200" to be "Succeeded or Failed"
Jan 30 12:18:27.234: INFO: Pod "downwardapi-volume-3f22f44b-aa5a-4900-a91e-ac3c49f5e470": Phase="Pending", Reason="", readiness=false. Elapsed: 1.695423ms
Jan 30 12:18:29.236: INFO: Pod "downwardapi-volume-3f22f44b-aa5a-4900-a91e-ac3c49f5e470": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003808712s
Jan 30 12:18:31.237: INFO: Pod "downwardapi-volume-3f22f44b-aa5a-4900-a91e-ac3c49f5e470": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004839878s
STEP: Saw pod success 01/30/23 12:18:31.237
Jan 30 12:18:31.237: INFO: Pod "downwardapi-volume-3f22f44b-aa5a-4900-a91e-ac3c49f5e470" satisfied condition "Succeeded or Failed"
Jan 30 12:18:31.239: INFO: Trying to get logs from node pubt2-nks-for-dev1.dg.163.org pod downwardapi-volume-3f22f44b-aa5a-4900-a91e-ac3c49f5e470 container client-container: <nil>
STEP: delete the pod 01/30/23 12:18:31.243
Jan 30 12:18:31.248: INFO: Waiting for pod downwardapi-volume-3f22f44b-aa5a-4900-a91e-ac3c49f5e470 to disappear
Jan 30 12:18:31.250: INFO: Pod downwardapi-volume-3f22f44b-aa5a-4900-a91e-ac3c49f5e470 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Jan 30 12:18:31.250: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-1200" for this suite. 01/30/23 12:18:31.252
------------------------------
• [4.036 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:235

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 12:18:27.218
    Jan 30 12:18:27.218: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename projected 01/30/23 12:18:27.219
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:18:27.224
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:18:27.226
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide container's memory request [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:235
    STEP: Creating a pod to test downward API volume plugin 01/30/23 12:18:27.228
    Jan 30 12:18:27.233: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3f22f44b-aa5a-4900-a91e-ac3c49f5e470" in namespace "projected-1200" to be "Succeeded or Failed"
    Jan 30 12:18:27.234: INFO: Pod "downwardapi-volume-3f22f44b-aa5a-4900-a91e-ac3c49f5e470": Phase="Pending", Reason="", readiness=false. Elapsed: 1.695423ms
    Jan 30 12:18:29.236: INFO: Pod "downwardapi-volume-3f22f44b-aa5a-4900-a91e-ac3c49f5e470": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003808712s
    Jan 30 12:18:31.237: INFO: Pod "downwardapi-volume-3f22f44b-aa5a-4900-a91e-ac3c49f5e470": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004839878s
    STEP: Saw pod success 01/30/23 12:18:31.237
    Jan 30 12:18:31.237: INFO: Pod "downwardapi-volume-3f22f44b-aa5a-4900-a91e-ac3c49f5e470" satisfied condition "Succeeded or Failed"
    Jan 30 12:18:31.239: INFO: Trying to get logs from node pubt2-nks-for-dev1.dg.163.org pod downwardapi-volume-3f22f44b-aa5a-4900-a91e-ac3c49f5e470 container client-container: <nil>
    STEP: delete the pod 01/30/23 12:18:31.243
    Jan 30 12:18:31.248: INFO: Waiting for pod downwardapi-volume-3f22f44b-aa5a-4900-a91e-ac3c49f5e470 to disappear
    Jan 30 12:18:31.250: INFO: Pod downwardapi-volume-3f22f44b-aa5a-4900-a91e-ac3c49f5e470 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Jan 30 12:18:31.250: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-1200" for this suite. 01/30/23 12:18:31.252
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:177
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 12:18:31.255
Jan 30 12:18:31.255: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename emptydir 01/30/23 12:18:31.255
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:18:31.261
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:18:31.263
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:177
STEP: Creating a pod to test emptydir 0666 on node default medium 01/30/23 12:18:31.265
Jan 30 12:18:31.269: INFO: Waiting up to 5m0s for pod "pod-254bb10d-3940-415c-9ebf-15a4b310a575" in namespace "emptydir-7499" to be "Succeeded or Failed"
Jan 30 12:18:31.271: INFO: Pod "pod-254bb10d-3940-415c-9ebf-15a4b310a575": Phase="Pending", Reason="", readiness=false. Elapsed: 1.574871ms
Jan 30 12:18:33.272: INFO: Pod "pod-254bb10d-3940-415c-9ebf-15a4b310a575": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003301253s
Jan 30 12:18:35.275: INFO: Pod "pod-254bb10d-3940-415c-9ebf-15a4b310a575": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00543068s
STEP: Saw pod success 01/30/23 12:18:35.275
Jan 30 12:18:35.275: INFO: Pod "pod-254bb10d-3940-415c-9ebf-15a4b310a575" satisfied condition "Succeeded or Failed"
Jan 30 12:18:35.276: INFO: Trying to get logs from node pubt2-nks-for-dev1.dg.163.org pod pod-254bb10d-3940-415c-9ebf-15a4b310a575 container test-container: <nil>
STEP: delete the pod 01/30/23 12:18:35.281
Jan 30 12:18:35.285: INFO: Waiting for pod pod-254bb10d-3940-415c-9ebf-15a4b310a575 to disappear
Jan 30 12:18:35.287: INFO: Pod pod-254bb10d-3940-415c-9ebf-15a4b310a575 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jan 30 12:18:35.287: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-7499" for this suite. 01/30/23 12:18:35.289
------------------------------
• [4.036 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:177

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 12:18:31.255
    Jan 30 12:18:31.255: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename emptydir 01/30/23 12:18:31.255
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:18:31.261
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:18:31.263
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:177
    STEP: Creating a pod to test emptydir 0666 on node default medium 01/30/23 12:18:31.265
    Jan 30 12:18:31.269: INFO: Waiting up to 5m0s for pod "pod-254bb10d-3940-415c-9ebf-15a4b310a575" in namespace "emptydir-7499" to be "Succeeded or Failed"
    Jan 30 12:18:31.271: INFO: Pod "pod-254bb10d-3940-415c-9ebf-15a4b310a575": Phase="Pending", Reason="", readiness=false. Elapsed: 1.574871ms
    Jan 30 12:18:33.272: INFO: Pod "pod-254bb10d-3940-415c-9ebf-15a4b310a575": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003301253s
    Jan 30 12:18:35.275: INFO: Pod "pod-254bb10d-3940-415c-9ebf-15a4b310a575": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00543068s
    STEP: Saw pod success 01/30/23 12:18:35.275
    Jan 30 12:18:35.275: INFO: Pod "pod-254bb10d-3940-415c-9ebf-15a4b310a575" satisfied condition "Succeeded or Failed"
    Jan 30 12:18:35.276: INFO: Trying to get logs from node pubt2-nks-for-dev1.dg.163.org pod pod-254bb10d-3940-415c-9ebf-15a4b310a575 container test-container: <nil>
    STEP: delete the pod 01/30/23 12:18:35.281
    Jan 30 12:18:35.285: INFO: Waiting for pod pod-254bb10d-3940-415c-9ebf-15a4b310a575 to disappear
    Jan 30 12:18:35.287: INFO: Pod pod-254bb10d-3940-415c-9ebf-15a4b310a575 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jan 30 12:18:35.287: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-7499" for this suite. 01/30/23 12:18:35.289
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:153
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 12:18:35.292
Jan 30 12:18:35.292: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename crd-publish-openapi 01/30/23 12:18:35.292
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:18:35.298
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:18:35.3
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:153
Jan 30 12:18:35.302: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 01/30/23 12:18:37.126
Jan 30 12:18:37.126: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=crd-publish-openapi-2273 --namespace=crd-publish-openapi-2273 create -f -'
Jan 30 12:18:38.267: INFO: stderr: ""
Jan 30 12:18:38.267: INFO: stdout: "e2e-test-crd-publish-openapi-9443-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Jan 30 12:18:38.267: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=crd-publish-openapi-2273 --namespace=crd-publish-openapi-2273 delete e2e-test-crd-publish-openapi-9443-crds test-cr'
Jan 30 12:18:38.331: INFO: stderr: ""
Jan 30 12:18:38.331: INFO: stdout: "e2e-test-crd-publish-openapi-9443-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Jan 30 12:18:38.331: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=crd-publish-openapi-2273 --namespace=crd-publish-openapi-2273 apply -f -'
Jan 30 12:18:38.498: INFO: stderr: ""
Jan 30 12:18:38.498: INFO: stdout: "e2e-test-crd-publish-openapi-9443-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Jan 30 12:18:38.498: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=crd-publish-openapi-2273 --namespace=crd-publish-openapi-2273 delete e2e-test-crd-publish-openapi-9443-crds test-cr'
Jan 30 12:18:38.559: INFO: stderr: ""
Jan 30 12:18:38.559: INFO: stdout: "e2e-test-crd-publish-openapi-9443-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema 01/30/23 12:18:38.559
Jan 30 12:18:38.559: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=crd-publish-openapi-2273 explain e2e-test-crd-publish-openapi-9443-crds'
Jan 30 12:18:38.722: INFO: stderr: ""
Jan 30 12:18:38.722: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-9443-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 30 12:18:40.524: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-2273" for this suite. 01/30/23 12:18:40.531
------------------------------
• [SLOW TEST] [5.241 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:153

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 12:18:35.292
    Jan 30 12:18:35.292: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename crd-publish-openapi 01/30/23 12:18:35.292
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:18:35.298
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:18:35.3
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for CRD without validation schema [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:153
    Jan 30 12:18:35.302: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 01/30/23 12:18:37.126
    Jan 30 12:18:37.126: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=crd-publish-openapi-2273 --namespace=crd-publish-openapi-2273 create -f -'
    Jan 30 12:18:38.267: INFO: stderr: ""
    Jan 30 12:18:38.267: INFO: stdout: "e2e-test-crd-publish-openapi-9443-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
    Jan 30 12:18:38.267: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=crd-publish-openapi-2273 --namespace=crd-publish-openapi-2273 delete e2e-test-crd-publish-openapi-9443-crds test-cr'
    Jan 30 12:18:38.331: INFO: stderr: ""
    Jan 30 12:18:38.331: INFO: stdout: "e2e-test-crd-publish-openapi-9443-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
    Jan 30 12:18:38.331: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=crd-publish-openapi-2273 --namespace=crd-publish-openapi-2273 apply -f -'
    Jan 30 12:18:38.498: INFO: stderr: ""
    Jan 30 12:18:38.498: INFO: stdout: "e2e-test-crd-publish-openapi-9443-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
    Jan 30 12:18:38.498: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=crd-publish-openapi-2273 --namespace=crd-publish-openapi-2273 delete e2e-test-crd-publish-openapi-9443-crds test-cr'
    Jan 30 12:18:38.559: INFO: stderr: ""
    Jan 30 12:18:38.559: INFO: stdout: "e2e-test-crd-publish-openapi-9443-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR without validation schema 01/30/23 12:18:38.559
    Jan 30 12:18:38.559: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=crd-publish-openapi-2273 explain e2e-test-crd-publish-openapi-9443-crds'
    Jan 30 12:18:38.722: INFO: stderr: ""
    Jan 30 12:18:38.722: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-9443-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 30 12:18:40.524: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-2273" for this suite. 01/30/23 12:18:40.531
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:117
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 12:18:40.534
Jan 30 12:18:40.534: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename webhook 01/30/23 12:18:40.535
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:18:40.541
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:18:40.543
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 01/30/23 12:18:40.551
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/30/23 12:18:40.88
STEP: Deploying the webhook pod 01/30/23 12:18:40.884
STEP: Wait for the deployment to be ready 01/30/23 12:18:40.889
Jan 30 12:18:40.892: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 01/30/23 12:18:42.897
STEP: Verifying the service has paired with the endpoint 01/30/23 12:18:42.901
Jan 30 12:18:43.901: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:117
STEP: fetching the /apis discovery document 01/30/23 12:18:43.904
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document 01/30/23 12:18:43.905
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document 01/30/23 12:18:43.905
STEP: fetching the /apis/admissionregistration.k8s.io discovery document 01/30/23 12:18:43.905
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document 01/30/23 12:18:43.906
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document 01/30/23 12:18:43.906
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document 01/30/23 12:18:43.907
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 30 12:18:43.907: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-750" for this suite. 01/30/23 12:18:43.922
STEP: Destroying namespace "webhook-750-markers" for this suite. 01/30/23 12:18:43.924
------------------------------
• [3.393 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:117

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 12:18:40.534
    Jan 30 12:18:40.534: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename webhook 01/30/23 12:18:40.535
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:18:40.541
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:18:40.543
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 01/30/23 12:18:40.551
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/30/23 12:18:40.88
    STEP: Deploying the webhook pod 01/30/23 12:18:40.884
    STEP: Wait for the deployment to be ready 01/30/23 12:18:40.889
    Jan 30 12:18:40.892: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 01/30/23 12:18:42.897
    STEP: Verifying the service has paired with the endpoint 01/30/23 12:18:42.901
    Jan 30 12:18:43.901: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should include webhook resources in discovery documents [Conformance]
      test/e2e/apimachinery/webhook.go:117
    STEP: fetching the /apis discovery document 01/30/23 12:18:43.904
    STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document 01/30/23 12:18:43.905
    STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document 01/30/23 12:18:43.905
    STEP: fetching the /apis/admissionregistration.k8s.io discovery document 01/30/23 12:18:43.905
    STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document 01/30/23 12:18:43.906
    STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document 01/30/23 12:18:43.906
    STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document 01/30/23 12:18:43.907
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 30 12:18:43.907: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-750" for this suite. 01/30/23 12:18:43.922
    STEP: Destroying namespace "webhook-750-markers" for this suite. 01/30/23 12:18:43.924
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with configmap pod [Conformance]
  test/e2e/storage/subpath.go:70
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 12:18:43.927
Jan 30 12:18:43.927: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename subpath 01/30/23 12:18:43.928
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:18:43.936
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:18:43.938
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 01/30/23 12:18:43.94
[It] should support subpaths with configmap pod [Conformance]
  test/e2e/storage/subpath.go:70
STEP: Creating pod pod-subpath-test-configmap-4fv7 01/30/23 12:18:43.944
STEP: Creating a pod to test atomic-volume-subpath 01/30/23 12:18:43.944
Jan 30 12:18:43.949: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-4fv7" in namespace "subpath-5849" to be "Succeeded or Failed"
Jan 30 12:18:43.950: INFO: Pod "pod-subpath-test-configmap-4fv7": Phase="Pending", Reason="", readiness=false. Elapsed: 1.591006ms
Jan 30 12:18:45.953: INFO: Pod "pod-subpath-test-configmap-4fv7": Phase="Running", Reason="", readiness=true. Elapsed: 2.004451913s
Jan 30 12:18:47.954: INFO: Pod "pod-subpath-test-configmap-4fv7": Phase="Running", Reason="", readiness=true. Elapsed: 4.005302518s
Jan 30 12:18:49.954: INFO: Pod "pod-subpath-test-configmap-4fv7": Phase="Running", Reason="", readiness=true. Elapsed: 6.005540915s
Jan 30 12:18:51.953: INFO: Pod "pod-subpath-test-configmap-4fv7": Phase="Running", Reason="", readiness=true. Elapsed: 8.004435241s
Jan 30 12:18:53.954: INFO: Pod "pod-subpath-test-configmap-4fv7": Phase="Running", Reason="", readiness=true. Elapsed: 10.005208627s
Jan 30 12:18:55.952: INFO: Pod "pod-subpath-test-configmap-4fv7": Phase="Running", Reason="", readiness=true. Elapsed: 12.003915901s
Jan 30 12:18:57.954: INFO: Pod "pod-subpath-test-configmap-4fv7": Phase="Running", Reason="", readiness=true. Elapsed: 14.005141432s
Jan 30 12:18:59.954: INFO: Pod "pod-subpath-test-configmap-4fv7": Phase="Running", Reason="", readiness=true. Elapsed: 16.005402947s
Jan 30 12:19:01.954: INFO: Pod "pod-subpath-test-configmap-4fv7": Phase="Running", Reason="", readiness=true. Elapsed: 18.005478593s
Jan 30 12:19:03.953: INFO: Pod "pod-subpath-test-configmap-4fv7": Phase="Running", Reason="", readiness=true. Elapsed: 20.004392273s
Jan 30 12:19:05.954: INFO: Pod "pod-subpath-test-configmap-4fv7": Phase="Running", Reason="", readiness=false. Elapsed: 22.00519864s
Jan 30 12:19:07.954: INFO: Pod "pod-subpath-test-configmap-4fv7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.005005845s
STEP: Saw pod success 01/30/23 12:19:07.954
Jan 30 12:19:07.954: INFO: Pod "pod-subpath-test-configmap-4fv7" satisfied condition "Succeeded or Failed"
Jan 30 12:19:07.956: INFO: Trying to get logs from node pubt2-nks-for-dev1.dg.163.org pod pod-subpath-test-configmap-4fv7 container test-container-subpath-configmap-4fv7: <nil>
STEP: delete the pod 01/30/23 12:19:07.961
Jan 30 12:19:07.965: INFO: Waiting for pod pod-subpath-test-configmap-4fv7 to disappear
Jan 30 12:19:07.967: INFO: Pod pod-subpath-test-configmap-4fv7 no longer exists
STEP: Deleting pod pod-subpath-test-configmap-4fv7 01/30/23 12:19:07.967
Jan 30 12:19:07.967: INFO: Deleting pod "pod-subpath-test-configmap-4fv7" in namespace "subpath-5849"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Jan 30 12:19:07.968: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-5849" for this suite. 01/30/23 12:19:07.97
------------------------------
• [SLOW TEST] [24.046 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with configmap pod [Conformance]
    test/e2e/storage/subpath.go:70

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 12:18:43.927
    Jan 30 12:18:43.927: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename subpath 01/30/23 12:18:43.928
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:18:43.936
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:18:43.938
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 01/30/23 12:18:43.94
    [It] should support subpaths with configmap pod [Conformance]
      test/e2e/storage/subpath.go:70
    STEP: Creating pod pod-subpath-test-configmap-4fv7 01/30/23 12:18:43.944
    STEP: Creating a pod to test atomic-volume-subpath 01/30/23 12:18:43.944
    Jan 30 12:18:43.949: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-4fv7" in namespace "subpath-5849" to be "Succeeded or Failed"
    Jan 30 12:18:43.950: INFO: Pod "pod-subpath-test-configmap-4fv7": Phase="Pending", Reason="", readiness=false. Elapsed: 1.591006ms
    Jan 30 12:18:45.953: INFO: Pod "pod-subpath-test-configmap-4fv7": Phase="Running", Reason="", readiness=true. Elapsed: 2.004451913s
    Jan 30 12:18:47.954: INFO: Pod "pod-subpath-test-configmap-4fv7": Phase="Running", Reason="", readiness=true. Elapsed: 4.005302518s
    Jan 30 12:18:49.954: INFO: Pod "pod-subpath-test-configmap-4fv7": Phase="Running", Reason="", readiness=true. Elapsed: 6.005540915s
    Jan 30 12:18:51.953: INFO: Pod "pod-subpath-test-configmap-4fv7": Phase="Running", Reason="", readiness=true. Elapsed: 8.004435241s
    Jan 30 12:18:53.954: INFO: Pod "pod-subpath-test-configmap-4fv7": Phase="Running", Reason="", readiness=true. Elapsed: 10.005208627s
    Jan 30 12:18:55.952: INFO: Pod "pod-subpath-test-configmap-4fv7": Phase="Running", Reason="", readiness=true. Elapsed: 12.003915901s
    Jan 30 12:18:57.954: INFO: Pod "pod-subpath-test-configmap-4fv7": Phase="Running", Reason="", readiness=true. Elapsed: 14.005141432s
    Jan 30 12:18:59.954: INFO: Pod "pod-subpath-test-configmap-4fv7": Phase="Running", Reason="", readiness=true. Elapsed: 16.005402947s
    Jan 30 12:19:01.954: INFO: Pod "pod-subpath-test-configmap-4fv7": Phase="Running", Reason="", readiness=true. Elapsed: 18.005478593s
    Jan 30 12:19:03.953: INFO: Pod "pod-subpath-test-configmap-4fv7": Phase="Running", Reason="", readiness=true. Elapsed: 20.004392273s
    Jan 30 12:19:05.954: INFO: Pod "pod-subpath-test-configmap-4fv7": Phase="Running", Reason="", readiness=false. Elapsed: 22.00519864s
    Jan 30 12:19:07.954: INFO: Pod "pod-subpath-test-configmap-4fv7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.005005845s
    STEP: Saw pod success 01/30/23 12:19:07.954
    Jan 30 12:19:07.954: INFO: Pod "pod-subpath-test-configmap-4fv7" satisfied condition "Succeeded or Failed"
    Jan 30 12:19:07.956: INFO: Trying to get logs from node pubt2-nks-for-dev1.dg.163.org pod pod-subpath-test-configmap-4fv7 container test-container-subpath-configmap-4fv7: <nil>
    STEP: delete the pod 01/30/23 12:19:07.961
    Jan 30 12:19:07.965: INFO: Waiting for pod pod-subpath-test-configmap-4fv7 to disappear
    Jan 30 12:19:07.967: INFO: Pod pod-subpath-test-configmap-4fv7 no longer exists
    STEP: Deleting pod pod-subpath-test-configmap-4fv7 01/30/23 12:19:07.967
    Jan 30 12:19:07.967: INFO: Deleting pod "pod-subpath-test-configmap-4fv7" in namespace "subpath-5849"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Jan 30 12:19:07.968: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-5849" for this suite. 01/30/23 12:19:07.97
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] CSIInlineVolumes
  should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
  test/e2e/storage/csi_inline.go:46
[BeforeEach] [sig-storage] CSIInlineVolumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 12:19:07.973
Jan 30 12:19:07.973: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename csiinlinevolumes 01/30/23 12:19:07.974
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:19:07.982
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:19:07.985
[BeforeEach] [sig-storage] CSIInlineVolumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
  test/e2e/storage/csi_inline.go:46
STEP: creating 01/30/23 12:19:07.987
STEP: getting 01/30/23 12:19:07.993
STEP: listing 01/30/23 12:19:07.996
STEP: deleting 01/30/23 12:19:07.997
[AfterEach] [sig-storage] CSIInlineVolumes
  test/e2e/framework/node/init/init.go:32
Jan 30 12:19:08.004: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  tear down framework | framework.go:193
STEP: Destroying namespace "csiinlinevolumes-5941" for this suite. 01/30/23 12:19:08.006
------------------------------
• [0.035 seconds]
[sig-storage] CSIInlineVolumes
test/e2e/storage/utils/framework.go:23
  should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
  test/e2e/storage/csi_inline.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] CSIInlineVolumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 12:19:07.973
    Jan 30 12:19:07.973: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename csiinlinevolumes 01/30/23 12:19:07.974
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:19:07.982
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:19:07.985
    [BeforeEach] [sig-storage] CSIInlineVolumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
      test/e2e/storage/csi_inline.go:46
    STEP: creating 01/30/23 12:19:07.987
    STEP: getting 01/30/23 12:19:07.993
    STEP: listing 01/30/23 12:19:07.996
    STEP: deleting 01/30/23 12:19:07.997
    [AfterEach] [sig-storage] CSIInlineVolumes
      test/e2e/framework/node/init/init.go:32
    Jan 30 12:19:08.004: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "csiinlinevolumes-5941" for this suite. 01/30/23 12:19:08.006
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:334
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 12:19:08.009
Jan 30 12:19:08.009: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename init-container 01/30/23 12:19:08.01
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:19:08.015
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:19:08.017
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:165
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:334
STEP: creating the pod 01/30/23 12:19:08.019
Jan 30 12:19:08.019: INFO: PodSpec: initContainers in spec.initContainers
Jan 30 12:19:55.509: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-33f6cdbd-79e9-4251-b2e2-1ab7850f7ba6", GenerateName:"", Namespace:"init-container-8173", SelfLink:"", UID:"28d7f5c6-1b0f-4a20-b636-5545a531fb8e", ResourceVersion:"33293", Generation:0, CreationTimestamp:time.Date(2023, time.January, 30, 12, 19, 8, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"19489667"}, Annotations:map[string]string{"cni.projectcalico.org/podIP":"10.178.151.122/32", "cni.projectcalico.org/podIPs":"10.178.151.122/32"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"calico", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.January, 30, 12, 19, 8, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc000291a58), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.January, 30, 12, 19, 8, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc000291ab8), Subresource:""}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.January, 30, 12, 19, 55, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc000291ce0), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-pm69j", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc00497d760), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"harbor.cloud.netease.com/qzprod-k8s/e2e/busybox:1.29-4", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-pm69j", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"harbor.cloud.netease.com/qzprod-k8s/e2e/busybox:1.29-4", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-pm69j", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"harbor.cloud.netease.com/qzprod-k8s/e2e/pause:3.9", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-pm69j", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc004c2b538), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"pubt2-nks-for-dev1.dg.163.org", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc000401730), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc004c2b5b0)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc004c2b5e0)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc004c2b5e8), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc004c2b5ec), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc0017c3800), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil), HostUsers:(*bool)(nil), SchedulingGates:[]v1.PodSchedulingGate(nil), ResourceClaims:[]v1.PodResourceClaim(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.January, 30, 12, 19, 8, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.January, 30, 12, 19, 8, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.January, 30, 12, 19, 8, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.January, 30, 12, 19, 8, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.182.0.82", PodIP:"10.178.151.122", PodIPs:[]v1.PodIP{v1.PodIP{IP:"10.178.151.122"}}, StartTime:time.Date(2023, time.January, 30, 12, 19, 8, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc000401810)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0004018f0)}, Ready:false, RestartCount:3, Image:"harbor.cloud.netease.com/qzprod-k8s/e2e/busybox:1.29-4", ImageID:"docker-pullable://harbor.cloud.netease.com/qzprod-k8s/e2e/busybox@sha256:7ddd6b83e44b8f6e2f1fccace9562f5600b71e7717515ebd2131bdb94ad8634c", ContainerID:"docker://8cebaeae6eb76130433af3954431cc3fe45152c2d5d55669eab9d3926f572123", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc00497d7e0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"harbor.cloud.netease.com/qzprod-k8s/e2e/busybox:1.29-4", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc00497d7c0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"harbor.cloud.netease.com/qzprod-k8s/e2e/pause:3.9", ImageID:"", ContainerID:"", Started:(*bool)(0xc004c2b66f)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Jan 30 12:19:55.510: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "init-container-8173" for this suite. 01/30/23 12:19:55.512
------------------------------
• [SLOW TEST] [47.506 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:334

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 12:19:08.009
    Jan 30 12:19:08.009: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename init-container 01/30/23 12:19:08.01
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:19:08.015
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:19:08.017
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:165
    [It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
      test/e2e/common/node/init_container.go:334
    STEP: creating the pod 01/30/23 12:19:08.019
    Jan 30 12:19:08.019: INFO: PodSpec: initContainers in spec.initContainers
    Jan 30 12:19:55.509: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-33f6cdbd-79e9-4251-b2e2-1ab7850f7ba6", GenerateName:"", Namespace:"init-container-8173", SelfLink:"", UID:"28d7f5c6-1b0f-4a20-b636-5545a531fb8e", ResourceVersion:"33293", Generation:0, CreationTimestamp:time.Date(2023, time.January, 30, 12, 19, 8, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"19489667"}, Annotations:map[string]string{"cni.projectcalico.org/podIP":"10.178.151.122/32", "cni.projectcalico.org/podIPs":"10.178.151.122/32"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"calico", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.January, 30, 12, 19, 8, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc000291a58), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.January, 30, 12, 19, 8, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc000291ab8), Subresource:""}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.January, 30, 12, 19, 55, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc000291ce0), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-pm69j", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc00497d760), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"harbor.cloud.netease.com/qzprod-k8s/e2e/busybox:1.29-4", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-pm69j", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"harbor.cloud.netease.com/qzprod-k8s/e2e/busybox:1.29-4", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-pm69j", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"harbor.cloud.netease.com/qzprod-k8s/e2e/pause:3.9", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-pm69j", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc004c2b538), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"pubt2-nks-for-dev1.dg.163.org", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc000401730), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc004c2b5b0)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc004c2b5e0)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc004c2b5e8), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc004c2b5ec), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc0017c3800), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil), HostUsers:(*bool)(nil), SchedulingGates:[]v1.PodSchedulingGate(nil), ResourceClaims:[]v1.PodResourceClaim(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.January, 30, 12, 19, 8, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.January, 30, 12, 19, 8, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.January, 30, 12, 19, 8, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.January, 30, 12, 19, 8, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.182.0.82", PodIP:"10.178.151.122", PodIPs:[]v1.PodIP{v1.PodIP{IP:"10.178.151.122"}}, StartTime:time.Date(2023, time.January, 30, 12, 19, 8, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc000401810)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0004018f0)}, Ready:false, RestartCount:3, Image:"harbor.cloud.netease.com/qzprod-k8s/e2e/busybox:1.29-4", ImageID:"docker-pullable://harbor.cloud.netease.com/qzprod-k8s/e2e/busybox@sha256:7ddd6b83e44b8f6e2f1fccace9562f5600b71e7717515ebd2131bdb94ad8634c", ContainerID:"docker://8cebaeae6eb76130433af3954431cc3fe45152c2d5d55669eab9d3926f572123", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc00497d7e0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"harbor.cloud.netease.com/qzprod-k8s/e2e/busybox:1.29-4", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc00497d7c0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"harbor.cloud.netease.com/qzprod-k8s/e2e/pause:3.9", ImageID:"", ContainerID:"", Started:(*bool)(0xc004c2b66f)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Jan 30 12:19:55.510: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "init-container-8173" for this suite. 01/30/23 12:19:55.512
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-apps] Job
  should delete a job [Conformance]
  test/e2e/apps/job.go:481
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 12:19:55.515
Jan 30 12:19:55.515: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename job 01/30/23 12:19:55.516
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:19:55.522
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:19:55.524
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should delete a job [Conformance]
  test/e2e/apps/job.go:481
STEP: Creating a job 01/30/23 12:19:55.526
STEP: Ensuring active pods == parallelism 01/30/23 12:19:55.529
STEP: delete a job 01/30/23 12:19:59.533
STEP: deleting Job.batch foo in namespace job-5241, will wait for the garbage collector to delete the pods 01/30/23 12:19:59.533
Jan 30 12:19:59.587: INFO: Deleting Job.batch foo took: 2.42952ms
Jan 30 12:19:59.687: INFO: Terminating Job.batch foo pods took: 100.168647ms
STEP: Ensuring job was deleted 01/30/23 12:20:31.388
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Jan 30 12:20:31.389: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-5241" for this suite. 01/30/23 12:20:31.392
------------------------------
• [SLOW TEST] [35.879 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  test/e2e/apps/job.go:481

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 12:19:55.515
    Jan 30 12:19:55.515: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename job 01/30/23 12:19:55.516
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:19:55.522
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:19:55.524
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete a job [Conformance]
      test/e2e/apps/job.go:481
    STEP: Creating a job 01/30/23 12:19:55.526
    STEP: Ensuring active pods == parallelism 01/30/23 12:19:55.529
    STEP: delete a job 01/30/23 12:19:59.533
    STEP: deleting Job.batch foo in namespace job-5241, will wait for the garbage collector to delete the pods 01/30/23 12:19:59.533
    Jan 30 12:19:59.587: INFO: Deleting Job.batch foo took: 2.42952ms
    Jan 30 12:19:59.687: INFO: Terminating Job.batch foo pods took: 100.168647ms
    STEP: Ensuring job was deleted 01/30/23 12:20:31.388
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Jan 30 12:20:31.389: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-5241" for this suite. 01/30/23 12:20:31.392
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 12:20:31.394
Jan 30 12:20:31.395: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename custom-resource-definition 01/30/23 12:20:31.395
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:20:31.401
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:20:31.403
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269
Jan 30 12:20:31.405: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 30 12:20:34.558: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-4296" for this suite. 01/30/23 12:20:34.56
------------------------------
• [3.168 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 12:20:31.394
    Jan 30 12:20:31.395: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename custom-resource-definition 01/30/23 12:20:31.395
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:20:31.401
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:20:31.403
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] custom resource defaulting for requests and from storage works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:269
    Jan 30 12:20:31.405: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 30 12:20:34.558: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-4296" for this suite. 01/30/23 12:20:34.56
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:239
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 12:20:34.563
Jan 30 12:20:34.563: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename webhook 01/30/23 12:20:34.564
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:20:34.57
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:20:34.572
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 01/30/23 12:20:34.579
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/30/23 12:20:35.032
STEP: Deploying the webhook pod 01/30/23 12:20:35.037
STEP: Wait for the deployment to be ready 01/30/23 12:20:35.042
Jan 30 12:20:35.045: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 01/30/23 12:20:37.052
STEP: Verifying the service has paired with the endpoint 01/30/23 12:20:37.055
Jan 30 12:20:38.057: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:239
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API 01/30/23 12:20:38.059
STEP: create a namespace for the webhook 01/30/23 12:20:38.068
STEP: create a configmap should be unconditionally rejected by the webhook 01/30/23 12:20:38.072
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 30 12:20:38.082: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-8723" for this suite. 01/30/23 12:20:38.101
STEP: Destroying namespace "webhook-8723-markers" for this suite. 01/30/23 12:20:38.103
------------------------------
• [3.542 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:239

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 12:20:34.563
    Jan 30 12:20:34.563: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename webhook 01/30/23 12:20:34.564
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:20:34.57
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:20:34.572
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 01/30/23 12:20:34.579
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/30/23 12:20:35.032
    STEP: Deploying the webhook pod 01/30/23 12:20:35.037
    STEP: Wait for the deployment to be ready 01/30/23 12:20:35.042
    Jan 30 12:20:35.045: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 01/30/23 12:20:37.052
    STEP: Verifying the service has paired with the endpoint 01/30/23 12:20:37.055
    Jan 30 12:20:38.057: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should unconditionally reject operations on fail closed webhook [Conformance]
      test/e2e/apimachinery/webhook.go:239
    STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API 01/30/23 12:20:38.059
    STEP: create a namespace for the webhook 01/30/23 12:20:38.068
    STEP: create a configmap should be unconditionally rejected by the webhook 01/30/23 12:20:38.072
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 30 12:20:38.082: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-8723" for this suite. 01/30/23 12:20:38.101
    STEP: Destroying namespace "webhook-8723-markers" for this suite. 01/30/23 12:20:38.103
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:536
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 12:20:38.107
Jan 30 12:20:38.107: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename pods 01/30/23 12:20:38.108
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:20:38.113
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:20:38.115
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:536
Jan 30 12:20:38.117: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: creating the pod 01/30/23 12:20:38.118
STEP: submitting the pod to kubernetes 01/30/23 12:20:38.118
Jan 30 12:20:38.122: INFO: Waiting up to 5m0s for pod "pod-exec-websocket-226fe8b2-ed17-43ab-b97f-778f95878835" in namespace "pods-6314" to be "running and ready"
Jan 30 12:20:38.124: INFO: Pod "pod-exec-websocket-226fe8b2-ed17-43ab-b97f-778f95878835": Phase="Pending", Reason="", readiness=false. Elapsed: 1.581277ms
Jan 30 12:20:38.124: INFO: The phase of Pod pod-exec-websocket-226fe8b2-ed17-43ab-b97f-778f95878835 is Pending, waiting for it to be Running (with Ready = true)
Jan 30 12:20:40.127: INFO: Pod "pod-exec-websocket-226fe8b2-ed17-43ab-b97f-778f95878835": Phase="Running", Reason="", readiness=true. Elapsed: 2.00550176s
Jan 30 12:20:40.128: INFO: The phase of Pod pod-exec-websocket-226fe8b2-ed17-43ab-b97f-778f95878835 is Running (Ready = true)
Jan 30 12:20:40.128: INFO: Pod "pod-exec-websocket-226fe8b2-ed17-43ab-b97f-778f95878835" satisfied condition "running and ready"
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Jan 30 12:20:40.209: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-6314" for this suite. 01/30/23 12:20:40.211
------------------------------
• [2.107 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:536

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 12:20:38.107
    Jan 30 12:20:38.107: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename pods 01/30/23 12:20:38.108
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:20:38.113
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:20:38.115
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should support remote command execution over websockets [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:536
    Jan 30 12:20:38.117: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: creating the pod 01/30/23 12:20:38.118
    STEP: submitting the pod to kubernetes 01/30/23 12:20:38.118
    Jan 30 12:20:38.122: INFO: Waiting up to 5m0s for pod "pod-exec-websocket-226fe8b2-ed17-43ab-b97f-778f95878835" in namespace "pods-6314" to be "running and ready"
    Jan 30 12:20:38.124: INFO: Pod "pod-exec-websocket-226fe8b2-ed17-43ab-b97f-778f95878835": Phase="Pending", Reason="", readiness=false. Elapsed: 1.581277ms
    Jan 30 12:20:38.124: INFO: The phase of Pod pod-exec-websocket-226fe8b2-ed17-43ab-b97f-778f95878835 is Pending, waiting for it to be Running (with Ready = true)
    Jan 30 12:20:40.127: INFO: Pod "pod-exec-websocket-226fe8b2-ed17-43ab-b97f-778f95878835": Phase="Running", Reason="", readiness=true. Elapsed: 2.00550176s
    Jan 30 12:20:40.128: INFO: The phase of Pod pod-exec-websocket-226fe8b2-ed17-43ab-b97f-778f95878835 is Running (Ready = true)
    Jan 30 12:20:40.128: INFO: Pod "pod-exec-websocket-226fe8b2-ed17-43ab-b97f-778f95878835" satisfied condition "running and ready"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Jan 30 12:20:40.209: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-6314" for this suite. 01/30/23 12:20:40.211
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes
  should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 12:20:40.214
Jan 30 12:20:40.214: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename emptydir-wrapper 01/30/23 12:20:40.215
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:20:40.221
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:20:40.223
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189
STEP: Creating 50 configmaps 01/30/23 12:20:40.225
STEP: Creating RC which spawns configmap-volume pods 01/30/23 12:20:40.469
Jan 30 12:20:40.571: INFO: Pod name wrapped-volume-race-21d6cf48-2141-476e-9a62-34fa61a97b18: Found 5 pods out of 5
STEP: Ensuring each pod is running 01/30/23 12:20:40.571
Jan 30 12:20:40.571: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-21d6cf48-2141-476e-9a62-34fa61a97b18-b87v2" in namespace "emptydir-wrapper-1245" to be "running"
Jan 30 12:20:40.619: INFO: Pod "wrapped-volume-race-21d6cf48-2141-476e-9a62-34fa61a97b18-b87v2": Phase="Pending", Reason="", readiness=false. Elapsed: 47.756285ms
Jan 30 12:20:42.622: INFO: Pod "wrapped-volume-race-21d6cf48-2141-476e-9a62-34fa61a97b18-b87v2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.050819924s
Jan 30 12:20:44.622: INFO: Pod "wrapped-volume-race-21d6cf48-2141-476e-9a62-34fa61a97b18-b87v2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.050748316s
Jan 30 12:20:46.622: INFO: Pod "wrapped-volume-race-21d6cf48-2141-476e-9a62-34fa61a97b18-b87v2": Phase="Pending", Reason="", readiness=false. Elapsed: 6.051519725s
Jan 30 12:20:48.622: INFO: Pod "wrapped-volume-race-21d6cf48-2141-476e-9a62-34fa61a97b18-b87v2": Phase="Pending", Reason="", readiness=false. Elapsed: 8.050849744s
Jan 30 12:20:50.622: INFO: Pod "wrapped-volume-race-21d6cf48-2141-476e-9a62-34fa61a97b18-b87v2": Phase="Pending", Reason="", readiness=false. Elapsed: 10.051645574s
Jan 30 12:20:52.623: INFO: Pod "wrapped-volume-race-21d6cf48-2141-476e-9a62-34fa61a97b18-b87v2": Phase="Pending", Reason="", readiness=false. Elapsed: 12.052037649s
Jan 30 12:20:54.622: INFO: Pod "wrapped-volume-race-21d6cf48-2141-476e-9a62-34fa61a97b18-b87v2": Phase="Running", Reason="", readiness=true. Elapsed: 14.051502072s
Jan 30 12:20:54.622: INFO: Pod "wrapped-volume-race-21d6cf48-2141-476e-9a62-34fa61a97b18-b87v2" satisfied condition "running"
Jan 30 12:20:54.622: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-21d6cf48-2141-476e-9a62-34fa61a97b18-bzpsl" in namespace "emptydir-wrapper-1245" to be "running"
Jan 30 12:20:54.625: INFO: Pod "wrapped-volume-race-21d6cf48-2141-476e-9a62-34fa61a97b18-bzpsl": Phase="Running", Reason="", readiness=true. Elapsed: 2.395327ms
Jan 30 12:20:54.625: INFO: Pod "wrapped-volume-race-21d6cf48-2141-476e-9a62-34fa61a97b18-bzpsl" satisfied condition "running"
Jan 30 12:20:54.625: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-21d6cf48-2141-476e-9a62-34fa61a97b18-c9wjr" in namespace "emptydir-wrapper-1245" to be "running"
Jan 30 12:20:54.627: INFO: Pod "wrapped-volume-race-21d6cf48-2141-476e-9a62-34fa61a97b18-c9wjr": Phase="Running", Reason="", readiness=true. Elapsed: 2.362567ms
Jan 30 12:20:54.627: INFO: Pod "wrapped-volume-race-21d6cf48-2141-476e-9a62-34fa61a97b18-c9wjr" satisfied condition "running"
Jan 30 12:20:54.627: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-21d6cf48-2141-476e-9a62-34fa61a97b18-ctw2x" in namespace "emptydir-wrapper-1245" to be "running"
Jan 30 12:20:54.629: INFO: Pod "wrapped-volume-race-21d6cf48-2141-476e-9a62-34fa61a97b18-ctw2x": Phase="Running", Reason="", readiness=true. Elapsed: 2.164493ms
Jan 30 12:20:54.629: INFO: Pod "wrapped-volume-race-21d6cf48-2141-476e-9a62-34fa61a97b18-ctw2x" satisfied condition "running"
Jan 30 12:20:54.629: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-21d6cf48-2141-476e-9a62-34fa61a97b18-nl2gw" in namespace "emptydir-wrapper-1245" to be "running"
Jan 30 12:20:54.632: INFO: Pod "wrapped-volume-race-21d6cf48-2141-476e-9a62-34fa61a97b18-nl2gw": Phase="Running", Reason="", readiness=true. Elapsed: 2.243138ms
Jan 30 12:20:54.632: INFO: Pod "wrapped-volume-race-21d6cf48-2141-476e-9a62-34fa61a97b18-nl2gw" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-21d6cf48-2141-476e-9a62-34fa61a97b18 in namespace emptydir-wrapper-1245, will wait for the garbage collector to delete the pods 01/30/23 12:20:54.632
Jan 30 12:20:54.688: INFO: Deleting ReplicationController wrapped-volume-race-21d6cf48-2141-476e-9a62-34fa61a97b18 took: 3.010872ms
Jan 30 12:20:54.788: INFO: Terminating ReplicationController wrapped-volume-race-21d6cf48-2141-476e-9a62-34fa61a97b18 pods took: 100.787105ms
STEP: Creating RC which spawns configmap-volume pods 01/30/23 12:20:59.491
Jan 30 12:20:59.500: INFO: Pod name wrapped-volume-race-d2752d8e-29f3-416a-896e-97028729671a: Found 0 pods out of 5
Jan 30 12:21:04.506: INFO: Pod name wrapped-volume-race-d2752d8e-29f3-416a-896e-97028729671a: Found 5 pods out of 5
STEP: Ensuring each pod is running 01/30/23 12:21:04.506
Jan 30 12:21:04.506: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-d2752d8e-29f3-416a-896e-97028729671a-794f7" in namespace "emptydir-wrapper-1245" to be "running"
Jan 30 12:21:04.508: INFO: Pod "wrapped-volume-race-d2752d8e-29f3-416a-896e-97028729671a-794f7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.197083ms
Jan 30 12:21:06.512: INFO: Pod "wrapped-volume-race-d2752d8e-29f3-416a-896e-97028729671a-794f7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005886768s
Jan 30 12:21:08.512: INFO: Pod "wrapped-volume-race-d2752d8e-29f3-416a-896e-97028729671a-794f7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.005522648s
Jan 30 12:21:10.513: INFO: Pod "wrapped-volume-race-d2752d8e-29f3-416a-896e-97028729671a-794f7": Phase="Pending", Reason="", readiness=false. Elapsed: 6.006866249s
Jan 30 12:21:12.512: INFO: Pod "wrapped-volume-race-d2752d8e-29f3-416a-896e-97028729671a-794f7": Phase="Pending", Reason="", readiness=false. Elapsed: 8.005804727s
Jan 30 12:21:14.513: INFO: Pod "wrapped-volume-race-d2752d8e-29f3-416a-896e-97028729671a-794f7": Phase="Running", Reason="", readiness=true. Elapsed: 10.006761002s
Jan 30 12:21:14.513: INFO: Pod "wrapped-volume-race-d2752d8e-29f3-416a-896e-97028729671a-794f7" satisfied condition "running"
Jan 30 12:21:14.513: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-d2752d8e-29f3-416a-896e-97028729671a-b7vzr" in namespace "emptydir-wrapper-1245" to be "running"
Jan 30 12:21:14.515: INFO: Pod "wrapped-volume-race-d2752d8e-29f3-416a-896e-97028729671a-b7vzr": Phase="Running", Reason="", readiness=true. Elapsed: 2.035022ms
Jan 30 12:21:14.515: INFO: Pod "wrapped-volume-race-d2752d8e-29f3-416a-896e-97028729671a-b7vzr" satisfied condition "running"
Jan 30 12:21:14.515: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-d2752d8e-29f3-416a-896e-97028729671a-k7cj6" in namespace "emptydir-wrapper-1245" to be "running"
Jan 30 12:21:14.517: INFO: Pod "wrapped-volume-race-d2752d8e-29f3-416a-896e-97028729671a-k7cj6": Phase="Running", Reason="", readiness=true. Elapsed: 2.04497ms
Jan 30 12:21:14.517: INFO: Pod "wrapped-volume-race-d2752d8e-29f3-416a-896e-97028729671a-k7cj6" satisfied condition "running"
Jan 30 12:21:14.517: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-d2752d8e-29f3-416a-896e-97028729671a-tmgpk" in namespace "emptydir-wrapper-1245" to be "running"
Jan 30 12:21:14.519: INFO: Pod "wrapped-volume-race-d2752d8e-29f3-416a-896e-97028729671a-tmgpk": Phase="Running", Reason="", readiness=true. Elapsed: 2.036546ms
Jan 30 12:21:14.519: INFO: Pod "wrapped-volume-race-d2752d8e-29f3-416a-896e-97028729671a-tmgpk" satisfied condition "running"
Jan 30 12:21:14.519: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-d2752d8e-29f3-416a-896e-97028729671a-x5n4q" in namespace "emptydir-wrapper-1245" to be "running"
Jan 30 12:21:14.521: INFO: Pod "wrapped-volume-race-d2752d8e-29f3-416a-896e-97028729671a-x5n4q": Phase="Pending", Reason="", readiness=false. Elapsed: 2.154695ms
Jan 30 12:21:16.525: INFO: Pod "wrapped-volume-race-d2752d8e-29f3-416a-896e-97028729671a-x5n4q": Phase="Running", Reason="", readiness=true. Elapsed: 2.006042254s
Jan 30 12:21:16.525: INFO: Pod "wrapped-volume-race-d2752d8e-29f3-416a-896e-97028729671a-x5n4q" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-d2752d8e-29f3-416a-896e-97028729671a in namespace emptydir-wrapper-1245, will wait for the garbage collector to delete the pods 01/30/23 12:21:16.525
Jan 30 12:21:16.581: INFO: Deleting ReplicationController wrapped-volume-race-d2752d8e-29f3-416a-896e-97028729671a took: 3.252005ms
Jan 30 12:21:16.681: INFO: Terminating ReplicationController wrapped-volume-race-d2752d8e-29f3-416a-896e-97028729671a pods took: 100.086888ms
STEP: Creating RC which spawns configmap-volume pods 01/30/23 12:21:19.484
Jan 30 12:21:19.495: INFO: Pod name wrapped-volume-race-96eb1f39-c59a-449c-81c5-00f31c49d9c0: Found 0 pods out of 5
Jan 30 12:21:24.501: INFO: Pod name wrapped-volume-race-96eb1f39-c59a-449c-81c5-00f31c49d9c0: Found 5 pods out of 5
STEP: Ensuring each pod is running 01/30/23 12:21:24.501
Jan 30 12:21:24.502: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-96eb1f39-c59a-449c-81c5-00f31c49d9c0-8pwkj" in namespace "emptydir-wrapper-1245" to be "running"
Jan 30 12:21:24.504: INFO: Pod "wrapped-volume-race-96eb1f39-c59a-449c-81c5-00f31c49d9c0-8pwkj": Phase="Pending", Reason="", readiness=false. Elapsed: 2.230426ms
Jan 30 12:21:26.508: INFO: Pod "wrapped-volume-race-96eb1f39-c59a-449c-81c5-00f31c49d9c0-8pwkj": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006319237s
Jan 30 12:21:28.507: INFO: Pod "wrapped-volume-race-96eb1f39-c59a-449c-81c5-00f31c49d9c0-8pwkj": Phase="Pending", Reason="", readiness=false. Elapsed: 4.005693477s
Jan 30 12:21:30.507: INFO: Pod "wrapped-volume-race-96eb1f39-c59a-449c-81c5-00f31c49d9c0-8pwkj": Phase="Pending", Reason="", readiness=false. Elapsed: 6.005766453s
Jan 30 12:21:32.507: INFO: Pod "wrapped-volume-race-96eb1f39-c59a-449c-81c5-00f31c49d9c0-8pwkj": Phase="Pending", Reason="", readiness=false. Elapsed: 8.00531547s
Jan 30 12:21:34.507: INFO: Pod "wrapped-volume-race-96eb1f39-c59a-449c-81c5-00f31c49d9c0-8pwkj": Phase="Running", Reason="", readiness=true. Elapsed: 10.005780201s
Jan 30 12:21:34.507: INFO: Pod "wrapped-volume-race-96eb1f39-c59a-449c-81c5-00f31c49d9c0-8pwkj" satisfied condition "running"
Jan 30 12:21:34.507: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-96eb1f39-c59a-449c-81c5-00f31c49d9c0-dwctt" in namespace "emptydir-wrapper-1245" to be "running"
Jan 30 12:21:34.510: INFO: Pod "wrapped-volume-race-96eb1f39-c59a-449c-81c5-00f31c49d9c0-dwctt": Phase="Running", Reason="", readiness=true. Elapsed: 2.144957ms
Jan 30 12:21:34.510: INFO: Pod "wrapped-volume-race-96eb1f39-c59a-449c-81c5-00f31c49d9c0-dwctt" satisfied condition "running"
Jan 30 12:21:34.510: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-96eb1f39-c59a-449c-81c5-00f31c49d9c0-jhdr6" in namespace "emptydir-wrapper-1245" to be "running"
Jan 30 12:21:34.512: INFO: Pod "wrapped-volume-race-96eb1f39-c59a-449c-81c5-00f31c49d9c0-jhdr6": Phase="Running", Reason="", readiness=true. Elapsed: 2.155752ms
Jan 30 12:21:34.512: INFO: Pod "wrapped-volume-race-96eb1f39-c59a-449c-81c5-00f31c49d9c0-jhdr6" satisfied condition "running"
Jan 30 12:21:34.512: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-96eb1f39-c59a-449c-81c5-00f31c49d9c0-pndmc" in namespace "emptydir-wrapper-1245" to be "running"
Jan 30 12:21:34.514: INFO: Pod "wrapped-volume-race-96eb1f39-c59a-449c-81c5-00f31c49d9c0-pndmc": Phase="Running", Reason="", readiness=true. Elapsed: 2.14481ms
Jan 30 12:21:34.514: INFO: Pod "wrapped-volume-race-96eb1f39-c59a-449c-81c5-00f31c49d9c0-pndmc" satisfied condition "running"
Jan 30 12:21:34.514: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-96eb1f39-c59a-449c-81c5-00f31c49d9c0-xwwfv" in namespace "emptydir-wrapper-1245" to be "running"
Jan 30 12:21:34.516: INFO: Pod "wrapped-volume-race-96eb1f39-c59a-449c-81c5-00f31c49d9c0-xwwfv": Phase="Running", Reason="", readiness=true. Elapsed: 2.10506ms
Jan 30 12:21:34.516: INFO: Pod "wrapped-volume-race-96eb1f39-c59a-449c-81c5-00f31c49d9c0-xwwfv" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-96eb1f39-c59a-449c-81c5-00f31c49d9c0 in namespace emptydir-wrapper-1245, will wait for the garbage collector to delete the pods 01/30/23 12:21:34.516
Jan 30 12:21:34.572: INFO: Deleting ReplicationController wrapped-volume-race-96eb1f39-c59a-449c-81c5-00f31c49d9c0 took: 2.811217ms
Jan 30 12:21:34.673: INFO: Terminating ReplicationController wrapped-volume-race-96eb1f39-c59a-449c-81c5-00f31c49d9c0 pods took: 100.70004ms
STEP: Cleaning up the configMaps 01/30/23 12:21:38.273
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/node/init/init.go:32
Jan 30 12:21:38.374: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-wrapper-1245" for this suite. 01/30/23 12:21:38.376
------------------------------
• [SLOW TEST] [58.164 seconds]
[sig-storage] EmptyDir wrapper volumes
test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 12:20:40.214
    Jan 30 12:20:40.214: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename emptydir-wrapper 01/30/23 12:20:40.215
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:20:40.221
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:20:40.223
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should not cause race condition when used for configmaps [Serial] [Conformance]
      test/e2e/storage/empty_dir_wrapper.go:189
    STEP: Creating 50 configmaps 01/30/23 12:20:40.225
    STEP: Creating RC which spawns configmap-volume pods 01/30/23 12:20:40.469
    Jan 30 12:20:40.571: INFO: Pod name wrapped-volume-race-21d6cf48-2141-476e-9a62-34fa61a97b18: Found 5 pods out of 5
    STEP: Ensuring each pod is running 01/30/23 12:20:40.571
    Jan 30 12:20:40.571: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-21d6cf48-2141-476e-9a62-34fa61a97b18-b87v2" in namespace "emptydir-wrapper-1245" to be "running"
    Jan 30 12:20:40.619: INFO: Pod "wrapped-volume-race-21d6cf48-2141-476e-9a62-34fa61a97b18-b87v2": Phase="Pending", Reason="", readiness=false. Elapsed: 47.756285ms
    Jan 30 12:20:42.622: INFO: Pod "wrapped-volume-race-21d6cf48-2141-476e-9a62-34fa61a97b18-b87v2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.050819924s
    Jan 30 12:20:44.622: INFO: Pod "wrapped-volume-race-21d6cf48-2141-476e-9a62-34fa61a97b18-b87v2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.050748316s
    Jan 30 12:20:46.622: INFO: Pod "wrapped-volume-race-21d6cf48-2141-476e-9a62-34fa61a97b18-b87v2": Phase="Pending", Reason="", readiness=false. Elapsed: 6.051519725s
    Jan 30 12:20:48.622: INFO: Pod "wrapped-volume-race-21d6cf48-2141-476e-9a62-34fa61a97b18-b87v2": Phase="Pending", Reason="", readiness=false. Elapsed: 8.050849744s
    Jan 30 12:20:50.622: INFO: Pod "wrapped-volume-race-21d6cf48-2141-476e-9a62-34fa61a97b18-b87v2": Phase="Pending", Reason="", readiness=false. Elapsed: 10.051645574s
    Jan 30 12:20:52.623: INFO: Pod "wrapped-volume-race-21d6cf48-2141-476e-9a62-34fa61a97b18-b87v2": Phase="Pending", Reason="", readiness=false. Elapsed: 12.052037649s
    Jan 30 12:20:54.622: INFO: Pod "wrapped-volume-race-21d6cf48-2141-476e-9a62-34fa61a97b18-b87v2": Phase="Running", Reason="", readiness=true. Elapsed: 14.051502072s
    Jan 30 12:20:54.622: INFO: Pod "wrapped-volume-race-21d6cf48-2141-476e-9a62-34fa61a97b18-b87v2" satisfied condition "running"
    Jan 30 12:20:54.622: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-21d6cf48-2141-476e-9a62-34fa61a97b18-bzpsl" in namespace "emptydir-wrapper-1245" to be "running"
    Jan 30 12:20:54.625: INFO: Pod "wrapped-volume-race-21d6cf48-2141-476e-9a62-34fa61a97b18-bzpsl": Phase="Running", Reason="", readiness=true. Elapsed: 2.395327ms
    Jan 30 12:20:54.625: INFO: Pod "wrapped-volume-race-21d6cf48-2141-476e-9a62-34fa61a97b18-bzpsl" satisfied condition "running"
    Jan 30 12:20:54.625: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-21d6cf48-2141-476e-9a62-34fa61a97b18-c9wjr" in namespace "emptydir-wrapper-1245" to be "running"
    Jan 30 12:20:54.627: INFO: Pod "wrapped-volume-race-21d6cf48-2141-476e-9a62-34fa61a97b18-c9wjr": Phase="Running", Reason="", readiness=true. Elapsed: 2.362567ms
    Jan 30 12:20:54.627: INFO: Pod "wrapped-volume-race-21d6cf48-2141-476e-9a62-34fa61a97b18-c9wjr" satisfied condition "running"
    Jan 30 12:20:54.627: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-21d6cf48-2141-476e-9a62-34fa61a97b18-ctw2x" in namespace "emptydir-wrapper-1245" to be "running"
    Jan 30 12:20:54.629: INFO: Pod "wrapped-volume-race-21d6cf48-2141-476e-9a62-34fa61a97b18-ctw2x": Phase="Running", Reason="", readiness=true. Elapsed: 2.164493ms
    Jan 30 12:20:54.629: INFO: Pod "wrapped-volume-race-21d6cf48-2141-476e-9a62-34fa61a97b18-ctw2x" satisfied condition "running"
    Jan 30 12:20:54.629: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-21d6cf48-2141-476e-9a62-34fa61a97b18-nl2gw" in namespace "emptydir-wrapper-1245" to be "running"
    Jan 30 12:20:54.632: INFO: Pod "wrapped-volume-race-21d6cf48-2141-476e-9a62-34fa61a97b18-nl2gw": Phase="Running", Reason="", readiness=true. Elapsed: 2.243138ms
    Jan 30 12:20:54.632: INFO: Pod "wrapped-volume-race-21d6cf48-2141-476e-9a62-34fa61a97b18-nl2gw" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-21d6cf48-2141-476e-9a62-34fa61a97b18 in namespace emptydir-wrapper-1245, will wait for the garbage collector to delete the pods 01/30/23 12:20:54.632
    Jan 30 12:20:54.688: INFO: Deleting ReplicationController wrapped-volume-race-21d6cf48-2141-476e-9a62-34fa61a97b18 took: 3.010872ms
    Jan 30 12:20:54.788: INFO: Terminating ReplicationController wrapped-volume-race-21d6cf48-2141-476e-9a62-34fa61a97b18 pods took: 100.787105ms
    STEP: Creating RC which spawns configmap-volume pods 01/30/23 12:20:59.491
    Jan 30 12:20:59.500: INFO: Pod name wrapped-volume-race-d2752d8e-29f3-416a-896e-97028729671a: Found 0 pods out of 5
    Jan 30 12:21:04.506: INFO: Pod name wrapped-volume-race-d2752d8e-29f3-416a-896e-97028729671a: Found 5 pods out of 5
    STEP: Ensuring each pod is running 01/30/23 12:21:04.506
    Jan 30 12:21:04.506: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-d2752d8e-29f3-416a-896e-97028729671a-794f7" in namespace "emptydir-wrapper-1245" to be "running"
    Jan 30 12:21:04.508: INFO: Pod "wrapped-volume-race-d2752d8e-29f3-416a-896e-97028729671a-794f7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.197083ms
    Jan 30 12:21:06.512: INFO: Pod "wrapped-volume-race-d2752d8e-29f3-416a-896e-97028729671a-794f7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005886768s
    Jan 30 12:21:08.512: INFO: Pod "wrapped-volume-race-d2752d8e-29f3-416a-896e-97028729671a-794f7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.005522648s
    Jan 30 12:21:10.513: INFO: Pod "wrapped-volume-race-d2752d8e-29f3-416a-896e-97028729671a-794f7": Phase="Pending", Reason="", readiness=false. Elapsed: 6.006866249s
    Jan 30 12:21:12.512: INFO: Pod "wrapped-volume-race-d2752d8e-29f3-416a-896e-97028729671a-794f7": Phase="Pending", Reason="", readiness=false. Elapsed: 8.005804727s
    Jan 30 12:21:14.513: INFO: Pod "wrapped-volume-race-d2752d8e-29f3-416a-896e-97028729671a-794f7": Phase="Running", Reason="", readiness=true. Elapsed: 10.006761002s
    Jan 30 12:21:14.513: INFO: Pod "wrapped-volume-race-d2752d8e-29f3-416a-896e-97028729671a-794f7" satisfied condition "running"
    Jan 30 12:21:14.513: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-d2752d8e-29f3-416a-896e-97028729671a-b7vzr" in namespace "emptydir-wrapper-1245" to be "running"
    Jan 30 12:21:14.515: INFO: Pod "wrapped-volume-race-d2752d8e-29f3-416a-896e-97028729671a-b7vzr": Phase="Running", Reason="", readiness=true. Elapsed: 2.035022ms
    Jan 30 12:21:14.515: INFO: Pod "wrapped-volume-race-d2752d8e-29f3-416a-896e-97028729671a-b7vzr" satisfied condition "running"
    Jan 30 12:21:14.515: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-d2752d8e-29f3-416a-896e-97028729671a-k7cj6" in namespace "emptydir-wrapper-1245" to be "running"
    Jan 30 12:21:14.517: INFO: Pod "wrapped-volume-race-d2752d8e-29f3-416a-896e-97028729671a-k7cj6": Phase="Running", Reason="", readiness=true. Elapsed: 2.04497ms
    Jan 30 12:21:14.517: INFO: Pod "wrapped-volume-race-d2752d8e-29f3-416a-896e-97028729671a-k7cj6" satisfied condition "running"
    Jan 30 12:21:14.517: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-d2752d8e-29f3-416a-896e-97028729671a-tmgpk" in namespace "emptydir-wrapper-1245" to be "running"
    Jan 30 12:21:14.519: INFO: Pod "wrapped-volume-race-d2752d8e-29f3-416a-896e-97028729671a-tmgpk": Phase="Running", Reason="", readiness=true. Elapsed: 2.036546ms
    Jan 30 12:21:14.519: INFO: Pod "wrapped-volume-race-d2752d8e-29f3-416a-896e-97028729671a-tmgpk" satisfied condition "running"
    Jan 30 12:21:14.519: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-d2752d8e-29f3-416a-896e-97028729671a-x5n4q" in namespace "emptydir-wrapper-1245" to be "running"
    Jan 30 12:21:14.521: INFO: Pod "wrapped-volume-race-d2752d8e-29f3-416a-896e-97028729671a-x5n4q": Phase="Pending", Reason="", readiness=false. Elapsed: 2.154695ms
    Jan 30 12:21:16.525: INFO: Pod "wrapped-volume-race-d2752d8e-29f3-416a-896e-97028729671a-x5n4q": Phase="Running", Reason="", readiness=true. Elapsed: 2.006042254s
    Jan 30 12:21:16.525: INFO: Pod "wrapped-volume-race-d2752d8e-29f3-416a-896e-97028729671a-x5n4q" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-d2752d8e-29f3-416a-896e-97028729671a in namespace emptydir-wrapper-1245, will wait for the garbage collector to delete the pods 01/30/23 12:21:16.525
    Jan 30 12:21:16.581: INFO: Deleting ReplicationController wrapped-volume-race-d2752d8e-29f3-416a-896e-97028729671a took: 3.252005ms
    Jan 30 12:21:16.681: INFO: Terminating ReplicationController wrapped-volume-race-d2752d8e-29f3-416a-896e-97028729671a pods took: 100.086888ms
    STEP: Creating RC which spawns configmap-volume pods 01/30/23 12:21:19.484
    Jan 30 12:21:19.495: INFO: Pod name wrapped-volume-race-96eb1f39-c59a-449c-81c5-00f31c49d9c0: Found 0 pods out of 5
    Jan 30 12:21:24.501: INFO: Pod name wrapped-volume-race-96eb1f39-c59a-449c-81c5-00f31c49d9c0: Found 5 pods out of 5
    STEP: Ensuring each pod is running 01/30/23 12:21:24.501
    Jan 30 12:21:24.502: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-96eb1f39-c59a-449c-81c5-00f31c49d9c0-8pwkj" in namespace "emptydir-wrapper-1245" to be "running"
    Jan 30 12:21:24.504: INFO: Pod "wrapped-volume-race-96eb1f39-c59a-449c-81c5-00f31c49d9c0-8pwkj": Phase="Pending", Reason="", readiness=false. Elapsed: 2.230426ms
    Jan 30 12:21:26.508: INFO: Pod "wrapped-volume-race-96eb1f39-c59a-449c-81c5-00f31c49d9c0-8pwkj": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006319237s
    Jan 30 12:21:28.507: INFO: Pod "wrapped-volume-race-96eb1f39-c59a-449c-81c5-00f31c49d9c0-8pwkj": Phase="Pending", Reason="", readiness=false. Elapsed: 4.005693477s
    Jan 30 12:21:30.507: INFO: Pod "wrapped-volume-race-96eb1f39-c59a-449c-81c5-00f31c49d9c0-8pwkj": Phase="Pending", Reason="", readiness=false. Elapsed: 6.005766453s
    Jan 30 12:21:32.507: INFO: Pod "wrapped-volume-race-96eb1f39-c59a-449c-81c5-00f31c49d9c0-8pwkj": Phase="Pending", Reason="", readiness=false. Elapsed: 8.00531547s
    Jan 30 12:21:34.507: INFO: Pod "wrapped-volume-race-96eb1f39-c59a-449c-81c5-00f31c49d9c0-8pwkj": Phase="Running", Reason="", readiness=true. Elapsed: 10.005780201s
    Jan 30 12:21:34.507: INFO: Pod "wrapped-volume-race-96eb1f39-c59a-449c-81c5-00f31c49d9c0-8pwkj" satisfied condition "running"
    Jan 30 12:21:34.507: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-96eb1f39-c59a-449c-81c5-00f31c49d9c0-dwctt" in namespace "emptydir-wrapper-1245" to be "running"
    Jan 30 12:21:34.510: INFO: Pod "wrapped-volume-race-96eb1f39-c59a-449c-81c5-00f31c49d9c0-dwctt": Phase="Running", Reason="", readiness=true. Elapsed: 2.144957ms
    Jan 30 12:21:34.510: INFO: Pod "wrapped-volume-race-96eb1f39-c59a-449c-81c5-00f31c49d9c0-dwctt" satisfied condition "running"
    Jan 30 12:21:34.510: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-96eb1f39-c59a-449c-81c5-00f31c49d9c0-jhdr6" in namespace "emptydir-wrapper-1245" to be "running"
    Jan 30 12:21:34.512: INFO: Pod "wrapped-volume-race-96eb1f39-c59a-449c-81c5-00f31c49d9c0-jhdr6": Phase="Running", Reason="", readiness=true. Elapsed: 2.155752ms
    Jan 30 12:21:34.512: INFO: Pod "wrapped-volume-race-96eb1f39-c59a-449c-81c5-00f31c49d9c0-jhdr6" satisfied condition "running"
    Jan 30 12:21:34.512: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-96eb1f39-c59a-449c-81c5-00f31c49d9c0-pndmc" in namespace "emptydir-wrapper-1245" to be "running"
    Jan 30 12:21:34.514: INFO: Pod "wrapped-volume-race-96eb1f39-c59a-449c-81c5-00f31c49d9c0-pndmc": Phase="Running", Reason="", readiness=true. Elapsed: 2.14481ms
    Jan 30 12:21:34.514: INFO: Pod "wrapped-volume-race-96eb1f39-c59a-449c-81c5-00f31c49d9c0-pndmc" satisfied condition "running"
    Jan 30 12:21:34.514: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-96eb1f39-c59a-449c-81c5-00f31c49d9c0-xwwfv" in namespace "emptydir-wrapper-1245" to be "running"
    Jan 30 12:21:34.516: INFO: Pod "wrapped-volume-race-96eb1f39-c59a-449c-81c5-00f31c49d9c0-xwwfv": Phase="Running", Reason="", readiness=true. Elapsed: 2.10506ms
    Jan 30 12:21:34.516: INFO: Pod "wrapped-volume-race-96eb1f39-c59a-449c-81c5-00f31c49d9c0-xwwfv" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-96eb1f39-c59a-449c-81c5-00f31c49d9c0 in namespace emptydir-wrapper-1245, will wait for the garbage collector to delete the pods 01/30/23 12:21:34.516
    Jan 30 12:21:34.572: INFO: Deleting ReplicationController wrapped-volume-race-96eb1f39-c59a-449c-81c5-00f31c49d9c0 took: 2.811217ms
    Jan 30 12:21:34.673: INFO: Terminating ReplicationController wrapped-volume-race-96eb1f39-c59a-449c-81c5-00f31c49d9c0 pods took: 100.70004ms
    STEP: Cleaning up the configMaps 01/30/23 12:21:38.273
    [AfterEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/node/init/init.go:32
    Jan 30 12:21:38.374: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-wrapper-1245" for this suite. 01/30/23 12:21:38.376
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:184
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 12:21:38.38
Jan 30 12:21:38.380: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename container-probe 01/30/23 12:21:38.38
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:21:38.387
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:21:38.389
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:184
STEP: Creating pod liveness-ea24c42b-1ea6-4f07-bd1b-606829d4fb60 in namespace container-probe-3272 01/30/23 12:21:38.391
Jan 30 12:21:38.396: INFO: Waiting up to 5m0s for pod "liveness-ea24c42b-1ea6-4f07-bd1b-606829d4fb60" in namespace "container-probe-3272" to be "not pending"
Jan 30 12:21:38.397: INFO: Pod "liveness-ea24c42b-1ea6-4f07-bd1b-606829d4fb60": Phase="Pending", Reason="", readiness=false. Elapsed: 1.471224ms
Jan 30 12:21:40.399: INFO: Pod "liveness-ea24c42b-1ea6-4f07-bd1b-606829d4fb60": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003754306s
Jan 30 12:21:42.400: INFO: Pod "liveness-ea24c42b-1ea6-4f07-bd1b-606829d4fb60": Phase="Running", Reason="", readiness=true. Elapsed: 4.00396725s
Jan 30 12:21:42.400: INFO: Pod "liveness-ea24c42b-1ea6-4f07-bd1b-606829d4fb60" satisfied condition "not pending"
Jan 30 12:21:42.400: INFO: Started pod liveness-ea24c42b-1ea6-4f07-bd1b-606829d4fb60 in namespace container-probe-3272
STEP: checking the pod's current state and verifying that restartCount is present 01/30/23 12:21:42.4
Jan 30 12:21:42.401: INFO: Initial restart count of pod liveness-ea24c42b-1ea6-4f07-bd1b-606829d4fb60 is 0
STEP: deleting the pod 01/30/23 12:25:42.785
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Jan 30 12:25:42.790: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-3272" for this suite. 01/30/23 12:25:42.792
------------------------------
• [SLOW TEST] [244.415 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 12:21:38.38
    Jan 30 12:21:38.380: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename container-probe 01/30/23 12:21:38.38
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:21:38.387
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:21:38.389
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:184
    STEP: Creating pod liveness-ea24c42b-1ea6-4f07-bd1b-606829d4fb60 in namespace container-probe-3272 01/30/23 12:21:38.391
    Jan 30 12:21:38.396: INFO: Waiting up to 5m0s for pod "liveness-ea24c42b-1ea6-4f07-bd1b-606829d4fb60" in namespace "container-probe-3272" to be "not pending"
    Jan 30 12:21:38.397: INFO: Pod "liveness-ea24c42b-1ea6-4f07-bd1b-606829d4fb60": Phase="Pending", Reason="", readiness=false. Elapsed: 1.471224ms
    Jan 30 12:21:40.399: INFO: Pod "liveness-ea24c42b-1ea6-4f07-bd1b-606829d4fb60": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003754306s
    Jan 30 12:21:42.400: INFO: Pod "liveness-ea24c42b-1ea6-4f07-bd1b-606829d4fb60": Phase="Running", Reason="", readiness=true. Elapsed: 4.00396725s
    Jan 30 12:21:42.400: INFO: Pod "liveness-ea24c42b-1ea6-4f07-bd1b-606829d4fb60" satisfied condition "not pending"
    Jan 30 12:21:42.400: INFO: Started pod liveness-ea24c42b-1ea6-4f07-bd1b-606829d4fb60 in namespace container-probe-3272
    STEP: checking the pod's current state and verifying that restartCount is present 01/30/23 12:21:42.4
    Jan 30 12:21:42.401: INFO: Initial restart count of pod liveness-ea24c42b-1ea6-4f07-bd1b-606829d4fb60 is 0
    STEP: deleting the pod 01/30/23 12:25:42.785
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Jan 30 12:25:42.790: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-3272" for this suite. 01/30/23 12:25:42.792
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial]
  validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:130
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 12:25:42.795
Jan 30 12:25:42.795: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename sched-preemption 01/30/23 12:25:42.796
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:25:42.802
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:25:42.804
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:97
Jan 30 12:25:42.812: INFO: Waiting up to 1m0s for all nodes to be ready
Jan 30 12:26:42.829: INFO: Waiting for terminating namespaces to be deleted...
[It] validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:130
STEP: Create pods that use 4/5 of node resources. 01/30/23 12:26:42.831
Jan 30 12:26:42.846: INFO: Created pod: pod0-0-sched-preemption-low-priority
Jan 30 12:26:42.849: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Jan 30 12:26:42.860: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Jan 30 12:26:42.862: INFO: Created pod: pod1-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled. 01/30/23 12:26:42.862
Jan 30 12:26:42.863: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-1911" to be "running"
Jan 30 12:26:42.864: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 1.796718ms
Jan 30 12:26:44.867: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004809553s
Jan 30 12:26:46.868: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.005735458s
Jan 30 12:26:46.868: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
Jan 30 12:26:46.868: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-1911" to be "running"
Jan 30 12:26:46.870: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 1.579882ms
Jan 30 12:26:46.870: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
Jan 30 12:26:46.870: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-1911" to be "running"
Jan 30 12:26:46.872: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 1.658527ms
Jan 30 12:26:46.872: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
Jan 30 12:26:46.872: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-1911" to be "running"
Jan 30 12:26:46.873: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 1.635396ms
Jan 30 12:26:46.873: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
STEP: Run a high priority pod that has same requirements as that of lower priority pod 01/30/23 12:26:46.873
Jan 30 12:26:46.876: INFO: Waiting up to 2m0s for pod "preemptor-pod" in namespace "sched-preemption-1911" to be "running"
Jan 30 12:26:46.878: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 1.522636ms
Jan 30 12:26:48.881: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00493314s
Jan 30 12:26:50.881: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.004846587s
Jan 30 12:26:52.882: INFO: Pod "preemptor-pod": Phase="Running", Reason="", readiness=true. Elapsed: 6.006005819s
Jan 30 12:26:52.882: INFO: Pod "preemptor-pod" satisfied condition "running"
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/node/init/init.go:32
Jan 30 12:26:52.891: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:84
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-1911" for this suite. 01/30/23 12:26:52.918
------------------------------
• [SLOW TEST] [70.125 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:130

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 12:25:42.795
    Jan 30 12:25:42.795: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename sched-preemption 01/30/23 12:25:42.796
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:25:42.802
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:25:42.804
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:97
    Jan 30 12:25:42.812: INFO: Waiting up to 1m0s for all nodes to be ready
    Jan 30 12:26:42.829: INFO: Waiting for terminating namespaces to be deleted...
    [It] validates basic preemption works [Conformance]
      test/e2e/scheduling/preemption.go:130
    STEP: Create pods that use 4/5 of node resources. 01/30/23 12:26:42.831
    Jan 30 12:26:42.846: INFO: Created pod: pod0-0-sched-preemption-low-priority
    Jan 30 12:26:42.849: INFO: Created pod: pod0-1-sched-preemption-medium-priority
    Jan 30 12:26:42.860: INFO: Created pod: pod1-0-sched-preemption-medium-priority
    Jan 30 12:26:42.862: INFO: Created pod: pod1-1-sched-preemption-medium-priority
    STEP: Wait for pods to be scheduled. 01/30/23 12:26:42.862
    Jan 30 12:26:42.863: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-1911" to be "running"
    Jan 30 12:26:42.864: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 1.796718ms
    Jan 30 12:26:44.867: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004809553s
    Jan 30 12:26:46.868: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.005735458s
    Jan 30 12:26:46.868: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
    Jan 30 12:26:46.868: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-1911" to be "running"
    Jan 30 12:26:46.870: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 1.579882ms
    Jan 30 12:26:46.870: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
    Jan 30 12:26:46.870: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-1911" to be "running"
    Jan 30 12:26:46.872: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 1.658527ms
    Jan 30 12:26:46.872: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
    Jan 30 12:26:46.872: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-1911" to be "running"
    Jan 30 12:26:46.873: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 1.635396ms
    Jan 30 12:26:46.873: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
    STEP: Run a high priority pod that has same requirements as that of lower priority pod 01/30/23 12:26:46.873
    Jan 30 12:26:46.876: INFO: Waiting up to 2m0s for pod "preemptor-pod" in namespace "sched-preemption-1911" to be "running"
    Jan 30 12:26:46.878: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 1.522636ms
    Jan 30 12:26:48.881: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00493314s
    Jan 30 12:26:50.881: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.004846587s
    Jan 30 12:26:52.882: INFO: Pod "preemptor-pod": Phase="Running", Reason="", readiness=true. Elapsed: 6.006005819s
    Jan 30 12:26:52.882: INFO: Pod "preemptor-pod" satisfied condition "running"
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan 30 12:26:52.891: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:84
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-1911" for this suite. 01/30/23 12:26:52.918
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 12:26:52.921
Jan 30 12:26:52.921: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename watch 01/30/23 12:26:52.922
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:26:52.929
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:26:52.93
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257
STEP: creating a watch on configmaps with a certain label 01/30/23 12:26:52.932
STEP: creating a new configmap 01/30/23 12:26:52.933
STEP: modifying the configmap once 01/30/23 12:26:52.935
STEP: changing the label value of the configmap 01/30/23 12:26:52.939
STEP: Expecting to observe a delete notification for the watched object 01/30/23 12:26:52.942
Jan 30 12:26:52.942: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5971  25bc59d0-c43e-4e5f-8306-5d8ff591f5a0 35257 0 2023-01-30 12:26:52 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-30 12:26:52 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 30 12:26:52.942: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5971  25bc59d0-c43e-4e5f-8306-5d8ff591f5a0 35258 0 2023-01-30 12:26:52 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-30 12:26:52 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 30 12:26:52.942: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5971  25bc59d0-c43e-4e5f-8306-5d8ff591f5a0 35259 0 2023-01-30 12:26:52 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-30 12:26:52 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time 01/30/23 12:26:52.942
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements 01/30/23 12:26:52.946
STEP: changing the label value of the configmap back 01/30/23 12:27:02.947
STEP: modifying the configmap a third time 01/30/23 12:27:02.951
STEP: deleting the configmap 01/30/23 12:27:02.955
STEP: Expecting to observe an add notification for the watched object when the label value was restored 01/30/23 12:27:02.957
Jan 30 12:27:02.958: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5971  25bc59d0-c43e-4e5f-8306-5d8ff591f5a0 35339 0 2023-01-30 12:26:52 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-30 12:27:02 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 30 12:27:02.958: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5971  25bc59d0-c43e-4e5f-8306-5d8ff591f5a0 35340 0 2023-01-30 12:26:52 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-30 12:27:02 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 30 12:27:02.958: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5971  25bc59d0-c43e-4e5f-8306-5d8ff591f5a0 35341 0 2023-01-30 12:26:52 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-30 12:27:02 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Jan 30 12:27:02.958: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-5971" for this suite. 01/30/23 12:27:02.96
------------------------------
• [SLOW TEST] [10.041 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 12:26:52.921
    Jan 30 12:26:52.921: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename watch 01/30/23 12:26:52.922
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:26:52.929
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:26:52.93
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
      test/e2e/apimachinery/watch.go:257
    STEP: creating a watch on configmaps with a certain label 01/30/23 12:26:52.932
    STEP: creating a new configmap 01/30/23 12:26:52.933
    STEP: modifying the configmap once 01/30/23 12:26:52.935
    STEP: changing the label value of the configmap 01/30/23 12:26:52.939
    STEP: Expecting to observe a delete notification for the watched object 01/30/23 12:26:52.942
    Jan 30 12:26:52.942: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5971  25bc59d0-c43e-4e5f-8306-5d8ff591f5a0 35257 0 2023-01-30 12:26:52 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-30 12:26:52 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan 30 12:26:52.942: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5971  25bc59d0-c43e-4e5f-8306-5d8ff591f5a0 35258 0 2023-01-30 12:26:52 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-30 12:26:52 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan 30 12:26:52.942: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5971  25bc59d0-c43e-4e5f-8306-5d8ff591f5a0 35259 0 2023-01-30 12:26:52 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-30 12:26:52 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying the configmap a second time 01/30/23 12:26:52.942
    STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements 01/30/23 12:26:52.946
    STEP: changing the label value of the configmap back 01/30/23 12:27:02.947
    STEP: modifying the configmap a third time 01/30/23 12:27:02.951
    STEP: deleting the configmap 01/30/23 12:27:02.955
    STEP: Expecting to observe an add notification for the watched object when the label value was restored 01/30/23 12:27:02.957
    Jan 30 12:27:02.958: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5971  25bc59d0-c43e-4e5f-8306-5d8ff591f5a0 35339 0 2023-01-30 12:26:52 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-30 12:27:02 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan 30 12:27:02.958: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5971  25bc59d0-c43e-4e5f-8306-5d8ff591f5a0 35340 0 2023-01-30 12:26:52 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-30 12:27:02 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan 30 12:27:02.958: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5971  25bc59d0-c43e-4e5f-8306-5d8ff591f5a0 35341 0 2023-01-30 12:26:52 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-30 12:27:02 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Jan 30 12:27:02.958: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-5971" for this suite. 01/30/23 12:27:02.96
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:276
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 12:27:02.964
Jan 30 12:27:02.964: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename crd-publish-openapi 01/30/23 12:27:02.965
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:27:02.971
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:27:02.973
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:276
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation 01/30/23 12:27:02.978
Jan 30 12:27:02.978: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
Jan 30 12:27:05.795: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 30 12:27:13.281: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-9351" for this suite. 01/30/23 12:27:13.287
------------------------------
• [SLOW TEST] [10.326 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:276

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 12:27:02.964
    Jan 30 12:27:02.964: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename crd-publish-openapi 01/30/23 12:27:02.965
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:27:02.971
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:27:02.973
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for multiple CRDs of different groups [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:276
    STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation 01/30/23 12:27:02.978
    Jan 30 12:27:02.978: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    Jan 30 12:27:05.795: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 30 12:27:13.281: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-9351" for this suite. 01/30/23 12:27:13.287
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-apps] ReplicationController
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:83
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 12:27:13.29
Jan 30 12:27:13.290: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename replication-controller 01/30/23 12:27:13.291
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:27:13.297
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:27:13.299
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:83
Jan 30 12:27:13.301: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota 01/30/23 12:27:14.307
STEP: Checking rc "condition-test" has the desired failure condition set 01/30/23 12:27:14.31
STEP: Scaling down rc "condition-test" to satisfy pod quota 01/30/23 12:27:15.315
Jan 30 12:27:15.320: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set 01/30/23 12:27:15.32
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Jan 30 12:27:16.324: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-925" for this suite. 01/30/23 12:27:16.326
------------------------------
• [3.038 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:83

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 12:27:13.29
    Jan 30 12:27:13.290: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename replication-controller 01/30/23 12:27:13.291
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:27:13.297
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:27:13.299
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should surface a failure condition on a common issue like exceeded quota [Conformance]
      test/e2e/apps/rc.go:83
    Jan 30 12:27:13.301: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
    STEP: Creating rc "condition-test" that asks for more than the allowed pod quota 01/30/23 12:27:14.307
    STEP: Checking rc "condition-test" has the desired failure condition set 01/30/23 12:27:14.31
    STEP: Scaling down rc "condition-test" to satisfy pod quota 01/30/23 12:27:15.315
    Jan 30 12:27:15.320: INFO: Updating replication controller "condition-test"
    STEP: Checking rc "condition-test" has no failure condition set 01/30/23 12:27:15.32
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Jan 30 12:27:16.324: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-925" for this suite. 01/30/23 12:27:16.326
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:72
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 12:27:16.329
Jan 30 12:27:16.329: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename container-probe 01/30/23 12:27:16.33
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:27:16.336
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:27:16.338
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:72
Jan 30 12:27:16.344: INFO: Waiting up to 5m0s for pod "test-webserver-a4ef2f6b-224b-459f-8dae-4e30e00d38db" in namespace "container-probe-4847" to be "running and ready"
Jan 30 12:27:16.345: INFO: Pod "test-webserver-a4ef2f6b-224b-459f-8dae-4e30e00d38db": Phase="Pending", Reason="", readiness=false. Elapsed: 1.579405ms
Jan 30 12:27:16.345: INFO: The phase of Pod test-webserver-a4ef2f6b-224b-459f-8dae-4e30e00d38db is Pending, waiting for it to be Running (with Ready = true)
Jan 30 12:27:18.348: INFO: Pod "test-webserver-a4ef2f6b-224b-459f-8dae-4e30e00d38db": Phase="Running", Reason="", readiness=false. Elapsed: 2.004071869s
Jan 30 12:27:18.348: INFO: The phase of Pod test-webserver-a4ef2f6b-224b-459f-8dae-4e30e00d38db is Running (Ready = false)
Jan 30 12:27:20.349: INFO: Pod "test-webserver-a4ef2f6b-224b-459f-8dae-4e30e00d38db": Phase="Running", Reason="", readiness=false. Elapsed: 4.004981076s
Jan 30 12:27:20.349: INFO: The phase of Pod test-webserver-a4ef2f6b-224b-459f-8dae-4e30e00d38db is Running (Ready = false)
Jan 30 12:27:22.348: INFO: Pod "test-webserver-a4ef2f6b-224b-459f-8dae-4e30e00d38db": Phase="Running", Reason="", readiness=false. Elapsed: 6.003954363s
Jan 30 12:27:22.348: INFO: The phase of Pod test-webserver-a4ef2f6b-224b-459f-8dae-4e30e00d38db is Running (Ready = false)
Jan 30 12:27:24.348: INFO: Pod "test-webserver-a4ef2f6b-224b-459f-8dae-4e30e00d38db": Phase="Running", Reason="", readiness=false. Elapsed: 8.004486626s
Jan 30 12:27:24.348: INFO: The phase of Pod test-webserver-a4ef2f6b-224b-459f-8dae-4e30e00d38db is Running (Ready = false)
Jan 30 12:27:26.349: INFO: Pod "test-webserver-a4ef2f6b-224b-459f-8dae-4e30e00d38db": Phase="Running", Reason="", readiness=false. Elapsed: 10.00504107s
Jan 30 12:27:26.349: INFO: The phase of Pod test-webserver-a4ef2f6b-224b-459f-8dae-4e30e00d38db is Running (Ready = false)
Jan 30 12:27:28.348: INFO: Pod "test-webserver-a4ef2f6b-224b-459f-8dae-4e30e00d38db": Phase="Running", Reason="", readiness=false. Elapsed: 12.004420504s
Jan 30 12:27:28.348: INFO: The phase of Pod test-webserver-a4ef2f6b-224b-459f-8dae-4e30e00d38db is Running (Ready = false)
Jan 30 12:27:30.349: INFO: Pod "test-webserver-a4ef2f6b-224b-459f-8dae-4e30e00d38db": Phase="Running", Reason="", readiness=false. Elapsed: 14.005427016s
Jan 30 12:27:30.349: INFO: The phase of Pod test-webserver-a4ef2f6b-224b-459f-8dae-4e30e00d38db is Running (Ready = false)
Jan 30 12:27:32.347: INFO: Pod "test-webserver-a4ef2f6b-224b-459f-8dae-4e30e00d38db": Phase="Running", Reason="", readiness=false. Elapsed: 16.003622188s
Jan 30 12:27:32.347: INFO: The phase of Pod test-webserver-a4ef2f6b-224b-459f-8dae-4e30e00d38db is Running (Ready = false)
Jan 30 12:27:34.349: INFO: Pod "test-webserver-a4ef2f6b-224b-459f-8dae-4e30e00d38db": Phase="Running", Reason="", readiness=false. Elapsed: 18.005342478s
Jan 30 12:27:34.349: INFO: The phase of Pod test-webserver-a4ef2f6b-224b-459f-8dae-4e30e00d38db is Running (Ready = false)
Jan 30 12:27:36.348: INFO: Pod "test-webserver-a4ef2f6b-224b-459f-8dae-4e30e00d38db": Phase="Running", Reason="", readiness=false. Elapsed: 20.004743663s
Jan 30 12:27:36.348: INFO: The phase of Pod test-webserver-a4ef2f6b-224b-459f-8dae-4e30e00d38db is Running (Ready = false)
Jan 30 12:27:38.347: INFO: Pod "test-webserver-a4ef2f6b-224b-459f-8dae-4e30e00d38db": Phase="Running", Reason="", readiness=true. Elapsed: 22.003832864s
Jan 30 12:27:38.347: INFO: The phase of Pod test-webserver-a4ef2f6b-224b-459f-8dae-4e30e00d38db is Running (Ready = true)
Jan 30 12:27:38.347: INFO: Pod "test-webserver-a4ef2f6b-224b-459f-8dae-4e30e00d38db" satisfied condition "running and ready"
Jan 30 12:27:38.350: INFO: Container started at 2023-01-30 12:27:17 +0000 UTC, pod became ready at 2023-01-30 12:27:36 +0000 UTC
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Jan 30 12:27:38.350: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-4847" for this suite. 01/30/23 12:27:38.352
------------------------------
• [SLOW TEST] [22.026 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:72

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 12:27:16.329
    Jan 30 12:27:16.329: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename container-probe 01/30/23 12:27:16.33
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:27:16.336
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:27:16.338
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:72
    Jan 30 12:27:16.344: INFO: Waiting up to 5m0s for pod "test-webserver-a4ef2f6b-224b-459f-8dae-4e30e00d38db" in namespace "container-probe-4847" to be "running and ready"
    Jan 30 12:27:16.345: INFO: Pod "test-webserver-a4ef2f6b-224b-459f-8dae-4e30e00d38db": Phase="Pending", Reason="", readiness=false. Elapsed: 1.579405ms
    Jan 30 12:27:16.345: INFO: The phase of Pod test-webserver-a4ef2f6b-224b-459f-8dae-4e30e00d38db is Pending, waiting for it to be Running (with Ready = true)
    Jan 30 12:27:18.348: INFO: Pod "test-webserver-a4ef2f6b-224b-459f-8dae-4e30e00d38db": Phase="Running", Reason="", readiness=false. Elapsed: 2.004071869s
    Jan 30 12:27:18.348: INFO: The phase of Pod test-webserver-a4ef2f6b-224b-459f-8dae-4e30e00d38db is Running (Ready = false)
    Jan 30 12:27:20.349: INFO: Pod "test-webserver-a4ef2f6b-224b-459f-8dae-4e30e00d38db": Phase="Running", Reason="", readiness=false. Elapsed: 4.004981076s
    Jan 30 12:27:20.349: INFO: The phase of Pod test-webserver-a4ef2f6b-224b-459f-8dae-4e30e00d38db is Running (Ready = false)
    Jan 30 12:27:22.348: INFO: Pod "test-webserver-a4ef2f6b-224b-459f-8dae-4e30e00d38db": Phase="Running", Reason="", readiness=false. Elapsed: 6.003954363s
    Jan 30 12:27:22.348: INFO: The phase of Pod test-webserver-a4ef2f6b-224b-459f-8dae-4e30e00d38db is Running (Ready = false)
    Jan 30 12:27:24.348: INFO: Pod "test-webserver-a4ef2f6b-224b-459f-8dae-4e30e00d38db": Phase="Running", Reason="", readiness=false. Elapsed: 8.004486626s
    Jan 30 12:27:24.348: INFO: The phase of Pod test-webserver-a4ef2f6b-224b-459f-8dae-4e30e00d38db is Running (Ready = false)
    Jan 30 12:27:26.349: INFO: Pod "test-webserver-a4ef2f6b-224b-459f-8dae-4e30e00d38db": Phase="Running", Reason="", readiness=false. Elapsed: 10.00504107s
    Jan 30 12:27:26.349: INFO: The phase of Pod test-webserver-a4ef2f6b-224b-459f-8dae-4e30e00d38db is Running (Ready = false)
    Jan 30 12:27:28.348: INFO: Pod "test-webserver-a4ef2f6b-224b-459f-8dae-4e30e00d38db": Phase="Running", Reason="", readiness=false. Elapsed: 12.004420504s
    Jan 30 12:27:28.348: INFO: The phase of Pod test-webserver-a4ef2f6b-224b-459f-8dae-4e30e00d38db is Running (Ready = false)
    Jan 30 12:27:30.349: INFO: Pod "test-webserver-a4ef2f6b-224b-459f-8dae-4e30e00d38db": Phase="Running", Reason="", readiness=false. Elapsed: 14.005427016s
    Jan 30 12:27:30.349: INFO: The phase of Pod test-webserver-a4ef2f6b-224b-459f-8dae-4e30e00d38db is Running (Ready = false)
    Jan 30 12:27:32.347: INFO: Pod "test-webserver-a4ef2f6b-224b-459f-8dae-4e30e00d38db": Phase="Running", Reason="", readiness=false. Elapsed: 16.003622188s
    Jan 30 12:27:32.347: INFO: The phase of Pod test-webserver-a4ef2f6b-224b-459f-8dae-4e30e00d38db is Running (Ready = false)
    Jan 30 12:27:34.349: INFO: Pod "test-webserver-a4ef2f6b-224b-459f-8dae-4e30e00d38db": Phase="Running", Reason="", readiness=false. Elapsed: 18.005342478s
    Jan 30 12:27:34.349: INFO: The phase of Pod test-webserver-a4ef2f6b-224b-459f-8dae-4e30e00d38db is Running (Ready = false)
    Jan 30 12:27:36.348: INFO: Pod "test-webserver-a4ef2f6b-224b-459f-8dae-4e30e00d38db": Phase="Running", Reason="", readiness=false. Elapsed: 20.004743663s
    Jan 30 12:27:36.348: INFO: The phase of Pod test-webserver-a4ef2f6b-224b-459f-8dae-4e30e00d38db is Running (Ready = false)
    Jan 30 12:27:38.347: INFO: Pod "test-webserver-a4ef2f6b-224b-459f-8dae-4e30e00d38db": Phase="Running", Reason="", readiness=true. Elapsed: 22.003832864s
    Jan 30 12:27:38.347: INFO: The phase of Pod test-webserver-a4ef2f6b-224b-459f-8dae-4e30e00d38db is Running (Ready = true)
    Jan 30 12:27:38.347: INFO: Pod "test-webserver-a4ef2f6b-224b-459f-8dae-4e30e00d38db" satisfied condition "running and ready"
    Jan 30 12:27:38.350: INFO: Container started at 2023-01-30 12:27:17 +0000 UTC, pod became ready at 2023-01-30 12:27:36 +0000 UTC
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Jan 30 12:27:38.350: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-4847" for this suite. 01/30/23 12:27:38.352
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] ReplicaSet
  Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 12:27:38.355
Jan 30 12:27:38.355: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename replicaset 01/30/23 12:27:38.356
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:27:38.363
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:27:38.365
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143
STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota 01/30/23 12:27:38.367
Jan 30 12:27:38.371: INFO: Pod name sample-pod: Found 0 pods out of 1
Jan 30 12:27:43.374: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 01/30/23 12:27:43.374
STEP: getting scale subresource 01/30/23 12:27:43.374
STEP: updating a scale subresource 01/30/23 12:27:43.376
STEP: verifying the replicaset Spec.Replicas was modified 01/30/23 12:27:43.379
STEP: Patch a scale subresource 01/30/23 12:27:43.38
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Jan 30 12:27:43.385: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-2496" for this suite. 01/30/23 12:27:43.388
------------------------------
• [SLOW TEST] [5.035 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 12:27:38.355
    Jan 30 12:27:38.355: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename replicaset 01/30/23 12:27:38.356
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:27:38.363
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:27:38.365
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] Replicaset should have a working scale subresource [Conformance]
      test/e2e/apps/replica_set.go:143
    STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota 01/30/23 12:27:38.367
    Jan 30 12:27:38.371: INFO: Pod name sample-pod: Found 0 pods out of 1
    Jan 30 12:27:43.374: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 01/30/23 12:27:43.374
    STEP: getting scale subresource 01/30/23 12:27:43.374
    STEP: updating a scale subresource 01/30/23 12:27:43.376
    STEP: verifying the replicaset Spec.Replicas was modified 01/30/23 12:27:43.379
    STEP: Patch a scale subresource 01/30/23 12:27:43.38
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Jan 30 12:27:43.385: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-2496" for this suite. 01/30/23 12:27:43.388
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] Variable Expansion
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:225
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 12:27:43.391
Jan 30 12:27:43.391: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename var-expansion 01/30/23 12:27:43.391
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:27:43.398
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:27:43.4
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:225
STEP: creating the pod with failed condition 01/30/23 12:27:43.402
Jan 30 12:27:43.407: INFO: Waiting up to 2m0s for pod "var-expansion-e6d063a8-3b74-42be-a730-6dabd2a63e25" in namespace "var-expansion-8736" to be "running"
Jan 30 12:27:43.408: INFO: Pod "var-expansion-e6d063a8-3b74-42be-a730-6dabd2a63e25": Phase="Pending", Reason="", readiness=false. Elapsed: 1.566449ms
Jan 30 12:27:45.412: INFO: Pod "var-expansion-e6d063a8-3b74-42be-a730-6dabd2a63e25": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004942342s
Jan 30 12:27:47.412: INFO: Pod "var-expansion-e6d063a8-3b74-42be-a730-6dabd2a63e25": Phase="Pending", Reason="", readiness=false. Elapsed: 4.004925448s
Jan 30 12:27:49.411: INFO: Pod "var-expansion-e6d063a8-3b74-42be-a730-6dabd2a63e25": Phase="Pending", Reason="", readiness=false. Elapsed: 6.004371006s
Jan 30 12:27:51.411: INFO: Pod "var-expansion-e6d063a8-3b74-42be-a730-6dabd2a63e25": Phase="Pending", Reason="", readiness=false. Elapsed: 8.00446981s
Jan 30 12:27:53.411: INFO: Pod "var-expansion-e6d063a8-3b74-42be-a730-6dabd2a63e25": Phase="Pending", Reason="", readiness=false. Elapsed: 10.003936228s
Jan 30 12:27:55.411: INFO: Pod "var-expansion-e6d063a8-3b74-42be-a730-6dabd2a63e25": Phase="Pending", Reason="", readiness=false. Elapsed: 12.003884928s
Jan 30 12:27:57.412: INFO: Pod "var-expansion-e6d063a8-3b74-42be-a730-6dabd2a63e25": Phase="Pending", Reason="", readiness=false. Elapsed: 14.005396805s
Jan 30 12:27:59.412: INFO: Pod "var-expansion-e6d063a8-3b74-42be-a730-6dabd2a63e25": Phase="Pending", Reason="", readiness=false. Elapsed: 16.004943403s
Jan 30 12:28:01.412: INFO: Pod "var-expansion-e6d063a8-3b74-42be-a730-6dabd2a63e25": Phase="Pending", Reason="", readiness=false. Elapsed: 18.004752174s
Jan 30 12:28:03.411: INFO: Pod "var-expansion-e6d063a8-3b74-42be-a730-6dabd2a63e25": Phase="Pending", Reason="", readiness=false. Elapsed: 20.004196699s
Jan 30 12:28:05.412: INFO: Pod "var-expansion-e6d063a8-3b74-42be-a730-6dabd2a63e25": Phase="Pending", Reason="", readiness=false. Elapsed: 22.005243552s
Jan 30 12:28:07.411: INFO: Pod "var-expansion-e6d063a8-3b74-42be-a730-6dabd2a63e25": Phase="Pending", Reason="", readiness=false. Elapsed: 24.004412322s
Jan 30 12:28:09.411: INFO: Pod "var-expansion-e6d063a8-3b74-42be-a730-6dabd2a63e25": Phase="Pending", Reason="", readiness=false. Elapsed: 26.00460217s
Jan 30 12:28:11.411: INFO: Pod "var-expansion-e6d063a8-3b74-42be-a730-6dabd2a63e25": Phase="Pending", Reason="", readiness=false. Elapsed: 28.004439519s
Jan 30 12:28:13.411: INFO: Pod "var-expansion-e6d063a8-3b74-42be-a730-6dabd2a63e25": Phase="Pending", Reason="", readiness=false. Elapsed: 30.004151507s
Jan 30 12:28:15.412: INFO: Pod "var-expansion-e6d063a8-3b74-42be-a730-6dabd2a63e25": Phase="Pending", Reason="", readiness=false. Elapsed: 32.004931429s
Jan 30 12:28:17.413: INFO: Pod "var-expansion-e6d063a8-3b74-42be-a730-6dabd2a63e25": Phase="Pending", Reason="", readiness=false. Elapsed: 34.005700018s
Jan 30 12:28:19.413: INFO: Pod "var-expansion-e6d063a8-3b74-42be-a730-6dabd2a63e25": Phase="Pending", Reason="", readiness=false. Elapsed: 36.00564568s
Jan 30 12:28:21.412: INFO: Pod "var-expansion-e6d063a8-3b74-42be-a730-6dabd2a63e25": Phase="Pending", Reason="", readiness=false. Elapsed: 38.005410812s
Jan 30 12:28:23.411: INFO: Pod "var-expansion-e6d063a8-3b74-42be-a730-6dabd2a63e25": Phase="Pending", Reason="", readiness=false. Elapsed: 40.004283158s
Jan 30 12:28:25.412: INFO: Pod "var-expansion-e6d063a8-3b74-42be-a730-6dabd2a63e25": Phase="Pending", Reason="", readiness=false. Elapsed: 42.005568876s
Jan 30 12:28:27.412: INFO: Pod "var-expansion-e6d063a8-3b74-42be-a730-6dabd2a63e25": Phase="Pending", Reason="", readiness=false. Elapsed: 44.0055596s
Jan 30 12:28:29.412: INFO: Pod "var-expansion-e6d063a8-3b74-42be-a730-6dabd2a63e25": Phase="Pending", Reason="", readiness=false. Elapsed: 46.004904965s
Jan 30 12:28:31.412: INFO: Pod "var-expansion-e6d063a8-3b74-42be-a730-6dabd2a63e25": Phase="Pending", Reason="", readiness=false. Elapsed: 48.005063046s
Jan 30 12:28:33.411: INFO: Pod "var-expansion-e6d063a8-3b74-42be-a730-6dabd2a63e25": Phase="Pending", Reason="", readiness=false. Elapsed: 50.004381686s
Jan 30 12:28:35.413: INFO: Pod "var-expansion-e6d063a8-3b74-42be-a730-6dabd2a63e25": Phase="Pending", Reason="", readiness=false. Elapsed: 52.005668225s
Jan 30 12:28:37.412: INFO: Pod "var-expansion-e6d063a8-3b74-42be-a730-6dabd2a63e25": Phase="Pending", Reason="", readiness=false. Elapsed: 54.005588234s
Jan 30 12:28:39.412: INFO: Pod "var-expansion-e6d063a8-3b74-42be-a730-6dabd2a63e25": Phase="Pending", Reason="", readiness=false. Elapsed: 56.004733011s
Jan 30 12:28:41.411: INFO: Pod "var-expansion-e6d063a8-3b74-42be-a730-6dabd2a63e25": Phase="Pending", Reason="", readiness=false. Elapsed: 58.003958706s
Jan 30 12:28:43.411: INFO: Pod "var-expansion-e6d063a8-3b74-42be-a730-6dabd2a63e25": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.004144132s
Jan 30 12:28:45.411: INFO: Pod "var-expansion-e6d063a8-3b74-42be-a730-6dabd2a63e25": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.004293694s
Jan 30 12:28:47.412: INFO: Pod "var-expansion-e6d063a8-3b74-42be-a730-6dabd2a63e25": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.005480997s
Jan 30 12:28:49.412: INFO: Pod "var-expansion-e6d063a8-3b74-42be-a730-6dabd2a63e25": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.004641316s
Jan 30 12:28:51.412: INFO: Pod "var-expansion-e6d063a8-3b74-42be-a730-6dabd2a63e25": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.005470913s
Jan 30 12:28:53.412: INFO: Pod "var-expansion-e6d063a8-3b74-42be-a730-6dabd2a63e25": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.004821625s
Jan 30 12:28:55.412: INFO: Pod "var-expansion-e6d063a8-3b74-42be-a730-6dabd2a63e25": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.005120653s
Jan 30 12:28:57.411: INFO: Pod "var-expansion-e6d063a8-3b74-42be-a730-6dabd2a63e25": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.004569788s
Jan 30 12:28:59.411: INFO: Pod "var-expansion-e6d063a8-3b74-42be-a730-6dabd2a63e25": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.004542653s
Jan 30 12:29:01.412: INFO: Pod "var-expansion-e6d063a8-3b74-42be-a730-6dabd2a63e25": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.005577525s
Jan 30 12:29:03.411: INFO: Pod "var-expansion-e6d063a8-3b74-42be-a730-6dabd2a63e25": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.003960958s
Jan 30 12:29:05.412: INFO: Pod "var-expansion-e6d063a8-3b74-42be-a730-6dabd2a63e25": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.005246269s
Jan 30 12:29:07.412: INFO: Pod "var-expansion-e6d063a8-3b74-42be-a730-6dabd2a63e25": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.005260925s
Jan 30 12:29:09.411: INFO: Pod "var-expansion-e6d063a8-3b74-42be-a730-6dabd2a63e25": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.00444815s
Jan 30 12:29:11.411: INFO: Pod "var-expansion-e6d063a8-3b74-42be-a730-6dabd2a63e25": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.004062739s
Jan 30 12:29:13.411: INFO: Pod "var-expansion-e6d063a8-3b74-42be-a730-6dabd2a63e25": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.004465511s
Jan 30 12:29:15.412: INFO: Pod "var-expansion-e6d063a8-3b74-42be-a730-6dabd2a63e25": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.004997014s
Jan 30 12:29:17.412: INFO: Pod "var-expansion-e6d063a8-3b74-42be-a730-6dabd2a63e25": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.004743729s
Jan 30 12:29:19.413: INFO: Pod "var-expansion-e6d063a8-3b74-42be-a730-6dabd2a63e25": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.005838781s
Jan 30 12:29:21.411: INFO: Pod "var-expansion-e6d063a8-3b74-42be-a730-6dabd2a63e25": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.004558749s
Jan 30 12:29:23.411: INFO: Pod "var-expansion-e6d063a8-3b74-42be-a730-6dabd2a63e25": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.003860002s
Jan 30 12:29:25.412: INFO: Pod "var-expansion-e6d063a8-3b74-42be-a730-6dabd2a63e25": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.004803759s
Jan 30 12:29:27.411: INFO: Pod "var-expansion-e6d063a8-3b74-42be-a730-6dabd2a63e25": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.004479276s
Jan 30 12:29:29.411: INFO: Pod "var-expansion-e6d063a8-3b74-42be-a730-6dabd2a63e25": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.004592371s
Jan 30 12:29:31.412: INFO: Pod "var-expansion-e6d063a8-3b74-42be-a730-6dabd2a63e25": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.005411219s
Jan 30 12:29:33.411: INFO: Pod "var-expansion-e6d063a8-3b74-42be-a730-6dabd2a63e25": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.003764584s
Jan 30 12:29:35.411: INFO: Pod "var-expansion-e6d063a8-3b74-42be-a730-6dabd2a63e25": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.004549247s
Jan 30 12:29:37.412: INFO: Pod "var-expansion-e6d063a8-3b74-42be-a730-6dabd2a63e25": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.005488327s
Jan 30 12:29:39.412: INFO: Pod "var-expansion-e6d063a8-3b74-42be-a730-6dabd2a63e25": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.004974666s
Jan 30 12:29:41.411: INFO: Pod "var-expansion-e6d063a8-3b74-42be-a730-6dabd2a63e25": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.004237158s
Jan 30 12:29:43.412: INFO: Pod "var-expansion-e6d063a8-3b74-42be-a730-6dabd2a63e25": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.004636249s
Jan 30 12:29:43.413: INFO: Pod "var-expansion-e6d063a8-3b74-42be-a730-6dabd2a63e25": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.006319183s
STEP: updating the pod 01/30/23 12:29:43.413
Jan 30 12:29:43.923: INFO: Successfully updated pod "var-expansion-e6d063a8-3b74-42be-a730-6dabd2a63e25"
STEP: waiting for pod running 01/30/23 12:29:43.923
Jan 30 12:29:43.923: INFO: Waiting up to 2m0s for pod "var-expansion-e6d063a8-3b74-42be-a730-6dabd2a63e25" in namespace "var-expansion-8736" to be "running"
Jan 30 12:29:43.925: INFO: Pod "var-expansion-e6d063a8-3b74-42be-a730-6dabd2a63e25": Phase="Pending", Reason="", readiness=false. Elapsed: 1.846127ms
Jan 30 12:29:45.928: INFO: Pod "var-expansion-e6d063a8-3b74-42be-a730-6dabd2a63e25": Phase="Running", Reason="", readiness=true. Elapsed: 2.005275672s
Jan 30 12:29:45.928: INFO: Pod "var-expansion-e6d063a8-3b74-42be-a730-6dabd2a63e25" satisfied condition "running"
STEP: deleting the pod gracefully 01/30/23 12:29:45.928
Jan 30 12:29:45.928: INFO: Deleting pod "var-expansion-e6d063a8-3b74-42be-a730-6dabd2a63e25" in namespace "var-expansion-8736"
Jan 30 12:29:45.931: INFO: Wait up to 5m0s for pod "var-expansion-e6d063a8-3b74-42be-a730-6dabd2a63e25" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Jan 30 12:30:17.936: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-8736" for this suite. 01/30/23 12:30:17.938
------------------------------
• [SLOW TEST] [154.550 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:225

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 12:27:43.391
    Jan 30 12:27:43.391: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename var-expansion 01/30/23 12:27:43.391
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:27:43.398
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:27:43.4
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
      test/e2e/common/node/expansion.go:225
    STEP: creating the pod with failed condition 01/30/23 12:27:43.402
    Jan 30 12:27:43.407: INFO: Waiting up to 2m0s for pod "var-expansion-e6d063a8-3b74-42be-a730-6dabd2a63e25" in namespace "var-expansion-8736" to be "running"
    Jan 30 12:27:43.408: INFO: Pod "var-expansion-e6d063a8-3b74-42be-a730-6dabd2a63e25": Phase="Pending", Reason="", readiness=false. Elapsed: 1.566449ms
    Jan 30 12:27:45.412: INFO: Pod "var-expansion-e6d063a8-3b74-42be-a730-6dabd2a63e25": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004942342s
    Jan 30 12:27:47.412: INFO: Pod "var-expansion-e6d063a8-3b74-42be-a730-6dabd2a63e25": Phase="Pending", Reason="", readiness=false. Elapsed: 4.004925448s
    Jan 30 12:27:49.411: INFO: Pod "var-expansion-e6d063a8-3b74-42be-a730-6dabd2a63e25": Phase="Pending", Reason="", readiness=false. Elapsed: 6.004371006s
    Jan 30 12:27:51.411: INFO: Pod "var-expansion-e6d063a8-3b74-42be-a730-6dabd2a63e25": Phase="Pending", Reason="", readiness=false. Elapsed: 8.00446981s
    Jan 30 12:27:53.411: INFO: Pod "var-expansion-e6d063a8-3b74-42be-a730-6dabd2a63e25": Phase="Pending", Reason="", readiness=false. Elapsed: 10.003936228s
    Jan 30 12:27:55.411: INFO: Pod "var-expansion-e6d063a8-3b74-42be-a730-6dabd2a63e25": Phase="Pending", Reason="", readiness=false. Elapsed: 12.003884928s
    Jan 30 12:27:57.412: INFO: Pod "var-expansion-e6d063a8-3b74-42be-a730-6dabd2a63e25": Phase="Pending", Reason="", readiness=false. Elapsed: 14.005396805s
    Jan 30 12:27:59.412: INFO: Pod "var-expansion-e6d063a8-3b74-42be-a730-6dabd2a63e25": Phase="Pending", Reason="", readiness=false. Elapsed: 16.004943403s
    Jan 30 12:28:01.412: INFO: Pod "var-expansion-e6d063a8-3b74-42be-a730-6dabd2a63e25": Phase="Pending", Reason="", readiness=false. Elapsed: 18.004752174s
    Jan 30 12:28:03.411: INFO: Pod "var-expansion-e6d063a8-3b74-42be-a730-6dabd2a63e25": Phase="Pending", Reason="", readiness=false. Elapsed: 20.004196699s
    Jan 30 12:28:05.412: INFO: Pod "var-expansion-e6d063a8-3b74-42be-a730-6dabd2a63e25": Phase="Pending", Reason="", readiness=false. Elapsed: 22.005243552s
    Jan 30 12:28:07.411: INFO: Pod "var-expansion-e6d063a8-3b74-42be-a730-6dabd2a63e25": Phase="Pending", Reason="", readiness=false. Elapsed: 24.004412322s
    Jan 30 12:28:09.411: INFO: Pod "var-expansion-e6d063a8-3b74-42be-a730-6dabd2a63e25": Phase="Pending", Reason="", readiness=false. Elapsed: 26.00460217s
    Jan 30 12:28:11.411: INFO: Pod "var-expansion-e6d063a8-3b74-42be-a730-6dabd2a63e25": Phase="Pending", Reason="", readiness=false. Elapsed: 28.004439519s
    Jan 30 12:28:13.411: INFO: Pod "var-expansion-e6d063a8-3b74-42be-a730-6dabd2a63e25": Phase="Pending", Reason="", readiness=false. Elapsed: 30.004151507s
    Jan 30 12:28:15.412: INFO: Pod "var-expansion-e6d063a8-3b74-42be-a730-6dabd2a63e25": Phase="Pending", Reason="", readiness=false. Elapsed: 32.004931429s
    Jan 30 12:28:17.413: INFO: Pod "var-expansion-e6d063a8-3b74-42be-a730-6dabd2a63e25": Phase="Pending", Reason="", readiness=false. Elapsed: 34.005700018s
    Jan 30 12:28:19.413: INFO: Pod "var-expansion-e6d063a8-3b74-42be-a730-6dabd2a63e25": Phase="Pending", Reason="", readiness=false. Elapsed: 36.00564568s
    Jan 30 12:28:21.412: INFO: Pod "var-expansion-e6d063a8-3b74-42be-a730-6dabd2a63e25": Phase="Pending", Reason="", readiness=false. Elapsed: 38.005410812s
    Jan 30 12:28:23.411: INFO: Pod "var-expansion-e6d063a8-3b74-42be-a730-6dabd2a63e25": Phase="Pending", Reason="", readiness=false. Elapsed: 40.004283158s
    Jan 30 12:28:25.412: INFO: Pod "var-expansion-e6d063a8-3b74-42be-a730-6dabd2a63e25": Phase="Pending", Reason="", readiness=false. Elapsed: 42.005568876s
    Jan 30 12:28:27.412: INFO: Pod "var-expansion-e6d063a8-3b74-42be-a730-6dabd2a63e25": Phase="Pending", Reason="", readiness=false. Elapsed: 44.0055596s
    Jan 30 12:28:29.412: INFO: Pod "var-expansion-e6d063a8-3b74-42be-a730-6dabd2a63e25": Phase="Pending", Reason="", readiness=false. Elapsed: 46.004904965s
    Jan 30 12:28:31.412: INFO: Pod "var-expansion-e6d063a8-3b74-42be-a730-6dabd2a63e25": Phase="Pending", Reason="", readiness=false. Elapsed: 48.005063046s
    Jan 30 12:28:33.411: INFO: Pod "var-expansion-e6d063a8-3b74-42be-a730-6dabd2a63e25": Phase="Pending", Reason="", readiness=false. Elapsed: 50.004381686s
    Jan 30 12:28:35.413: INFO: Pod "var-expansion-e6d063a8-3b74-42be-a730-6dabd2a63e25": Phase="Pending", Reason="", readiness=false. Elapsed: 52.005668225s
    Jan 30 12:28:37.412: INFO: Pod "var-expansion-e6d063a8-3b74-42be-a730-6dabd2a63e25": Phase="Pending", Reason="", readiness=false. Elapsed: 54.005588234s
    Jan 30 12:28:39.412: INFO: Pod "var-expansion-e6d063a8-3b74-42be-a730-6dabd2a63e25": Phase="Pending", Reason="", readiness=false. Elapsed: 56.004733011s
    Jan 30 12:28:41.411: INFO: Pod "var-expansion-e6d063a8-3b74-42be-a730-6dabd2a63e25": Phase="Pending", Reason="", readiness=false. Elapsed: 58.003958706s
    Jan 30 12:28:43.411: INFO: Pod "var-expansion-e6d063a8-3b74-42be-a730-6dabd2a63e25": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.004144132s
    Jan 30 12:28:45.411: INFO: Pod "var-expansion-e6d063a8-3b74-42be-a730-6dabd2a63e25": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.004293694s
    Jan 30 12:28:47.412: INFO: Pod "var-expansion-e6d063a8-3b74-42be-a730-6dabd2a63e25": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.005480997s
    Jan 30 12:28:49.412: INFO: Pod "var-expansion-e6d063a8-3b74-42be-a730-6dabd2a63e25": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.004641316s
    Jan 30 12:28:51.412: INFO: Pod "var-expansion-e6d063a8-3b74-42be-a730-6dabd2a63e25": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.005470913s
    Jan 30 12:28:53.412: INFO: Pod "var-expansion-e6d063a8-3b74-42be-a730-6dabd2a63e25": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.004821625s
    Jan 30 12:28:55.412: INFO: Pod "var-expansion-e6d063a8-3b74-42be-a730-6dabd2a63e25": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.005120653s
    Jan 30 12:28:57.411: INFO: Pod "var-expansion-e6d063a8-3b74-42be-a730-6dabd2a63e25": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.004569788s
    Jan 30 12:28:59.411: INFO: Pod "var-expansion-e6d063a8-3b74-42be-a730-6dabd2a63e25": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.004542653s
    Jan 30 12:29:01.412: INFO: Pod "var-expansion-e6d063a8-3b74-42be-a730-6dabd2a63e25": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.005577525s
    Jan 30 12:29:03.411: INFO: Pod "var-expansion-e6d063a8-3b74-42be-a730-6dabd2a63e25": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.003960958s
    Jan 30 12:29:05.412: INFO: Pod "var-expansion-e6d063a8-3b74-42be-a730-6dabd2a63e25": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.005246269s
    Jan 30 12:29:07.412: INFO: Pod "var-expansion-e6d063a8-3b74-42be-a730-6dabd2a63e25": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.005260925s
    Jan 30 12:29:09.411: INFO: Pod "var-expansion-e6d063a8-3b74-42be-a730-6dabd2a63e25": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.00444815s
    Jan 30 12:29:11.411: INFO: Pod "var-expansion-e6d063a8-3b74-42be-a730-6dabd2a63e25": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.004062739s
    Jan 30 12:29:13.411: INFO: Pod "var-expansion-e6d063a8-3b74-42be-a730-6dabd2a63e25": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.004465511s
    Jan 30 12:29:15.412: INFO: Pod "var-expansion-e6d063a8-3b74-42be-a730-6dabd2a63e25": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.004997014s
    Jan 30 12:29:17.412: INFO: Pod "var-expansion-e6d063a8-3b74-42be-a730-6dabd2a63e25": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.004743729s
    Jan 30 12:29:19.413: INFO: Pod "var-expansion-e6d063a8-3b74-42be-a730-6dabd2a63e25": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.005838781s
    Jan 30 12:29:21.411: INFO: Pod "var-expansion-e6d063a8-3b74-42be-a730-6dabd2a63e25": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.004558749s
    Jan 30 12:29:23.411: INFO: Pod "var-expansion-e6d063a8-3b74-42be-a730-6dabd2a63e25": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.003860002s
    Jan 30 12:29:25.412: INFO: Pod "var-expansion-e6d063a8-3b74-42be-a730-6dabd2a63e25": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.004803759s
    Jan 30 12:29:27.411: INFO: Pod "var-expansion-e6d063a8-3b74-42be-a730-6dabd2a63e25": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.004479276s
    Jan 30 12:29:29.411: INFO: Pod "var-expansion-e6d063a8-3b74-42be-a730-6dabd2a63e25": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.004592371s
    Jan 30 12:29:31.412: INFO: Pod "var-expansion-e6d063a8-3b74-42be-a730-6dabd2a63e25": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.005411219s
    Jan 30 12:29:33.411: INFO: Pod "var-expansion-e6d063a8-3b74-42be-a730-6dabd2a63e25": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.003764584s
    Jan 30 12:29:35.411: INFO: Pod "var-expansion-e6d063a8-3b74-42be-a730-6dabd2a63e25": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.004549247s
    Jan 30 12:29:37.412: INFO: Pod "var-expansion-e6d063a8-3b74-42be-a730-6dabd2a63e25": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.005488327s
    Jan 30 12:29:39.412: INFO: Pod "var-expansion-e6d063a8-3b74-42be-a730-6dabd2a63e25": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.004974666s
    Jan 30 12:29:41.411: INFO: Pod "var-expansion-e6d063a8-3b74-42be-a730-6dabd2a63e25": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.004237158s
    Jan 30 12:29:43.412: INFO: Pod "var-expansion-e6d063a8-3b74-42be-a730-6dabd2a63e25": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.004636249s
    Jan 30 12:29:43.413: INFO: Pod "var-expansion-e6d063a8-3b74-42be-a730-6dabd2a63e25": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.006319183s
    STEP: updating the pod 01/30/23 12:29:43.413
    Jan 30 12:29:43.923: INFO: Successfully updated pod "var-expansion-e6d063a8-3b74-42be-a730-6dabd2a63e25"
    STEP: waiting for pod running 01/30/23 12:29:43.923
    Jan 30 12:29:43.923: INFO: Waiting up to 2m0s for pod "var-expansion-e6d063a8-3b74-42be-a730-6dabd2a63e25" in namespace "var-expansion-8736" to be "running"
    Jan 30 12:29:43.925: INFO: Pod "var-expansion-e6d063a8-3b74-42be-a730-6dabd2a63e25": Phase="Pending", Reason="", readiness=false. Elapsed: 1.846127ms
    Jan 30 12:29:45.928: INFO: Pod "var-expansion-e6d063a8-3b74-42be-a730-6dabd2a63e25": Phase="Running", Reason="", readiness=true. Elapsed: 2.005275672s
    Jan 30 12:29:45.928: INFO: Pod "var-expansion-e6d063a8-3b74-42be-a730-6dabd2a63e25" satisfied condition "running"
    STEP: deleting the pod gracefully 01/30/23 12:29:45.928
    Jan 30 12:29:45.928: INFO: Deleting pod "var-expansion-e6d063a8-3b74-42be-a730-6dabd2a63e25" in namespace "var-expansion-8736"
    Jan 30 12:29:45.931: INFO: Wait up to 5m0s for pod "var-expansion-e6d063a8-3b74-42be-a730-6dabd2a63e25" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Jan 30 12:30:17.936: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-8736" for this suite. 01/30/23 12:30:17.938
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:690
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 12:30:17.943
Jan 30 12:30:17.943: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename resourcequota 01/30/23 12:30:17.943
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:30:17.95
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:30:17.952
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:690
STEP: Creating a ResourceQuota with terminating scope 01/30/23 12:30:17.954
STEP: Ensuring ResourceQuota status is calculated 01/30/23 12:30:17.956
STEP: Creating a ResourceQuota with not terminating scope 01/30/23 12:30:19.958
STEP: Ensuring ResourceQuota status is calculated 01/30/23 12:30:19.961
STEP: Creating a long running pod 01/30/23 12:30:21.963
STEP: Ensuring resource quota with not terminating scope captures the pod usage 01/30/23 12:30:21.971
STEP: Ensuring resource quota with terminating scope ignored the pod usage 01/30/23 12:30:23.975
STEP: Deleting the pod 01/30/23 12:30:25.979
STEP: Ensuring resource quota status released the pod usage 01/30/23 12:30:25.984
STEP: Creating a terminating pod 01/30/23 12:30:27.986
STEP: Ensuring resource quota with terminating scope captures the pod usage 01/30/23 12:30:27.992
STEP: Ensuring resource quota with not terminating scope ignored the pod usage 01/30/23 12:30:29.995
STEP: Deleting the pod 01/30/23 12:30:31.998
STEP: Ensuring resource quota status released the pod usage 01/30/23 12:30:32.001
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Jan 30 12:30:34.004: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-8931" for this suite. 01/30/23 12:30:34.007
------------------------------
• [SLOW TEST] [16.067 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:690

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 12:30:17.943
    Jan 30 12:30:17.943: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename resourcequota 01/30/23 12:30:17.943
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:30:17.95
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:30:17.952
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should verify ResourceQuota with terminating scopes. [Conformance]
      test/e2e/apimachinery/resource_quota.go:690
    STEP: Creating a ResourceQuota with terminating scope 01/30/23 12:30:17.954
    STEP: Ensuring ResourceQuota status is calculated 01/30/23 12:30:17.956
    STEP: Creating a ResourceQuota with not terminating scope 01/30/23 12:30:19.958
    STEP: Ensuring ResourceQuota status is calculated 01/30/23 12:30:19.961
    STEP: Creating a long running pod 01/30/23 12:30:21.963
    STEP: Ensuring resource quota with not terminating scope captures the pod usage 01/30/23 12:30:21.971
    STEP: Ensuring resource quota with terminating scope ignored the pod usage 01/30/23 12:30:23.975
    STEP: Deleting the pod 01/30/23 12:30:25.979
    STEP: Ensuring resource quota status released the pod usage 01/30/23 12:30:25.984
    STEP: Creating a terminating pod 01/30/23 12:30:27.986
    STEP: Ensuring resource quota with terminating scope captures the pod usage 01/30/23 12:30:27.992
    STEP: Ensuring resource quota with not terminating scope ignored the pod usage 01/30/23 12:30:29.995
    STEP: Deleting the pod 01/30/23 12:30:31.998
    STEP: Ensuring resource quota status released the pod usage 01/30/23 12:30:32.001
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Jan 30 12:30:34.004: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-8931" for this suite. 01/30/23 12:30:34.007
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 12:30:34.01
Jan 30 12:30:34.010: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename custom-resource-definition 01/30/23 12:30:34.011
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:30:34.017
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:30:34.019
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198
STEP: fetching the /apis discovery document 01/30/23 12:30:34.021
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document 01/30/23 12:30:34.022
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document 01/30/23 12:30:34.022
STEP: fetching the /apis/apiextensions.k8s.io discovery document 01/30/23 12:30:34.022
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document 01/30/23 12:30:34.023
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document 01/30/23 12:30:34.023
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document 01/30/23 12:30:34.024
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 30 12:30:34.024: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-6867" for this suite. 01/30/23 12:30:34.026
------------------------------
• [0.018 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 12:30:34.01
    Jan 30 12:30:34.010: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename custom-resource-definition 01/30/23 12:30:34.011
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:30:34.017
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:30:34.019
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] should include custom resource definition resources in discovery documents [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:198
    STEP: fetching the /apis discovery document 01/30/23 12:30:34.021
    STEP: finding the apiextensions.k8s.io API group in the /apis discovery document 01/30/23 12:30:34.022
    STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document 01/30/23 12:30:34.022
    STEP: fetching the /apis/apiextensions.k8s.io discovery document 01/30/23 12:30:34.022
    STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document 01/30/23 12:30:34.023
    STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document 01/30/23 12:30:34.023
    STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document 01/30/23 12:30:34.024
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 30 12:30:34.024: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-6867" for this suite. 01/30/23 12:30:34.026
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1302
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 12:30:34.029
Jan 30 12:30:34.029: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename services 01/30/23 12:30:34.029
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:30:34.034
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:30:34.036
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1302
STEP: creating service nodeport-test with type=NodePort in namespace services-7586 01/30/23 12:30:34.038
STEP: creating replication controller nodeport-test in namespace services-7586 01/30/23 12:30:34.043
I0130 12:30:34.046105      23 runners.go:193] Created replication controller with name: nodeport-test, namespace: services-7586, replica count: 2
I0130 12:30:37.097548      23 runners.go:193] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 30 12:30:37.097: INFO: Creating new exec pod
Jan 30 12:30:37.100: INFO: Waiting up to 5m0s for pod "execpodk4bxl" in namespace "services-7586" to be "running"
Jan 30 12:30:37.101: INFO: Pod "execpodk4bxl": Phase="Pending", Reason="", readiness=false. Elapsed: 1.488945ms
Jan 30 12:30:39.104: INFO: Pod "execpodk4bxl": Phase="Running", Reason="", readiness=true. Elapsed: 2.004593584s
Jan 30 12:30:39.104: INFO: Pod "execpodk4bxl" satisfied condition "running"
Jan 30 12:30:40.107: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=services-7586 exec execpodk4bxl -- /bin/sh -x -c nc -v -z -w 2 nodeport-test 80'
Jan 30 12:30:40.276: INFO: stderr: "+ nc -v -z -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Jan 30 12:30:40.276: INFO: stdout: ""
Jan 30 12:30:40.276: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=services-7586 exec execpodk4bxl -- /bin/sh -x -c nc -v -z -w 2 10.178.93.231 80'
Jan 30 12:30:40.402: INFO: stderr: "+ nc -v -z -w 2 10.178.93.231 80\nConnection to 10.178.93.231 80 port [tcp/http] succeeded!\n"
Jan 30 12:30:40.402: INFO: stdout: ""
Jan 30 12:30:40.402: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=services-7586 exec execpodk4bxl -- /bin/sh -x -c nc -v -z -w 2 10.182.0.82 30713'
Jan 30 12:30:40.526: INFO: stderr: "+ nc -v -z -w 2 10.182.0.82 30713\nConnection to 10.182.0.82 30713 port [tcp/*] succeeded!\n"
Jan 30 12:30:40.526: INFO: stdout: ""
Jan 30 12:30:40.526: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=services-7586 exec execpodk4bxl -- /bin/sh -x -c nc -v -z -w 2 10.182.0.84 30713'
Jan 30 12:30:40.647: INFO: stderr: "+ nc -v -z -w 2 10.182.0.84 30713\nConnection to 10.182.0.84 30713 port [tcp/*] succeeded!\n"
Jan 30 12:30:40.647: INFO: stdout: ""
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jan 30 12:30:40.647: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-7586" for this suite. 01/30/23 12:30:40.649
------------------------------
• [SLOW TEST] [6.623 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1302

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 12:30:34.029
    Jan 30 12:30:34.029: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename services 01/30/23 12:30:34.029
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:30:34.034
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:30:34.036
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to create a functioning NodePort service [Conformance]
      test/e2e/network/service.go:1302
    STEP: creating service nodeport-test with type=NodePort in namespace services-7586 01/30/23 12:30:34.038
    STEP: creating replication controller nodeport-test in namespace services-7586 01/30/23 12:30:34.043
    I0130 12:30:34.046105      23 runners.go:193] Created replication controller with name: nodeport-test, namespace: services-7586, replica count: 2
    I0130 12:30:37.097548      23 runners.go:193] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan 30 12:30:37.097: INFO: Creating new exec pod
    Jan 30 12:30:37.100: INFO: Waiting up to 5m0s for pod "execpodk4bxl" in namespace "services-7586" to be "running"
    Jan 30 12:30:37.101: INFO: Pod "execpodk4bxl": Phase="Pending", Reason="", readiness=false. Elapsed: 1.488945ms
    Jan 30 12:30:39.104: INFO: Pod "execpodk4bxl": Phase="Running", Reason="", readiness=true. Elapsed: 2.004593584s
    Jan 30 12:30:39.104: INFO: Pod "execpodk4bxl" satisfied condition "running"
    Jan 30 12:30:40.107: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=services-7586 exec execpodk4bxl -- /bin/sh -x -c nc -v -z -w 2 nodeport-test 80'
    Jan 30 12:30:40.276: INFO: stderr: "+ nc -v -z -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
    Jan 30 12:30:40.276: INFO: stdout: ""
    Jan 30 12:30:40.276: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=services-7586 exec execpodk4bxl -- /bin/sh -x -c nc -v -z -w 2 10.178.93.231 80'
    Jan 30 12:30:40.402: INFO: stderr: "+ nc -v -z -w 2 10.178.93.231 80\nConnection to 10.178.93.231 80 port [tcp/http] succeeded!\n"
    Jan 30 12:30:40.402: INFO: stdout: ""
    Jan 30 12:30:40.402: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=services-7586 exec execpodk4bxl -- /bin/sh -x -c nc -v -z -w 2 10.182.0.82 30713'
    Jan 30 12:30:40.526: INFO: stderr: "+ nc -v -z -w 2 10.182.0.82 30713\nConnection to 10.182.0.82 30713 port [tcp/*] succeeded!\n"
    Jan 30 12:30:40.526: INFO: stdout: ""
    Jan 30 12:30:40.526: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=services-7586 exec execpodk4bxl -- /bin/sh -x -c nc -v -z -w 2 10.182.0.84 30713'
    Jan 30 12:30:40.647: INFO: stderr: "+ nc -v -z -w 2 10.182.0.84 30713\nConnection to 10.182.0.84 30713 port [tcp/*] succeeded!\n"
    Jan 30 12:30:40.647: INFO: stdout: ""
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jan 30 12:30:40.647: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-7586" for this suite. 01/30/23 12:30:40.649
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:207
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 12:30:40.653
Jan 30 12:30:40.653: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename emptydir 01/30/23 12:30:40.653
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:30:40.659
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:30:40.661
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:207
STEP: Creating a pod to test emptydir 0666 on node default medium 01/30/23 12:30:40.663
Jan 30 12:30:40.667: INFO: Waiting up to 5m0s for pod "pod-ec3c164d-a51e-4319-a4bb-24db260955e4" in namespace "emptydir-1676" to be "Succeeded or Failed"
Jan 30 12:30:40.668: INFO: Pod "pod-ec3c164d-a51e-4319-a4bb-24db260955e4": Phase="Pending", Reason="", readiness=false. Elapsed: 1.445888ms
Jan 30 12:30:42.671: INFO: Pod "pod-ec3c164d-a51e-4319-a4bb-24db260955e4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00389243s
Jan 30 12:30:44.672: INFO: Pod "pod-ec3c164d-a51e-4319-a4bb-24db260955e4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.004684777s
Jan 30 12:30:46.671: INFO: Pod "pod-ec3c164d-a51e-4319-a4bb-24db260955e4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.003992423s
STEP: Saw pod success 01/30/23 12:30:46.671
Jan 30 12:30:46.671: INFO: Pod "pod-ec3c164d-a51e-4319-a4bb-24db260955e4" satisfied condition "Succeeded or Failed"
Jan 30 12:30:46.673: INFO: Trying to get logs from node pubt2-nks-for-dev1.dg.163.org pod pod-ec3c164d-a51e-4319-a4bb-24db260955e4 container test-container: <nil>
STEP: delete the pod 01/30/23 12:30:46.687
Jan 30 12:30:46.691: INFO: Waiting for pod pod-ec3c164d-a51e-4319-a4bb-24db260955e4 to disappear
Jan 30 12:30:46.693: INFO: Pod pod-ec3c164d-a51e-4319-a4bb-24db260955e4 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jan 30 12:30:46.693: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-1676" for this suite. 01/30/23 12:30:46.695
------------------------------
• [SLOW TEST] [6.045 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 12:30:40.653
    Jan 30 12:30:40.653: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename emptydir 01/30/23 12:30:40.653
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:30:40.659
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:30:40.661
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:207
    STEP: Creating a pod to test emptydir 0666 on node default medium 01/30/23 12:30:40.663
    Jan 30 12:30:40.667: INFO: Waiting up to 5m0s for pod "pod-ec3c164d-a51e-4319-a4bb-24db260955e4" in namespace "emptydir-1676" to be "Succeeded or Failed"
    Jan 30 12:30:40.668: INFO: Pod "pod-ec3c164d-a51e-4319-a4bb-24db260955e4": Phase="Pending", Reason="", readiness=false. Elapsed: 1.445888ms
    Jan 30 12:30:42.671: INFO: Pod "pod-ec3c164d-a51e-4319-a4bb-24db260955e4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00389243s
    Jan 30 12:30:44.672: INFO: Pod "pod-ec3c164d-a51e-4319-a4bb-24db260955e4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.004684777s
    Jan 30 12:30:46.671: INFO: Pod "pod-ec3c164d-a51e-4319-a4bb-24db260955e4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.003992423s
    STEP: Saw pod success 01/30/23 12:30:46.671
    Jan 30 12:30:46.671: INFO: Pod "pod-ec3c164d-a51e-4319-a4bb-24db260955e4" satisfied condition "Succeeded or Failed"
    Jan 30 12:30:46.673: INFO: Trying to get logs from node pubt2-nks-for-dev1.dg.163.org pod pod-ec3c164d-a51e-4319-a4bb-24db260955e4 container test-container: <nil>
    STEP: delete the pod 01/30/23 12:30:46.687
    Jan 30 12:30:46.691: INFO: Waiting for pod pod-ec3c164d-a51e-4319-a4bb-24db260955e4 to disappear
    Jan 30 12:30:46.693: INFO: Pod pod-ec3c164d-a51e-4319-a4bb-24db260955e4 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jan 30 12:30:46.693: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-1676" for this suite. 01/30/23 12:30:46.695
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] Probing container
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:199
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 12:30:46.697
Jan 30 12:30:46.698: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename container-probe 01/30/23 12:30:46.698
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:30:46.704
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:30:46.706
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:199
STEP: Creating pod liveness-e3ba0ae4-fe63-49cc-aae3-ea961c7eab9c in namespace container-probe-9172 01/30/23 12:30:46.708
Jan 30 12:30:46.712: INFO: Waiting up to 5m0s for pod "liveness-e3ba0ae4-fe63-49cc-aae3-ea961c7eab9c" in namespace "container-probe-9172" to be "not pending"
Jan 30 12:30:46.714: INFO: Pod "liveness-e3ba0ae4-fe63-49cc-aae3-ea961c7eab9c": Phase="Pending", Reason="", readiness=false. Elapsed: 1.725689ms
Jan 30 12:30:48.716: INFO: Pod "liveness-e3ba0ae4-fe63-49cc-aae3-ea961c7eab9c": Phase="Running", Reason="", readiness=true. Elapsed: 2.003746885s
Jan 30 12:30:48.716: INFO: Pod "liveness-e3ba0ae4-fe63-49cc-aae3-ea961c7eab9c" satisfied condition "not pending"
Jan 30 12:30:48.716: INFO: Started pod liveness-e3ba0ae4-fe63-49cc-aae3-ea961c7eab9c in namespace container-probe-9172
STEP: checking the pod's current state and verifying that restartCount is present 01/30/23 12:30:48.716
Jan 30 12:30:48.717: INFO: Initial restart count of pod liveness-e3ba0ae4-fe63-49cc-aae3-ea961c7eab9c is 0
Jan 30 12:31:08.748: INFO: Restart count of pod container-probe-9172/liveness-e3ba0ae4-fe63-49cc-aae3-ea961c7eab9c is now 1 (20.030965702s elapsed)
Jan 30 12:31:28.779: INFO: Restart count of pod container-probe-9172/liveness-e3ba0ae4-fe63-49cc-aae3-ea961c7eab9c is now 2 (40.061407323s elapsed)
Jan 30 12:31:48.809: INFO: Restart count of pod container-probe-9172/liveness-e3ba0ae4-fe63-49cc-aae3-ea961c7eab9c is now 3 (1m0.091347272s elapsed)
Jan 30 12:32:08.840: INFO: Restart count of pod container-probe-9172/liveness-e3ba0ae4-fe63-49cc-aae3-ea961c7eab9c is now 4 (1m20.12276776s elapsed)
Jan 30 12:33:10.941: INFO: Restart count of pod container-probe-9172/liveness-e3ba0ae4-fe63-49cc-aae3-ea961c7eab9c is now 5 (2m22.223299041s elapsed)
STEP: deleting the pod 01/30/23 12:33:10.941
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Jan 30 12:33:10.945: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-9172" for this suite. 01/30/23 12:33:10.947
------------------------------
• [SLOW TEST] [144.253 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:199

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 12:30:46.697
    Jan 30 12:30:46.698: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename container-probe 01/30/23 12:30:46.698
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:30:46.704
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:30:46.706
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should have monotonically increasing restart count [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:199
    STEP: Creating pod liveness-e3ba0ae4-fe63-49cc-aae3-ea961c7eab9c in namespace container-probe-9172 01/30/23 12:30:46.708
    Jan 30 12:30:46.712: INFO: Waiting up to 5m0s for pod "liveness-e3ba0ae4-fe63-49cc-aae3-ea961c7eab9c" in namespace "container-probe-9172" to be "not pending"
    Jan 30 12:30:46.714: INFO: Pod "liveness-e3ba0ae4-fe63-49cc-aae3-ea961c7eab9c": Phase="Pending", Reason="", readiness=false. Elapsed: 1.725689ms
    Jan 30 12:30:48.716: INFO: Pod "liveness-e3ba0ae4-fe63-49cc-aae3-ea961c7eab9c": Phase="Running", Reason="", readiness=true. Elapsed: 2.003746885s
    Jan 30 12:30:48.716: INFO: Pod "liveness-e3ba0ae4-fe63-49cc-aae3-ea961c7eab9c" satisfied condition "not pending"
    Jan 30 12:30:48.716: INFO: Started pod liveness-e3ba0ae4-fe63-49cc-aae3-ea961c7eab9c in namespace container-probe-9172
    STEP: checking the pod's current state and verifying that restartCount is present 01/30/23 12:30:48.716
    Jan 30 12:30:48.717: INFO: Initial restart count of pod liveness-e3ba0ae4-fe63-49cc-aae3-ea961c7eab9c is 0
    Jan 30 12:31:08.748: INFO: Restart count of pod container-probe-9172/liveness-e3ba0ae4-fe63-49cc-aae3-ea961c7eab9c is now 1 (20.030965702s elapsed)
    Jan 30 12:31:28.779: INFO: Restart count of pod container-probe-9172/liveness-e3ba0ae4-fe63-49cc-aae3-ea961c7eab9c is now 2 (40.061407323s elapsed)
    Jan 30 12:31:48.809: INFO: Restart count of pod container-probe-9172/liveness-e3ba0ae4-fe63-49cc-aae3-ea961c7eab9c is now 3 (1m0.091347272s elapsed)
    Jan 30 12:32:08.840: INFO: Restart count of pod container-probe-9172/liveness-e3ba0ae4-fe63-49cc-aae3-ea961c7eab9c is now 4 (1m20.12276776s elapsed)
    Jan 30 12:33:10.941: INFO: Restart count of pod container-probe-9172/liveness-e3ba0ae4-fe63-49cc-aae3-ea961c7eab9c is now 5 (2m22.223299041s elapsed)
    STEP: deleting the pod 01/30/23 12:33:10.941
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Jan 30 12:33:10.945: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-9172" for this suite. 01/30/23 12:33:10.947
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:175
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 12:33:10.951
Jan 30 12:33:10.951: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename configmap 01/30/23 12:33:10.952
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:33:10.958
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:33:10.959
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:175
STEP: Creating configMap with name configmap-test-upd-ab74820a-890a-497e-a42f-9a39d41a27cd 01/30/23 12:33:10.965
STEP: Creating the pod 01/30/23 12:33:10.967
Jan 30 12:33:10.971: INFO: Waiting up to 5m0s for pod "pod-configmaps-e5534544-5343-422c-ac69-b3139a9ea737" in namespace "configmap-5608" to be "running"
Jan 30 12:33:10.973: INFO: Pod "pod-configmaps-e5534544-5343-422c-ac69-b3139a9ea737": Phase="Pending", Reason="", readiness=false. Elapsed: 1.530733ms
Jan 30 12:33:12.978: INFO: Pod "pod-configmaps-e5534544-5343-422c-ac69-b3139a9ea737": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00675482s
Jan 30 12:33:14.980: INFO: Pod "pod-configmaps-e5534544-5343-422c-ac69-b3139a9ea737": Phase="Running", Reason="", readiness=false. Elapsed: 4.008299161s
Jan 30 12:33:14.980: INFO: Pod "pod-configmaps-e5534544-5343-422c-ac69-b3139a9ea737" satisfied condition "running"
STEP: Waiting for pod with text data 01/30/23 12:33:14.98
STEP: Waiting for pod with binary data 01/30/23 12:33:14.993
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jan 30 12:33:14.997: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-5608" for this suite. 01/30/23 12:33:14.999
------------------------------
• [4.051 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:175

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 12:33:10.951
    Jan 30 12:33:10.951: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename configmap 01/30/23 12:33:10.952
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:33:10.958
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:33:10.959
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] binary data should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:175
    STEP: Creating configMap with name configmap-test-upd-ab74820a-890a-497e-a42f-9a39d41a27cd 01/30/23 12:33:10.965
    STEP: Creating the pod 01/30/23 12:33:10.967
    Jan 30 12:33:10.971: INFO: Waiting up to 5m0s for pod "pod-configmaps-e5534544-5343-422c-ac69-b3139a9ea737" in namespace "configmap-5608" to be "running"
    Jan 30 12:33:10.973: INFO: Pod "pod-configmaps-e5534544-5343-422c-ac69-b3139a9ea737": Phase="Pending", Reason="", readiness=false. Elapsed: 1.530733ms
    Jan 30 12:33:12.978: INFO: Pod "pod-configmaps-e5534544-5343-422c-ac69-b3139a9ea737": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00675482s
    Jan 30 12:33:14.980: INFO: Pod "pod-configmaps-e5534544-5343-422c-ac69-b3139a9ea737": Phase="Running", Reason="", readiness=false. Elapsed: 4.008299161s
    Jan 30 12:33:14.980: INFO: Pod "pod-configmaps-e5534544-5343-422c-ac69-b3139a9ea737" satisfied condition "running"
    STEP: Waiting for pod with text data 01/30/23 12:33:14.98
    STEP: Waiting for pod with binary data 01/30/23 12:33:14.993
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jan 30 12:33:14.997: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-5608" for this suite. 01/30/23 12:33:14.999
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:255
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 12:33:15.003
Jan 30 12:33:15.003: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename init-container 01/30/23 12:33:15.003
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:33:15.01
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:33:15.011
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:165
[It] should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:255
STEP: creating the pod 01/30/23 12:33:15.014
Jan 30 12:33:15.014: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Jan 30 12:33:18.452: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "init-container-1802" for this suite. 01/30/23 12:33:18.454
------------------------------
• [3.454 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:255

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 12:33:15.003
    Jan 30 12:33:15.003: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename init-container 01/30/23 12:33:15.003
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:33:15.01
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:33:15.011
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:165
    [It] should invoke init containers on a RestartAlways pod [Conformance]
      test/e2e/common/node/init_container.go:255
    STEP: creating the pod 01/30/23 12:33:15.014
    Jan 30 12:33:15.014: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Jan 30 12:33:18.452: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "init-container-1802" for this suite. 01/30/23 12:33:18.454
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-network] Services
  should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1557
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 12:33:18.457
Jan 30 12:33:18.457: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename services 01/30/23 12:33:18.458
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:33:18.463
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:33:18.465
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1557
STEP: creating a service nodeport-service with the type=NodePort in namespace services-9160 01/30/23 12:33:18.467
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 01/30/23 12:33:18.471
STEP: creating service externalsvc in namespace services-9160 01/30/23 12:33:18.471
STEP: creating replication controller externalsvc in namespace services-9160 01/30/23 12:33:18.475
I0130 12:33:18.478285      23 runners.go:193] Created replication controller with name: externalsvc, namespace: services-9160, replica count: 2
I0130 12:33:21.528717      23 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName 01/30/23 12:33:21.53
Jan 30 12:33:21.536: INFO: Creating new exec pod
Jan 30 12:33:21.538: INFO: Waiting up to 5m0s for pod "execpodxvxwg" in namespace "services-9160" to be "running"
Jan 30 12:33:21.540: INFO: Pod "execpodxvxwg": Phase="Pending", Reason="", readiness=false. Elapsed: 1.488761ms
Jan 30 12:33:23.542: INFO: Pod "execpodxvxwg": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003660155s
Jan 30 12:33:25.543: INFO: Pod "execpodxvxwg": Phase="Running", Reason="", readiness=true. Elapsed: 4.004005286s
Jan 30 12:33:25.543: INFO: Pod "execpodxvxwg" satisfied condition "running"
Jan 30 12:33:25.543: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=services-9160 exec execpodxvxwg -- /bin/sh -x -c nslookup nodeport-service.services-9160.svc.cluster.local'
Jan 30 12:33:25.726: INFO: stderr: "+ nslookup nodeport-service.services-9160.svc.cluster.local\n"
Jan 30 12:33:25.726: INFO: stdout: "Server:\t\t10.178.64.64\nAddress:\t10.178.64.64#53\n\nnodeport-service.services-9160.svc.cluster.local\tcanonical name = externalsvc.services-9160.svc.cluster.local.\nName:\texternalsvc.services-9160.svc.cluster.local\nAddress: 10.178.68.225\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-9160, will wait for the garbage collector to delete the pods 01/30/23 12:33:25.726
Jan 30 12:33:25.781: INFO: Deleting ReplicationController externalsvc took: 2.39958ms
Jan 30 12:33:25.882: INFO: Terminating ReplicationController externalsvc pods took: 100.975498ms
Jan 30 12:33:27.987: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jan 30 12:33:27.991: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-9160" for this suite. 01/30/23 12:33:27.993
------------------------------
• [SLOW TEST] [9.539 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1557

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 12:33:18.457
    Jan 30 12:33:18.457: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename services 01/30/23 12:33:18.458
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:33:18.463
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:33:18.465
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to change the type from NodePort to ExternalName [Conformance]
      test/e2e/network/service.go:1557
    STEP: creating a service nodeport-service with the type=NodePort in namespace services-9160 01/30/23 12:33:18.467
    STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 01/30/23 12:33:18.471
    STEP: creating service externalsvc in namespace services-9160 01/30/23 12:33:18.471
    STEP: creating replication controller externalsvc in namespace services-9160 01/30/23 12:33:18.475
    I0130 12:33:18.478285      23 runners.go:193] Created replication controller with name: externalsvc, namespace: services-9160, replica count: 2
    I0130 12:33:21.528717      23 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    STEP: changing the NodePort service to type=ExternalName 01/30/23 12:33:21.53
    Jan 30 12:33:21.536: INFO: Creating new exec pod
    Jan 30 12:33:21.538: INFO: Waiting up to 5m0s for pod "execpodxvxwg" in namespace "services-9160" to be "running"
    Jan 30 12:33:21.540: INFO: Pod "execpodxvxwg": Phase="Pending", Reason="", readiness=false. Elapsed: 1.488761ms
    Jan 30 12:33:23.542: INFO: Pod "execpodxvxwg": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003660155s
    Jan 30 12:33:25.543: INFO: Pod "execpodxvxwg": Phase="Running", Reason="", readiness=true. Elapsed: 4.004005286s
    Jan 30 12:33:25.543: INFO: Pod "execpodxvxwg" satisfied condition "running"
    Jan 30 12:33:25.543: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=services-9160 exec execpodxvxwg -- /bin/sh -x -c nslookup nodeport-service.services-9160.svc.cluster.local'
    Jan 30 12:33:25.726: INFO: stderr: "+ nslookup nodeport-service.services-9160.svc.cluster.local\n"
    Jan 30 12:33:25.726: INFO: stdout: "Server:\t\t10.178.64.64\nAddress:\t10.178.64.64#53\n\nnodeport-service.services-9160.svc.cluster.local\tcanonical name = externalsvc.services-9160.svc.cluster.local.\nName:\texternalsvc.services-9160.svc.cluster.local\nAddress: 10.178.68.225\n\n"
    STEP: deleting ReplicationController externalsvc in namespace services-9160, will wait for the garbage collector to delete the pods 01/30/23 12:33:25.726
    Jan 30 12:33:25.781: INFO: Deleting ReplicationController externalsvc took: 2.39958ms
    Jan 30 12:33:25.882: INFO: Terminating ReplicationController externalsvc pods took: 100.975498ms
    Jan 30 12:33:27.987: INFO: Cleaning up the NodePort to ExternalName test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jan 30 12:33:27.991: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-9160" for this suite. 01/30/23 12:33:27.993
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace
  should update a single-container pod's image  [Conformance]
  test/e2e/kubectl/kubectl.go:1747
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 12:33:27.996
Jan 30 12:33:27.996: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename kubectl 01/30/23 12:33:27.997
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:33:28.003
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:33:28.004
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Kubectl replace
  test/e2e/kubectl/kubectl.go:1734
[It] should update a single-container pod's image  [Conformance]
  test/e2e/kubectl/kubectl.go:1747
STEP: running the image harbor.cloud.netease.com/qzprod-k8s/e2e/httpd:2.4.38-4 01/30/23 12:33:28.006
Jan 30 12:33:28.006: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=kubectl-349 run e2e-test-httpd-pod --image=harbor.cloud.netease.com/qzprod-k8s/e2e/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Jan 30 12:33:28.068: INFO: stderr: ""
Jan 30 12:33:28.068: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running 01/30/23 12:33:28.068
STEP: verifying the pod e2e-test-httpd-pod was created 01/30/23 12:33:33.12
Jan 30 12:33:33.120: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=kubectl-349 get pod e2e-test-httpd-pod -o json'
Jan 30 12:33:33.182: INFO: stderr: ""
Jan 30 12:33:33.182: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/podIP\": \"10.178.151.147/32\",\n            \"cni.projectcalico.org/podIPs\": \"10.178.151.147/32\"\n        },\n        \"creationTimestamp\": \"2023-01-30T12:33:28Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-349\",\n        \"resourceVersion\": \"36680\",\n        \"uid\": \"36cd91a9-90eb-4c87-8b1e-79a24aa8e566\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"harbor.cloud.netease.com/qzprod-k8s/e2e/httpd:2.4.38-4\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-59pzr\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"pubt2-nks-for-dev1.dg.163.org\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-59pzr\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-01-30T12:33:28Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-01-30T12:33:29Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-01-30T12:33:29Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-01-30T12:33:28Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"docker://9c8aeebf7799ccad818c50350352eaa2834e08cb354b6b202347a6605e896b42\",\n                \"image\": \"harbor.cloud.netease.com/qzprod-k8s/e2e/httpd:2.4.38-4\",\n                \"imageID\": \"docker-pullable://harbor.cloud.netease.com/qzprod-k8s/e2e/httpd@sha256:8b745b6e9cb6a6184353110aabe28d8d6f6145d3fe7c0e1899aa70c8228c0fd3\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2023-01-30T12:33:29Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.182.0.82\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.178.151.147\",\n        \"podIPs\": [\n            {\n                \"ip\": \"10.178.151.147\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2023-01-30T12:33:28Z\"\n    }\n}\n"
STEP: replace the image in the pod 01/30/23 12:33:33.182
Jan 30 12:33:33.182: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=kubectl-349 replace -f -'
Jan 30 12:33:34.295: INFO: stderr: ""
Jan 30 12:33:34.295: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image harbor.cloud.netease.com/qzprod-k8s/e2e/busybox:1.29-4 01/30/23 12:33:34.295
[AfterEach] Kubectl replace
  test/e2e/kubectl/kubectl.go:1738
Jan 30 12:33:34.297: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=kubectl-349 delete pods e2e-test-httpd-pod'
Jan 30 12:33:36.066: INFO: stderr: ""
Jan 30 12:33:36.066: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jan 30 12:33:36.066: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-349" for this suite. 01/30/23 12:33:36.069
------------------------------
• [SLOW TEST] [8.075 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl replace
  test/e2e/kubectl/kubectl.go:1731
    should update a single-container pod's image  [Conformance]
    test/e2e/kubectl/kubectl.go:1747

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 12:33:27.996
    Jan 30 12:33:27.996: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename kubectl 01/30/23 12:33:27.997
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:33:28.003
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:33:28.004
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Kubectl replace
      test/e2e/kubectl/kubectl.go:1734
    [It] should update a single-container pod's image  [Conformance]
      test/e2e/kubectl/kubectl.go:1747
    STEP: running the image harbor.cloud.netease.com/qzprod-k8s/e2e/httpd:2.4.38-4 01/30/23 12:33:28.006
    Jan 30 12:33:28.006: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=kubectl-349 run e2e-test-httpd-pod --image=harbor.cloud.netease.com/qzprod-k8s/e2e/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
    Jan 30 12:33:28.068: INFO: stderr: ""
    Jan 30 12:33:28.068: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: verifying the pod e2e-test-httpd-pod is running 01/30/23 12:33:28.068
    STEP: verifying the pod e2e-test-httpd-pod was created 01/30/23 12:33:33.12
    Jan 30 12:33:33.120: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=kubectl-349 get pod e2e-test-httpd-pod -o json'
    Jan 30 12:33:33.182: INFO: stderr: ""
    Jan 30 12:33:33.182: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/podIP\": \"10.178.151.147/32\",\n            \"cni.projectcalico.org/podIPs\": \"10.178.151.147/32\"\n        },\n        \"creationTimestamp\": \"2023-01-30T12:33:28Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-349\",\n        \"resourceVersion\": \"36680\",\n        \"uid\": \"36cd91a9-90eb-4c87-8b1e-79a24aa8e566\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"harbor.cloud.netease.com/qzprod-k8s/e2e/httpd:2.4.38-4\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-59pzr\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"pubt2-nks-for-dev1.dg.163.org\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-59pzr\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-01-30T12:33:28Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-01-30T12:33:29Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-01-30T12:33:29Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-01-30T12:33:28Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"docker://9c8aeebf7799ccad818c50350352eaa2834e08cb354b6b202347a6605e896b42\",\n                \"image\": \"harbor.cloud.netease.com/qzprod-k8s/e2e/httpd:2.4.38-4\",\n                \"imageID\": \"docker-pullable://harbor.cloud.netease.com/qzprod-k8s/e2e/httpd@sha256:8b745b6e9cb6a6184353110aabe28d8d6f6145d3fe7c0e1899aa70c8228c0fd3\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2023-01-30T12:33:29Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.182.0.82\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.178.151.147\",\n        \"podIPs\": [\n            {\n                \"ip\": \"10.178.151.147\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2023-01-30T12:33:28Z\"\n    }\n}\n"
    STEP: replace the image in the pod 01/30/23 12:33:33.182
    Jan 30 12:33:33.182: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=kubectl-349 replace -f -'
    Jan 30 12:33:34.295: INFO: stderr: ""
    Jan 30 12:33:34.295: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
    STEP: verifying the pod e2e-test-httpd-pod has the right image harbor.cloud.netease.com/qzprod-k8s/e2e/busybox:1.29-4 01/30/23 12:33:34.295
    [AfterEach] Kubectl replace
      test/e2e/kubectl/kubectl.go:1738
    Jan 30 12:33:34.297: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=kubectl-349 delete pods e2e-test-httpd-pod'
    Jan 30 12:33:36.066: INFO: stderr: ""
    Jan 30 12:33:36.066: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jan 30 12:33:36.066: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-349" for this suite. 01/30/23 12:33:36.069
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3219
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 12:33:36.072
Jan 30 12:33:36.072: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename services 01/30/23 12:33:36.072
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:33:36.078
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:33:36.08
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3219
STEP: fetching services 01/30/23 12:33:36.082
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jan 30 12:33:36.083: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-2230" for this suite. 01/30/23 12:33:36.085
------------------------------
• [0.016 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3219

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 12:33:36.072
    Jan 30 12:33:36.072: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename services 01/30/23 12:33:36.072
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:33:36.078
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:33:36.08
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should find a service from listing all namespaces [Conformance]
      test/e2e/network/service.go:3219
    STEP: fetching services 01/30/23 12:33:36.082
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jan 30 12:33:36.083: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-2230" for this suite. 01/30/23 12:33:36.085
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-network] DNS
  should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 12:33:36.088
Jan 30 12:33:36.088: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename dns 01/30/23 12:33:36.088
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:33:36.093
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:33:36.095
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-6313.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-6313.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done
 01/30/23 12:33:36.097
STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-6313.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-6313.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done
 01/30/23 12:33:36.097
STEP: creating a pod to probe /etc/hosts 01/30/23 12:33:36.097
STEP: submitting the pod to kubernetes 01/30/23 12:33:36.097
Jan 30 12:33:36.102: INFO: Waiting up to 15m0s for pod "dns-test-a347e3a5-8a60-449a-95d9-5b5f3e710556" in namespace "dns-6313" to be "running"
Jan 30 12:33:36.103: INFO: Pod "dns-test-a347e3a5-8a60-449a-95d9-5b5f3e710556": Phase="Pending", Reason="", readiness=false. Elapsed: 1.575638ms
Jan 30 12:33:38.106: INFO: Pod "dns-test-a347e3a5-8a60-449a-95d9-5b5f3e710556": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004358155s
Jan 30 12:33:40.107: INFO: Pod "dns-test-a347e3a5-8a60-449a-95d9-5b5f3e710556": Phase="Running", Reason="", readiness=true. Elapsed: 4.005671185s
Jan 30 12:33:40.107: INFO: Pod "dns-test-a347e3a5-8a60-449a-95d9-5b5f3e710556" satisfied condition "running"
STEP: retrieving the pod 01/30/23 12:33:40.107
STEP: looking for the results for each expected name from probers 01/30/23 12:33:40.109
Jan 30 12:33:40.117: INFO: DNS probes using dns-6313/dns-test-a347e3a5-8a60-449a-95d9-5b5f3e710556 succeeded

STEP: deleting the pod 01/30/23 12:33:40.117
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Jan 30 12:33:40.122: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-6313" for this suite. 01/30/23 12:33:40.125
------------------------------
• [4.040 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 12:33:36.088
    Jan 30 12:33:36.088: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename dns 01/30/23 12:33:36.088
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:33:36.093
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:33:36.095
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide /etc/hosts entries for the cluster [Conformance]
      test/e2e/network/dns.go:117
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-6313.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-6313.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done
     01/30/23 12:33:36.097
    STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-6313.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-6313.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done
     01/30/23 12:33:36.097
    STEP: creating a pod to probe /etc/hosts 01/30/23 12:33:36.097
    STEP: submitting the pod to kubernetes 01/30/23 12:33:36.097
    Jan 30 12:33:36.102: INFO: Waiting up to 15m0s for pod "dns-test-a347e3a5-8a60-449a-95d9-5b5f3e710556" in namespace "dns-6313" to be "running"
    Jan 30 12:33:36.103: INFO: Pod "dns-test-a347e3a5-8a60-449a-95d9-5b5f3e710556": Phase="Pending", Reason="", readiness=false. Elapsed: 1.575638ms
    Jan 30 12:33:38.106: INFO: Pod "dns-test-a347e3a5-8a60-449a-95d9-5b5f3e710556": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004358155s
    Jan 30 12:33:40.107: INFO: Pod "dns-test-a347e3a5-8a60-449a-95d9-5b5f3e710556": Phase="Running", Reason="", readiness=true. Elapsed: 4.005671185s
    Jan 30 12:33:40.107: INFO: Pod "dns-test-a347e3a5-8a60-449a-95d9-5b5f3e710556" satisfied condition "running"
    STEP: retrieving the pod 01/30/23 12:33:40.107
    STEP: looking for the results for each expected name from probers 01/30/23 12:33:40.109
    Jan 30 12:33:40.117: INFO: DNS probes using dns-6313/dns-test-a347e3a5-8a60-449a-95d9-5b5f3e710556 succeeded

    STEP: deleting the pod 01/30/23 12:33:40.117
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Jan 30 12:33:40.122: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-6313" for this suite. 01/30/23 12:33:40.125
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  test/e2e/apps/statefulset.go:587
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 12:33:40.128
Jan 30 12:33:40.128: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename statefulset 01/30/23 12:33:40.129
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:33:40.135
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:33:40.137
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-463 01/30/23 12:33:40.139
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  test/e2e/apps/statefulset.go:587
STEP: Initializing watcher for selector baz=blah,foo=bar 01/30/23 12:33:40.141
STEP: Creating stateful set ss in namespace statefulset-463 01/30/23 12:33:40.143
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-463 01/30/23 12:33:40.146
Jan 30 12:33:40.147: INFO: Found 0 stateful pods, waiting for 1
Jan 30 12:33:50.149: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod 01/30/23 12:33:50.149
Jan 30 12:33:50.151: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=statefulset-463 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 30 12:33:50.317: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 30 12:33:50.317: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 30 12:33:50.317: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 30 12:33:50.319: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Jan 30 12:34:00.324: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jan 30 12:34:00.324: INFO: Waiting for statefulset status.replicas updated to 0
Jan 30 12:34:00.332: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999815s
Jan 30 12:34:01.334: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.998048264s
Jan 30 12:34:02.337: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.995907399s
Jan 30 12:34:03.339: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.993543351s
Jan 30 12:34:04.342: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.991311322s
Jan 30 12:34:05.344: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.988671192s
Jan 30 12:34:06.346: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.986481149s
Jan 30 12:34:07.347: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.98470365s
Jan 30 12:34:08.351: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.981874614s
Jan 30 12:34:09.353: INFO: Verifying statefulset ss doesn't scale past 1 for another 979.220366ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-463 01/30/23 12:34:10.353
Jan 30 12:34:10.356: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=statefulset-463 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 30 12:34:10.521: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan 30 12:34:10.521: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 30 12:34:10.521: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 30 12:34:10.522: INFO: Found 1 stateful pods, waiting for 3
Jan 30 12:34:20.528: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jan 30 12:34:20.528: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Jan 30 12:34:20.528: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order 01/30/23 12:34:20.528
STEP: Scale down will halt with unhealthy stateful pod 01/30/23 12:34:20.528
Jan 30 12:34:20.532: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=statefulset-463 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 30 12:34:20.679: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 30 12:34:20.679: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 30 12:34:20.679: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 30 12:34:20.679: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=statefulset-463 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 30 12:34:20.827: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 30 12:34:20.827: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 30 12:34:20.827: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 30 12:34:20.828: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=statefulset-463 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 30 12:34:20.964: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 30 12:34:20.964: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 30 12:34:20.964: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 30 12:34:20.964: INFO: Waiting for statefulset status.replicas updated to 0
Jan 30 12:34:20.966: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Jan 30 12:34:30.971: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jan 30 12:34:30.971: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Jan 30 12:34:30.971: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Jan 30 12:34:30.979: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.99999981s
Jan 30 12:34:31.981: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.997678959s
Jan 30 12:34:32.984: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.995010449s
Jan 30 12:34:33.987: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.992209659s
Jan 30 12:34:34.990: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.989405998s
Jan 30 12:34:35.992: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.986588967s
Jan 30 12:34:36.995: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.983988642s
Jan 30 12:34:37.999: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.980374737s
Jan 30 12:34:39.001: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.977743474s
Jan 30 12:34:40.004: INFO: Verifying statefulset ss doesn't scale past 3 for another 975.139576ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-463 01/30/23 12:34:41.004
Jan 30 12:34:41.007: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=statefulset-463 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 30 12:34:41.168: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan 30 12:34:41.168: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 30 12:34:41.168: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 30 12:34:41.168: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=statefulset-463 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 30 12:34:41.330: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan 30 12:34:41.330: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 30 12:34:41.330: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 30 12:34:41.330: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=statefulset-463 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 30 12:34:41.479: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan 30 12:34:41.479: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 30 12:34:41.479: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 30 12:34:41.479: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order 01/30/23 12:34:51.492
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Jan 30 12:34:51.492: INFO: Deleting all statefulset in ns statefulset-463
Jan 30 12:34:51.493: INFO: Scaling statefulset ss to 0
Jan 30 12:34:51.499: INFO: Waiting for statefulset status.replicas updated to 0
Jan 30 12:34:51.501: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Jan 30 12:34:51.506: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-463" for this suite. 01/30/23 12:34:51.508
------------------------------
• [SLOW TEST] [71.382 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    test/e2e/apps/statefulset.go:587

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 12:33:40.128
    Jan 30 12:33:40.128: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename statefulset 01/30/23 12:33:40.129
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:33:40.135
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:33:40.137
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-463 01/30/23 12:33:40.139
    [It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
      test/e2e/apps/statefulset.go:587
    STEP: Initializing watcher for selector baz=blah,foo=bar 01/30/23 12:33:40.141
    STEP: Creating stateful set ss in namespace statefulset-463 01/30/23 12:33:40.143
    STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-463 01/30/23 12:33:40.146
    Jan 30 12:33:40.147: INFO: Found 0 stateful pods, waiting for 1
    Jan 30 12:33:50.149: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod 01/30/23 12:33:50.149
    Jan 30 12:33:50.151: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=statefulset-463 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan 30 12:33:50.317: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan 30 12:33:50.317: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan 30 12:33:50.317: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jan 30 12:33:50.319: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
    Jan 30 12:34:00.324: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Jan 30 12:34:00.324: INFO: Waiting for statefulset status.replicas updated to 0
    Jan 30 12:34:00.332: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999815s
    Jan 30 12:34:01.334: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.998048264s
    Jan 30 12:34:02.337: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.995907399s
    Jan 30 12:34:03.339: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.993543351s
    Jan 30 12:34:04.342: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.991311322s
    Jan 30 12:34:05.344: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.988671192s
    Jan 30 12:34:06.346: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.986481149s
    Jan 30 12:34:07.347: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.98470365s
    Jan 30 12:34:08.351: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.981874614s
    Jan 30 12:34:09.353: INFO: Verifying statefulset ss doesn't scale past 1 for another 979.220366ms
    STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-463 01/30/23 12:34:10.353
    Jan 30 12:34:10.356: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=statefulset-463 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan 30 12:34:10.521: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Jan 30 12:34:10.521: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jan 30 12:34:10.521: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jan 30 12:34:10.522: INFO: Found 1 stateful pods, waiting for 3
    Jan 30 12:34:20.528: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Jan 30 12:34:20.528: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
    Jan 30 12:34:20.528: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Verifying that stateful set ss was scaled up in order 01/30/23 12:34:20.528
    STEP: Scale down will halt with unhealthy stateful pod 01/30/23 12:34:20.528
    Jan 30 12:34:20.532: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=statefulset-463 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan 30 12:34:20.679: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan 30 12:34:20.679: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan 30 12:34:20.679: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jan 30 12:34:20.679: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=statefulset-463 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan 30 12:34:20.827: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan 30 12:34:20.827: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan 30 12:34:20.827: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jan 30 12:34:20.828: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=statefulset-463 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan 30 12:34:20.964: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan 30 12:34:20.964: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan 30 12:34:20.964: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jan 30 12:34:20.964: INFO: Waiting for statefulset status.replicas updated to 0
    Jan 30 12:34:20.966: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
    Jan 30 12:34:30.971: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Jan 30 12:34:30.971: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
    Jan 30 12:34:30.971: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
    Jan 30 12:34:30.979: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.99999981s
    Jan 30 12:34:31.981: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.997678959s
    Jan 30 12:34:32.984: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.995010449s
    Jan 30 12:34:33.987: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.992209659s
    Jan 30 12:34:34.990: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.989405998s
    Jan 30 12:34:35.992: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.986588967s
    Jan 30 12:34:36.995: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.983988642s
    Jan 30 12:34:37.999: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.980374737s
    Jan 30 12:34:39.001: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.977743474s
    Jan 30 12:34:40.004: INFO: Verifying statefulset ss doesn't scale past 3 for another 975.139576ms
    STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-463 01/30/23 12:34:41.004
    Jan 30 12:34:41.007: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=statefulset-463 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan 30 12:34:41.168: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Jan 30 12:34:41.168: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jan 30 12:34:41.168: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jan 30 12:34:41.168: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=statefulset-463 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan 30 12:34:41.330: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Jan 30 12:34:41.330: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jan 30 12:34:41.330: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jan 30 12:34:41.330: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=statefulset-463 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan 30 12:34:41.479: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Jan 30 12:34:41.479: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jan 30 12:34:41.479: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jan 30 12:34:41.479: INFO: Scaling statefulset ss to 0
    STEP: Verifying that stateful set ss was scaled down in reverse order 01/30/23 12:34:51.492
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Jan 30 12:34:51.492: INFO: Deleting all statefulset in ns statefulset-463
    Jan 30 12:34:51.493: INFO: Scaling statefulset ss to 0
    Jan 30 12:34:51.499: INFO: Waiting for statefulset status.replicas updated to 0
    Jan 30 12:34:51.501: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Jan 30 12:34:51.506: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-463" for this suite. 01/30/23 12:34:51.508
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:195
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 12:34:51.511
Jan 30 12:34:51.511: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename container-runtime 01/30/23 12:34:51.512
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:34:51.518
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:34:51.52
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:195
STEP: create the container 01/30/23 12:34:51.522
STEP: wait for the container to reach Succeeded 01/30/23 12:34:51.526
STEP: get the container status 01/30/23 12:34:56.537
STEP: the container should be terminated 01/30/23 12:34:56.539
STEP: the termination message should be set 01/30/23 12:34:56.539
Jan 30 12:34:56.539: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container 01/30/23 12:34:56.539
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Jan 30 12:34:56.545: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-8105" for this suite. 01/30/23 12:34:56.547
------------------------------
• [SLOW TEST] [5.038 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    on terminated container
    test/e2e/common/node/runtime.go:137
      should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:195

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 12:34:51.511
    Jan 30 12:34:51.511: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename container-runtime 01/30/23 12:34:51.512
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:34:51.518
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:34:51.52
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:195
    STEP: create the container 01/30/23 12:34:51.522
    STEP: wait for the container to reach Succeeded 01/30/23 12:34:51.526
    STEP: get the container status 01/30/23 12:34:56.537
    STEP: the container should be terminated 01/30/23 12:34:56.539
    STEP: the termination message should be set 01/30/23 12:34:56.539
    Jan 30 12:34:56.539: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
    STEP: delete the container 01/30/23 12:34:56.539
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Jan 30 12:34:56.545: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-8105" for this suite. 01/30/23 12:34:56.547
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:704
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 12:34:56.551
Jan 30 12:34:56.551: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename sched-pred 01/30/23 12:34:56.552
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:34:56.557
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:34:56.559
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:97
Jan 30 12:34:56.561: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jan 30 12:34:56.565: INFO: Waiting for terminating namespaces to be deleted...
Jan 30 12:34:56.567: INFO: 
Logging pods the apiserver thinks is on node pubt2-nks-for-dev1.dg.163.org before test
Jan 30 12:34:56.571: INFO: calico-node-5xgvr from kube-system started at 2023-01-30 11:14:53 +0000 UTC (1 container statuses recorded)
Jan 30 12:34:56.571: INFO: 	Container calico-node ready: true, restart count 0
Jan 30 12:34:56.571: INFO: cleanlog-w9znh from kube-system started at 2023-01-30 11:02:22 +0000 UTC (1 container statuses recorded)
Jan 30 12:34:56.571: INFO: 	Container cleanlog ready: true, restart count 0
Jan 30 12:34:56.571: INFO: coredns-5bf7dfc67-4chrr from kube-system started at 2023-01-30 11:33:54 +0000 UTC (1 container statuses recorded)
Jan 30 12:34:56.571: INFO: 	Container coredns ready: true, restart count 0
Jan 30 12:34:56.571: INFO: kube-proxy-fvt68 from kube-system started at 2023-01-30 11:02:22 +0000 UTC (1 container statuses recorded)
Jan 30 12:34:56.571: INFO: 	Container kube-proxy ready: true, restart count 0
Jan 30 12:34:56.571: INFO: node-local-dns-8jh9v from kube-system started at 2023-01-30 11:02:22 +0000 UTC (1 container statuses recorded)
Jan 30 12:34:56.571: INFO: 	Container node-cache ready: true, restart count 5
Jan 30 12:34:56.571: INFO: sonobuoy-systemd-logs-daemon-set-6d9e6fe3a8a74fcc-st9q5 from sonobuoy started at 2023-01-30 11:17:19 +0000 UTC (2 container statuses recorded)
Jan 30 12:34:56.571: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 30 12:34:56.571: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 30 12:34:56.571: INFO: 
Logging pods the apiserver thinks is on node pubt2-nks-for-dev3.dg.163.org before test
Jan 30 12:34:56.577: INFO: calico-kube-controllers-568668f974-5gtkq from kube-system started at 2023-01-30 09:59:07 +0000 UTC (1 container statuses recorded)
Jan 30 12:34:56.577: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Jan 30 12:34:56.577: INFO: calico-node-h7jwg from kube-system started at 2023-01-30 11:14:53 +0000 UTC (1 container statuses recorded)
Jan 30 12:34:56.577: INFO: 	Container calico-node ready: true, restart count 0
Jan 30 12:34:56.577: INFO: cleanlog-crksz from kube-system started at 2023-01-30 09:56:18 +0000 UTC (1 container statuses recorded)
Jan 30 12:34:56.577: INFO: 	Container cleanlog ready: true, restart count 0
Jan 30 12:34:56.577: INFO: coredns-5bf7dfc67-2gl8f from kube-system started at 2023-01-30 10:08:32 +0000 UTC (1 container statuses recorded)
Jan 30 12:34:56.577: INFO: 	Container coredns ready: true, restart count 0
Jan 30 12:34:56.577: INFO: etcd-main-pubt2-nks-for-dev3.dg.163.org from kube-system started at 2023-01-30 09:56:17 +0000 UTC (1 container statuses recorded)
Jan 30 12:34:56.577: INFO: 	Container etcd ready: true, restart count 1
Jan 30 12:34:56.577: INFO: kube-apiserver-pubt2-nks-for-dev3.dg.163.org from kube-system started at 2023-01-30 09:56:18 +0000 UTC (1 container statuses recorded)
Jan 30 12:34:56.577: INFO: 	Container kube-apiserver ready: true, restart count 1
Jan 30 12:34:56.577: INFO: kube-controller-manager-pubt2-nks-for-dev3.dg.163.org from kube-system started at 2023-01-30 09:56:18 +0000 UTC (1 container statuses recorded)
Jan 30 12:34:56.577: INFO: 	Container kube-controller-manager ready: true, restart count 1
Jan 30 12:34:56.577: INFO: kube-proxy-br2mf from kube-system started at 2023-01-30 09:58:40 +0000 UTC (1 container statuses recorded)
Jan 30 12:34:56.577: INFO: 	Container kube-proxy ready: true, restart count 0
Jan 30 12:34:56.577: INFO: kube-scheduler-pubt2-nks-for-dev3.dg.163.org from kube-system started at 2023-01-30 09:56:17 +0000 UTC (1 container statuses recorded)
Jan 30 12:34:56.577: INFO: 	Container kube-scheduler ready: true, restart count 1
Jan 30 12:34:56.577: INFO: node-local-dns-txwnd from kube-system started at 2023-01-30 10:09:12 +0000 UTC (1 container statuses recorded)
Jan 30 12:34:56.577: INFO: 	Container node-cache ready: true, restart count 0
Jan 30 12:34:56.577: INFO: sonobuoy from sonobuoy started at 2023-01-30 11:17:15 +0000 UTC (1 container statuses recorded)
Jan 30 12:34:56.577: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jan 30 12:34:56.577: INFO: sonobuoy-e2e-job-a0572830995342a7 from sonobuoy started at 2023-01-30 11:17:19 +0000 UTC (2 container statuses recorded)
Jan 30 12:34:56.577: INFO: 	Container e2e ready: true, restart count 0
Jan 30 12:34:56.577: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 30 12:34:56.577: INFO: sonobuoy-systemd-logs-daemon-set-6d9e6fe3a8a74fcc-nkw5p from sonobuoy started at 2023-01-30 11:17:19 +0000 UTC (2 container statuses recorded)
Jan 30 12:34:56.577: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 30 12:34:56.577: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:704
STEP: Trying to launch a pod without a label to get a node which can launch it. 01/30/23 12:34:56.577
Jan 30 12:34:56.581: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-8303" to be "running"
Jan 30 12:34:56.582: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 1.392895ms
Jan 30 12:34:58.584: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003689903s
Jan 30 12:35:00.585: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 4.004577562s
Jan 30 12:35:00.585: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 01/30/23 12:35:00.587
STEP: Trying to apply a random label on the found node. 01/30/23 12:35:00.593
STEP: verifying the node has the label kubernetes.io/e2e-715c9ba5-e63c-4ee7-bce6-a96319f5a36e 95 01/30/23 12:35:00.604
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled 01/30/23 12:35:00.606
Jan 30 12:35:00.608: INFO: Waiting up to 5m0s for pod "pod4" in namespace "sched-pred-8303" to be "not pending"
Jan 30 12:35:00.610: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 1.721016ms
Jan 30 12:35:02.613: INFO: Pod "pod4": Phase="Running", Reason="", readiness=true. Elapsed: 2.004600391s
Jan 30 12:35:02.613: INFO: Pod "pod4" satisfied condition "not pending"
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 10.182.0.82 on the node which pod4 resides and expect not scheduled 01/30/23 12:35:02.613
Jan 30 12:35:02.616: INFO: Waiting up to 5m0s for pod "pod5" in namespace "sched-pred-8303" to be "not pending"
Jan 30 12:35:02.618: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1.675143ms
Jan 30 12:35:04.621: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005033653s
Jan 30 12:35:06.620: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.003964643s
Jan 30 12:35:08.621: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.00463365s
Jan 30 12:35:10.622: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.006093817s
Jan 30 12:35:12.620: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.004460864s
Jan 30 12:35:14.621: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.004754275s
Jan 30 12:35:16.621: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.004799291s
Jan 30 12:35:18.620: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 16.004564221s
Jan 30 12:35:20.622: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 18.00593805s
Jan 30 12:35:22.620: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 20.004539392s
Jan 30 12:35:24.621: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 22.005012412s
Jan 30 12:35:26.620: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 24.004003359s
Jan 30 12:35:28.620: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 26.004310115s
Jan 30 12:35:30.621: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 28.005526433s
Jan 30 12:35:32.620: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 30.00437931s
Jan 30 12:35:34.621: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 32.005419272s
Jan 30 12:35:36.621: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 34.005222034s
Jan 30 12:35:38.621: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 36.00487333s
Jan 30 12:35:40.621: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 38.004855389s
Jan 30 12:35:42.620: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 40.004187931s
Jan 30 12:35:44.622: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 42.006325619s
Jan 30 12:35:46.621: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 44.005240424s
Jan 30 12:35:48.621: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 46.004803739s
Jan 30 12:35:50.621: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 48.004738956s
Jan 30 12:35:52.620: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 50.00369267s
Jan 30 12:35:54.621: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 52.005376502s
Jan 30 12:35:56.622: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 54.005662679s
Jan 30 12:35:58.621: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 56.005094659s
Jan 30 12:36:00.621: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 58.005084667s
Jan 30 12:36:02.620: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.004166415s
Jan 30 12:36:04.622: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.00557988s
Jan 30 12:36:06.621: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.005450401s
Jan 30 12:36:08.621: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.004638727s
Jan 30 12:36:10.621: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.005534061s
Jan 30 12:36:12.620: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.003673264s
Jan 30 12:36:14.621: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.004941585s
Jan 30 12:36:16.621: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.004751179s
Jan 30 12:36:18.620: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.003971748s
Jan 30 12:36:20.621: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.004897004s
Jan 30 12:36:22.620: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.004102569s
Jan 30 12:36:24.621: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.005544001s
Jan 30 12:36:26.620: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.004470014s
Jan 30 12:36:28.622: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.005652637s
Jan 30 12:36:30.622: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.005645109s
Jan 30 12:36:32.620: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.003935265s
Jan 30 12:36:34.621: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.005483648s
Jan 30 12:36:36.622: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.005644094s
Jan 30 12:36:38.621: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.005253466s
Jan 30 12:36:40.622: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.006099875s
Jan 30 12:36:42.621: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.004686096s
Jan 30 12:36:44.621: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.004855906s
Jan 30 12:36:46.621: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.004832772s
Jan 30 12:36:48.620: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.003871435s
Jan 30 12:36:50.621: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.005115145s
Jan 30 12:36:52.621: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.004703417s
Jan 30 12:36:54.621: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.005300838s
Jan 30 12:36:56.621: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.005043526s
Jan 30 12:36:58.621: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.004650092s
Jan 30 12:37:00.622: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.005851345s
Jan 30 12:37:02.621: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.004660222s
Jan 30 12:37:04.621: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m2.005093441s
Jan 30 12:37:06.621: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m4.005210854s
Jan 30 12:37:08.621: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m6.004811176s
Jan 30 12:37:10.621: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m8.005398677s
Jan 30 12:37:12.621: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m10.004577931s
Jan 30 12:37:14.621: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m12.004954801s
Jan 30 12:37:16.621: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m14.004998263s
Jan 30 12:37:18.621: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m16.005446858s
Jan 30 12:37:20.621: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m18.005514888s
Jan 30 12:37:22.620: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m20.004470797s
Jan 30 12:37:24.622: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m22.00562464s
Jan 30 12:37:26.620: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m24.004489298s
Jan 30 12:37:28.621: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m26.004626326s
Jan 30 12:37:30.621: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m28.005531069s
Jan 30 12:37:32.620: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m30.004102152s
Jan 30 12:37:34.622: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m32.005797023s
Jan 30 12:37:36.620: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m34.003705606s
Jan 30 12:37:38.620: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m36.003933188s
Jan 30 12:37:40.621: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m38.004752045s
Jan 30 12:37:42.621: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m40.004746674s
Jan 30 12:37:44.621: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m42.004583404s
Jan 30 12:37:46.622: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m44.005802031s
Jan 30 12:37:48.620: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m46.004400675s
Jan 30 12:37:50.621: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m48.005385609s
Jan 30 12:37:52.620: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m50.004360952s
Jan 30 12:37:54.620: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m52.004521662s
Jan 30 12:37:56.621: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m54.005480625s
Jan 30 12:37:58.621: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m56.005431111s
Jan 30 12:38:00.621: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m58.00537173s
Jan 30 12:38:02.621: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m0.004578848s
Jan 30 12:38:04.621: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m2.004903825s
Jan 30 12:38:06.621: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m4.004681118s
Jan 30 12:38:08.621: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m6.00465437s
Jan 30 12:38:10.622: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m8.00585283s
Jan 30 12:38:12.621: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m10.005209065s
Jan 30 12:38:14.622: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m12.005860762s
Jan 30 12:38:16.621: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m14.004717481s
Jan 30 12:38:18.620: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m16.004300269s
Jan 30 12:38:20.622: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m18.006080269s
Jan 30 12:38:22.621: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m20.005411007s
Jan 30 12:38:24.621: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m22.004642572s
Jan 30 12:38:26.621: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m24.005447925s
Jan 30 12:38:28.621: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m26.005056125s
Jan 30 12:38:30.621: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m28.005292799s
Jan 30 12:38:32.621: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m30.005283059s
Jan 30 12:38:34.620: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m32.004453005s
Jan 30 12:38:36.621: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m34.005184084s
Jan 30 12:38:38.620: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m36.004326293s
Jan 30 12:38:40.621: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m38.005488157s
Jan 30 12:38:42.621: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m40.004631954s
Jan 30 12:38:44.621: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m42.005054971s
Jan 30 12:38:46.620: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m44.003639954s
Jan 30 12:38:48.621: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m46.005175416s
Jan 30 12:38:50.621: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m48.005110504s
Jan 30 12:38:52.621: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m50.00492412s
Jan 30 12:38:54.622: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m52.005617812s
Jan 30 12:38:56.621: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m54.004832503s
Jan 30 12:38:58.620: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m56.004239084s
Jan 30 12:39:00.621: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m58.004592026s
Jan 30 12:39:02.620: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m0.003580034s
Jan 30 12:39:04.621: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m2.004878191s
Jan 30 12:39:06.621: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m4.005559432s
Jan 30 12:39:08.621: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m6.005054943s
Jan 30 12:39:10.621: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m8.005027055s
Jan 30 12:39:12.620: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m10.004374747s
Jan 30 12:39:14.620: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m12.004557027s
Jan 30 12:39:16.622: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m14.005865828s
Jan 30 12:39:18.622: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m16.005686728s
Jan 30 12:39:20.620: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m18.00401624s
Jan 30 12:39:22.621: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m20.004876355s
Jan 30 12:39:24.621: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m22.004908137s
Jan 30 12:39:26.620: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m24.003820998s
Jan 30 12:39:28.620: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m26.004515036s
Jan 30 12:39:30.621: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m28.004933146s
Jan 30 12:39:32.620: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m30.00442613s
Jan 30 12:39:34.621: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m32.005017316s
Jan 30 12:39:36.621: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m34.005011434s
Jan 30 12:39:38.621: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m36.00495428s
Jan 30 12:39:40.621: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m38.005424565s
Jan 30 12:39:42.620: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m40.004545989s
Jan 30 12:39:44.621: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m42.00461109s
Jan 30 12:39:46.621: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m44.005450992s
Jan 30 12:39:48.621: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m46.005285121s
Jan 30 12:39:50.621: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m48.005175953s
Jan 30 12:39:52.620: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m50.003958984s
Jan 30 12:39:54.621: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m52.005325135s
Jan 30 12:39:56.620: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m54.004155843s
Jan 30 12:39:58.620: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m56.004467653s
Jan 30 12:40:00.622: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m58.006122933s
Jan 30 12:40:02.620: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.00434263s
Jan 30 12:40:02.622: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.005922987s
STEP: removing the label kubernetes.io/e2e-715c9ba5-e63c-4ee7-bce6-a96319f5a36e off the node pubt2-nks-for-dev1.dg.163.org 01/30/23 12:40:02.622
STEP: verifying the node doesn't have the label kubernetes.io/e2e-715c9ba5-e63c-4ee7-bce6-a96319f5a36e 01/30/23 12:40:02.632
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/node/init/init.go:32
Jan 30 12:40:02.634: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:88
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-pred-8303" for this suite. 01/30/23 12:40:02.636
------------------------------
• [SLOW TEST] [306.088 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:704

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 12:34:56.551
    Jan 30 12:34:56.551: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename sched-pred 01/30/23 12:34:56.552
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:34:56.557
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:34:56.559
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:97
    Jan 30 12:34:56.561: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Jan 30 12:34:56.565: INFO: Waiting for terminating namespaces to be deleted...
    Jan 30 12:34:56.567: INFO: 
    Logging pods the apiserver thinks is on node pubt2-nks-for-dev1.dg.163.org before test
    Jan 30 12:34:56.571: INFO: calico-node-5xgvr from kube-system started at 2023-01-30 11:14:53 +0000 UTC (1 container statuses recorded)
    Jan 30 12:34:56.571: INFO: 	Container calico-node ready: true, restart count 0
    Jan 30 12:34:56.571: INFO: cleanlog-w9znh from kube-system started at 2023-01-30 11:02:22 +0000 UTC (1 container statuses recorded)
    Jan 30 12:34:56.571: INFO: 	Container cleanlog ready: true, restart count 0
    Jan 30 12:34:56.571: INFO: coredns-5bf7dfc67-4chrr from kube-system started at 2023-01-30 11:33:54 +0000 UTC (1 container statuses recorded)
    Jan 30 12:34:56.571: INFO: 	Container coredns ready: true, restart count 0
    Jan 30 12:34:56.571: INFO: kube-proxy-fvt68 from kube-system started at 2023-01-30 11:02:22 +0000 UTC (1 container statuses recorded)
    Jan 30 12:34:56.571: INFO: 	Container kube-proxy ready: true, restart count 0
    Jan 30 12:34:56.571: INFO: node-local-dns-8jh9v from kube-system started at 2023-01-30 11:02:22 +0000 UTC (1 container statuses recorded)
    Jan 30 12:34:56.571: INFO: 	Container node-cache ready: true, restart count 5
    Jan 30 12:34:56.571: INFO: sonobuoy-systemd-logs-daemon-set-6d9e6fe3a8a74fcc-st9q5 from sonobuoy started at 2023-01-30 11:17:19 +0000 UTC (2 container statuses recorded)
    Jan 30 12:34:56.571: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 30 12:34:56.571: INFO: 	Container systemd-logs ready: true, restart count 0
    Jan 30 12:34:56.571: INFO: 
    Logging pods the apiserver thinks is on node pubt2-nks-for-dev3.dg.163.org before test
    Jan 30 12:34:56.577: INFO: calico-kube-controllers-568668f974-5gtkq from kube-system started at 2023-01-30 09:59:07 +0000 UTC (1 container statuses recorded)
    Jan 30 12:34:56.577: INFO: 	Container calico-kube-controllers ready: true, restart count 0
    Jan 30 12:34:56.577: INFO: calico-node-h7jwg from kube-system started at 2023-01-30 11:14:53 +0000 UTC (1 container statuses recorded)
    Jan 30 12:34:56.577: INFO: 	Container calico-node ready: true, restart count 0
    Jan 30 12:34:56.577: INFO: cleanlog-crksz from kube-system started at 2023-01-30 09:56:18 +0000 UTC (1 container statuses recorded)
    Jan 30 12:34:56.577: INFO: 	Container cleanlog ready: true, restart count 0
    Jan 30 12:34:56.577: INFO: coredns-5bf7dfc67-2gl8f from kube-system started at 2023-01-30 10:08:32 +0000 UTC (1 container statuses recorded)
    Jan 30 12:34:56.577: INFO: 	Container coredns ready: true, restart count 0
    Jan 30 12:34:56.577: INFO: etcd-main-pubt2-nks-for-dev3.dg.163.org from kube-system started at 2023-01-30 09:56:17 +0000 UTC (1 container statuses recorded)
    Jan 30 12:34:56.577: INFO: 	Container etcd ready: true, restart count 1
    Jan 30 12:34:56.577: INFO: kube-apiserver-pubt2-nks-for-dev3.dg.163.org from kube-system started at 2023-01-30 09:56:18 +0000 UTC (1 container statuses recorded)
    Jan 30 12:34:56.577: INFO: 	Container kube-apiserver ready: true, restart count 1
    Jan 30 12:34:56.577: INFO: kube-controller-manager-pubt2-nks-for-dev3.dg.163.org from kube-system started at 2023-01-30 09:56:18 +0000 UTC (1 container statuses recorded)
    Jan 30 12:34:56.577: INFO: 	Container kube-controller-manager ready: true, restart count 1
    Jan 30 12:34:56.577: INFO: kube-proxy-br2mf from kube-system started at 2023-01-30 09:58:40 +0000 UTC (1 container statuses recorded)
    Jan 30 12:34:56.577: INFO: 	Container kube-proxy ready: true, restart count 0
    Jan 30 12:34:56.577: INFO: kube-scheduler-pubt2-nks-for-dev3.dg.163.org from kube-system started at 2023-01-30 09:56:17 +0000 UTC (1 container statuses recorded)
    Jan 30 12:34:56.577: INFO: 	Container kube-scheduler ready: true, restart count 1
    Jan 30 12:34:56.577: INFO: node-local-dns-txwnd from kube-system started at 2023-01-30 10:09:12 +0000 UTC (1 container statuses recorded)
    Jan 30 12:34:56.577: INFO: 	Container node-cache ready: true, restart count 0
    Jan 30 12:34:56.577: INFO: sonobuoy from sonobuoy started at 2023-01-30 11:17:15 +0000 UTC (1 container statuses recorded)
    Jan 30 12:34:56.577: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Jan 30 12:34:56.577: INFO: sonobuoy-e2e-job-a0572830995342a7 from sonobuoy started at 2023-01-30 11:17:19 +0000 UTC (2 container statuses recorded)
    Jan 30 12:34:56.577: INFO: 	Container e2e ready: true, restart count 0
    Jan 30 12:34:56.577: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 30 12:34:56.577: INFO: sonobuoy-systemd-logs-daemon-set-6d9e6fe3a8a74fcc-nkw5p from sonobuoy started at 2023-01-30 11:17:19 +0000 UTC (2 container statuses recorded)
    Jan 30 12:34:56.577: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 30 12:34:56.577: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
      test/e2e/scheduling/predicates.go:704
    STEP: Trying to launch a pod without a label to get a node which can launch it. 01/30/23 12:34:56.577
    Jan 30 12:34:56.581: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-8303" to be "running"
    Jan 30 12:34:56.582: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 1.392895ms
    Jan 30 12:34:58.584: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003689903s
    Jan 30 12:35:00.585: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 4.004577562s
    Jan 30 12:35:00.585: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 01/30/23 12:35:00.587
    STEP: Trying to apply a random label on the found node. 01/30/23 12:35:00.593
    STEP: verifying the node has the label kubernetes.io/e2e-715c9ba5-e63c-4ee7-bce6-a96319f5a36e 95 01/30/23 12:35:00.604
    STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled 01/30/23 12:35:00.606
    Jan 30 12:35:00.608: INFO: Waiting up to 5m0s for pod "pod4" in namespace "sched-pred-8303" to be "not pending"
    Jan 30 12:35:00.610: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 1.721016ms
    Jan 30 12:35:02.613: INFO: Pod "pod4": Phase="Running", Reason="", readiness=true. Elapsed: 2.004600391s
    Jan 30 12:35:02.613: INFO: Pod "pod4" satisfied condition "not pending"
    STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 10.182.0.82 on the node which pod4 resides and expect not scheduled 01/30/23 12:35:02.613
    Jan 30 12:35:02.616: INFO: Waiting up to 5m0s for pod "pod5" in namespace "sched-pred-8303" to be "not pending"
    Jan 30 12:35:02.618: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1.675143ms
    Jan 30 12:35:04.621: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005033653s
    Jan 30 12:35:06.620: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.003964643s
    Jan 30 12:35:08.621: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.00463365s
    Jan 30 12:35:10.622: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.006093817s
    Jan 30 12:35:12.620: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.004460864s
    Jan 30 12:35:14.621: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.004754275s
    Jan 30 12:35:16.621: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.004799291s
    Jan 30 12:35:18.620: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 16.004564221s
    Jan 30 12:35:20.622: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 18.00593805s
    Jan 30 12:35:22.620: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 20.004539392s
    Jan 30 12:35:24.621: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 22.005012412s
    Jan 30 12:35:26.620: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 24.004003359s
    Jan 30 12:35:28.620: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 26.004310115s
    Jan 30 12:35:30.621: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 28.005526433s
    Jan 30 12:35:32.620: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 30.00437931s
    Jan 30 12:35:34.621: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 32.005419272s
    Jan 30 12:35:36.621: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 34.005222034s
    Jan 30 12:35:38.621: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 36.00487333s
    Jan 30 12:35:40.621: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 38.004855389s
    Jan 30 12:35:42.620: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 40.004187931s
    Jan 30 12:35:44.622: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 42.006325619s
    Jan 30 12:35:46.621: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 44.005240424s
    Jan 30 12:35:48.621: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 46.004803739s
    Jan 30 12:35:50.621: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 48.004738956s
    Jan 30 12:35:52.620: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 50.00369267s
    Jan 30 12:35:54.621: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 52.005376502s
    Jan 30 12:35:56.622: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 54.005662679s
    Jan 30 12:35:58.621: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 56.005094659s
    Jan 30 12:36:00.621: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 58.005084667s
    Jan 30 12:36:02.620: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.004166415s
    Jan 30 12:36:04.622: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.00557988s
    Jan 30 12:36:06.621: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.005450401s
    Jan 30 12:36:08.621: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.004638727s
    Jan 30 12:36:10.621: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.005534061s
    Jan 30 12:36:12.620: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.003673264s
    Jan 30 12:36:14.621: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.004941585s
    Jan 30 12:36:16.621: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.004751179s
    Jan 30 12:36:18.620: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.003971748s
    Jan 30 12:36:20.621: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.004897004s
    Jan 30 12:36:22.620: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.004102569s
    Jan 30 12:36:24.621: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.005544001s
    Jan 30 12:36:26.620: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.004470014s
    Jan 30 12:36:28.622: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.005652637s
    Jan 30 12:36:30.622: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.005645109s
    Jan 30 12:36:32.620: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.003935265s
    Jan 30 12:36:34.621: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.005483648s
    Jan 30 12:36:36.622: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.005644094s
    Jan 30 12:36:38.621: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.005253466s
    Jan 30 12:36:40.622: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.006099875s
    Jan 30 12:36:42.621: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.004686096s
    Jan 30 12:36:44.621: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.004855906s
    Jan 30 12:36:46.621: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.004832772s
    Jan 30 12:36:48.620: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.003871435s
    Jan 30 12:36:50.621: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.005115145s
    Jan 30 12:36:52.621: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.004703417s
    Jan 30 12:36:54.621: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.005300838s
    Jan 30 12:36:56.621: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.005043526s
    Jan 30 12:36:58.621: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.004650092s
    Jan 30 12:37:00.622: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.005851345s
    Jan 30 12:37:02.621: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.004660222s
    Jan 30 12:37:04.621: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m2.005093441s
    Jan 30 12:37:06.621: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m4.005210854s
    Jan 30 12:37:08.621: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m6.004811176s
    Jan 30 12:37:10.621: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m8.005398677s
    Jan 30 12:37:12.621: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m10.004577931s
    Jan 30 12:37:14.621: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m12.004954801s
    Jan 30 12:37:16.621: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m14.004998263s
    Jan 30 12:37:18.621: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m16.005446858s
    Jan 30 12:37:20.621: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m18.005514888s
    Jan 30 12:37:22.620: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m20.004470797s
    Jan 30 12:37:24.622: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m22.00562464s
    Jan 30 12:37:26.620: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m24.004489298s
    Jan 30 12:37:28.621: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m26.004626326s
    Jan 30 12:37:30.621: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m28.005531069s
    Jan 30 12:37:32.620: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m30.004102152s
    Jan 30 12:37:34.622: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m32.005797023s
    Jan 30 12:37:36.620: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m34.003705606s
    Jan 30 12:37:38.620: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m36.003933188s
    Jan 30 12:37:40.621: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m38.004752045s
    Jan 30 12:37:42.621: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m40.004746674s
    Jan 30 12:37:44.621: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m42.004583404s
    Jan 30 12:37:46.622: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m44.005802031s
    Jan 30 12:37:48.620: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m46.004400675s
    Jan 30 12:37:50.621: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m48.005385609s
    Jan 30 12:37:52.620: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m50.004360952s
    Jan 30 12:37:54.620: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m52.004521662s
    Jan 30 12:37:56.621: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m54.005480625s
    Jan 30 12:37:58.621: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m56.005431111s
    Jan 30 12:38:00.621: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m58.00537173s
    Jan 30 12:38:02.621: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m0.004578848s
    Jan 30 12:38:04.621: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m2.004903825s
    Jan 30 12:38:06.621: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m4.004681118s
    Jan 30 12:38:08.621: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m6.00465437s
    Jan 30 12:38:10.622: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m8.00585283s
    Jan 30 12:38:12.621: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m10.005209065s
    Jan 30 12:38:14.622: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m12.005860762s
    Jan 30 12:38:16.621: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m14.004717481s
    Jan 30 12:38:18.620: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m16.004300269s
    Jan 30 12:38:20.622: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m18.006080269s
    Jan 30 12:38:22.621: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m20.005411007s
    Jan 30 12:38:24.621: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m22.004642572s
    Jan 30 12:38:26.621: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m24.005447925s
    Jan 30 12:38:28.621: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m26.005056125s
    Jan 30 12:38:30.621: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m28.005292799s
    Jan 30 12:38:32.621: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m30.005283059s
    Jan 30 12:38:34.620: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m32.004453005s
    Jan 30 12:38:36.621: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m34.005184084s
    Jan 30 12:38:38.620: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m36.004326293s
    Jan 30 12:38:40.621: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m38.005488157s
    Jan 30 12:38:42.621: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m40.004631954s
    Jan 30 12:38:44.621: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m42.005054971s
    Jan 30 12:38:46.620: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m44.003639954s
    Jan 30 12:38:48.621: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m46.005175416s
    Jan 30 12:38:50.621: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m48.005110504s
    Jan 30 12:38:52.621: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m50.00492412s
    Jan 30 12:38:54.622: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m52.005617812s
    Jan 30 12:38:56.621: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m54.004832503s
    Jan 30 12:38:58.620: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m56.004239084s
    Jan 30 12:39:00.621: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m58.004592026s
    Jan 30 12:39:02.620: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m0.003580034s
    Jan 30 12:39:04.621: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m2.004878191s
    Jan 30 12:39:06.621: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m4.005559432s
    Jan 30 12:39:08.621: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m6.005054943s
    Jan 30 12:39:10.621: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m8.005027055s
    Jan 30 12:39:12.620: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m10.004374747s
    Jan 30 12:39:14.620: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m12.004557027s
    Jan 30 12:39:16.622: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m14.005865828s
    Jan 30 12:39:18.622: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m16.005686728s
    Jan 30 12:39:20.620: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m18.00401624s
    Jan 30 12:39:22.621: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m20.004876355s
    Jan 30 12:39:24.621: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m22.004908137s
    Jan 30 12:39:26.620: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m24.003820998s
    Jan 30 12:39:28.620: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m26.004515036s
    Jan 30 12:39:30.621: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m28.004933146s
    Jan 30 12:39:32.620: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m30.00442613s
    Jan 30 12:39:34.621: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m32.005017316s
    Jan 30 12:39:36.621: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m34.005011434s
    Jan 30 12:39:38.621: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m36.00495428s
    Jan 30 12:39:40.621: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m38.005424565s
    Jan 30 12:39:42.620: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m40.004545989s
    Jan 30 12:39:44.621: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m42.00461109s
    Jan 30 12:39:46.621: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m44.005450992s
    Jan 30 12:39:48.621: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m46.005285121s
    Jan 30 12:39:50.621: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m48.005175953s
    Jan 30 12:39:52.620: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m50.003958984s
    Jan 30 12:39:54.621: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m52.005325135s
    Jan 30 12:39:56.620: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m54.004155843s
    Jan 30 12:39:58.620: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m56.004467653s
    Jan 30 12:40:00.622: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m58.006122933s
    Jan 30 12:40:02.620: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.00434263s
    Jan 30 12:40:02.622: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.005922987s
    STEP: removing the label kubernetes.io/e2e-715c9ba5-e63c-4ee7-bce6-a96319f5a36e off the node pubt2-nks-for-dev1.dg.163.org 01/30/23 12:40:02.622
    STEP: verifying the node doesn't have the label kubernetes.io/e2e-715c9ba5-e63c-4ee7-bce6-a96319f5a36e 01/30/23 12:40:02.632
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan 30 12:40:02.634: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:88
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-pred-8303" for this suite. 01/30/23 12:40:02.636
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:742
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 12:40:02.64
Jan 30 12:40:02.641: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename svcaccounts 01/30/23 12:40:02.641
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:40:02.647
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:40:02.649
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:742
Jan 30 12:40:02.652: INFO: Got root ca configmap in namespace "svcaccounts-5003"
Jan 30 12:40:02.655: INFO: Deleted root ca configmap in namespace "svcaccounts-5003"
STEP: waiting for a new root ca configmap created 01/30/23 12:40:03.155
Jan 30 12:40:03.158: INFO: Recreated root ca configmap in namespace "svcaccounts-5003"
Jan 30 12:40:03.161: INFO: Updated root ca configmap in namespace "svcaccounts-5003"
STEP: waiting for the root ca configmap reconciled 01/30/23 12:40:03.662
Jan 30 12:40:03.663: INFO: Reconciled root ca configmap in namespace "svcaccounts-5003"
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Jan 30 12:40:03.663: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-5003" for this suite. 01/30/23 12:40:03.666
------------------------------
• [1.028 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:742

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 12:40:02.64
    Jan 30 12:40:02.641: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename svcaccounts 01/30/23 12:40:02.641
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:40:02.647
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:40:02.649
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
      test/e2e/auth/service_accounts.go:742
    Jan 30 12:40:02.652: INFO: Got root ca configmap in namespace "svcaccounts-5003"
    Jan 30 12:40:02.655: INFO: Deleted root ca configmap in namespace "svcaccounts-5003"
    STEP: waiting for a new root ca configmap created 01/30/23 12:40:03.155
    Jan 30 12:40:03.158: INFO: Recreated root ca configmap in namespace "svcaccounts-5003"
    Jan 30 12:40:03.161: INFO: Updated root ca configmap in namespace "svcaccounts-5003"
    STEP: waiting for the root ca configmap reconciled 01/30/23 12:40:03.662
    Jan 30 12:40:03.663: INFO: Reconciled root ca configmap in namespace "svcaccounts-5003"
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Jan 30 12:40:03.663: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-5003" for this suite. 01/30/23 12:40:03.666
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Lease
  lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72
[BeforeEach] [sig-node] Lease
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 12:40:03.669
Jan 30 12:40:03.669: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename lease-test 01/30/23 12:40:03.67
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:40:03.676
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:40:03.678
[BeforeEach] [sig-node] Lease
  test/e2e/framework/metrics/init/init.go:31
[It] lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72
[AfterEach] [sig-node] Lease
  test/e2e/framework/node/init/init.go:32
Jan 30 12:40:03.705: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Lease
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Lease
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Lease
  tear down framework | framework.go:193
STEP: Destroying namespace "lease-test-3136" for this suite. 01/30/23 12:40:03.707
------------------------------
• [0.040 seconds]
[sig-node] Lease
test/e2e/common/node/framework.go:23
  lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Lease
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 12:40:03.669
    Jan 30 12:40:03.669: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename lease-test 01/30/23 12:40:03.67
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:40:03.676
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:40:03.678
    [BeforeEach] [sig-node] Lease
      test/e2e/framework/metrics/init/init.go:31
    [It] lease API should be available [Conformance]
      test/e2e/common/node/lease.go:72
    [AfterEach] [sig-node] Lease
      test/e2e/framework/node/init/init.go:32
    Jan 30 12:40:03.705: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Lease
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Lease
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Lease
      tear down framework | framework.go:193
    STEP: Destroying namespace "lease-test-3136" for this suite. 01/30/23 12:40:03.707
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:134
[BeforeEach] [sig-node] Container Lifecycle Hook
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 12:40:03.71
Jan 30 12:40:03.710: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename container-lifecycle-hook 01/30/23 12:40:03.71
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:40:03.716
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:40:03.718
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:77
STEP: create the container to handle the HTTPGet hook request. 01/30/23 12:40:03.721
Jan 30 12:40:03.726: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-9614" to be "running and ready"
Jan 30 12:40:03.727: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 1.538885ms
Jan 30 12:40:03.727: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jan 30 12:40:05.729: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.003611673s
Jan 30 12:40:05.729: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Jan 30 12:40:05.729: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:134
STEP: create the pod with lifecycle hook 01/30/23 12:40:05.731
Jan 30 12:40:05.734: INFO: Waiting up to 5m0s for pod "pod-with-poststart-exec-hook" in namespace "container-lifecycle-hook-9614" to be "running and ready"
Jan 30 12:40:05.735: INFO: Pod "pod-with-poststart-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 1.518922ms
Jan 30 12:40:05.735: INFO: The phase of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Jan 30 12:40:07.738: INFO: Pod "pod-with-poststart-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004131649s
Jan 30 12:40:07.738: INFO: The phase of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Jan 30 12:40:09.739: INFO: Pod "pod-with-poststart-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 4.005229188s
Jan 30 12:40:09.739: INFO: The phase of Pod pod-with-poststart-exec-hook is Running (Ready = true)
Jan 30 12:40:09.739: INFO: Pod "pod-with-poststart-exec-hook" satisfied condition "running and ready"
STEP: check poststart hook 01/30/23 12:40:09.74
STEP: delete the pod with lifecycle hook 01/30/23 12:40:09.755
Jan 30 12:40:09.757: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan 30 12:40:09.759: INFO: Pod pod-with-poststart-exec-hook still exists
Jan 30 12:40:11.759: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan 30 12:40:11.762: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/node/init/init.go:32
Jan 30 12:40:11.762: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  tear down framework | framework.go:193
STEP: Destroying namespace "container-lifecycle-hook-9614" for this suite. 01/30/23 12:40:11.764
------------------------------
• [SLOW TEST] [8.057 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:134

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 12:40:03.71
    Jan 30 12:40:03.710: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename container-lifecycle-hook 01/30/23 12:40:03.71
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:40:03.716
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:40:03.718
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:77
    STEP: create the container to handle the HTTPGet hook request. 01/30/23 12:40:03.721
    Jan 30 12:40:03.726: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-9614" to be "running and ready"
    Jan 30 12:40:03.727: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 1.538885ms
    Jan 30 12:40:03.727: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Jan 30 12:40:05.729: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.003611673s
    Jan 30 12:40:05.729: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Jan 30 12:40:05.729: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute poststart exec hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:134
    STEP: create the pod with lifecycle hook 01/30/23 12:40:05.731
    Jan 30 12:40:05.734: INFO: Waiting up to 5m0s for pod "pod-with-poststart-exec-hook" in namespace "container-lifecycle-hook-9614" to be "running and ready"
    Jan 30 12:40:05.735: INFO: Pod "pod-with-poststart-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 1.518922ms
    Jan 30 12:40:05.735: INFO: The phase of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
    Jan 30 12:40:07.738: INFO: Pod "pod-with-poststart-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004131649s
    Jan 30 12:40:07.738: INFO: The phase of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
    Jan 30 12:40:09.739: INFO: Pod "pod-with-poststart-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 4.005229188s
    Jan 30 12:40:09.739: INFO: The phase of Pod pod-with-poststart-exec-hook is Running (Ready = true)
    Jan 30 12:40:09.739: INFO: Pod "pod-with-poststart-exec-hook" satisfied condition "running and ready"
    STEP: check poststart hook 01/30/23 12:40:09.74
    STEP: delete the pod with lifecycle hook 01/30/23 12:40:09.755
    Jan 30 12:40:09.757: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    Jan 30 12:40:09.759: INFO: Pod pod-with-poststart-exec-hook still exists
    Jan 30 12:40:11.759: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    Jan 30 12:40:11.762: INFO: Pod pod-with-poststart-exec-hook no longer exists
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/node/init/init.go:32
    Jan 30 12:40:11.762: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-lifecycle-hook-9614" for this suite. 01/30/23 12:40:11.764
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server
  should support --unix-socket=/path  [Conformance]
  test/e2e/kubectl/kubectl.go:1812
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 12:40:11.767
Jan 30 12:40:11.767: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename kubectl 01/30/23 12:40:11.767
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:40:11.774
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:40:11.776
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should support --unix-socket=/path  [Conformance]
  test/e2e/kubectl/kubectl.go:1812
STEP: Starting the proxy 01/30/23 12:40:11.778
Jan 30 12:40:11.778: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=kubectl-5664 proxy --unix-socket=/tmp/kubectl-proxy-unix2971548362/test'
STEP: retrieving proxy /api/ output 01/30/23 12:40:11.825
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jan 30 12:40:11.825: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-5664" for this suite. 01/30/23 12:40:11.828
------------------------------
• [0.063 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Proxy server
  test/e2e/kubectl/kubectl.go:1780
    should support --unix-socket=/path  [Conformance]
    test/e2e/kubectl/kubectl.go:1812

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 12:40:11.767
    Jan 30 12:40:11.767: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename kubectl 01/30/23 12:40:11.767
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:40:11.774
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:40:11.776
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should support --unix-socket=/path  [Conformance]
      test/e2e/kubectl/kubectl.go:1812
    STEP: Starting the proxy 01/30/23 12:40:11.778
    Jan 30 12:40:11.778: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=kubectl-5664 proxy --unix-socket=/tmp/kubectl-proxy-unix2971548362/test'
    STEP: retrieving proxy /api/ output 01/30/23 12:40:11.825
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jan 30 12:40:11.825: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-5664" for this suite. 01/30/23 12:40:11.828
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:124
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 12:40:11.831
Jan 30 12:40:11.831: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename projected 01/30/23 12:40:11.831
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:40:11.837
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:40:11.839
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:124
STEP: Creating projection with configMap that has name projected-configmap-test-upd-896a4085-1d30-434a-8951-4abe2a6a6777 01/30/23 12:40:11.843
STEP: Creating the pod 01/30/23 12:40:11.845
Jan 30 12:40:11.849: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-449ad912-adf5-41ce-8158-816b945dd786" in namespace "projected-7761" to be "running and ready"
Jan 30 12:40:11.851: INFO: Pod "pod-projected-configmaps-449ad912-adf5-41ce-8158-816b945dd786": Phase="Pending", Reason="", readiness=false. Elapsed: 1.504414ms
Jan 30 12:40:11.851: INFO: The phase of Pod pod-projected-configmaps-449ad912-adf5-41ce-8158-816b945dd786 is Pending, waiting for it to be Running (with Ready = true)
Jan 30 12:40:13.853: INFO: Pod "pod-projected-configmaps-449ad912-adf5-41ce-8158-816b945dd786": Phase="Running", Reason="", readiness=true. Elapsed: 2.00420197s
Jan 30 12:40:13.853: INFO: The phase of Pod pod-projected-configmaps-449ad912-adf5-41ce-8158-816b945dd786 is Running (Ready = true)
Jan 30 12:40:13.853: INFO: Pod "pod-projected-configmaps-449ad912-adf5-41ce-8158-816b945dd786" satisfied condition "running and ready"
STEP: Updating configmap projected-configmap-test-upd-896a4085-1d30-434a-8951-4abe2a6a6777 01/30/23 12:40:13.86
STEP: waiting to observe update in volume 01/30/23 12:40:13.862
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Jan 30 12:40:15.873: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-7761" for this suite. 01/30/23 12:40:15.875
------------------------------
• [4.047 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 12:40:11.831
    Jan 30 12:40:11.831: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename projected 01/30/23 12:40:11.831
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:40:11.837
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:40:11.839
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:124
    STEP: Creating projection with configMap that has name projected-configmap-test-upd-896a4085-1d30-434a-8951-4abe2a6a6777 01/30/23 12:40:11.843
    STEP: Creating the pod 01/30/23 12:40:11.845
    Jan 30 12:40:11.849: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-449ad912-adf5-41ce-8158-816b945dd786" in namespace "projected-7761" to be "running and ready"
    Jan 30 12:40:11.851: INFO: Pod "pod-projected-configmaps-449ad912-adf5-41ce-8158-816b945dd786": Phase="Pending", Reason="", readiness=false. Elapsed: 1.504414ms
    Jan 30 12:40:11.851: INFO: The phase of Pod pod-projected-configmaps-449ad912-adf5-41ce-8158-816b945dd786 is Pending, waiting for it to be Running (with Ready = true)
    Jan 30 12:40:13.853: INFO: Pod "pod-projected-configmaps-449ad912-adf5-41ce-8158-816b945dd786": Phase="Running", Reason="", readiness=true. Elapsed: 2.00420197s
    Jan 30 12:40:13.853: INFO: The phase of Pod pod-projected-configmaps-449ad912-adf5-41ce-8158-816b945dd786 is Running (Ready = true)
    Jan 30 12:40:13.853: INFO: Pod "pod-projected-configmaps-449ad912-adf5-41ce-8158-816b945dd786" satisfied condition "running and ready"
    STEP: Updating configmap projected-configmap-test-upd-896a4085-1d30-434a-8951-4abe2a6a6777 01/30/23 12:40:13.86
    STEP: waiting to observe update in volume 01/30/23 12:40:13.862
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Jan 30 12:40:15.873: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-7761" for this suite. 01/30/23 12:40:15.875
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:93
[BeforeEach] [sig-node] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 12:40:15.879
Jan 30 12:40:15.879: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename configmap 01/30/23 12:40:15.879
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:40:15.886
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:40:15.888
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:93
STEP: Creating configMap configmap-2158/configmap-test-11996f92-86bc-4ffa-b5b3-f2072be01f52 01/30/23 12:40:15.89
STEP: Creating a pod to test consume configMaps 01/30/23 12:40:15.892
Jan 30 12:40:15.897: INFO: Waiting up to 5m0s for pod "pod-configmaps-eb5009a3-ac95-4d2f-9f92-aa69fd794c61" in namespace "configmap-2158" to be "Succeeded or Failed"
Jan 30 12:40:15.898: INFO: Pod "pod-configmaps-eb5009a3-ac95-4d2f-9f92-aa69fd794c61": Phase="Pending", Reason="", readiness=false. Elapsed: 1.539263ms
Jan 30 12:40:17.900: INFO: Pod "pod-configmaps-eb5009a3-ac95-4d2f-9f92-aa69fd794c61": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003466557s
Jan 30 12:40:19.902: INFO: Pod "pod-configmaps-eb5009a3-ac95-4d2f-9f92-aa69fd794c61": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005450102s
STEP: Saw pod success 01/30/23 12:40:19.902
Jan 30 12:40:19.902: INFO: Pod "pod-configmaps-eb5009a3-ac95-4d2f-9f92-aa69fd794c61" satisfied condition "Succeeded or Failed"
Jan 30 12:40:19.904: INFO: Trying to get logs from node pubt2-nks-for-dev1.dg.163.org pod pod-configmaps-eb5009a3-ac95-4d2f-9f92-aa69fd794c61 container env-test: <nil>
STEP: delete the pod 01/30/23 12:40:19.909
Jan 30 12:40:19.913: INFO: Waiting for pod pod-configmaps-eb5009a3-ac95-4d2f-9f92-aa69fd794c61 to disappear
Jan 30 12:40:19.914: INFO: Pod pod-configmaps-eb5009a3-ac95-4d2f-9f92-aa69fd794c61 no longer exists
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jan 30 12:40:19.914: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-2158" for this suite. 01/30/23 12:40:19.916
------------------------------
• [4.040 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:93

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 12:40:15.879
    Jan 30 12:40:15.879: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename configmap 01/30/23 12:40:15.879
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:40:15.886
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:40:15.888
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable via the environment [NodeConformance] [Conformance]
      test/e2e/common/node/configmap.go:93
    STEP: Creating configMap configmap-2158/configmap-test-11996f92-86bc-4ffa-b5b3-f2072be01f52 01/30/23 12:40:15.89
    STEP: Creating a pod to test consume configMaps 01/30/23 12:40:15.892
    Jan 30 12:40:15.897: INFO: Waiting up to 5m0s for pod "pod-configmaps-eb5009a3-ac95-4d2f-9f92-aa69fd794c61" in namespace "configmap-2158" to be "Succeeded or Failed"
    Jan 30 12:40:15.898: INFO: Pod "pod-configmaps-eb5009a3-ac95-4d2f-9f92-aa69fd794c61": Phase="Pending", Reason="", readiness=false. Elapsed: 1.539263ms
    Jan 30 12:40:17.900: INFO: Pod "pod-configmaps-eb5009a3-ac95-4d2f-9f92-aa69fd794c61": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003466557s
    Jan 30 12:40:19.902: INFO: Pod "pod-configmaps-eb5009a3-ac95-4d2f-9f92-aa69fd794c61": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005450102s
    STEP: Saw pod success 01/30/23 12:40:19.902
    Jan 30 12:40:19.902: INFO: Pod "pod-configmaps-eb5009a3-ac95-4d2f-9f92-aa69fd794c61" satisfied condition "Succeeded or Failed"
    Jan 30 12:40:19.904: INFO: Trying to get logs from node pubt2-nks-for-dev1.dg.163.org pod pod-configmaps-eb5009a3-ac95-4d2f-9f92-aa69fd794c61 container env-test: <nil>
    STEP: delete the pod 01/30/23 12:40:19.909
    Jan 30 12:40:19.913: INFO: Waiting for pod pod-configmaps-eb5009a3-ac95-4d2f-9f92-aa69fd794c61 to disappear
    Jan 30 12:40:19.914: INFO: Pod pod-configmaps-eb5009a3-ac95-4d2f-9f92-aa69fd794c61 no longer exists
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jan 30 12:40:19.914: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-2158" for this suite. 01/30/23 12:40:19.916
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-apps] Daemon set [Serial]
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:374
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 12:40:19.919
Jan 30 12:40:19.919: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename daemonsets 01/30/23 12:40:19.92
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:40:19.925
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:40:19.927
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:374
Jan 30 12:40:19.937: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster. 01/30/23 12:40:19.94
Jan 30 12:40:19.943: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 30 12:40:19.944: INFO: Node pubt2-nks-for-dev1.dg.163.org is running 0 daemon pod, expected 1
Jan 30 12:40:20.948: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 30 12:40:20.948: INFO: Node pubt2-nks-for-dev1.dg.163.org is running 0 daemon pod, expected 1
Jan 30 12:40:21.949: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan 30 12:40:21.949: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
STEP: Update daemon pods image. 01/30/23 12:40:21.956
STEP: Check that daemon pods images are updated. 01/30/23 12:40:21.962
Jan 30 12:40:21.964: INFO: Wrong image for pod: daemon-set-lqbdb. Expected: harbor.cloud.netease.com/qzprod-k8s/e2e/agnhost:2.43, got: harbor.cloud.netease.com/qzprod-k8s/e2e/httpd:2.4.38-4.
Jan 30 12:40:21.964: INFO: Wrong image for pod: daemon-set-lwcg8. Expected: harbor.cloud.netease.com/qzprod-k8s/e2e/agnhost:2.43, got: harbor.cloud.netease.com/qzprod-k8s/e2e/httpd:2.4.38-4.
Jan 30 12:40:22.969: INFO: Wrong image for pod: daemon-set-lwcg8. Expected: harbor.cloud.netease.com/qzprod-k8s/e2e/agnhost:2.43, got: harbor.cloud.netease.com/qzprod-k8s/e2e/httpd:2.4.38-4.
Jan 30 12:40:23.969: INFO: Wrong image for pod: daemon-set-lwcg8. Expected: harbor.cloud.netease.com/qzprod-k8s/e2e/agnhost:2.43, got: harbor.cloud.netease.com/qzprod-k8s/e2e/httpd:2.4.38-4.
Jan 30 12:40:24.969: INFO: Pod daemon-set-4tkvv is not available
Jan 30 12:40:24.969: INFO: Wrong image for pod: daemon-set-lwcg8. Expected: harbor.cloud.netease.com/qzprod-k8s/e2e/agnhost:2.43, got: harbor.cloud.netease.com/qzprod-k8s/e2e/httpd:2.4.38-4.
Jan 30 12:40:25.969: INFO: Pod daemon-set-4tkvv is not available
Jan 30 12:40:25.969: INFO: Wrong image for pod: daemon-set-lwcg8. Expected: harbor.cloud.netease.com/qzprod-k8s/e2e/agnhost:2.43, got: harbor.cloud.netease.com/qzprod-k8s/e2e/httpd:2.4.38-4.
Jan 30 12:40:27.969: INFO: Pod daemon-set-6jfjk is not available
STEP: Check that daemon pods are still running on every node of the cluster. 01/30/23 12:40:27.971
Jan 30 12:40:27.975: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 30 12:40:27.975: INFO: Node pubt2-nks-for-dev1.dg.163.org is running 0 daemon pod, expected 1
Jan 30 12:40:28.981: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 30 12:40:28.981: INFO: Node pubt2-nks-for-dev1.dg.163.org is running 0 daemon pod, expected 1
Jan 30 12:40:29.981: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 30 12:40:29.981: INFO: Node pubt2-nks-for-dev1.dg.163.org is running 0 daemon pod, expected 1
Jan 30 12:40:30.981: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan 30 12:40:30.981: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 01/30/23 12:40:30.989
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-375, will wait for the garbage collector to delete the pods 01/30/23 12:40:30.989
Jan 30 12:40:31.043: INFO: Deleting DaemonSet.extensions daemon-set took: 2.314948ms
Jan 30 12:40:31.144: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.709629ms
Jan 30 12:40:33.146: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 30 12:40:33.146: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jan 30 12:40:33.147: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"38080"},"items":null}

Jan 30 12:40:33.149: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"38080"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Jan 30 12:40:33.154: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-375" for this suite. 01/30/23 12:40:33.156
------------------------------
• [SLOW TEST] [13.239 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:374

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 12:40:19.919
    Jan 30 12:40:19.919: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename daemonsets 01/30/23 12:40:19.92
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:40:19.925
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:40:19.927
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
      test/e2e/apps/daemon_set.go:374
    Jan 30 12:40:19.937: INFO: Creating simple daemon set daemon-set
    STEP: Check that daemon pods launch on every node of the cluster. 01/30/23 12:40:19.94
    Jan 30 12:40:19.943: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 30 12:40:19.944: INFO: Node pubt2-nks-for-dev1.dg.163.org is running 0 daemon pod, expected 1
    Jan 30 12:40:20.948: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 30 12:40:20.948: INFO: Node pubt2-nks-for-dev1.dg.163.org is running 0 daemon pod, expected 1
    Jan 30 12:40:21.949: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jan 30 12:40:21.949: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    STEP: Update daemon pods image. 01/30/23 12:40:21.956
    STEP: Check that daemon pods images are updated. 01/30/23 12:40:21.962
    Jan 30 12:40:21.964: INFO: Wrong image for pod: daemon-set-lqbdb. Expected: harbor.cloud.netease.com/qzprod-k8s/e2e/agnhost:2.43, got: harbor.cloud.netease.com/qzprod-k8s/e2e/httpd:2.4.38-4.
    Jan 30 12:40:21.964: INFO: Wrong image for pod: daemon-set-lwcg8. Expected: harbor.cloud.netease.com/qzprod-k8s/e2e/agnhost:2.43, got: harbor.cloud.netease.com/qzprod-k8s/e2e/httpd:2.4.38-4.
    Jan 30 12:40:22.969: INFO: Wrong image for pod: daemon-set-lwcg8. Expected: harbor.cloud.netease.com/qzprod-k8s/e2e/agnhost:2.43, got: harbor.cloud.netease.com/qzprod-k8s/e2e/httpd:2.4.38-4.
    Jan 30 12:40:23.969: INFO: Wrong image for pod: daemon-set-lwcg8. Expected: harbor.cloud.netease.com/qzprod-k8s/e2e/agnhost:2.43, got: harbor.cloud.netease.com/qzprod-k8s/e2e/httpd:2.4.38-4.
    Jan 30 12:40:24.969: INFO: Pod daemon-set-4tkvv is not available
    Jan 30 12:40:24.969: INFO: Wrong image for pod: daemon-set-lwcg8. Expected: harbor.cloud.netease.com/qzprod-k8s/e2e/agnhost:2.43, got: harbor.cloud.netease.com/qzprod-k8s/e2e/httpd:2.4.38-4.
    Jan 30 12:40:25.969: INFO: Pod daemon-set-4tkvv is not available
    Jan 30 12:40:25.969: INFO: Wrong image for pod: daemon-set-lwcg8. Expected: harbor.cloud.netease.com/qzprod-k8s/e2e/agnhost:2.43, got: harbor.cloud.netease.com/qzprod-k8s/e2e/httpd:2.4.38-4.
    Jan 30 12:40:27.969: INFO: Pod daemon-set-6jfjk is not available
    STEP: Check that daemon pods are still running on every node of the cluster. 01/30/23 12:40:27.971
    Jan 30 12:40:27.975: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan 30 12:40:27.975: INFO: Node pubt2-nks-for-dev1.dg.163.org is running 0 daemon pod, expected 1
    Jan 30 12:40:28.981: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan 30 12:40:28.981: INFO: Node pubt2-nks-for-dev1.dg.163.org is running 0 daemon pod, expected 1
    Jan 30 12:40:29.981: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan 30 12:40:29.981: INFO: Node pubt2-nks-for-dev1.dg.163.org is running 0 daemon pod, expected 1
    Jan 30 12:40:30.981: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jan 30 12:40:30.981: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 01/30/23 12:40:30.989
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-375, will wait for the garbage collector to delete the pods 01/30/23 12:40:30.989
    Jan 30 12:40:31.043: INFO: Deleting DaemonSet.extensions daemon-set took: 2.314948ms
    Jan 30 12:40:31.144: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.709629ms
    Jan 30 12:40:33.146: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 30 12:40:33.146: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Jan 30 12:40:33.147: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"38080"},"items":null}

    Jan 30 12:40:33.149: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"38080"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan 30 12:40:33.154: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-375" for this suite. 01/30/23 12:40:33.156
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] Projected downwardAPI
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:68
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 12:40:33.159
Jan 30 12:40:33.159: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename projected 01/30/23 12:40:33.159
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:40:33.165
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:40:33.168
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:68
STEP: Creating a pod to test downward API volume plugin 01/30/23 12:40:33.17
Jan 30 12:40:33.174: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0aaa3f10-bbd2-4941-94e5-fd0da2816dbc" in namespace "projected-3021" to be "Succeeded or Failed"
Jan 30 12:40:33.176: INFO: Pod "downwardapi-volume-0aaa3f10-bbd2-4941-94e5-fd0da2816dbc": Phase="Pending", Reason="", readiness=false. Elapsed: 1.63733ms
Jan 30 12:40:35.179: INFO: Pod "downwardapi-volume-0aaa3f10-bbd2-4941-94e5-fd0da2816dbc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004766766s
Jan 30 12:40:37.179: INFO: Pod "downwardapi-volume-0aaa3f10-bbd2-4941-94e5-fd0da2816dbc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004952015s
STEP: Saw pod success 01/30/23 12:40:37.179
Jan 30 12:40:37.179: INFO: Pod "downwardapi-volume-0aaa3f10-bbd2-4941-94e5-fd0da2816dbc" satisfied condition "Succeeded or Failed"
Jan 30 12:40:37.181: INFO: Trying to get logs from node pubt2-nks-for-dev1.dg.163.org pod downwardapi-volume-0aaa3f10-bbd2-4941-94e5-fd0da2816dbc container client-container: <nil>
STEP: delete the pod 01/30/23 12:40:37.185
Jan 30 12:40:37.189: INFO: Waiting for pod downwardapi-volume-0aaa3f10-bbd2-4941-94e5-fd0da2816dbc to disappear
Jan 30 12:40:37.190: INFO: Pod downwardapi-volume-0aaa3f10-bbd2-4941-94e5-fd0da2816dbc no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Jan 30 12:40:37.190: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-3021" for this suite. 01/30/23 12:40:37.192
------------------------------
• [4.036 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 12:40:33.159
    Jan 30 12:40:33.159: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename projected 01/30/23 12:40:33.159
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:40:33.165
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:40:33.168
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:68
    STEP: Creating a pod to test downward API volume plugin 01/30/23 12:40:33.17
    Jan 30 12:40:33.174: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0aaa3f10-bbd2-4941-94e5-fd0da2816dbc" in namespace "projected-3021" to be "Succeeded or Failed"
    Jan 30 12:40:33.176: INFO: Pod "downwardapi-volume-0aaa3f10-bbd2-4941-94e5-fd0da2816dbc": Phase="Pending", Reason="", readiness=false. Elapsed: 1.63733ms
    Jan 30 12:40:35.179: INFO: Pod "downwardapi-volume-0aaa3f10-bbd2-4941-94e5-fd0da2816dbc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004766766s
    Jan 30 12:40:37.179: INFO: Pod "downwardapi-volume-0aaa3f10-bbd2-4941-94e5-fd0da2816dbc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004952015s
    STEP: Saw pod success 01/30/23 12:40:37.179
    Jan 30 12:40:37.179: INFO: Pod "downwardapi-volume-0aaa3f10-bbd2-4941-94e5-fd0da2816dbc" satisfied condition "Succeeded or Failed"
    Jan 30 12:40:37.181: INFO: Trying to get logs from node pubt2-nks-for-dev1.dg.163.org pod downwardapi-volume-0aaa3f10-bbd2-4941-94e5-fd0da2816dbc container client-container: <nil>
    STEP: delete the pod 01/30/23 12:40:37.185
    Jan 30 12:40:37.189: INFO: Waiting for pod downwardapi-volume-0aaa3f10-bbd2-4941-94e5-fd0da2816dbc to disappear
    Jan 30 12:40:37.190: INFO: Pod downwardapi-volume-0aaa3f10-bbd2-4941-94e5-fd0da2816dbc no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Jan 30 12:40:37.190: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-3021" for this suite. 01/30/23 12:40:37.192
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:252
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 12:40:37.195
Jan 30 12:40:37.195: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename webhook 01/30/23 12:40:37.196
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:40:37.201
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:40:37.203
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 01/30/23 12:40:37.211
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/30/23 12:40:37.742
STEP: Deploying the webhook pod 01/30/23 12:40:37.745
STEP: Wait for the deployment to be ready 01/30/23 12:40:37.75
Jan 30 12:40:37.753: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 01/30/23 12:40:39.76
STEP: Verifying the service has paired with the endpoint 01/30/23 12:40:39.763
Jan 30 12:40:40.764: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:252
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API 01/30/23 12:40:40.766
STEP: create a configmap that should be updated by the webhook 01/30/23 12:40:40.776
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 30 12:40:40.787: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-2388" for this suite. 01/30/23 12:40:40.802
STEP: Destroying namespace "webhook-2388-markers" for this suite. 01/30/23 12:40:40.804
------------------------------
• [3.611 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:252

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 12:40:37.195
    Jan 30 12:40:37.195: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename webhook 01/30/23 12:40:37.196
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:40:37.201
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:40:37.203
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 01/30/23 12:40:37.211
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/30/23 12:40:37.742
    STEP: Deploying the webhook pod 01/30/23 12:40:37.745
    STEP: Wait for the deployment to be ready 01/30/23 12:40:37.75
    Jan 30 12:40:37.753: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 01/30/23 12:40:39.76
    STEP: Verifying the service has paired with the endpoint 01/30/23 12:40:39.763
    Jan 30 12:40:40.764: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate configmap [Conformance]
      test/e2e/apimachinery/webhook.go:252
    STEP: Registering the mutating configmap webhook via the AdmissionRegistration API 01/30/23 12:40:40.766
    STEP: create a configmap that should be updated by the webhook 01/30/23 12:40:40.776
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 30 12:40:40.787: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-2388" for this suite. 01/30/23 12:40:40.802
    STEP: Destroying namespace "webhook-2388-markers" for this suite. 01/30/23 12:40:40.804
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:221
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 12:40:40.813
Jan 30 12:40:40.813: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename downward-api 01/30/23 12:40:40.814
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:40:40.82
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:40:40.822
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:221
STEP: Creating a pod to test downward API volume plugin 01/30/23 12:40:40.824
Jan 30 12:40:40.828: INFO: Waiting up to 5m0s for pod "downwardapi-volume-bdd447f0-4945-4a22-b554-479bf9e0ff85" in namespace "downward-api-2290" to be "Succeeded or Failed"
Jan 30 12:40:40.830: INFO: Pod "downwardapi-volume-bdd447f0-4945-4a22-b554-479bf9e0ff85": Phase="Pending", Reason="", readiness=false. Elapsed: 1.652043ms
Jan 30 12:40:42.832: INFO: Pod "downwardapi-volume-bdd447f0-4945-4a22-b554-479bf9e0ff85": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003734307s
Jan 30 12:40:44.833: INFO: Pod "downwardapi-volume-bdd447f0-4945-4a22-b554-479bf9e0ff85": Phase="Pending", Reason="", readiness=false. Elapsed: 4.004819559s
Jan 30 12:40:46.833: INFO: Pod "downwardapi-volume-bdd447f0-4945-4a22-b554-479bf9e0ff85": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.004669211s
STEP: Saw pod success 01/30/23 12:40:46.833
Jan 30 12:40:46.833: INFO: Pod "downwardapi-volume-bdd447f0-4945-4a22-b554-479bf9e0ff85" satisfied condition "Succeeded or Failed"
Jan 30 12:40:46.834: INFO: Trying to get logs from node pubt2-nks-for-dev1.dg.163.org pod downwardapi-volume-bdd447f0-4945-4a22-b554-479bf9e0ff85 container client-container: <nil>
STEP: delete the pod 01/30/23 12:40:46.839
Jan 30 12:40:46.844: INFO: Waiting for pod downwardapi-volume-bdd447f0-4945-4a22-b554-479bf9e0ff85 to disappear
Jan 30 12:40:46.845: INFO: Pod downwardapi-volume-bdd447f0-4945-4a22-b554-479bf9e0ff85 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Jan 30 12:40:46.845: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-2290" for this suite. 01/30/23 12:40:46.847
------------------------------
• [SLOW TEST] [6.036 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:221

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 12:40:40.813
    Jan 30 12:40:40.813: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename downward-api 01/30/23 12:40:40.814
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:40:40.82
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:40:40.822
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide container's cpu request [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:221
    STEP: Creating a pod to test downward API volume plugin 01/30/23 12:40:40.824
    Jan 30 12:40:40.828: INFO: Waiting up to 5m0s for pod "downwardapi-volume-bdd447f0-4945-4a22-b554-479bf9e0ff85" in namespace "downward-api-2290" to be "Succeeded or Failed"
    Jan 30 12:40:40.830: INFO: Pod "downwardapi-volume-bdd447f0-4945-4a22-b554-479bf9e0ff85": Phase="Pending", Reason="", readiness=false. Elapsed: 1.652043ms
    Jan 30 12:40:42.832: INFO: Pod "downwardapi-volume-bdd447f0-4945-4a22-b554-479bf9e0ff85": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003734307s
    Jan 30 12:40:44.833: INFO: Pod "downwardapi-volume-bdd447f0-4945-4a22-b554-479bf9e0ff85": Phase="Pending", Reason="", readiness=false. Elapsed: 4.004819559s
    Jan 30 12:40:46.833: INFO: Pod "downwardapi-volume-bdd447f0-4945-4a22-b554-479bf9e0ff85": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.004669211s
    STEP: Saw pod success 01/30/23 12:40:46.833
    Jan 30 12:40:46.833: INFO: Pod "downwardapi-volume-bdd447f0-4945-4a22-b554-479bf9e0ff85" satisfied condition "Succeeded or Failed"
    Jan 30 12:40:46.834: INFO: Trying to get logs from node pubt2-nks-for-dev1.dg.163.org pod downwardapi-volume-bdd447f0-4945-4a22-b554-479bf9e0ff85 container client-container: <nil>
    STEP: delete the pod 01/30/23 12:40:46.839
    Jan 30 12:40:46.844: INFO: Waiting for pod downwardapi-volume-bdd447f0-4945-4a22-b554-479bf9e0ff85 to disappear
    Jan 30 12:40:46.845: INFO: Pod downwardapi-volume-bdd447f0-4945-4a22-b554-479bf9e0ff85 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Jan 30 12:40:46.845: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-2290" for this suite. 01/30/23 12:40:46.847
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:508
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 12:40:46.85
Jan 30 12:40:46.851: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename webhook 01/30/23 12:40:46.851
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:40:46.857
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:40:46.859
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 01/30/23 12:40:46.867
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/30/23 12:40:47.117
STEP: Deploying the webhook pod 01/30/23 12:40:47.119
STEP: Wait for the deployment to be ready 01/30/23 12:40:47.124
Jan 30 12:40:47.127: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 01/30/23 12:40:49.133
STEP: Verifying the service has paired with the endpoint 01/30/23 12:40:49.137
Jan 30 12:40:50.137: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:508
STEP: Creating a mutating webhook configuration 01/30/23 12:40:50.14
STEP: Updating a mutating webhook configuration's rules to not include the create operation 01/30/23 12:40:50.151
STEP: Creating a configMap that should not be mutated 01/30/23 12:40:50.154
STEP: Patching a mutating webhook configuration's rules to include the create operation 01/30/23 12:40:50.158
STEP: Creating a configMap that should be mutated 01/30/23 12:40:50.163
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 30 12:40:50.174: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-6287" for this suite. 01/30/23 12:40:50.187
STEP: Destroying namespace "webhook-6287-markers" for this suite. 01/30/23 12:40:50.189
------------------------------
• [3.342 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:508

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 12:40:46.85
    Jan 30 12:40:46.851: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename webhook 01/30/23 12:40:46.851
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:40:46.857
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:40:46.859
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 01/30/23 12:40:46.867
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/30/23 12:40:47.117
    STEP: Deploying the webhook pod 01/30/23 12:40:47.119
    STEP: Wait for the deployment to be ready 01/30/23 12:40:47.124
    Jan 30 12:40:47.127: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 01/30/23 12:40:49.133
    STEP: Verifying the service has paired with the endpoint 01/30/23 12:40:49.137
    Jan 30 12:40:50.137: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] patching/updating a mutating webhook should work [Conformance]
      test/e2e/apimachinery/webhook.go:508
    STEP: Creating a mutating webhook configuration 01/30/23 12:40:50.14
    STEP: Updating a mutating webhook configuration's rules to not include the create operation 01/30/23 12:40:50.151
    STEP: Creating a configMap that should not be mutated 01/30/23 12:40:50.154
    STEP: Patching a mutating webhook configuration's rules to include the create operation 01/30/23 12:40:50.158
    STEP: Creating a configMap that should be mutated 01/30/23 12:40:50.163
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 30 12:40:50.174: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-6287" for this suite. 01/30/23 12:40:50.187
    STEP: Destroying namespace "webhook-6287-markers" for this suite. 01/30/23 12:40:50.189
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] CronJob
  should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 12:40:50.192
Jan 30 12:40:50.192: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename cronjob 01/30/23 12:40:50.193
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:40:50.199
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:40:50.2
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160
STEP: Creating a ReplaceConcurrent cronjob 01/30/23 12:40:50.202
STEP: Ensuring a job is scheduled 01/30/23 12:40:50.205
STEP: Ensuring exactly one is scheduled 01/30/23 12:41:00.207
STEP: Ensuring exactly one running job exists by listing jobs explicitly 01/30/23 12:41:00.209
STEP: Ensuring the job is replaced with a new one 01/30/23 12:41:00.211
STEP: Removing cronjob 01/30/23 12:42:00.214
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Jan 30 12:42:00.217: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-8534" for this suite. 01/30/23 12:42:00.22
------------------------------
• [SLOW TEST] [70.030 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 12:40:50.192
    Jan 30 12:40:50.192: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename cronjob 01/30/23 12:40:50.193
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:40:50.199
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:40:50.2
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should replace jobs when ReplaceConcurrent [Conformance]
      test/e2e/apps/cronjob.go:160
    STEP: Creating a ReplaceConcurrent cronjob 01/30/23 12:40:50.202
    STEP: Ensuring a job is scheduled 01/30/23 12:40:50.205
    STEP: Ensuring exactly one is scheduled 01/30/23 12:41:00.207
    STEP: Ensuring exactly one running job exists by listing jobs explicitly 01/30/23 12:41:00.209
    STEP: Ensuring the job is replaced with a new one 01/30/23 12:41:00.211
    STEP: Removing cronjob 01/30/23 12:42:00.214
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Jan 30 12:42:00.217: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-8534" for this suite. 01/30/23 12:42:00.22
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:124
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 12:42:00.222
Jan 30 12:42:00.222: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename configmap 01/30/23 12:42:00.223
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:42:00.229
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:42:00.231
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:124
STEP: Creating configMap with name configmap-test-upd-c7f69521-3e79-4e16-bde0-52c07bb3aa7c 01/30/23 12:42:00.235
STEP: Creating the pod 01/30/23 12:42:00.237
Jan 30 12:42:00.241: INFO: Waiting up to 5m0s for pod "pod-configmaps-df1b83d0-5e32-4b78-b2b2-42cab6bd9198" in namespace "configmap-1010" to be "running and ready"
Jan 30 12:42:00.243: INFO: Pod "pod-configmaps-df1b83d0-5e32-4b78-b2b2-42cab6bd9198": Phase="Pending", Reason="", readiness=false. Elapsed: 1.577501ms
Jan 30 12:42:00.243: INFO: The phase of Pod pod-configmaps-df1b83d0-5e32-4b78-b2b2-42cab6bd9198 is Pending, waiting for it to be Running (with Ready = true)
Jan 30 12:42:02.245: INFO: Pod "pod-configmaps-df1b83d0-5e32-4b78-b2b2-42cab6bd9198": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00392957s
Jan 30 12:42:02.245: INFO: The phase of Pod pod-configmaps-df1b83d0-5e32-4b78-b2b2-42cab6bd9198 is Pending, waiting for it to be Running (with Ready = true)
Jan 30 12:42:04.245: INFO: Pod "pod-configmaps-df1b83d0-5e32-4b78-b2b2-42cab6bd9198": Phase="Running", Reason="", readiness=true. Elapsed: 4.004050711s
Jan 30 12:42:04.245: INFO: The phase of Pod pod-configmaps-df1b83d0-5e32-4b78-b2b2-42cab6bd9198 is Running (Ready = true)
Jan 30 12:42:04.245: INFO: Pod "pod-configmaps-df1b83d0-5e32-4b78-b2b2-42cab6bd9198" satisfied condition "running and ready"
STEP: Updating configmap configmap-test-upd-c7f69521-3e79-4e16-bde0-52c07bb3aa7c 01/30/23 12:42:04.252
STEP: waiting to observe update in volume 01/30/23 12:42:04.254
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jan 30 12:43:20.519: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-1010" for this suite. 01/30/23 12:43:20.521
------------------------------
• [SLOW TEST] [80.301 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 12:42:00.222
    Jan 30 12:42:00.222: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename configmap 01/30/23 12:42:00.223
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:42:00.229
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:42:00.231
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:124
    STEP: Creating configMap with name configmap-test-upd-c7f69521-3e79-4e16-bde0-52c07bb3aa7c 01/30/23 12:42:00.235
    STEP: Creating the pod 01/30/23 12:42:00.237
    Jan 30 12:42:00.241: INFO: Waiting up to 5m0s for pod "pod-configmaps-df1b83d0-5e32-4b78-b2b2-42cab6bd9198" in namespace "configmap-1010" to be "running and ready"
    Jan 30 12:42:00.243: INFO: Pod "pod-configmaps-df1b83d0-5e32-4b78-b2b2-42cab6bd9198": Phase="Pending", Reason="", readiness=false. Elapsed: 1.577501ms
    Jan 30 12:42:00.243: INFO: The phase of Pod pod-configmaps-df1b83d0-5e32-4b78-b2b2-42cab6bd9198 is Pending, waiting for it to be Running (with Ready = true)
    Jan 30 12:42:02.245: INFO: Pod "pod-configmaps-df1b83d0-5e32-4b78-b2b2-42cab6bd9198": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00392957s
    Jan 30 12:42:02.245: INFO: The phase of Pod pod-configmaps-df1b83d0-5e32-4b78-b2b2-42cab6bd9198 is Pending, waiting for it to be Running (with Ready = true)
    Jan 30 12:42:04.245: INFO: Pod "pod-configmaps-df1b83d0-5e32-4b78-b2b2-42cab6bd9198": Phase="Running", Reason="", readiness=true. Elapsed: 4.004050711s
    Jan 30 12:42:04.245: INFO: The phase of Pod pod-configmaps-df1b83d0-5e32-4b78-b2b2-42cab6bd9198 is Running (Ready = true)
    Jan 30 12:42:04.245: INFO: Pod "pod-configmaps-df1b83d0-5e32-4b78-b2b2-42cab6bd9198" satisfied condition "running and ready"
    STEP: Updating configmap configmap-test-upd-c7f69521-3e79-4e16-bde0-52c07bb3aa7c 01/30/23 12:42:04.252
    STEP: waiting to observe update in volume 01/30/23 12:42:04.254
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jan 30 12:43:20.519: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-1010" for this suite. 01/30/23 12:43:20.521
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Probing container
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:169
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 12:43:20.524
Jan 30 12:43:20.524: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename container-probe 01/30/23 12:43:20.525
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:43:20.532
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:43:20.534
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:169
STEP: Creating pod liveness-b648c599-9c88-4f33-833b-594f023ecf2d in namespace container-probe-9147 01/30/23 12:43:20.536
Jan 30 12:43:20.540: INFO: Waiting up to 5m0s for pod "liveness-b648c599-9c88-4f33-833b-594f023ecf2d" in namespace "container-probe-9147" to be "not pending"
Jan 30 12:43:20.541: INFO: Pod "liveness-b648c599-9c88-4f33-833b-594f023ecf2d": Phase="Pending", Reason="", readiness=false. Elapsed: 1.428354ms
Jan 30 12:43:22.544: INFO: Pod "liveness-b648c599-9c88-4f33-833b-594f023ecf2d": Phase="Running", Reason="", readiness=true. Elapsed: 2.004368669s
Jan 30 12:43:22.544: INFO: Pod "liveness-b648c599-9c88-4f33-833b-594f023ecf2d" satisfied condition "not pending"
Jan 30 12:43:22.544: INFO: Started pod liveness-b648c599-9c88-4f33-833b-594f023ecf2d in namespace container-probe-9147
STEP: checking the pod's current state and verifying that restartCount is present 01/30/23 12:43:22.544
Jan 30 12:43:22.546: INFO: Initial restart count of pod liveness-b648c599-9c88-4f33-833b-594f023ecf2d is 0
Jan 30 12:43:42.579: INFO: Restart count of pod container-probe-9147/liveness-b648c599-9c88-4f33-833b-594f023ecf2d is now 1 (20.033494798s elapsed)
STEP: deleting the pod 01/30/23 12:43:42.579
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Jan 30 12:43:42.584: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-9147" for this suite. 01/30/23 12:43:42.586
------------------------------
• [SLOW TEST] [22.064 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:169

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 12:43:20.524
    Jan 30 12:43:20.524: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename container-probe 01/30/23 12:43:20.525
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:43:20.532
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:43:20.534
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:169
    STEP: Creating pod liveness-b648c599-9c88-4f33-833b-594f023ecf2d in namespace container-probe-9147 01/30/23 12:43:20.536
    Jan 30 12:43:20.540: INFO: Waiting up to 5m0s for pod "liveness-b648c599-9c88-4f33-833b-594f023ecf2d" in namespace "container-probe-9147" to be "not pending"
    Jan 30 12:43:20.541: INFO: Pod "liveness-b648c599-9c88-4f33-833b-594f023ecf2d": Phase="Pending", Reason="", readiness=false. Elapsed: 1.428354ms
    Jan 30 12:43:22.544: INFO: Pod "liveness-b648c599-9c88-4f33-833b-594f023ecf2d": Phase="Running", Reason="", readiness=true. Elapsed: 2.004368669s
    Jan 30 12:43:22.544: INFO: Pod "liveness-b648c599-9c88-4f33-833b-594f023ecf2d" satisfied condition "not pending"
    Jan 30 12:43:22.544: INFO: Started pod liveness-b648c599-9c88-4f33-833b-594f023ecf2d in namespace container-probe-9147
    STEP: checking the pod's current state and verifying that restartCount is present 01/30/23 12:43:22.544
    Jan 30 12:43:22.546: INFO: Initial restart count of pod liveness-b648c599-9c88-4f33-833b-594f023ecf2d is 0
    Jan 30 12:43:42.579: INFO: Restart count of pod container-probe-9147/liveness-b648c599-9c88-4f33-833b-594f023ecf2d is now 1 (20.033494798s elapsed)
    STEP: deleting the pod 01/30/23 12:43:42.579
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Jan 30 12:43:42.584: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-9147" for this suite. 01/30/23 12:43:42.586
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 12:43:42.589
Jan 30 12:43:42.589: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename dns 01/30/23 12:43:42.59
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:43:42.596
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:43:42.598
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290
STEP: Creating a test headless service 01/30/23 12:43:42.6
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-9168.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-9168.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-9168.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9168.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-9168.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-9168.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-9168.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-9168.svc.cluster.local;sleep 1; done
 01/30/23 12:43:42.602
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-9168.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-9168.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-9168.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-9168.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-9168.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-9168.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-9168.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-9168.svc.cluster.local;sleep 1; done
 01/30/23 12:43:42.602
STEP: creating a pod to probe DNS 01/30/23 12:43:42.602
STEP: submitting the pod to kubernetes 01/30/23 12:43:42.602
Jan 30 12:43:42.607: INFO: Waiting up to 15m0s for pod "dns-test-e829959f-2898-4508-ae41-2759874e00ad" in namespace "dns-9168" to be "running"
Jan 30 12:43:42.609: INFO: Pod "dns-test-e829959f-2898-4508-ae41-2759874e00ad": Phase="Pending", Reason="", readiness=false. Elapsed: 1.645993ms
Jan 30 12:43:44.611: INFO: Pod "dns-test-e829959f-2898-4508-ae41-2759874e00ad": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003881277s
Jan 30 12:43:46.612: INFO: Pod "dns-test-e829959f-2898-4508-ae41-2759874e00ad": Phase="Running", Reason="", readiness=true. Elapsed: 4.004946753s
Jan 30 12:43:46.612: INFO: Pod "dns-test-e829959f-2898-4508-ae41-2759874e00ad" satisfied condition "running"
STEP: retrieving the pod 01/30/23 12:43:46.612
STEP: looking for the results for each expected name from probers 01/30/23 12:43:46.614
Jan 30 12:43:46.616: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-9168.svc.cluster.local from pod dns-9168/dns-test-e829959f-2898-4508-ae41-2759874e00ad: the server could not find the requested resource (get pods dns-test-e829959f-2898-4508-ae41-2759874e00ad)
Jan 30 12:43:46.618: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9168.svc.cluster.local from pod dns-9168/dns-test-e829959f-2898-4508-ae41-2759874e00ad: the server could not find the requested resource (get pods dns-test-e829959f-2898-4508-ae41-2759874e00ad)
Jan 30 12:43:46.620: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-9168.svc.cluster.local from pod dns-9168/dns-test-e829959f-2898-4508-ae41-2759874e00ad: the server could not find the requested resource (get pods dns-test-e829959f-2898-4508-ae41-2759874e00ad)
Jan 30 12:43:46.624: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-9168.svc.cluster.local from pod dns-9168/dns-test-e829959f-2898-4508-ae41-2759874e00ad: the server could not find the requested resource (get pods dns-test-e829959f-2898-4508-ae41-2759874e00ad)
Jan 30 12:43:46.625: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-9168.svc.cluster.local from pod dns-9168/dns-test-e829959f-2898-4508-ae41-2759874e00ad: the server could not find the requested resource (get pods dns-test-e829959f-2898-4508-ae41-2759874e00ad)
Jan 30 12:43:46.627: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-9168.svc.cluster.local from pod dns-9168/dns-test-e829959f-2898-4508-ae41-2759874e00ad: the server could not find the requested resource (get pods dns-test-e829959f-2898-4508-ae41-2759874e00ad)
Jan 30 12:43:46.629: INFO: Unable to read jessie_udp@dns-test-service-2.dns-9168.svc.cluster.local from pod dns-9168/dns-test-e829959f-2898-4508-ae41-2759874e00ad: the server could not find the requested resource (get pods dns-test-e829959f-2898-4508-ae41-2759874e00ad)
Jan 30 12:43:46.631: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-9168.svc.cluster.local from pod dns-9168/dns-test-e829959f-2898-4508-ae41-2759874e00ad: the server could not find the requested resource (get pods dns-test-e829959f-2898-4508-ae41-2759874e00ad)
Jan 30 12:43:46.631: INFO: Lookups using dns-9168/dns-test-e829959f-2898-4508-ae41-2759874e00ad failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-9168.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9168.svc.cluster.local wheezy_udp@dns-test-service-2.dns-9168.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-9168.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-9168.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-9168.svc.cluster.local jessie_udp@dns-test-service-2.dns-9168.svc.cluster.local jessie_tcp@dns-test-service-2.dns-9168.svc.cluster.local]

Jan 30 12:43:51.649: INFO: DNS probes using dns-9168/dns-test-e829959f-2898-4508-ae41-2759874e00ad succeeded

STEP: deleting the pod 01/30/23 12:43:51.649
STEP: deleting the test headless service 01/30/23 12:43:51.656
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Jan 30 12:43:51.660: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-9168" for this suite. 01/30/23 12:43:51.662
------------------------------
• [SLOW TEST] [9.075 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 12:43:42.589
    Jan 30 12:43:42.589: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename dns 01/30/23 12:43:42.59
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:43:42.596
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:43:42.598
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for pods for Subdomain [Conformance]
      test/e2e/network/dns.go:290
    STEP: Creating a test headless service 01/30/23 12:43:42.6
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-9168.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-9168.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-9168.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9168.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-9168.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-9168.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-9168.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-9168.svc.cluster.local;sleep 1; done
     01/30/23 12:43:42.602
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-9168.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-9168.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-9168.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-9168.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-9168.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-9168.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-9168.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-9168.svc.cluster.local;sleep 1; done
     01/30/23 12:43:42.602
    STEP: creating a pod to probe DNS 01/30/23 12:43:42.602
    STEP: submitting the pod to kubernetes 01/30/23 12:43:42.602
    Jan 30 12:43:42.607: INFO: Waiting up to 15m0s for pod "dns-test-e829959f-2898-4508-ae41-2759874e00ad" in namespace "dns-9168" to be "running"
    Jan 30 12:43:42.609: INFO: Pod "dns-test-e829959f-2898-4508-ae41-2759874e00ad": Phase="Pending", Reason="", readiness=false. Elapsed: 1.645993ms
    Jan 30 12:43:44.611: INFO: Pod "dns-test-e829959f-2898-4508-ae41-2759874e00ad": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003881277s
    Jan 30 12:43:46.612: INFO: Pod "dns-test-e829959f-2898-4508-ae41-2759874e00ad": Phase="Running", Reason="", readiness=true. Elapsed: 4.004946753s
    Jan 30 12:43:46.612: INFO: Pod "dns-test-e829959f-2898-4508-ae41-2759874e00ad" satisfied condition "running"
    STEP: retrieving the pod 01/30/23 12:43:46.612
    STEP: looking for the results for each expected name from probers 01/30/23 12:43:46.614
    Jan 30 12:43:46.616: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-9168.svc.cluster.local from pod dns-9168/dns-test-e829959f-2898-4508-ae41-2759874e00ad: the server could not find the requested resource (get pods dns-test-e829959f-2898-4508-ae41-2759874e00ad)
    Jan 30 12:43:46.618: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9168.svc.cluster.local from pod dns-9168/dns-test-e829959f-2898-4508-ae41-2759874e00ad: the server could not find the requested resource (get pods dns-test-e829959f-2898-4508-ae41-2759874e00ad)
    Jan 30 12:43:46.620: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-9168.svc.cluster.local from pod dns-9168/dns-test-e829959f-2898-4508-ae41-2759874e00ad: the server could not find the requested resource (get pods dns-test-e829959f-2898-4508-ae41-2759874e00ad)
    Jan 30 12:43:46.624: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-9168.svc.cluster.local from pod dns-9168/dns-test-e829959f-2898-4508-ae41-2759874e00ad: the server could not find the requested resource (get pods dns-test-e829959f-2898-4508-ae41-2759874e00ad)
    Jan 30 12:43:46.625: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-9168.svc.cluster.local from pod dns-9168/dns-test-e829959f-2898-4508-ae41-2759874e00ad: the server could not find the requested resource (get pods dns-test-e829959f-2898-4508-ae41-2759874e00ad)
    Jan 30 12:43:46.627: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-9168.svc.cluster.local from pod dns-9168/dns-test-e829959f-2898-4508-ae41-2759874e00ad: the server could not find the requested resource (get pods dns-test-e829959f-2898-4508-ae41-2759874e00ad)
    Jan 30 12:43:46.629: INFO: Unable to read jessie_udp@dns-test-service-2.dns-9168.svc.cluster.local from pod dns-9168/dns-test-e829959f-2898-4508-ae41-2759874e00ad: the server could not find the requested resource (get pods dns-test-e829959f-2898-4508-ae41-2759874e00ad)
    Jan 30 12:43:46.631: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-9168.svc.cluster.local from pod dns-9168/dns-test-e829959f-2898-4508-ae41-2759874e00ad: the server could not find the requested resource (get pods dns-test-e829959f-2898-4508-ae41-2759874e00ad)
    Jan 30 12:43:46.631: INFO: Lookups using dns-9168/dns-test-e829959f-2898-4508-ae41-2759874e00ad failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-9168.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9168.svc.cluster.local wheezy_udp@dns-test-service-2.dns-9168.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-9168.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-9168.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-9168.svc.cluster.local jessie_udp@dns-test-service-2.dns-9168.svc.cluster.local jessie_tcp@dns-test-service-2.dns-9168.svc.cluster.local]

    Jan 30 12:43:51.649: INFO: DNS probes using dns-9168/dns-test-e829959f-2898-4508-ae41-2759874e00ad succeeded

    STEP: deleting the pod 01/30/23 12:43:51.649
    STEP: deleting the test headless service 01/30/23 12:43:51.656
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Jan 30 12:43:51.660: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-9168" for this suite. 01/30/23 12:43:51.662
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run
  should check if kubectl can dry-run update Pods [Conformance]
  test/e2e/kubectl/kubectl.go:962
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 12:43:51.665
Jan 30 12:43:51.666: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename kubectl 01/30/23 12:43:51.666
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:43:51.672
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:43:51.673
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if kubectl can dry-run update Pods [Conformance]
  test/e2e/kubectl/kubectl.go:962
STEP: running the image harbor.cloud.netease.com/qzprod-k8s/e2e/httpd:2.4.38-4 01/30/23 12:43:51.675
Jan 30 12:43:51.676: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=kubectl-552 run e2e-test-httpd-pod --image=harbor.cloud.netease.com/qzprod-k8s/e2e/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Jan 30 12:43:51.740: INFO: stderr: ""
Jan 30 12:43:51.740: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: replace the image in the pod with server-side dry-run 01/30/23 12:43:51.74
Jan 30 12:43:51.741: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=kubectl-552 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "harbor.cloud.netease.com/qzprod-k8s/e2e/busybox:1.29-4"}]}} --dry-run=server'
Jan 30 12:43:51.909: INFO: stderr: ""
Jan 30 12:43:51.909: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image harbor.cloud.netease.com/qzprod-k8s/e2e/httpd:2.4.38-4 01/30/23 12:43:51.909
Jan 30 12:43:51.911: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=kubectl-552 delete pods e2e-test-httpd-pod'
Jan 30 12:43:54.118: INFO: stderr: ""
Jan 30 12:43:54.118: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jan 30 12:43:54.118: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-552" for this suite. 01/30/23 12:43:54.121
------------------------------
• [2.458 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl server-side dry-run
  test/e2e/kubectl/kubectl.go:956
    should check if kubectl can dry-run update Pods [Conformance]
    test/e2e/kubectl/kubectl.go:962

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 12:43:51.665
    Jan 30 12:43:51.666: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename kubectl 01/30/23 12:43:51.666
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:43:51.672
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:43:51.673
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if kubectl can dry-run update Pods [Conformance]
      test/e2e/kubectl/kubectl.go:962
    STEP: running the image harbor.cloud.netease.com/qzprod-k8s/e2e/httpd:2.4.38-4 01/30/23 12:43:51.675
    Jan 30 12:43:51.676: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=kubectl-552 run e2e-test-httpd-pod --image=harbor.cloud.netease.com/qzprod-k8s/e2e/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
    Jan 30 12:43:51.740: INFO: stderr: ""
    Jan 30 12:43:51.740: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: replace the image in the pod with server-side dry-run 01/30/23 12:43:51.74
    Jan 30 12:43:51.741: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=kubectl-552 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "harbor.cloud.netease.com/qzprod-k8s/e2e/busybox:1.29-4"}]}} --dry-run=server'
    Jan 30 12:43:51.909: INFO: stderr: ""
    Jan 30 12:43:51.909: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
    STEP: verifying the pod e2e-test-httpd-pod has the right image harbor.cloud.netease.com/qzprod-k8s/e2e/httpd:2.4.38-4 01/30/23 12:43:51.909
    Jan 30 12:43:51.911: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=kubectl-552 delete pods e2e-test-httpd-pod'
    Jan 30 12:43:54.118: INFO: stderr: ""
    Jan 30 12:43:54.118: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jan 30 12:43:54.118: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-552" for this suite. 01/30/23 12:43:54.121
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:235
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 12:43:54.124
Jan 30 12:43:54.124: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename downward-api 01/30/23 12:43:54.125
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:43:54.131
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:43:54.133
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:235
STEP: Creating a pod to test downward API volume plugin 01/30/23 12:43:54.135
Jan 30 12:43:54.140: INFO: Waiting up to 5m0s for pod "downwardapi-volume-28c38046-2b94-4fb9-bc15-351f4c3c3772" in namespace "downward-api-6973" to be "Succeeded or Failed"
Jan 30 12:43:54.141: INFO: Pod "downwardapi-volume-28c38046-2b94-4fb9-bc15-351f4c3c3772": Phase="Pending", Reason="", readiness=false. Elapsed: 1.551404ms
Jan 30 12:43:56.144: INFO: Pod "downwardapi-volume-28c38046-2b94-4fb9-bc15-351f4c3c3772": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004455587s
Jan 30 12:43:58.144: INFO: Pod "downwardapi-volume-28c38046-2b94-4fb9-bc15-351f4c3c3772": Phase="Pending", Reason="", readiness=false. Elapsed: 4.004373028s
Jan 30 12:44:00.145: INFO: Pod "downwardapi-volume-28c38046-2b94-4fb9-bc15-351f4c3c3772": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.005368506s
STEP: Saw pod success 01/30/23 12:44:00.145
Jan 30 12:44:00.145: INFO: Pod "downwardapi-volume-28c38046-2b94-4fb9-bc15-351f4c3c3772" satisfied condition "Succeeded or Failed"
Jan 30 12:44:00.147: INFO: Trying to get logs from node pubt2-nks-for-dev1.dg.163.org pod downwardapi-volume-28c38046-2b94-4fb9-bc15-351f4c3c3772 container client-container: <nil>
STEP: delete the pod 01/30/23 12:44:00.152
Jan 30 12:44:00.156: INFO: Waiting for pod downwardapi-volume-28c38046-2b94-4fb9-bc15-351f4c3c3772 to disappear
Jan 30 12:44:00.158: INFO: Pod downwardapi-volume-28c38046-2b94-4fb9-bc15-351f4c3c3772 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Jan 30 12:44:00.158: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-6973" for this suite. 01/30/23 12:44:00.16
------------------------------
• [SLOW TEST] [6.038 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:235

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 12:43:54.124
    Jan 30 12:43:54.124: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename downward-api 01/30/23 12:43:54.125
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:43:54.131
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:43:54.133
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide container's memory request [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:235
    STEP: Creating a pod to test downward API volume plugin 01/30/23 12:43:54.135
    Jan 30 12:43:54.140: INFO: Waiting up to 5m0s for pod "downwardapi-volume-28c38046-2b94-4fb9-bc15-351f4c3c3772" in namespace "downward-api-6973" to be "Succeeded or Failed"
    Jan 30 12:43:54.141: INFO: Pod "downwardapi-volume-28c38046-2b94-4fb9-bc15-351f4c3c3772": Phase="Pending", Reason="", readiness=false. Elapsed: 1.551404ms
    Jan 30 12:43:56.144: INFO: Pod "downwardapi-volume-28c38046-2b94-4fb9-bc15-351f4c3c3772": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004455587s
    Jan 30 12:43:58.144: INFO: Pod "downwardapi-volume-28c38046-2b94-4fb9-bc15-351f4c3c3772": Phase="Pending", Reason="", readiness=false. Elapsed: 4.004373028s
    Jan 30 12:44:00.145: INFO: Pod "downwardapi-volume-28c38046-2b94-4fb9-bc15-351f4c3c3772": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.005368506s
    STEP: Saw pod success 01/30/23 12:44:00.145
    Jan 30 12:44:00.145: INFO: Pod "downwardapi-volume-28c38046-2b94-4fb9-bc15-351f4c3c3772" satisfied condition "Succeeded or Failed"
    Jan 30 12:44:00.147: INFO: Trying to get logs from node pubt2-nks-for-dev1.dg.163.org pod downwardapi-volume-28c38046-2b94-4fb9-bc15-351f4c3c3772 container client-container: <nil>
    STEP: delete the pod 01/30/23 12:44:00.152
    Jan 30 12:44:00.156: INFO: Waiting for pod downwardapi-volume-28c38046-2b94-4fb9-bc15-351f4c3c3772 to disappear
    Jan 30 12:44:00.158: INFO: Pod downwardapi-volume-28c38046-2b94-4fb9-bc15-351f4c3c3772 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Jan 30 12:44:00.158: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-6973" for this suite. 01/30/23 12:44:00.16
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  test/e2e/kubectl/kubectl.go:1276
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 12:44:00.163
Jan 30 12:44:00.163: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename kubectl 01/30/23 12:44:00.164
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:44:00.17
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:44:00.171
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  test/e2e/kubectl/kubectl.go:1276
Jan 30 12:44:00.174: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=kubectl-214 create -f -'
Jan 30 12:44:00.337: INFO: stderr: ""
Jan 30 12:44:00.337: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
Jan 30 12:44:00.337: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=kubectl-214 create -f -'
Jan 30 12:44:00.507: INFO: stderr: ""
Jan 30 12:44:00.507: INFO: stdout: "service/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 01/30/23 12:44:00.507
Jan 30 12:44:01.511: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 30 12:44:01.511: INFO: Found 0 / 1
Jan 30 12:44:02.511: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 30 12:44:02.511: INFO: Found 1 / 1
Jan 30 12:44:02.511: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jan 30 12:44:02.513: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 30 12:44:02.513: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jan 30 12:44:02.513: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=kubectl-214 describe pod agnhost-primary-47smg'
Jan 30 12:44:02.578: INFO: stderr: ""
Jan 30 12:44:02.578: INFO: stdout: "Name:             agnhost-primary-47smg\nNamespace:        kubectl-214\nPriority:         0\nService Account:  default\nNode:             pubt2-nks-for-dev1.dg.163.org/10.182.0.82\nStart Time:       Mon, 30 Jan 2023 12:44:00 +0000\nLabels:           app=agnhost\n                  role=primary\nAnnotations:      cni.projectcalico.org/podIP: 10.178.151.191/32\n                  cni.projectcalico.org/podIPs: 10.178.151.191/32\nStatus:           Running\nIP:               10.178.151.191\nIPs:\n  IP:           10.178.151.191\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   docker://e0b0038e621a06745fe2b1fbd98f9f5b34ac5a3f7356e079ae9f1ae70ed01bd1\n    Image:          harbor.cloud.netease.com/qzprod-k8s/e2e/agnhost:2.43\n    Image ID:       docker-pullable://harbor.cloud.netease.com/qzprod-k8s/e2e/agnhost@sha256:c338e9c1470d7e0e40b21e2fe191b99c1855ce0a6c265a2eacf70f7ed6a979f6\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Mon, 30 Jan 2023 12:44:02 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-hdz4t (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-hdz4t:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  2s    default-scheduler  Successfully assigned kubectl-214/agnhost-primary-47smg to pubt2-nks-for-dev1.dg.163.org\n  Normal  Pulled     1s    kubelet            Container image \"harbor.cloud.netease.com/qzprod-k8s/e2e/agnhost:2.43\" already present on machine\n  Normal  Created    1s    kubelet            Created container agnhost-primary\n  Normal  Started    0s    kubelet            Started container agnhost-primary\n"
Jan 30 12:44:02.578: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=kubectl-214 describe rc agnhost-primary'
Jan 30 12:44:02.643: INFO: stderr: ""
Jan 30 12:44:02.643: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-214\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        harbor.cloud.netease.com/qzprod-k8s/e2e/agnhost:2.43\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  2s    replication-controller  Created pod: agnhost-primary-47smg\n"
Jan 30 12:44:02.643: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=kubectl-214 describe service agnhost-primary'
Jan 30 12:44:02.706: INFO: stderr: ""
Jan 30 12:44:02.706: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-214\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                10.178.92.133\nIPs:               10.178.92.133\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         10.178.151.191:6379\nSession Affinity:  None\nEvents:            <none>\n"
Jan 30 12:44:02.708: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=kubectl-214 describe node pubt2-nks-for-dev1.dg.163.org'
Jan 30 12:44:02.797: INFO: stderr: ""
Jan 30 12:44:02.797: INFO: stdout: "Name:               pubt2-nks-for-dev1.dg.163.org\nRoles:              node\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=pubt2-nks-for-dev1.dg.163.org\n                    kubernetes.io/os=linux\n                    network.netease.com/zone=az\n                    node-role.kubernetes.io/node=\n                    node.kubernetes.io/instance-type=bare-metal\n                    node.netease.com/phase=checking\n                    topology.kubernetes.io/zone=az\nAnnotations:        node.alpha.kubernetes.io/ttl: 0\n                    projectcalico.org/IPv4Address: 10.182.0.82/22\n                    projectcalico.org/IPv4IPIPTunnelAddr: 10.178.151.0\n                    projectcalico.org/RouteReflectorClusterID: 224.0.0.1\n                    projectcalico.org/labels: {\"calico-peer-role/route-reflector\":\"true\"}\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Mon, 30 Jan 2023 11:02:16 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  pubt2-nks-for-dev1.dg.163.org\n  AcquireTime:     <unset>\n  RenewTime:       Mon, 30 Jan 2023 12:44:00 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Mon, 30 Jan 2023 11:14:56 +0000   Mon, 30 Jan 2023 11:14:56 +0000   CalicoIsUp                   Calico is running on this node\n  MemoryPressure       False   Mon, 30 Jan 2023 12:44:01 +0000   Mon, 30 Jan 2023 11:02:15 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Mon, 30 Jan 2023 12:44:01 +0000   Mon, 30 Jan 2023 11:02:15 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Mon, 30 Jan 2023 12:44:01 +0000   Mon, 30 Jan 2023 11:02:15 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Mon, 30 Jan 2023 12:44:01 +0000   Mon, 30 Jan 2023 11:02:36 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:  10.182.0.82\n  Hostname:    pubt2-nks-for-dev1.dg.163.org\nCapacity:\n  cpu:                    56\n  ephemeral-storage:      1135634272Ki\n  example.com/fakecpu:    1k\n  hugepages-1Gi:          0\n  hugepages-2Mi:          0\n  memory:                 264018468Ki\n  pods:                   110\n  scheduling.k8s.io/foo:  5\nAllocatable:\n  cpu:                    55\n  ephemeral-storage:      1098934112Ki\n  example.com/fakecpu:    1k\n  hugepages-1Gi:          0\n  hugepages-2Mi:          0\n  memory:                 256678436Ki\n  pods:                   110\n  scheduling.k8s.io/foo:  5\nSystem Info:\n  Machine ID:                 28f8da91f34c42c2a7a300f30e54be9d\n  System UUID:                f10462ce-c548-11e7-a291-71dfaea11504\n  Boot ID:                    a2f7609e-e901-4735-86c1-be3623cc5e6d\n  Kernel Version:             4.19.0-9-amd64\n  OS Image:                   Debian GNU/Linux 10 (buster)\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  docker://20.10.17-nks.2\n  Kubelet Version:            v1.26.1-nks.1\n  Kube-Proxy Version:         v1.26.1-nks.1\nPodCIDR:                      10.178.129.0/24\nPodCIDRs:                     10.178.129.0/24\nNon-terminated Pods:          (8 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  cronjob-8534                replace-27918042-klm47                                     0 (0%)        0 (0%)      0 (0%)           0 (0%)         2m2s\n  kube-system                 calico-node-5xgvr                                          250m (0%)     0 (0%)      0 (0%)           0 (0%)         89m\n  kube-system                 cleanlog-w9znh                                             100m (0%)     100m (0%)   40Mi (0%)        40Mi (0%)      101m\n  kube-system                 coredns-5bf7dfc67-4chrr                                    100m (0%)     0 (0%)      70Mi (0%)        170Mi (0%)     70m\n  kube-system                 kube-proxy-fvt68                                           200m (0%)     0 (0%)      200Mi (0%)       0 (0%)         101m\n  kube-system                 node-local-dns-8jh9v                                       25m (0%)      0 (0%)      5Mi (0%)         0 (0%)         101m\n  kubectl-214                 agnhost-primary-47smg                                      0 (0%)        0 (0%)      0 (0%)           0 (0%)         2s\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-6d9e6fe3a8a74fcc-st9q5    0 (0%)        0 (0%)      0 (0%)           0 (0%)         86m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource               Requests    Limits\n  --------               --------    ------\n  cpu                    675m (1%)   100m (0%)\n  memory                 315Mi (0%)  210Mi (0%)\n  ephemeral-storage      0 (0%)      0 (0%)\n  hugepages-1Gi          0 (0%)      0 (0%)\n  hugepages-2Mi          0 (0%)      0 (0%)\n  example.com/fakecpu    0           0\n  scheduling.k8s.io/foo  0           0\nEvents:\n  Type     Reason                   Age   From             Message\n  ----     ------                   ----  ----             -------\n  Normal   Starting                 101m  kube-proxy       \n  Normal   Starting                 101m  kubelet          Starting kubelet.\n  Warning  InvalidDiskCapacity      101m  kubelet          invalid capacity 0 on image filesystem\n  Normal   NodeHasSufficientMemory  101m  kubelet          Node pubt2-nks-for-dev1.dg.163.org status is now: NodeHasSufficientMemory\n  Normal   NodeHasNoDiskPressure    101m  kubelet          Node pubt2-nks-for-dev1.dg.163.org status is now: NodeHasNoDiskPressure\n  Normal   NodeHasSufficientPID     101m  kubelet          Node pubt2-nks-for-dev1.dg.163.org status is now: NodeHasSufficientPID\n  Normal   NodeAllocatableEnforced  101m  kubelet          Updated Node Allocatable limit across pods\n  Normal   RegisteredNode           101m  node-controller  Node pubt2-nks-for-dev1.dg.163.org event: Registered Node pubt2-nks-for-dev1.dg.163.org in Controller\n  Normal   NodeReady                101m  kubelet          Node pubt2-nks-for-dev1.dg.163.org status is now: NodeReady\n"
Jan 30 12:44:02.797: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=kubectl-214 describe namespace kubectl-214'
Jan 30 12:44:02.860: INFO: stderr: ""
Jan 30 12:44:02.860: INFO: stdout: "Name:         kubectl-214\nLabels:       e2e-framework=kubectl\n              e2e-run=195808ce-feca-480c-8c47-644dc9db1d74\n              kubernetes.io/metadata.name=kubectl-214\n              pod-security.kubernetes.io/enforce=baseline\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jan 30 12:44:02.860: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-214" for this suite. 01/30/23 12:44:02.862
------------------------------
• [2.701 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl describe
  test/e2e/kubectl/kubectl.go:1270
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    test/e2e/kubectl/kubectl.go:1276

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 12:44:00.163
    Jan 30 12:44:00.163: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename kubectl 01/30/23 12:44:00.164
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:44:00.17
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:44:00.171
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
      test/e2e/kubectl/kubectl.go:1276
    Jan 30 12:44:00.174: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=kubectl-214 create -f -'
    Jan 30 12:44:00.337: INFO: stderr: ""
    Jan 30 12:44:00.337: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    Jan 30 12:44:00.337: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=kubectl-214 create -f -'
    Jan 30 12:44:00.507: INFO: stderr: ""
    Jan 30 12:44:00.507: INFO: stdout: "service/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 01/30/23 12:44:00.507
    Jan 30 12:44:01.511: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan 30 12:44:01.511: INFO: Found 0 / 1
    Jan 30 12:44:02.511: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan 30 12:44:02.511: INFO: Found 1 / 1
    Jan 30 12:44:02.511: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    Jan 30 12:44:02.513: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan 30 12:44:02.513: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Jan 30 12:44:02.513: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=kubectl-214 describe pod agnhost-primary-47smg'
    Jan 30 12:44:02.578: INFO: stderr: ""
    Jan 30 12:44:02.578: INFO: stdout: "Name:             agnhost-primary-47smg\nNamespace:        kubectl-214\nPriority:         0\nService Account:  default\nNode:             pubt2-nks-for-dev1.dg.163.org/10.182.0.82\nStart Time:       Mon, 30 Jan 2023 12:44:00 +0000\nLabels:           app=agnhost\n                  role=primary\nAnnotations:      cni.projectcalico.org/podIP: 10.178.151.191/32\n                  cni.projectcalico.org/podIPs: 10.178.151.191/32\nStatus:           Running\nIP:               10.178.151.191\nIPs:\n  IP:           10.178.151.191\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   docker://e0b0038e621a06745fe2b1fbd98f9f5b34ac5a3f7356e079ae9f1ae70ed01bd1\n    Image:          harbor.cloud.netease.com/qzprod-k8s/e2e/agnhost:2.43\n    Image ID:       docker-pullable://harbor.cloud.netease.com/qzprod-k8s/e2e/agnhost@sha256:c338e9c1470d7e0e40b21e2fe191b99c1855ce0a6c265a2eacf70f7ed6a979f6\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Mon, 30 Jan 2023 12:44:02 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-hdz4t (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-hdz4t:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  2s    default-scheduler  Successfully assigned kubectl-214/agnhost-primary-47smg to pubt2-nks-for-dev1.dg.163.org\n  Normal  Pulled     1s    kubelet            Container image \"harbor.cloud.netease.com/qzprod-k8s/e2e/agnhost:2.43\" already present on machine\n  Normal  Created    1s    kubelet            Created container agnhost-primary\n  Normal  Started    0s    kubelet            Started container agnhost-primary\n"
    Jan 30 12:44:02.578: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=kubectl-214 describe rc agnhost-primary'
    Jan 30 12:44:02.643: INFO: stderr: ""
    Jan 30 12:44:02.643: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-214\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        harbor.cloud.netease.com/qzprod-k8s/e2e/agnhost:2.43\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  2s    replication-controller  Created pod: agnhost-primary-47smg\n"
    Jan 30 12:44:02.643: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=kubectl-214 describe service agnhost-primary'
    Jan 30 12:44:02.706: INFO: stderr: ""
    Jan 30 12:44:02.706: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-214\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                10.178.92.133\nIPs:               10.178.92.133\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         10.178.151.191:6379\nSession Affinity:  None\nEvents:            <none>\n"
    Jan 30 12:44:02.708: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=kubectl-214 describe node pubt2-nks-for-dev1.dg.163.org'
    Jan 30 12:44:02.797: INFO: stderr: ""
    Jan 30 12:44:02.797: INFO: stdout: "Name:               pubt2-nks-for-dev1.dg.163.org\nRoles:              node\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=pubt2-nks-for-dev1.dg.163.org\n                    kubernetes.io/os=linux\n                    network.netease.com/zone=az\n                    node-role.kubernetes.io/node=\n                    node.kubernetes.io/instance-type=bare-metal\n                    node.netease.com/phase=checking\n                    topology.kubernetes.io/zone=az\nAnnotations:        node.alpha.kubernetes.io/ttl: 0\n                    projectcalico.org/IPv4Address: 10.182.0.82/22\n                    projectcalico.org/IPv4IPIPTunnelAddr: 10.178.151.0\n                    projectcalico.org/RouteReflectorClusterID: 224.0.0.1\n                    projectcalico.org/labels: {\"calico-peer-role/route-reflector\":\"true\"}\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Mon, 30 Jan 2023 11:02:16 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  pubt2-nks-for-dev1.dg.163.org\n  AcquireTime:     <unset>\n  RenewTime:       Mon, 30 Jan 2023 12:44:00 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Mon, 30 Jan 2023 11:14:56 +0000   Mon, 30 Jan 2023 11:14:56 +0000   CalicoIsUp                   Calico is running on this node\n  MemoryPressure       False   Mon, 30 Jan 2023 12:44:01 +0000   Mon, 30 Jan 2023 11:02:15 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Mon, 30 Jan 2023 12:44:01 +0000   Mon, 30 Jan 2023 11:02:15 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Mon, 30 Jan 2023 12:44:01 +0000   Mon, 30 Jan 2023 11:02:15 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Mon, 30 Jan 2023 12:44:01 +0000   Mon, 30 Jan 2023 11:02:36 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:  10.182.0.82\n  Hostname:    pubt2-nks-for-dev1.dg.163.org\nCapacity:\n  cpu:                    56\n  ephemeral-storage:      1135634272Ki\n  example.com/fakecpu:    1k\n  hugepages-1Gi:          0\n  hugepages-2Mi:          0\n  memory:                 264018468Ki\n  pods:                   110\n  scheduling.k8s.io/foo:  5\nAllocatable:\n  cpu:                    55\n  ephemeral-storage:      1098934112Ki\n  example.com/fakecpu:    1k\n  hugepages-1Gi:          0\n  hugepages-2Mi:          0\n  memory:                 256678436Ki\n  pods:                   110\n  scheduling.k8s.io/foo:  5\nSystem Info:\n  Machine ID:                 28f8da91f34c42c2a7a300f30e54be9d\n  System UUID:                f10462ce-c548-11e7-a291-71dfaea11504\n  Boot ID:                    a2f7609e-e901-4735-86c1-be3623cc5e6d\n  Kernel Version:             4.19.0-9-amd64\n  OS Image:                   Debian GNU/Linux 10 (buster)\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  docker://20.10.17-nks.2\n  Kubelet Version:            v1.26.1-nks.1\n  Kube-Proxy Version:         v1.26.1-nks.1\nPodCIDR:                      10.178.129.0/24\nPodCIDRs:                     10.178.129.0/24\nNon-terminated Pods:          (8 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  cronjob-8534                replace-27918042-klm47                                     0 (0%)        0 (0%)      0 (0%)           0 (0%)         2m2s\n  kube-system                 calico-node-5xgvr                                          250m (0%)     0 (0%)      0 (0%)           0 (0%)         89m\n  kube-system                 cleanlog-w9znh                                             100m (0%)     100m (0%)   40Mi (0%)        40Mi (0%)      101m\n  kube-system                 coredns-5bf7dfc67-4chrr                                    100m (0%)     0 (0%)      70Mi (0%)        170Mi (0%)     70m\n  kube-system                 kube-proxy-fvt68                                           200m (0%)     0 (0%)      200Mi (0%)       0 (0%)         101m\n  kube-system                 node-local-dns-8jh9v                                       25m (0%)      0 (0%)      5Mi (0%)         0 (0%)         101m\n  kubectl-214                 agnhost-primary-47smg                                      0 (0%)        0 (0%)      0 (0%)           0 (0%)         2s\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-6d9e6fe3a8a74fcc-st9q5    0 (0%)        0 (0%)      0 (0%)           0 (0%)         86m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource               Requests    Limits\n  --------               --------    ------\n  cpu                    675m (1%)   100m (0%)\n  memory                 315Mi (0%)  210Mi (0%)\n  ephemeral-storage      0 (0%)      0 (0%)\n  hugepages-1Gi          0 (0%)      0 (0%)\n  hugepages-2Mi          0 (0%)      0 (0%)\n  example.com/fakecpu    0           0\n  scheduling.k8s.io/foo  0           0\nEvents:\n  Type     Reason                   Age   From             Message\n  ----     ------                   ----  ----             -------\n  Normal   Starting                 101m  kube-proxy       \n  Normal   Starting                 101m  kubelet          Starting kubelet.\n  Warning  InvalidDiskCapacity      101m  kubelet          invalid capacity 0 on image filesystem\n  Normal   NodeHasSufficientMemory  101m  kubelet          Node pubt2-nks-for-dev1.dg.163.org status is now: NodeHasSufficientMemory\n  Normal   NodeHasNoDiskPressure    101m  kubelet          Node pubt2-nks-for-dev1.dg.163.org status is now: NodeHasNoDiskPressure\n  Normal   NodeHasSufficientPID     101m  kubelet          Node pubt2-nks-for-dev1.dg.163.org status is now: NodeHasSufficientPID\n  Normal   NodeAllocatableEnforced  101m  kubelet          Updated Node Allocatable limit across pods\n  Normal   RegisteredNode           101m  node-controller  Node pubt2-nks-for-dev1.dg.163.org event: Registered Node pubt2-nks-for-dev1.dg.163.org in Controller\n  Normal   NodeReady                101m  kubelet          Node pubt2-nks-for-dev1.dg.163.org status is now: NodeReady\n"
    Jan 30 12:44:02.797: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=kubectl-214 describe namespace kubectl-214'
    Jan 30 12:44:02.860: INFO: stderr: ""
    Jan 30 12:44:02.860: INFO: stdout: "Name:         kubectl-214\nLabels:       e2e-framework=kubectl\n              e2e-run=195808ce-feca-480c-8c47-644dc9db1d74\n              kubernetes.io/metadata.name=kubectl-214\n              pod-security.kubernetes.io/enforce=baseline\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jan 30 12:44:02.860: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-214" for this suite. 01/30/23 12:44:02.862
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] Downward API volume
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:249
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 12:44:02.865
Jan 30 12:44:02.865: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename downward-api 01/30/23 12:44:02.866
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:44:02.871
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:44:02.874
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:249
STEP: Creating a pod to test downward API volume plugin 01/30/23 12:44:02.876
Jan 30 12:44:02.881: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d0d1fb87-d71b-4453-8907-7d0eefed699a" in namespace "downward-api-6416" to be "Succeeded or Failed"
Jan 30 12:44:02.882: INFO: Pod "downwardapi-volume-d0d1fb87-d71b-4453-8907-7d0eefed699a": Phase="Pending", Reason="", readiness=false. Elapsed: 1.569491ms
Jan 30 12:44:04.885: INFO: Pod "downwardapi-volume-d0d1fb87-d71b-4453-8907-7d0eefed699a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004226882s
Jan 30 12:44:06.886: INFO: Pod "downwardapi-volume-d0d1fb87-d71b-4453-8907-7d0eefed699a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005373615s
STEP: Saw pod success 01/30/23 12:44:06.886
Jan 30 12:44:06.886: INFO: Pod "downwardapi-volume-d0d1fb87-d71b-4453-8907-7d0eefed699a" satisfied condition "Succeeded or Failed"
Jan 30 12:44:06.888: INFO: Trying to get logs from node pubt2-nks-for-dev1.dg.163.org pod downwardapi-volume-d0d1fb87-d71b-4453-8907-7d0eefed699a container client-container: <nil>
STEP: delete the pod 01/30/23 12:44:06.893
Jan 30 12:44:06.897: INFO: Waiting for pod downwardapi-volume-d0d1fb87-d71b-4453-8907-7d0eefed699a to disappear
Jan 30 12:44:06.898: INFO: Pod downwardapi-volume-d0d1fb87-d71b-4453-8907-7d0eefed699a no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Jan 30 12:44:06.898: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-6416" for this suite. 01/30/23 12:44:06.9
------------------------------
• [4.038 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:249

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 12:44:02.865
    Jan 30 12:44:02.865: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename downward-api 01/30/23 12:44:02.866
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:44:02.871
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:44:02.874
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:249
    STEP: Creating a pod to test downward API volume plugin 01/30/23 12:44:02.876
    Jan 30 12:44:02.881: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d0d1fb87-d71b-4453-8907-7d0eefed699a" in namespace "downward-api-6416" to be "Succeeded or Failed"
    Jan 30 12:44:02.882: INFO: Pod "downwardapi-volume-d0d1fb87-d71b-4453-8907-7d0eefed699a": Phase="Pending", Reason="", readiness=false. Elapsed: 1.569491ms
    Jan 30 12:44:04.885: INFO: Pod "downwardapi-volume-d0d1fb87-d71b-4453-8907-7d0eefed699a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004226882s
    Jan 30 12:44:06.886: INFO: Pod "downwardapi-volume-d0d1fb87-d71b-4453-8907-7d0eefed699a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005373615s
    STEP: Saw pod success 01/30/23 12:44:06.886
    Jan 30 12:44:06.886: INFO: Pod "downwardapi-volume-d0d1fb87-d71b-4453-8907-7d0eefed699a" satisfied condition "Succeeded or Failed"
    Jan 30 12:44:06.888: INFO: Trying to get logs from node pubt2-nks-for-dev1.dg.163.org pod downwardapi-volume-d0d1fb87-d71b-4453-8907-7d0eefed699a container client-container: <nil>
    STEP: delete the pod 01/30/23 12:44:06.893
    Jan 30 12:44:06.897: INFO: Waiting for pod downwardapi-volume-d0d1fb87-d71b-4453-8907-7d0eefed699a to disappear
    Jan 30 12:44:06.898: INFO: Pod downwardapi-volume-d0d1fb87-d71b-4453-8907-7d0eefed699a no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Jan 30 12:44:06.898: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-6416" for this suite. 01/30/23 12:44:06.9
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-apps] ControllerRevision [Serial]
  should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 12:44:06.903
Jan 30 12:44:06.903: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename controllerrevisions 01/30/23 12:44:06.904
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:44:06.91
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:44:06.911
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/apps/controller_revision.go:93
[It] should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124
STEP: Creating DaemonSet "e2e-9w6ln-daemon-set" 01/30/23 12:44:06.921
STEP: Check that daemon pods launch on every node of the cluster. 01/30/23 12:44:06.924
Jan 30 12:44:06.927: INFO: Number of nodes with available pods controlled by daemonset e2e-9w6ln-daemon-set: 0
Jan 30 12:44:06.927: INFO: Node pubt2-nks-for-dev1.dg.163.org is running 0 daemon pod, expected 1
Jan 30 12:44:07.932: INFO: Number of nodes with available pods controlled by daemonset e2e-9w6ln-daemon-set: 0
Jan 30 12:44:07.932: INFO: Node pubt2-nks-for-dev1.dg.163.org is running 0 daemon pod, expected 1
Jan 30 12:44:08.931: INFO: Number of nodes with available pods controlled by daemonset e2e-9w6ln-daemon-set: 1
Jan 30 12:44:08.931: INFO: Node pubt2-nks-for-dev3.dg.163.org is running 0 daemon pod, expected 1
Jan 30 12:44:09.932: INFO: Number of nodes with available pods controlled by daemonset e2e-9w6ln-daemon-set: 2
Jan 30 12:44:09.932: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset e2e-9w6ln-daemon-set
STEP: Confirm DaemonSet "e2e-9w6ln-daemon-set" successfully created with "daemonset-name=e2e-9w6ln-daemon-set" label 01/30/23 12:44:09.934
STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-9w6ln-daemon-set" 01/30/23 12:44:09.938
Jan 30 12:44:09.940: INFO: Located ControllerRevision: "e2e-9w6ln-daemon-set-8556c679df"
STEP: Patching ControllerRevision "e2e-9w6ln-daemon-set-8556c679df" 01/30/23 12:44:09.941
Jan 30 12:44:09.946: INFO: e2e-9w6ln-daemon-set-8556c679df has been patched
STEP: Create a new ControllerRevision 01/30/23 12:44:09.946
Jan 30 12:44:09.948: INFO: Created ControllerRevision: e2e-9w6ln-daemon-set-5ff8d54489
STEP: Confirm that there are two ControllerRevisions 01/30/23 12:44:09.948
Jan 30 12:44:09.948: INFO: Requesting list of ControllerRevisions to confirm quantity
Jan 30 12:44:09.950: INFO: Found 2 ControllerRevisions
STEP: Deleting ControllerRevision "e2e-9w6ln-daemon-set-8556c679df" 01/30/23 12:44:09.95
STEP: Confirm that there is only one ControllerRevision 01/30/23 12:44:09.952
Jan 30 12:44:09.952: INFO: Requesting list of ControllerRevisions to confirm quantity
Jan 30 12:44:09.954: INFO: Found 1 ControllerRevisions
STEP: Updating ControllerRevision "e2e-9w6ln-daemon-set-5ff8d54489" 01/30/23 12:44:09.955
Jan 30 12:44:09.960: INFO: e2e-9w6ln-daemon-set-5ff8d54489 has been updated
STEP: Generate another ControllerRevision by patching the Daemonset 01/30/23 12:44:09.96
W0130 12:44:09.964366      23 warnings.go:70] unknown field "updateStrategy"
STEP: Confirm that there are two ControllerRevisions 01/30/23 12:44:09.964
Jan 30 12:44:09.964: INFO: Requesting list of ControllerRevisions to confirm quantity
Jan 30 12:44:10.967: INFO: Requesting list of ControllerRevisions to confirm quantity
Jan 30 12:44:10.969: INFO: Found 2 ControllerRevisions
STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-9w6ln-daemon-set-5ff8d54489=updated" 01/30/23 12:44:10.969
STEP: Confirm that there is only one ControllerRevision 01/30/23 12:44:10.972
Jan 30 12:44:10.972: INFO: Requesting list of ControllerRevisions to confirm quantity
Jan 30 12:44:10.974: INFO: Found 1 ControllerRevisions
Jan 30 12:44:10.975: INFO: ControllerRevision "e2e-9w6ln-daemon-set-66c8968c76" has revision 3
[AfterEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/apps/controller_revision.go:58
STEP: Deleting DaemonSet "e2e-9w6ln-daemon-set" 01/30/23 12:44:10.977
STEP: deleting DaemonSet.extensions e2e-9w6ln-daemon-set in namespace controllerrevisions-695, will wait for the garbage collector to delete the pods 01/30/23 12:44:10.977
Jan 30 12:44:11.031: INFO: Deleting DaemonSet.extensions e2e-9w6ln-daemon-set took: 2.494547ms
Jan 30 12:44:11.132: INFO: Terminating DaemonSet.extensions e2e-9w6ln-daemon-set pods took: 100.910002ms
Jan 30 12:44:12.135: INFO: Number of nodes with available pods controlled by daemonset e2e-9w6ln-daemon-set: 0
Jan 30 12:44:12.135: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset e2e-9w6ln-daemon-set
Jan 30 12:44:12.136: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"39075"},"items":null}

Jan 30 12:44:12.137: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"39075"},"items":null}

[AfterEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/node/init/init.go:32
Jan 30 12:44:12.143: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "controllerrevisions-695" for this suite. 01/30/23 12:44:12.145
------------------------------
• [SLOW TEST] [5.244 seconds]
[sig-apps] ControllerRevision [Serial]
test/e2e/apps/framework.go:23
  should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 12:44:06.903
    Jan 30 12:44:06.903: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename controllerrevisions 01/30/23 12:44:06.904
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:44:06.91
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:44:06.911
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/apps/controller_revision.go:93
    [It] should manage the lifecycle of a ControllerRevision [Conformance]
      test/e2e/apps/controller_revision.go:124
    STEP: Creating DaemonSet "e2e-9w6ln-daemon-set" 01/30/23 12:44:06.921
    STEP: Check that daemon pods launch on every node of the cluster. 01/30/23 12:44:06.924
    Jan 30 12:44:06.927: INFO: Number of nodes with available pods controlled by daemonset e2e-9w6ln-daemon-set: 0
    Jan 30 12:44:06.927: INFO: Node pubt2-nks-for-dev1.dg.163.org is running 0 daemon pod, expected 1
    Jan 30 12:44:07.932: INFO: Number of nodes with available pods controlled by daemonset e2e-9w6ln-daemon-set: 0
    Jan 30 12:44:07.932: INFO: Node pubt2-nks-for-dev1.dg.163.org is running 0 daemon pod, expected 1
    Jan 30 12:44:08.931: INFO: Number of nodes with available pods controlled by daemonset e2e-9w6ln-daemon-set: 1
    Jan 30 12:44:08.931: INFO: Node pubt2-nks-for-dev3.dg.163.org is running 0 daemon pod, expected 1
    Jan 30 12:44:09.932: INFO: Number of nodes with available pods controlled by daemonset e2e-9w6ln-daemon-set: 2
    Jan 30 12:44:09.932: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset e2e-9w6ln-daemon-set
    STEP: Confirm DaemonSet "e2e-9w6ln-daemon-set" successfully created with "daemonset-name=e2e-9w6ln-daemon-set" label 01/30/23 12:44:09.934
    STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-9w6ln-daemon-set" 01/30/23 12:44:09.938
    Jan 30 12:44:09.940: INFO: Located ControllerRevision: "e2e-9w6ln-daemon-set-8556c679df"
    STEP: Patching ControllerRevision "e2e-9w6ln-daemon-set-8556c679df" 01/30/23 12:44:09.941
    Jan 30 12:44:09.946: INFO: e2e-9w6ln-daemon-set-8556c679df has been patched
    STEP: Create a new ControllerRevision 01/30/23 12:44:09.946
    Jan 30 12:44:09.948: INFO: Created ControllerRevision: e2e-9w6ln-daemon-set-5ff8d54489
    STEP: Confirm that there are two ControllerRevisions 01/30/23 12:44:09.948
    Jan 30 12:44:09.948: INFO: Requesting list of ControllerRevisions to confirm quantity
    Jan 30 12:44:09.950: INFO: Found 2 ControllerRevisions
    STEP: Deleting ControllerRevision "e2e-9w6ln-daemon-set-8556c679df" 01/30/23 12:44:09.95
    STEP: Confirm that there is only one ControllerRevision 01/30/23 12:44:09.952
    Jan 30 12:44:09.952: INFO: Requesting list of ControllerRevisions to confirm quantity
    Jan 30 12:44:09.954: INFO: Found 1 ControllerRevisions
    STEP: Updating ControllerRevision "e2e-9w6ln-daemon-set-5ff8d54489" 01/30/23 12:44:09.955
    Jan 30 12:44:09.960: INFO: e2e-9w6ln-daemon-set-5ff8d54489 has been updated
    STEP: Generate another ControllerRevision by patching the Daemonset 01/30/23 12:44:09.96
    W0130 12:44:09.964366      23 warnings.go:70] unknown field "updateStrategy"
    STEP: Confirm that there are two ControllerRevisions 01/30/23 12:44:09.964
    Jan 30 12:44:09.964: INFO: Requesting list of ControllerRevisions to confirm quantity
    Jan 30 12:44:10.967: INFO: Requesting list of ControllerRevisions to confirm quantity
    Jan 30 12:44:10.969: INFO: Found 2 ControllerRevisions
    STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-9w6ln-daemon-set-5ff8d54489=updated" 01/30/23 12:44:10.969
    STEP: Confirm that there is only one ControllerRevision 01/30/23 12:44:10.972
    Jan 30 12:44:10.972: INFO: Requesting list of ControllerRevisions to confirm quantity
    Jan 30 12:44:10.974: INFO: Found 1 ControllerRevisions
    Jan 30 12:44:10.975: INFO: ControllerRevision "e2e-9w6ln-daemon-set-66c8968c76" has revision 3
    [AfterEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/apps/controller_revision.go:58
    STEP: Deleting DaemonSet "e2e-9w6ln-daemon-set" 01/30/23 12:44:10.977
    STEP: deleting DaemonSet.extensions e2e-9w6ln-daemon-set in namespace controllerrevisions-695, will wait for the garbage collector to delete the pods 01/30/23 12:44:10.977
    Jan 30 12:44:11.031: INFO: Deleting DaemonSet.extensions e2e-9w6ln-daemon-set took: 2.494547ms
    Jan 30 12:44:11.132: INFO: Terminating DaemonSet.extensions e2e-9w6ln-daemon-set pods took: 100.910002ms
    Jan 30 12:44:12.135: INFO: Number of nodes with available pods controlled by daemonset e2e-9w6ln-daemon-set: 0
    Jan 30 12:44:12.135: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset e2e-9w6ln-daemon-set
    Jan 30 12:44:12.136: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"39075"},"items":null}

    Jan 30 12:44:12.137: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"39075"},"items":null}

    [AfterEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan 30 12:44:12.143: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "controllerrevisions-695" for this suite. 01/30/23 12:44:12.145
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  listing custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:85
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 12:44:12.148
Jan 30 12:44:12.148: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename custom-resource-definition 01/30/23 12:44:12.149
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:44:12.155
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:44:12.157
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] listing custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:85
Jan 30 12:44:12.159: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 30 12:44:18.298: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-5353" for this suite. 01/30/23 12:44:18.301
------------------------------
• [SLOW TEST] [6.155 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    listing custom resource definition objects works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:85

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 12:44:12.148
    Jan 30 12:44:12.148: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename custom-resource-definition 01/30/23 12:44:12.149
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:44:12.155
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:44:12.157
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] listing custom resource definition objects works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:85
    Jan 30 12:44:12.159: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 30 12:44:18.298: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-5353" for this suite. 01/30/23 12:44:18.301
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:100
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 12:44:18.304
Jan 30 12:44:18.304: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename resourcequota 01/30/23 12:44:18.305
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:44:18.311
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:44:18.313
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:100
STEP: Counting existing ResourceQuota 01/30/23 12:44:18.315
STEP: Creating a ResourceQuota 01/30/23 12:44:23.317
STEP: Ensuring resource quota status is calculated 01/30/23 12:44:23.32
STEP: Creating a Service 01/30/23 12:44:25.323
STEP: Creating a NodePort Service 01/30/23 12:44:25.331
STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota 01/30/23 12:44:25.341
STEP: Ensuring resource quota status captures service creation 01/30/23 12:44:25.348
STEP: Deleting Services 01/30/23 12:44:27.35
STEP: Ensuring resource quota status released usage 01/30/23 12:44:27.36
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Jan 30 12:44:29.363: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-9413" for this suite. 01/30/23 12:44:29.365
------------------------------
• [SLOW TEST] [11.064 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:100

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 12:44:18.304
    Jan 30 12:44:18.304: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename resourcequota 01/30/23 12:44:18.305
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:44:18.311
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:44:18.313
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a service. [Conformance]
      test/e2e/apimachinery/resource_quota.go:100
    STEP: Counting existing ResourceQuota 01/30/23 12:44:18.315
    STEP: Creating a ResourceQuota 01/30/23 12:44:23.317
    STEP: Ensuring resource quota status is calculated 01/30/23 12:44:23.32
    STEP: Creating a Service 01/30/23 12:44:25.323
    STEP: Creating a NodePort Service 01/30/23 12:44:25.331
    STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota 01/30/23 12:44:25.341
    STEP: Ensuring resource quota status captures service creation 01/30/23 12:44:25.348
    STEP: Deleting Services 01/30/23 12:44:27.35
    STEP: Ensuring resource quota status released usage 01/30/23 12:44:27.36
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Jan 30 12:44:29.363: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-9413" for this suite. 01/30/23 12:44:29.365
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:67
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 12:44:29.368
Jan 30 12:44:29.368: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename projected 01/30/23 12:44:29.369
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:44:29.375
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:44:29.377
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:67
STEP: Creating projection with secret that has name projected-secret-test-ca101af7-24b6-4819-86df-de9923edb89f 01/30/23 12:44:29.379
STEP: Creating a pod to test consume secrets 01/30/23 12:44:29.381
Jan 30 12:44:29.386: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-40333319-9647-44fe-8506-dc21e9431b9f" in namespace "projected-8679" to be "Succeeded or Failed"
Jan 30 12:44:29.387: INFO: Pod "pod-projected-secrets-40333319-9647-44fe-8506-dc21e9431b9f": Phase="Pending", Reason="", readiness=false. Elapsed: 1.558542ms
Jan 30 12:44:31.390: INFO: Pod "pod-projected-secrets-40333319-9647-44fe-8506-dc21e9431b9f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00454372s
Jan 30 12:44:33.390: INFO: Pod "pod-projected-secrets-40333319-9647-44fe-8506-dc21e9431b9f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004288897s
STEP: Saw pod success 01/30/23 12:44:33.39
Jan 30 12:44:33.390: INFO: Pod "pod-projected-secrets-40333319-9647-44fe-8506-dc21e9431b9f" satisfied condition "Succeeded or Failed"
Jan 30 12:44:33.392: INFO: Trying to get logs from node pubt2-nks-for-dev1.dg.163.org pod pod-projected-secrets-40333319-9647-44fe-8506-dc21e9431b9f container projected-secret-volume-test: <nil>
STEP: delete the pod 01/30/23 12:44:33.397
Jan 30 12:44:33.401: INFO: Waiting for pod pod-projected-secrets-40333319-9647-44fe-8506-dc21e9431b9f to disappear
Jan 30 12:44:33.403: INFO: Pod pod-projected-secrets-40333319-9647-44fe-8506-dc21e9431b9f no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Jan 30 12:44:33.403: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-8679" for this suite. 01/30/23 12:44:33.405
------------------------------
• [4.039 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 12:44:29.368
    Jan 30 12:44:29.368: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename projected 01/30/23 12:44:29.369
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:44:29.375
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:44:29.377
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:67
    STEP: Creating projection with secret that has name projected-secret-test-ca101af7-24b6-4819-86df-de9923edb89f 01/30/23 12:44:29.379
    STEP: Creating a pod to test consume secrets 01/30/23 12:44:29.381
    Jan 30 12:44:29.386: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-40333319-9647-44fe-8506-dc21e9431b9f" in namespace "projected-8679" to be "Succeeded or Failed"
    Jan 30 12:44:29.387: INFO: Pod "pod-projected-secrets-40333319-9647-44fe-8506-dc21e9431b9f": Phase="Pending", Reason="", readiness=false. Elapsed: 1.558542ms
    Jan 30 12:44:31.390: INFO: Pod "pod-projected-secrets-40333319-9647-44fe-8506-dc21e9431b9f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00454372s
    Jan 30 12:44:33.390: INFO: Pod "pod-projected-secrets-40333319-9647-44fe-8506-dc21e9431b9f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004288897s
    STEP: Saw pod success 01/30/23 12:44:33.39
    Jan 30 12:44:33.390: INFO: Pod "pod-projected-secrets-40333319-9647-44fe-8506-dc21e9431b9f" satisfied condition "Succeeded or Failed"
    Jan 30 12:44:33.392: INFO: Trying to get logs from node pubt2-nks-for-dev1.dg.163.org pod pod-projected-secrets-40333319-9647-44fe-8506-dc21e9431b9f container projected-secret-volume-test: <nil>
    STEP: delete the pod 01/30/23 12:44:33.397
    Jan 30 12:44:33.401: INFO: Waiting for pod pod-projected-secrets-40333319-9647-44fe-8506-dc21e9431b9f to disappear
    Jan 30 12:44:33.403: INFO: Pod pod-projected-secrets-40333319-9647-44fe-8506-dc21e9431b9f no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Jan 30 12:44:33.403: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-8679" for this suite. 01/30/23 12:44:33.405
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:105
[BeforeEach] [sig-network] Networking
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 12:44:33.41
Jan 30 12:44:33.410: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename pod-network-test 01/30/23 12:44:33.411
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:44:33.416
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:44:33.418
[BeforeEach] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:31
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:105
STEP: Performing setup for networking test in namespace pod-network-test-5450 01/30/23 12:44:33.42
STEP: creating a selector 01/30/23 12:44:33.42
STEP: Creating the service pods in kubernetes 01/30/23 12:44:33.42
Jan 30 12:44:33.420: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jan 30 12:44:33.430: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-5450" to be "running and ready"
Jan 30 12:44:33.432: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 1.573425ms
Jan 30 12:44:33.432: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jan 30 12:44:35.435: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.004699207s
Jan 30 12:44:35.435: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 30 12:44:37.436: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.005896081s
Jan 30 12:44:37.436: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 30 12:44:39.434: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.003999943s
Jan 30 12:44:39.434: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 30 12:44:41.435: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.004271886s
Jan 30 12:44:41.435: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 30 12:44:43.435: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.004932654s
Jan 30 12:44:43.435: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 30 12:44:45.436: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.005696669s
Jan 30 12:44:45.436: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 30 12:44:47.436: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.005663687s
Jan 30 12:44:47.436: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 30 12:44:49.435: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.004725474s
Jan 30 12:44:49.435: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 30 12:44:51.436: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.00562421s
Jan 30 12:44:51.436: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 30 12:44:53.434: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.004022115s
Jan 30 12:44:53.434: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 30 12:44:55.436: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.005525221s
Jan 30 12:44:55.436: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Jan 30 12:44:55.436: INFO: Pod "netserver-0" satisfied condition "running and ready"
Jan 30 12:44:55.437: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-5450" to be "running and ready"
Jan 30 12:44:55.439: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 1.629113ms
Jan 30 12:44:55.439: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Jan 30 12:44:55.439: INFO: Pod "netserver-1" satisfied condition "running and ready"
STEP: Creating test pods 01/30/23 12:44:55.441
Jan 30 12:44:55.446: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-5450" to be "running"
Jan 30 12:44:55.447: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 1.595751ms
Jan 30 12:44:57.450: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.004639662s
Jan 30 12:44:57.450: INFO: Pod "test-container-pod" satisfied condition "running"
Jan 30 12:44:57.452: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-5450" to be "running"
Jan 30 12:44:57.454: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 1.58006ms
Jan 30 12:44:57.454: INFO: Pod "host-test-container-pod" satisfied condition "running"
Jan 30 12:44:57.455: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
Jan 30 12:44:57.455: INFO: Going to poll 10.178.151.185 on port 8083 at least 0 times, with a maximum of 34 tries before failing
Jan 30 12:44:57.457: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.178.151.185:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-5450 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 30 12:44:57.457: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
Jan 30 12:44:57.457: INFO: ExecWithOptions: Clientset creation
Jan 30 12:44:57.457: INFO: ExecWithOptions: execute(POST https://10.178.64.1:443/api/v1/namespaces/pod-network-test-5450/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.178.151.185%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jan 30 12:44:57.536: INFO: Found all 1 expected endpoints: [netserver-0]
Jan 30 12:44:57.536: INFO: Going to poll 10.178.197.194 on port 8083 at least 0 times, with a maximum of 34 tries before failing
Jan 30 12:44:57.538: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.178.197.194:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-5450 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 30 12:44:57.538: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
Jan 30 12:44:57.538: INFO: ExecWithOptions: Clientset creation
Jan 30 12:44:57.538: INFO: ExecWithOptions: execute(POST https://10.178.64.1:443/api/v1/namespaces/pod-network-test-5450/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.178.197.194%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jan 30 12:44:57.621: INFO: Found all 1 expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  test/e2e/framework/node/init/init.go:32
Jan 30 12:44:57.621: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Networking
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Networking
  tear down framework | framework.go:193
STEP: Destroying namespace "pod-network-test-5450" for this suite. 01/30/23 12:44:57.624
------------------------------
• [SLOW TEST] [24.217 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:105

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 12:44:33.41
    Jan 30 12:44:33.410: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename pod-network-test 01/30/23 12:44:33.411
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:44:33.416
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:44:33.418
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:31
    [It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:105
    STEP: Performing setup for networking test in namespace pod-network-test-5450 01/30/23 12:44:33.42
    STEP: creating a selector 01/30/23 12:44:33.42
    STEP: Creating the service pods in kubernetes 01/30/23 12:44:33.42
    Jan 30 12:44:33.420: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Jan 30 12:44:33.430: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-5450" to be "running and ready"
    Jan 30 12:44:33.432: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 1.573425ms
    Jan 30 12:44:33.432: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Jan 30 12:44:35.435: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.004699207s
    Jan 30 12:44:35.435: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 30 12:44:37.436: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.005896081s
    Jan 30 12:44:37.436: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 30 12:44:39.434: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.003999943s
    Jan 30 12:44:39.434: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 30 12:44:41.435: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.004271886s
    Jan 30 12:44:41.435: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 30 12:44:43.435: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.004932654s
    Jan 30 12:44:43.435: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 30 12:44:45.436: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.005696669s
    Jan 30 12:44:45.436: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 30 12:44:47.436: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.005663687s
    Jan 30 12:44:47.436: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 30 12:44:49.435: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.004725474s
    Jan 30 12:44:49.435: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 30 12:44:51.436: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.00562421s
    Jan 30 12:44:51.436: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 30 12:44:53.434: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.004022115s
    Jan 30 12:44:53.434: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 30 12:44:55.436: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.005525221s
    Jan 30 12:44:55.436: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Jan 30 12:44:55.436: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Jan 30 12:44:55.437: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-5450" to be "running and ready"
    Jan 30 12:44:55.439: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 1.629113ms
    Jan 30 12:44:55.439: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Jan 30 12:44:55.439: INFO: Pod "netserver-1" satisfied condition "running and ready"
    STEP: Creating test pods 01/30/23 12:44:55.441
    Jan 30 12:44:55.446: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-5450" to be "running"
    Jan 30 12:44:55.447: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 1.595751ms
    Jan 30 12:44:57.450: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.004639662s
    Jan 30 12:44:57.450: INFO: Pod "test-container-pod" satisfied condition "running"
    Jan 30 12:44:57.452: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-5450" to be "running"
    Jan 30 12:44:57.454: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 1.58006ms
    Jan 30 12:44:57.454: INFO: Pod "host-test-container-pod" satisfied condition "running"
    Jan 30 12:44:57.455: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
    Jan 30 12:44:57.455: INFO: Going to poll 10.178.151.185 on port 8083 at least 0 times, with a maximum of 34 tries before failing
    Jan 30 12:44:57.457: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.178.151.185:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-5450 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 30 12:44:57.457: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    Jan 30 12:44:57.457: INFO: ExecWithOptions: Clientset creation
    Jan 30 12:44:57.457: INFO: ExecWithOptions: execute(POST https://10.178.64.1:443/api/v1/namespaces/pod-network-test-5450/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.178.151.185%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Jan 30 12:44:57.536: INFO: Found all 1 expected endpoints: [netserver-0]
    Jan 30 12:44:57.536: INFO: Going to poll 10.178.197.194 on port 8083 at least 0 times, with a maximum of 34 tries before failing
    Jan 30 12:44:57.538: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.178.197.194:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-5450 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 30 12:44:57.538: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    Jan 30 12:44:57.538: INFO: ExecWithOptions: Clientset creation
    Jan 30 12:44:57.538: INFO: ExecWithOptions: execute(POST https://10.178.64.1:443/api/v1/namespaces/pod-network-test-5450/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.178.197.194%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Jan 30 12:44:57.621: INFO: Found all 1 expected endpoints: [netserver-1]
    [AfterEach] [sig-network] Networking
      test/e2e/framework/node/init/init.go:32
    Jan 30 12:44:57.621: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Networking
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Networking
      tear down framework | framework.go:193
    STEP: Destroying namespace "pod-network-test-5450" for this suite. 01/30/23 12:44:57.624
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:102
[BeforeEach] [sig-network] EndpointSlice
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 12:44:57.627
Jan 30 12:44:57.627: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename endpointslice 01/30/23 12:44:57.628
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:44:57.634
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:44:57.636
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:52
[It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:102
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/node/init/init.go:32
Jan 30 12:45:01.657: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSlice
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSlice
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslice-3652" for this suite. 01/30/23 12:45:01.66
------------------------------
• [4.035 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:102

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 12:44:57.627
    Jan 30 12:44:57.627: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename endpointslice 01/30/23 12:44:57.628
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:44:57.634
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:44:57.636
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:52
    [It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
      test/e2e/network/endpointslice.go:102
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/node/init/init.go:32
    Jan 30 12:45:01.657: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslice-3652" for this suite. 01/30/23 12:45:01.66
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] RuntimeClass
  should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 12:45:01.662
Jan 30 12:45:01.662: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename runtimeclass 01/30/23 12:45:01.663
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:45:01.669
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:45:01.671
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It] should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156
STEP: Deleting RuntimeClass runtimeclass-5701-delete-me 01/30/23 12:45:01.675
STEP: Waiting for the RuntimeClass to disappear 01/30/23 12:45:01.677
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Jan 30 12:45:01.683: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-5701" for this suite. 01/30/23 12:45:01.685
------------------------------
• [0.025 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 12:45:01.662
    Jan 30 12:45:01.662: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename runtimeclass 01/30/23 12:45:01.663
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:45:01.669
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:45:01.671
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It] should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:156
    STEP: Deleting RuntimeClass runtimeclass-5701-delete-me 01/30/23 12:45:01.675
    STEP: Waiting for the RuntimeClass to disappear 01/30/23 12:45:01.677
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Jan 30 12:45:01.683: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-5701" for this suite. 01/30/23 12:45:01.685
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:109
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 12:45:01.688
Jan 30 12:45:01.688: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename configmap 01/30/23 12:45:01.689
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:45:01.694
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:45:01.696
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:109
STEP: Creating configMap with name configmap-test-volume-map-0af6182a-5216-41db-873a-176b849e718a 01/30/23 12:45:01.698
STEP: Creating a pod to test consume configMaps 01/30/23 12:45:01.7
Jan 30 12:45:01.704: INFO: Waiting up to 5m0s for pod "pod-configmaps-7b2cf662-558a-4286-962e-7c8f287c96d9" in namespace "configmap-4969" to be "Succeeded or Failed"
Jan 30 12:45:01.706: INFO: Pod "pod-configmaps-7b2cf662-558a-4286-962e-7c8f287c96d9": Phase="Pending", Reason="", readiness=false. Elapsed: 1.51939ms
Jan 30 12:45:03.708: INFO: Pod "pod-configmaps-7b2cf662-558a-4286-962e-7c8f287c96d9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00393888s
Jan 30 12:45:05.709: INFO: Pod "pod-configmaps-7b2cf662-558a-4286-962e-7c8f287c96d9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004917151s
STEP: Saw pod success 01/30/23 12:45:05.709
Jan 30 12:45:05.709: INFO: Pod "pod-configmaps-7b2cf662-558a-4286-962e-7c8f287c96d9" satisfied condition "Succeeded or Failed"
Jan 30 12:45:05.711: INFO: Trying to get logs from node pubt2-nks-for-dev1.dg.163.org pod pod-configmaps-7b2cf662-558a-4286-962e-7c8f287c96d9 container agnhost-container: <nil>
STEP: delete the pod 01/30/23 12:45:05.715
Jan 30 12:45:05.720: INFO: Waiting for pod pod-configmaps-7b2cf662-558a-4286-962e-7c8f287c96d9 to disappear
Jan 30 12:45:05.721: INFO: Pod pod-configmaps-7b2cf662-558a-4286-962e-7c8f287c96d9 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jan 30 12:45:05.721: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-4969" for this suite. 01/30/23 12:45:05.724
------------------------------
• [4.038 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:109

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 12:45:01.688
    Jan 30 12:45:01.688: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename configmap 01/30/23 12:45:01.689
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:45:01.694
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:45:01.696
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:109
    STEP: Creating configMap with name configmap-test-volume-map-0af6182a-5216-41db-873a-176b849e718a 01/30/23 12:45:01.698
    STEP: Creating a pod to test consume configMaps 01/30/23 12:45:01.7
    Jan 30 12:45:01.704: INFO: Waiting up to 5m0s for pod "pod-configmaps-7b2cf662-558a-4286-962e-7c8f287c96d9" in namespace "configmap-4969" to be "Succeeded or Failed"
    Jan 30 12:45:01.706: INFO: Pod "pod-configmaps-7b2cf662-558a-4286-962e-7c8f287c96d9": Phase="Pending", Reason="", readiness=false. Elapsed: 1.51939ms
    Jan 30 12:45:03.708: INFO: Pod "pod-configmaps-7b2cf662-558a-4286-962e-7c8f287c96d9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00393888s
    Jan 30 12:45:05.709: INFO: Pod "pod-configmaps-7b2cf662-558a-4286-962e-7c8f287c96d9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004917151s
    STEP: Saw pod success 01/30/23 12:45:05.709
    Jan 30 12:45:05.709: INFO: Pod "pod-configmaps-7b2cf662-558a-4286-962e-7c8f287c96d9" satisfied condition "Succeeded or Failed"
    Jan 30 12:45:05.711: INFO: Trying to get logs from node pubt2-nks-for-dev1.dg.163.org pod pod-configmaps-7b2cf662-558a-4286-962e-7c8f287c96d9 container agnhost-container: <nil>
    STEP: delete the pod 01/30/23 12:45:05.715
    Jan 30 12:45:05.720: INFO: Waiting for pod pod-configmaps-7b2cf662-558a-4286-962e-7c8f287c96d9 to disappear
    Jan 30 12:45:05.721: INFO: Pod pod-configmaps-7b2cf662-558a-4286-962e-7c8f287c96d9 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jan 30 12:45:05.721: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-4969" for this suite. 01/30/23 12:45:05.724
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected combined
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:44
[BeforeEach] [sig-storage] Projected combined
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 12:45:05.727
Jan 30 12:45:05.727: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename projected 01/30/23 12:45:05.727
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:45:05.733
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:45:05.735
[BeforeEach] [sig-storage] Projected combined
  test/e2e/framework/metrics/init/init.go:31
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:44
STEP: Creating configMap with name configmap-projected-all-test-volume-4ffb940a-24ee-4ad5-b8f7-d67e9cb96b95 01/30/23 12:45:05.739
STEP: Creating secret with name secret-projected-all-test-volume-5bb34f61-e0fa-4ab4-8215-fe8e2da7635b 01/30/23 12:45:05.745
STEP: Creating a pod to test Check all projections for projected volume plugin 01/30/23 12:45:05.75
Jan 30 12:45:05.758: INFO: Waiting up to 5m0s for pod "projected-volume-94b646fa-f77e-4ad4-b5dc-0437167c6dcb" in namespace "projected-5929" to be "Succeeded or Failed"
Jan 30 12:45:05.760: INFO: Pod "projected-volume-94b646fa-f77e-4ad4-b5dc-0437167c6dcb": Phase="Pending", Reason="", readiness=false. Elapsed: 1.74345ms
Jan 30 12:45:07.763: INFO: Pod "projected-volume-94b646fa-f77e-4ad4-b5dc-0437167c6dcb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005347819s
Jan 30 12:45:09.763: INFO: Pod "projected-volume-94b646fa-f77e-4ad4-b5dc-0437167c6dcb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005204021s
STEP: Saw pod success 01/30/23 12:45:09.763
Jan 30 12:45:09.763: INFO: Pod "projected-volume-94b646fa-f77e-4ad4-b5dc-0437167c6dcb" satisfied condition "Succeeded or Failed"
Jan 30 12:45:09.765: INFO: Trying to get logs from node pubt2-nks-for-dev1.dg.163.org pod projected-volume-94b646fa-f77e-4ad4-b5dc-0437167c6dcb container projected-all-volume-test: <nil>
STEP: delete the pod 01/30/23 12:45:09.771
Jan 30 12:45:09.774: INFO: Waiting for pod projected-volume-94b646fa-f77e-4ad4-b5dc-0437167c6dcb to disappear
Jan 30 12:45:09.776: INFO: Pod projected-volume-94b646fa-f77e-4ad4-b5dc-0437167c6dcb no longer exists
[AfterEach] [sig-storage] Projected combined
  test/e2e/framework/node/init/init.go:32
Jan 30 12:45:09.776: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected combined
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected combined
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected combined
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-5929" for this suite. 01/30/23 12:45:09.778
------------------------------
• [4.054 seconds]
[sig-storage] Projected combined
test/e2e/common/storage/framework.go:23
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:44

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected combined
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 12:45:05.727
    Jan 30 12:45:05.727: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename projected 01/30/23 12:45:05.727
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:45:05.733
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:45:05.735
    [BeforeEach] [sig-storage] Projected combined
      test/e2e/framework/metrics/init/init.go:31
    [It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
      test/e2e/common/storage/projected_combined.go:44
    STEP: Creating configMap with name configmap-projected-all-test-volume-4ffb940a-24ee-4ad5-b8f7-d67e9cb96b95 01/30/23 12:45:05.739
    STEP: Creating secret with name secret-projected-all-test-volume-5bb34f61-e0fa-4ab4-8215-fe8e2da7635b 01/30/23 12:45:05.745
    STEP: Creating a pod to test Check all projections for projected volume plugin 01/30/23 12:45:05.75
    Jan 30 12:45:05.758: INFO: Waiting up to 5m0s for pod "projected-volume-94b646fa-f77e-4ad4-b5dc-0437167c6dcb" in namespace "projected-5929" to be "Succeeded or Failed"
    Jan 30 12:45:05.760: INFO: Pod "projected-volume-94b646fa-f77e-4ad4-b5dc-0437167c6dcb": Phase="Pending", Reason="", readiness=false. Elapsed: 1.74345ms
    Jan 30 12:45:07.763: INFO: Pod "projected-volume-94b646fa-f77e-4ad4-b5dc-0437167c6dcb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005347819s
    Jan 30 12:45:09.763: INFO: Pod "projected-volume-94b646fa-f77e-4ad4-b5dc-0437167c6dcb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005204021s
    STEP: Saw pod success 01/30/23 12:45:09.763
    Jan 30 12:45:09.763: INFO: Pod "projected-volume-94b646fa-f77e-4ad4-b5dc-0437167c6dcb" satisfied condition "Succeeded or Failed"
    Jan 30 12:45:09.765: INFO: Trying to get logs from node pubt2-nks-for-dev1.dg.163.org pod projected-volume-94b646fa-f77e-4ad4-b5dc-0437167c6dcb container projected-all-volume-test: <nil>
    STEP: delete the pod 01/30/23 12:45:09.771
    Jan 30 12:45:09.774: INFO: Waiting for pod projected-volume-94b646fa-f77e-4ad4-b5dc-0437167c6dcb to disappear
    Jan 30 12:45:09.776: INFO: Pod projected-volume-94b646fa-f77e-4ad4-b5dc-0437167c6dcb no longer exists
    [AfterEach] [sig-storage] Projected combined
      test/e2e/framework/node/init/init.go:32
    Jan 30 12:45:09.776: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected combined
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected combined
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected combined
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-5929" for this suite. 01/30/23 12:45:09.778
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 12:45:09.781
Jan 30 12:45:09.781: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename cronjob 01/30/23 12:45:09.782
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:45:09.788
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:45:09.789
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319
STEP: Creating a cronjob 01/30/23 12:45:09.791
STEP: creating 01/30/23 12:45:09.792
STEP: getting 01/30/23 12:45:09.794
STEP: listing 01/30/23 12:45:09.796
STEP: watching 01/30/23 12:45:09.798
Jan 30 12:45:09.798: INFO: starting watch
STEP: cluster-wide listing 01/30/23 12:45:09.799
STEP: cluster-wide watching 01/30/23 12:45:09.8
Jan 30 12:45:09.800: INFO: starting watch
STEP: patching 01/30/23 12:45:09.801
STEP: updating 01/30/23 12:45:09.805
Jan 30 12:45:09.810: INFO: waiting for watch events with expected annotations
Jan 30 12:45:09.810: INFO: saw patched and updated annotations
STEP: patching /status 01/30/23 12:45:09.81
STEP: updating /status 01/30/23 12:45:09.813
STEP: get /status 01/30/23 12:45:09.818
STEP: deleting 01/30/23 12:45:09.82
STEP: deleting a collection 01/30/23 12:45:09.826
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Jan 30 12:45:09.830: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-2820" for this suite. 01/30/23 12:45:09.832
------------------------------
• [0.053 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 12:45:09.781
    Jan 30 12:45:09.781: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename cronjob 01/30/23 12:45:09.782
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:45:09.788
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:45:09.789
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should support CronJob API operations [Conformance]
      test/e2e/apps/cronjob.go:319
    STEP: Creating a cronjob 01/30/23 12:45:09.791
    STEP: creating 01/30/23 12:45:09.792
    STEP: getting 01/30/23 12:45:09.794
    STEP: listing 01/30/23 12:45:09.796
    STEP: watching 01/30/23 12:45:09.798
    Jan 30 12:45:09.798: INFO: starting watch
    STEP: cluster-wide listing 01/30/23 12:45:09.799
    STEP: cluster-wide watching 01/30/23 12:45:09.8
    Jan 30 12:45:09.800: INFO: starting watch
    STEP: patching 01/30/23 12:45:09.801
    STEP: updating 01/30/23 12:45:09.805
    Jan 30 12:45:09.810: INFO: waiting for watch events with expected annotations
    Jan 30 12:45:09.810: INFO: saw patched and updated annotations
    STEP: patching /status 01/30/23 12:45:09.81
    STEP: updating /status 01/30/23 12:45:09.813
    STEP: get /status 01/30/23 12:45:09.818
    STEP: deleting 01/30/23 12:45:09.82
    STEP: deleting a collection 01/30/23 12:45:09.826
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Jan 30 12:45:09.830: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-2820" for this suite. 01/30/23 12:45:09.832
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:455
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 12:45:09.834
Jan 30 12:45:09.835: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename taint-multiple-pods 01/30/23 12:45:09.835
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:45:09.84
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:45:09.842
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/node/taints.go:383
Jan 30 12:45:09.844: INFO: Waiting up to 1m0s for all nodes to be ready
Jan 30 12:46:09.860: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:455
Jan 30 12:46:09.861: INFO: Starting informer...
STEP: Starting pods... 01/30/23 12:46:09.861
Jan 30 12:46:10.071: INFO: Pod1 is running on pubt2-nks-for-dev1.dg.163.org. Tainting Node
Jan 30 12:46:10.276: INFO: Waiting up to 5m0s for pod "taint-eviction-b1" in namespace "taint-multiple-pods-5531" to be "running"
Jan 30 12:46:10.278: INFO: Pod "taint-eviction-b1": Phase="Pending", Reason="", readiness=false. Elapsed: 1.664409ms
Jan 30 12:46:12.281: INFO: Pod "taint-eviction-b1": Phase="Running", Reason="", readiness=true. Elapsed: 2.004138929s
Jan 30 12:46:12.281: INFO: Pod "taint-eviction-b1" satisfied condition "running"
Jan 30 12:46:12.281: INFO: Waiting up to 5m0s for pod "taint-eviction-b2" in namespace "taint-multiple-pods-5531" to be "running"
Jan 30 12:46:12.282: INFO: Pod "taint-eviction-b2": Phase="Running", Reason="", readiness=true. Elapsed: 1.702337ms
Jan 30 12:46:12.282: INFO: Pod "taint-eviction-b2" satisfied condition "running"
Jan 30 12:46:12.282: INFO: Pod2 is running on pubt2-nks-for-dev1.dg.163.org. Tainting Node
STEP: Trying to apply a taint on the Node 01/30/23 12:46:12.282
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 01/30/23 12:46:12.293
STEP: Waiting for Pod1 and Pod2 to be deleted 01/30/23 12:46:12.295
Jan 30 12:46:18.239: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
Jan 30 12:46:38.731: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 01/30/23 12:46:38.743
[AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/node/init/init.go:32
Jan 30 12:46:38.745: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "taint-multiple-pods-5531" for this suite. 01/30/23 12:46:38.747
------------------------------
• [SLOW TEST] [88.915 seconds]
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
test/e2e/node/framework.go:23
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:455

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 12:45:09.834
    Jan 30 12:45:09.835: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename taint-multiple-pods 01/30/23 12:45:09.835
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:45:09.84
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:45:09.842
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/node/taints.go:383
    Jan 30 12:45:09.844: INFO: Waiting up to 1m0s for all nodes to be ready
    Jan 30 12:46:09.860: INFO: Waiting for terminating namespaces to be deleted...
    [It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
      test/e2e/node/taints.go:455
    Jan 30 12:46:09.861: INFO: Starting informer...
    STEP: Starting pods... 01/30/23 12:46:09.861
    Jan 30 12:46:10.071: INFO: Pod1 is running on pubt2-nks-for-dev1.dg.163.org. Tainting Node
    Jan 30 12:46:10.276: INFO: Waiting up to 5m0s for pod "taint-eviction-b1" in namespace "taint-multiple-pods-5531" to be "running"
    Jan 30 12:46:10.278: INFO: Pod "taint-eviction-b1": Phase="Pending", Reason="", readiness=false. Elapsed: 1.664409ms
    Jan 30 12:46:12.281: INFO: Pod "taint-eviction-b1": Phase="Running", Reason="", readiness=true. Elapsed: 2.004138929s
    Jan 30 12:46:12.281: INFO: Pod "taint-eviction-b1" satisfied condition "running"
    Jan 30 12:46:12.281: INFO: Waiting up to 5m0s for pod "taint-eviction-b2" in namespace "taint-multiple-pods-5531" to be "running"
    Jan 30 12:46:12.282: INFO: Pod "taint-eviction-b2": Phase="Running", Reason="", readiness=true. Elapsed: 1.702337ms
    Jan 30 12:46:12.282: INFO: Pod "taint-eviction-b2" satisfied condition "running"
    Jan 30 12:46:12.282: INFO: Pod2 is running on pubt2-nks-for-dev1.dg.163.org. Tainting Node
    STEP: Trying to apply a taint on the Node 01/30/23 12:46:12.282
    STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 01/30/23 12:46:12.293
    STEP: Waiting for Pod1 and Pod2 to be deleted 01/30/23 12:46:12.295
    Jan 30 12:46:18.239: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
    Jan 30 12:46:38.731: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
    STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 01/30/23 12:46:38.743
    [AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan 30 12:46:38.745: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "taint-multiple-pods-5531" for this suite. 01/30/23 12:46:38.747
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Pods
  should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:204
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 12:46:38.75
Jan 30 12:46:38.750: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename pods 01/30/23 12:46:38.75
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:46:38.757
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:46:38.759
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:204
STEP: creating pod 01/30/23 12:46:38.761
Jan 30 12:46:38.765: INFO: Waiting up to 5m0s for pod "pod-hostip-e9b28bb1-c852-4641-95e0-84e1edf5fb67" in namespace "pods-1031" to be "running and ready"
Jan 30 12:46:38.766: INFO: Pod "pod-hostip-e9b28bb1-c852-4641-95e0-84e1edf5fb67": Phase="Pending", Reason="", readiness=false. Elapsed: 1.494385ms
Jan 30 12:46:38.766: INFO: The phase of Pod pod-hostip-e9b28bb1-c852-4641-95e0-84e1edf5fb67 is Pending, waiting for it to be Running (with Ready = true)
Jan 30 12:46:40.770: INFO: Pod "pod-hostip-e9b28bb1-c852-4641-95e0-84e1edf5fb67": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004860057s
Jan 30 12:46:40.770: INFO: The phase of Pod pod-hostip-e9b28bb1-c852-4641-95e0-84e1edf5fb67 is Pending, waiting for it to be Running (with Ready = true)
Jan 30 12:46:42.770: INFO: Pod "pod-hostip-e9b28bb1-c852-4641-95e0-84e1edf5fb67": Phase="Running", Reason="", readiness=true. Elapsed: 4.0051355s
Jan 30 12:46:42.770: INFO: The phase of Pod pod-hostip-e9b28bb1-c852-4641-95e0-84e1edf5fb67 is Running (Ready = true)
Jan 30 12:46:42.770: INFO: Pod "pod-hostip-e9b28bb1-c852-4641-95e0-84e1edf5fb67" satisfied condition "running and ready"
Jan 30 12:46:42.773: INFO: Pod pod-hostip-e9b28bb1-c852-4641-95e0-84e1edf5fb67 has hostIP: 10.182.0.82
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Jan 30 12:46:42.773: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-1031" for this suite. 01/30/23 12:46:42.776
------------------------------
• [4.029 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:204

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 12:46:38.75
    Jan 30 12:46:38.750: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename pods 01/30/23 12:46:38.75
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:46:38.757
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:46:38.759
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should get a host IP [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:204
    STEP: creating pod 01/30/23 12:46:38.761
    Jan 30 12:46:38.765: INFO: Waiting up to 5m0s for pod "pod-hostip-e9b28bb1-c852-4641-95e0-84e1edf5fb67" in namespace "pods-1031" to be "running and ready"
    Jan 30 12:46:38.766: INFO: Pod "pod-hostip-e9b28bb1-c852-4641-95e0-84e1edf5fb67": Phase="Pending", Reason="", readiness=false. Elapsed: 1.494385ms
    Jan 30 12:46:38.766: INFO: The phase of Pod pod-hostip-e9b28bb1-c852-4641-95e0-84e1edf5fb67 is Pending, waiting for it to be Running (with Ready = true)
    Jan 30 12:46:40.770: INFO: Pod "pod-hostip-e9b28bb1-c852-4641-95e0-84e1edf5fb67": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004860057s
    Jan 30 12:46:40.770: INFO: The phase of Pod pod-hostip-e9b28bb1-c852-4641-95e0-84e1edf5fb67 is Pending, waiting for it to be Running (with Ready = true)
    Jan 30 12:46:42.770: INFO: Pod "pod-hostip-e9b28bb1-c852-4641-95e0-84e1edf5fb67": Phase="Running", Reason="", readiness=true. Elapsed: 4.0051355s
    Jan 30 12:46:42.770: INFO: The phase of Pod pod-hostip-e9b28bb1-c852-4641-95e0-84e1edf5fb67 is Running (Ready = true)
    Jan 30 12:46:42.770: INFO: Pod "pod-hostip-e9b28bb1-c852-4641-95e0-84e1edf5fb67" satisfied condition "running and ready"
    Jan 30 12:46:42.773: INFO: Pod pod-hostip-e9b28bb1-c852-4641-95e0-84e1edf5fb67 has hostIP: 10.182.0.82
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Jan 30 12:46:42.773: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-1031" for this suite. 01/30/23 12:46:42.776
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
  should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 12:46:42.779
Jan 30 12:46:42.780: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename runtimeclass 01/30/23 12:46:42.78
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:46:42.786
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:46:42.788
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It] should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129
Jan 30 12:46:42.796: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-3559 to be scheduled
Jan 30 12:46:42.798: INFO: 1 pods are not scheduled: [runtimeclass-3559/test-runtimeclass-runtimeclass-3559-preconfigured-handler-b5wt9(7e54b9c7-033a-4e56-a336-950f925e8a81)]
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Jan 30 12:46:44.803: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-3559" for this suite. 01/30/23 12:46:44.806
------------------------------
• [2.029 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 12:46:42.779
    Jan 30 12:46:42.780: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename runtimeclass 01/30/23 12:46:42.78
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:46:42.786
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:46:42.788
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It] should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:129
    Jan 30 12:46:42.796: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-3559 to be scheduled
    Jan 30 12:46:42.798: INFO: 1 pods are not scheduled: [runtimeclass-3559/test-runtimeclass-runtimeclass-3559-preconfigured-handler-b5wt9(7e54b9c7-033a-4e56-a336-950f925e8a81)]
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Jan 30 12:46:44.803: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-3559" for this suite. 01/30/23 12:46:44.806
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-cli] Kubectl client Guestbook application
  should create and stop a working application  [Conformance]
  test/e2e/kubectl/kubectl.go:394
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 12:46:44.808
Jan 30 12:46:44.808: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename kubectl 01/30/23 12:46:44.809
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:46:44.815
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:46:44.817
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should create and stop a working application  [Conformance]
  test/e2e/kubectl/kubectl.go:394
STEP: creating all guestbook components 01/30/23 12:46:44.819
Jan 30 12:46:44.819: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-replica
  labels:
    app: agnhost
    role: replica
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: replica
    tier: backend

Jan 30 12:46:44.819: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=kubectl-3921 create -f -'
Jan 30 12:46:46.345: INFO: stderr: ""
Jan 30 12:46:46.345: INFO: stdout: "service/agnhost-replica created\n"
Jan 30 12:46:46.345: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-primary
  labels:
    app: agnhost
    role: primary
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: primary
    tier: backend

Jan 30 12:46:46.345: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=kubectl-3921 create -f -'
Jan 30 12:46:46.516: INFO: stderr: ""
Jan 30 12:46:46.516: INFO: stdout: "service/agnhost-primary created\n"
Jan 30 12:46:46.516: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Jan 30 12:46:46.516: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=kubectl-3921 create -f -'
Jan 30 12:46:46.682: INFO: stderr: ""
Jan 30 12:46:46.682: INFO: stdout: "service/frontend created\n"
Jan 30 12:46:46.682: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: harbor.cloud.netease.com/qzprod-k8s/e2e/agnhost:2.43
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

Jan 30 12:46:46.682: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=kubectl-3921 create -f -'
Jan 30 12:46:46.846: INFO: stderr: ""
Jan 30 12:46:46.846: INFO: stdout: "deployment.apps/frontend created\n"
Jan 30 12:46:46.847: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-primary
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: primary
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      containers:
      - name: primary
        image: harbor.cloud.netease.com/qzprod-k8s/e2e/agnhost:2.43
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Jan 30 12:46:46.847: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=kubectl-3921 create -f -'
Jan 30 12:46:47.020: INFO: stderr: ""
Jan 30 12:46:47.020: INFO: stdout: "deployment.apps/agnhost-primary created\n"
Jan 30 12:46:47.020: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-replica
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: replica
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      containers:
      - name: replica
        image: harbor.cloud.netease.com/qzprod-k8s/e2e/agnhost:2.43
        args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Jan 30 12:46:47.020: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=kubectl-3921 create -f -'
Jan 30 12:46:47.190: INFO: stderr: ""
Jan 30 12:46:47.190: INFO: stdout: "deployment.apps/agnhost-replica created\n"
STEP: validating guestbook app 01/30/23 12:46:47.19
Jan 30 12:46:47.190: INFO: Waiting for all frontend pods to be Running.
Jan 30 12:46:52.243: INFO: Waiting for frontend to serve content.
Jan 30 12:46:52.249: INFO: Trying to add a new entry to the guestbook.
Jan 30 12:46:52.255: INFO: Verifying that added entry can be retrieved.
Jan 30 12:46:52.258: INFO: Failed to get response from guestbook. err: <nil>, response: {"data":""}
STEP: using delete to clean up resources 01/30/23 12:46:57.264
Jan 30 12:46:57.264: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=kubectl-3921 delete --grace-period=0 --force -f -'
Jan 30 12:46:57.327: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 30 12:46:57.327: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
STEP: using delete to clean up resources 01/30/23 12:46:57.327
Jan 30 12:46:57.327: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=kubectl-3921 delete --grace-period=0 --force -f -'
Jan 30 12:46:57.386: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 30 12:46:57.386: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources 01/30/23 12:46:57.386
Jan 30 12:46:57.386: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=kubectl-3921 delete --grace-period=0 --force -f -'
Jan 30 12:46:57.446: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 30 12:46:57.446: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources 01/30/23 12:46:57.446
Jan 30 12:46:57.446: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=kubectl-3921 delete --grace-period=0 --force -f -'
Jan 30 12:46:57.503: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 30 12:46:57.503: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources 01/30/23 12:46:57.503
Jan 30 12:46:57.503: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=kubectl-3921 delete --grace-period=0 --force -f -'
Jan 30 12:46:57.563: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 30 12:46:57.563: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources 01/30/23 12:46:57.564
Jan 30 12:46:57.564: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=kubectl-3921 delete --grace-period=0 --force -f -'
Jan 30 12:46:57.625: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 30 12:46:57.625: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jan 30 12:46:57.626: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-3921" for this suite. 01/30/23 12:46:57.628
------------------------------
• [SLOW TEST] [12.823 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Guestbook application
  test/e2e/kubectl/kubectl.go:369
    should create and stop a working application  [Conformance]
    test/e2e/kubectl/kubectl.go:394

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 12:46:44.808
    Jan 30 12:46:44.808: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename kubectl 01/30/23 12:46:44.809
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:46:44.815
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:46:44.817
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should create and stop a working application  [Conformance]
      test/e2e/kubectl/kubectl.go:394
    STEP: creating all guestbook components 01/30/23 12:46:44.819
    Jan 30 12:46:44.819: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: agnhost-replica
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      ports:
      - port: 6379
      selector:
        app: agnhost
        role: replica
        tier: backend

    Jan 30 12:46:44.819: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=kubectl-3921 create -f -'
    Jan 30 12:46:46.345: INFO: stderr: ""
    Jan 30 12:46:46.345: INFO: stdout: "service/agnhost-replica created\n"
    Jan 30 12:46:46.345: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: agnhost-primary
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      ports:
      - port: 6379
        targetPort: 6379
      selector:
        app: agnhost
        role: primary
        tier: backend

    Jan 30 12:46:46.345: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=kubectl-3921 create -f -'
    Jan 30 12:46:46.516: INFO: stderr: ""
    Jan 30 12:46:46.516: INFO: stdout: "service/agnhost-primary created\n"
    Jan 30 12:46:46.516: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: frontend
      labels:
        app: guestbook
        tier: frontend
    spec:
      # if your cluster supports it, uncomment the following to automatically create
      # an external load-balanced IP for the frontend service.
      # type: LoadBalancer
      ports:
      - port: 80
      selector:
        app: guestbook
        tier: frontend

    Jan 30 12:46:46.516: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=kubectl-3921 create -f -'
    Jan 30 12:46:46.682: INFO: stderr: ""
    Jan 30 12:46:46.682: INFO: stdout: "service/frontend created\n"
    Jan 30 12:46:46.682: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: frontend
    spec:
      replicas: 3
      selector:
        matchLabels:
          app: guestbook
          tier: frontend
      template:
        metadata:
          labels:
            app: guestbook
            tier: frontend
        spec:
          containers:
          - name: guestbook-frontend
            image: harbor.cloud.netease.com/qzprod-k8s/e2e/agnhost:2.43
            args: [ "guestbook", "--backend-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 80

    Jan 30 12:46:46.682: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=kubectl-3921 create -f -'
    Jan 30 12:46:46.846: INFO: stderr: ""
    Jan 30 12:46:46.846: INFO: stdout: "deployment.apps/frontend created\n"
    Jan 30 12:46:46.847: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: agnhost-primary
    spec:
      replicas: 1
      selector:
        matchLabels:
          app: agnhost
          role: primary
          tier: backend
      template:
        metadata:
          labels:
            app: agnhost
            role: primary
            tier: backend
        spec:
          containers:
          - name: primary
            image: harbor.cloud.netease.com/qzprod-k8s/e2e/agnhost:2.43
            args: [ "guestbook", "--http-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 6379

    Jan 30 12:46:46.847: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=kubectl-3921 create -f -'
    Jan 30 12:46:47.020: INFO: stderr: ""
    Jan 30 12:46:47.020: INFO: stdout: "deployment.apps/agnhost-primary created\n"
    Jan 30 12:46:47.020: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: agnhost-replica
    spec:
      replicas: 2
      selector:
        matchLabels:
          app: agnhost
          role: replica
          tier: backend
      template:
        metadata:
          labels:
            app: agnhost
            role: replica
            tier: backend
        spec:
          containers:
          - name: replica
            image: harbor.cloud.netease.com/qzprod-k8s/e2e/agnhost:2.43
            args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 6379

    Jan 30 12:46:47.020: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=kubectl-3921 create -f -'
    Jan 30 12:46:47.190: INFO: stderr: ""
    Jan 30 12:46:47.190: INFO: stdout: "deployment.apps/agnhost-replica created\n"
    STEP: validating guestbook app 01/30/23 12:46:47.19
    Jan 30 12:46:47.190: INFO: Waiting for all frontend pods to be Running.
    Jan 30 12:46:52.243: INFO: Waiting for frontend to serve content.
    Jan 30 12:46:52.249: INFO: Trying to add a new entry to the guestbook.
    Jan 30 12:46:52.255: INFO: Verifying that added entry can be retrieved.
    Jan 30 12:46:52.258: INFO: Failed to get response from guestbook. err: <nil>, response: {"data":""}
    STEP: using delete to clean up resources 01/30/23 12:46:57.264
    Jan 30 12:46:57.264: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=kubectl-3921 delete --grace-period=0 --force -f -'
    Jan 30 12:46:57.327: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jan 30 12:46:57.327: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
    STEP: using delete to clean up resources 01/30/23 12:46:57.327
    Jan 30 12:46:57.327: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=kubectl-3921 delete --grace-period=0 --force -f -'
    Jan 30 12:46:57.386: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jan 30 12:46:57.386: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
    STEP: using delete to clean up resources 01/30/23 12:46:57.386
    Jan 30 12:46:57.386: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=kubectl-3921 delete --grace-period=0 --force -f -'
    Jan 30 12:46:57.446: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jan 30 12:46:57.446: INFO: stdout: "service \"frontend\" force deleted\n"
    STEP: using delete to clean up resources 01/30/23 12:46:57.446
    Jan 30 12:46:57.446: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=kubectl-3921 delete --grace-period=0 --force -f -'
    Jan 30 12:46:57.503: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jan 30 12:46:57.503: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
    STEP: using delete to clean up resources 01/30/23 12:46:57.503
    Jan 30 12:46:57.503: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=kubectl-3921 delete --grace-period=0 --force -f -'
    Jan 30 12:46:57.563: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jan 30 12:46:57.563: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
    STEP: using delete to clean up resources 01/30/23 12:46:57.564
    Jan 30 12:46:57.564: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=kubectl-3921 delete --grace-period=0 --force -f -'
    Jan 30 12:46:57.625: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jan 30 12:46:57.625: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jan 30 12:46:57.626: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-3921" for this suite. 01/30/23 12:46:57.628
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:110
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 12:46:57.632
Jan 30 12:46:57.632: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename replication-controller 01/30/23 12:46:57.632
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:46:57.639
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:46:57.641
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:110
STEP: creating a ReplicationController 01/30/23 12:46:57.645
STEP: waiting for RC to be added 01/30/23 12:46:57.648
STEP: waiting for available Replicas 01/30/23 12:46:57.648
STEP: patching ReplicationController 01/30/23 12:46:59.957
STEP: waiting for RC to be modified 01/30/23 12:46:59.964
STEP: patching ReplicationController status 01/30/23 12:46:59.964
STEP: waiting for RC to be modified 01/30/23 12:46:59.968
STEP: waiting for available Replicas 01/30/23 12:46:59.968
STEP: fetching ReplicationController status 01/30/23 12:46:59.971
STEP: patching ReplicationController scale 01/30/23 12:46:59.973
STEP: waiting for RC to be modified 01/30/23 12:46:59.979
STEP: waiting for ReplicationController's scale to be the max amount 01/30/23 12:46:59.979
STEP: fetching ReplicationController; ensuring that it's patched 01/30/23 12:47:02.373
STEP: updating ReplicationController status 01/30/23 12:47:02.376
STEP: waiting for RC to be modified 01/30/23 12:47:02.38
STEP: listing all ReplicationControllers 01/30/23 12:47:02.38
STEP: checking that ReplicationController has expected values 01/30/23 12:47:02.382
STEP: deleting ReplicationControllers by collection 01/30/23 12:47:02.382
STEP: waiting for ReplicationController to have a DELETED watchEvent 01/30/23 12:47:02.386
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Jan 30 12:47:02.422: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-142" for this suite. 01/30/23 12:47:02.424
------------------------------
• [4.795 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:110

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 12:46:57.632
    Jan 30 12:46:57.632: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename replication-controller 01/30/23 12:46:57.632
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:46:57.639
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:46:57.641
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should test the lifecycle of a ReplicationController [Conformance]
      test/e2e/apps/rc.go:110
    STEP: creating a ReplicationController 01/30/23 12:46:57.645
    STEP: waiting for RC to be added 01/30/23 12:46:57.648
    STEP: waiting for available Replicas 01/30/23 12:46:57.648
    STEP: patching ReplicationController 01/30/23 12:46:59.957
    STEP: waiting for RC to be modified 01/30/23 12:46:59.964
    STEP: patching ReplicationController status 01/30/23 12:46:59.964
    STEP: waiting for RC to be modified 01/30/23 12:46:59.968
    STEP: waiting for available Replicas 01/30/23 12:46:59.968
    STEP: fetching ReplicationController status 01/30/23 12:46:59.971
    STEP: patching ReplicationController scale 01/30/23 12:46:59.973
    STEP: waiting for RC to be modified 01/30/23 12:46:59.979
    STEP: waiting for ReplicationController's scale to be the max amount 01/30/23 12:46:59.979
    STEP: fetching ReplicationController; ensuring that it's patched 01/30/23 12:47:02.373
    STEP: updating ReplicationController status 01/30/23 12:47:02.376
    STEP: waiting for RC to be modified 01/30/23 12:47:02.38
    STEP: listing all ReplicationControllers 01/30/23 12:47:02.38
    STEP: checking that ReplicationController has expected values 01/30/23 12:47:02.382
    STEP: deleting ReplicationControllers by collection 01/30/23 12:47:02.382
    STEP: waiting for ReplicationController to have a DELETED watchEvent 01/30/23 12:47:02.386
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Jan 30 12:47:02.422: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-142" for this suite. 01/30/23 12:47:02.424
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with privileged
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:528
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 12:47:02.427
Jan 30 12:47:02.427: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename security-context-test 01/30/23 12:47:02.428
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:47:02.435
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:47:02.437
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:50
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:528
Jan 30 12:47:02.444: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-6fc56bff-2187-40d1-8d82-dcc5b393d387" in namespace "security-context-test-194" to be "Succeeded or Failed"
Jan 30 12:47:02.445: INFO: Pod "busybox-privileged-false-6fc56bff-2187-40d1-8d82-dcc5b393d387": Phase="Pending", Reason="", readiness=false. Elapsed: 1.590135ms
Jan 30 12:47:04.447: INFO: Pod "busybox-privileged-false-6fc56bff-2187-40d1-8d82-dcc5b393d387": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003723517s
Jan 30 12:47:06.449: INFO: Pod "busybox-privileged-false-6fc56bff-2187-40d1-8d82-dcc5b393d387": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005572572s
Jan 30 12:47:06.449: INFO: Pod "busybox-privileged-false-6fc56bff-2187-40d1-8d82-dcc5b393d387" satisfied condition "Succeeded or Failed"
Jan 30 12:47:06.461: INFO: Got logs for pod "busybox-privileged-false-6fc56bff-2187-40d1-8d82-dcc5b393d387": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Jan 30 12:47:06.461: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-test-194" for this suite. 01/30/23 12:47:06.464
------------------------------
• [4.039 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a pod with privileged
  test/e2e/common/node/security_context.go:491
    should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:528

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 12:47:02.427
    Jan 30 12:47:02.427: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename security-context-test 01/30/23 12:47:02.428
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:47:02.435
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:47:02.437
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:50
    [It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:528
    Jan 30 12:47:02.444: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-6fc56bff-2187-40d1-8d82-dcc5b393d387" in namespace "security-context-test-194" to be "Succeeded or Failed"
    Jan 30 12:47:02.445: INFO: Pod "busybox-privileged-false-6fc56bff-2187-40d1-8d82-dcc5b393d387": Phase="Pending", Reason="", readiness=false. Elapsed: 1.590135ms
    Jan 30 12:47:04.447: INFO: Pod "busybox-privileged-false-6fc56bff-2187-40d1-8d82-dcc5b393d387": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003723517s
    Jan 30 12:47:06.449: INFO: Pod "busybox-privileged-false-6fc56bff-2187-40d1-8d82-dcc5b393d387": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005572572s
    Jan 30 12:47:06.449: INFO: Pod "busybox-privileged-false-6fc56bff-2187-40d1-8d82-dcc5b393d387" satisfied condition "Succeeded or Failed"
    Jan 30 12:47:06.461: INFO: Got logs for pod "busybox-privileged-false-6fc56bff-2187-40d1-8d82-dcc5b393d387": "ip: RTNETLINK answers: Operation not permitted\n"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Jan 30 12:47:06.461: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-test-194" for this suite. 01/30/23 12:47:06.464
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial]
  validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:224
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 12:47:06.467
Jan 30 12:47:06.467: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename sched-preemption 01/30/23 12:47:06.467
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:47:06.474
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:47:06.476
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:97
Jan 30 12:47:06.483: INFO: Waiting up to 1m0s for all nodes to be ready
Jan 30 12:48:06.503: INFO: Waiting for terminating namespaces to be deleted...
[It] validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:224
STEP: Create pods that use 4/5 of node resources. 01/30/23 12:48:06.505
Jan 30 12:48:06.520: INFO: Created pod: pod0-0-sched-preemption-low-priority
Jan 30 12:48:06.523: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Jan 30 12:48:06.534: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Jan 30 12:48:06.537: INFO: Created pod: pod1-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled. 01/30/23 12:48:06.537
Jan 30 12:48:06.537: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-6694" to be "running"
Jan 30 12:48:06.539: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 1.829628ms
Jan 30 12:48:08.542: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005753275s
Jan 30 12:48:10.542: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.005690566s
Jan 30 12:48:10.542: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
Jan 30 12:48:10.542: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-6694" to be "running"
Jan 30 12:48:10.544: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 1.639135ms
Jan 30 12:48:10.544: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
Jan 30 12:48:10.544: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-6694" to be "running"
Jan 30 12:48:10.546: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 1.575655ms
Jan 30 12:48:10.546: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
Jan 30 12:48:10.546: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-6694" to be "running"
Jan 30 12:48:10.547: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 1.682132ms
Jan 30 12:48:10.547: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
STEP: Run a critical pod that use same resources as that of a lower priority pod 01/30/23 12:48:10.547
Jan 30 12:48:10.553: INFO: Waiting up to 2m0s for pod "critical-pod" in namespace "kube-system" to be "running"
Jan 30 12:48:10.555: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 1.54712ms
Jan 30 12:48:12.557: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003960026s
Jan 30 12:48:14.559: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.005161657s
Jan 30 12:48:16.559: INFO: Pod "critical-pod": Phase="Running", Reason="", readiness=true. Elapsed: 6.005045801s
Jan 30 12:48:16.559: INFO: Pod "critical-pod" satisfied condition "running"
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/node/init/init.go:32
Jan 30 12:48:16.572: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:84
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-6694" for this suite. 01/30/23 12:48:16.598
------------------------------
• [SLOW TEST] [70.134 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:224

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 12:47:06.467
    Jan 30 12:47:06.467: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename sched-preemption 01/30/23 12:47:06.467
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:47:06.474
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:47:06.476
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:97
    Jan 30 12:47:06.483: INFO: Waiting up to 1m0s for all nodes to be ready
    Jan 30 12:48:06.503: INFO: Waiting for terminating namespaces to be deleted...
    [It] validates lower priority pod preemption by critical pod [Conformance]
      test/e2e/scheduling/preemption.go:224
    STEP: Create pods that use 4/5 of node resources. 01/30/23 12:48:06.505
    Jan 30 12:48:06.520: INFO: Created pod: pod0-0-sched-preemption-low-priority
    Jan 30 12:48:06.523: INFO: Created pod: pod0-1-sched-preemption-medium-priority
    Jan 30 12:48:06.534: INFO: Created pod: pod1-0-sched-preemption-medium-priority
    Jan 30 12:48:06.537: INFO: Created pod: pod1-1-sched-preemption-medium-priority
    STEP: Wait for pods to be scheduled. 01/30/23 12:48:06.537
    Jan 30 12:48:06.537: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-6694" to be "running"
    Jan 30 12:48:06.539: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 1.829628ms
    Jan 30 12:48:08.542: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005753275s
    Jan 30 12:48:10.542: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.005690566s
    Jan 30 12:48:10.542: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
    Jan 30 12:48:10.542: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-6694" to be "running"
    Jan 30 12:48:10.544: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 1.639135ms
    Jan 30 12:48:10.544: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
    Jan 30 12:48:10.544: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-6694" to be "running"
    Jan 30 12:48:10.546: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 1.575655ms
    Jan 30 12:48:10.546: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
    Jan 30 12:48:10.546: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-6694" to be "running"
    Jan 30 12:48:10.547: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 1.682132ms
    Jan 30 12:48:10.547: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
    STEP: Run a critical pod that use same resources as that of a lower priority pod 01/30/23 12:48:10.547
    Jan 30 12:48:10.553: INFO: Waiting up to 2m0s for pod "critical-pod" in namespace "kube-system" to be "running"
    Jan 30 12:48:10.555: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 1.54712ms
    Jan 30 12:48:12.557: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003960026s
    Jan 30 12:48:14.559: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.005161657s
    Jan 30 12:48:16.559: INFO: Pod "critical-pod": Phase="Running", Reason="", readiness=true. Elapsed: 6.005045801s
    Jan 30 12:48:16.559: INFO: Pod "critical-pod" satisfied condition "running"
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan 30 12:48:16.572: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:84
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-6694" for this suite. 01/30/23 12:48:16.598
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:442
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 12:48:16.601
Jan 30 12:48:16.602: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename crd-publish-openapi 01/30/23 12:48:16.602
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:48:16.609
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:48:16.611
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:442
STEP: set up a multi version CRD 01/30/23 12:48:16.613
Jan 30 12:48:16.613: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: mark a version not serverd 01/30/23 12:48:20.786
STEP: check the unserved version gets removed 01/30/23 12:48:20.802
STEP: check the other version is not changed 01/30/23 12:48:22.104
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 30 12:48:25.472: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-1774" for this suite. 01/30/23 12:48:25.479
------------------------------
• [SLOW TEST] [8.880 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:442

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 12:48:16.601
    Jan 30 12:48:16.602: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename crd-publish-openapi 01/30/23 12:48:16.602
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:48:16.609
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:48:16.611
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] removes definition from spec when one version gets changed to not be served [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:442
    STEP: set up a multi version CRD 01/30/23 12:48:16.613
    Jan 30 12:48:16.613: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: mark a version not serverd 01/30/23 12:48:20.786
    STEP: check the unserved version gets removed 01/30/23 12:48:20.802
    STEP: check the other version is not changed 01/30/23 12:48:22.104
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 30 12:48:25.472: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-1774" for this suite. 01/30/23 12:48:25.479
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:703
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 12:48:25.482
Jan 30 12:48:25.482: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename job 01/30/23 12:48:25.483
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:48:25.489
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:48:25.491
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:703
STEP: Creating a suspended job 01/30/23 12:48:25.495
STEP: Patching the Job 01/30/23 12:48:25.497
STEP: Watching for Job to be patched 01/30/23 12:48:25.511
Jan 30 12:48:25.513: INFO: Event ADDED observed for Job e2e-nc6cc in namespace job-8738 with labels: map[e2e-job-label:e2e-nc6cc] and annotations: map[batch.kubernetes.io/job-tracking:]
Jan 30 12:48:25.513: INFO: Event MODIFIED observed for Job e2e-nc6cc in namespace job-8738 with labels: map[e2e-job-label:e2e-nc6cc] and annotations: map[batch.kubernetes.io/job-tracking:]
Jan 30 12:48:25.513: INFO: Event MODIFIED found for Job e2e-nc6cc in namespace job-8738 with labels: map[e2e-job-label:e2e-nc6cc e2e-nc6cc:patched] and annotations: map[batch.kubernetes.io/job-tracking:]
STEP: Updating the job 01/30/23 12:48:25.513
STEP: Watching for Job to be updated 01/30/23 12:48:25.518
Jan 30 12:48:25.519: INFO: Event MODIFIED found for Job e2e-nc6cc in namespace job-8738 with labels: map[e2e-job-label:e2e-nc6cc e2e-nc6cc:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Jan 30 12:48:25.519: INFO: Found Job annotations: map[string]string{"batch.kubernetes.io/job-tracking":"", "updated":"true"}
STEP: Listing all Jobs with LabelSelector 01/30/23 12:48:25.519
Jan 30 12:48:25.521: INFO: Job: e2e-nc6cc as labels: map[e2e-job-label:e2e-nc6cc e2e-nc6cc:patched]
STEP: Waiting for job to complete 01/30/23 12:48:25.521
STEP: Delete a job collection with a labelselector 01/30/23 12:48:33.523
STEP: Watching for Job to be deleted 01/30/23 12:48:33.526
Jan 30 12:48:33.528: INFO: Event MODIFIED observed for Job e2e-nc6cc in namespace job-8738 with labels: map[e2e-job-label:e2e-nc6cc e2e-nc6cc:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Jan 30 12:48:33.528: INFO: Event MODIFIED observed for Job e2e-nc6cc in namespace job-8738 with labels: map[e2e-job-label:e2e-nc6cc e2e-nc6cc:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Jan 30 12:48:33.528: INFO: Event MODIFIED observed for Job e2e-nc6cc in namespace job-8738 with labels: map[e2e-job-label:e2e-nc6cc e2e-nc6cc:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Jan 30 12:48:33.528: INFO: Event MODIFIED observed for Job e2e-nc6cc in namespace job-8738 with labels: map[e2e-job-label:e2e-nc6cc e2e-nc6cc:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Jan 30 12:48:33.528: INFO: Event MODIFIED observed for Job e2e-nc6cc in namespace job-8738 with labels: map[e2e-job-label:e2e-nc6cc e2e-nc6cc:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Jan 30 12:48:33.528: INFO: Event DELETED found for Job e2e-nc6cc in namespace job-8738 with labels: map[e2e-job-label:e2e-nc6cc e2e-nc6cc:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
STEP: Relist jobs to confirm deletion 01/30/23 12:48:33.528
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Jan 30 12:48:33.530: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-8738" for this suite. 01/30/23 12:48:33.532
------------------------------
• [SLOW TEST] [8.052 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:703

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 12:48:25.482
    Jan 30 12:48:25.482: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename job 01/30/23 12:48:25.483
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:48:25.489
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:48:25.491
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should manage the lifecycle of a job [Conformance]
      test/e2e/apps/job.go:703
    STEP: Creating a suspended job 01/30/23 12:48:25.495
    STEP: Patching the Job 01/30/23 12:48:25.497
    STEP: Watching for Job to be patched 01/30/23 12:48:25.511
    Jan 30 12:48:25.513: INFO: Event ADDED observed for Job e2e-nc6cc in namespace job-8738 with labels: map[e2e-job-label:e2e-nc6cc] and annotations: map[batch.kubernetes.io/job-tracking:]
    Jan 30 12:48:25.513: INFO: Event MODIFIED observed for Job e2e-nc6cc in namespace job-8738 with labels: map[e2e-job-label:e2e-nc6cc] and annotations: map[batch.kubernetes.io/job-tracking:]
    Jan 30 12:48:25.513: INFO: Event MODIFIED found for Job e2e-nc6cc in namespace job-8738 with labels: map[e2e-job-label:e2e-nc6cc e2e-nc6cc:patched] and annotations: map[batch.kubernetes.io/job-tracking:]
    STEP: Updating the job 01/30/23 12:48:25.513
    STEP: Watching for Job to be updated 01/30/23 12:48:25.518
    Jan 30 12:48:25.519: INFO: Event MODIFIED found for Job e2e-nc6cc in namespace job-8738 with labels: map[e2e-job-label:e2e-nc6cc e2e-nc6cc:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Jan 30 12:48:25.519: INFO: Found Job annotations: map[string]string{"batch.kubernetes.io/job-tracking":"", "updated":"true"}
    STEP: Listing all Jobs with LabelSelector 01/30/23 12:48:25.519
    Jan 30 12:48:25.521: INFO: Job: e2e-nc6cc as labels: map[e2e-job-label:e2e-nc6cc e2e-nc6cc:patched]
    STEP: Waiting for job to complete 01/30/23 12:48:25.521
    STEP: Delete a job collection with a labelselector 01/30/23 12:48:33.523
    STEP: Watching for Job to be deleted 01/30/23 12:48:33.526
    Jan 30 12:48:33.528: INFO: Event MODIFIED observed for Job e2e-nc6cc in namespace job-8738 with labels: map[e2e-job-label:e2e-nc6cc e2e-nc6cc:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Jan 30 12:48:33.528: INFO: Event MODIFIED observed for Job e2e-nc6cc in namespace job-8738 with labels: map[e2e-job-label:e2e-nc6cc e2e-nc6cc:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Jan 30 12:48:33.528: INFO: Event MODIFIED observed for Job e2e-nc6cc in namespace job-8738 with labels: map[e2e-job-label:e2e-nc6cc e2e-nc6cc:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Jan 30 12:48:33.528: INFO: Event MODIFIED observed for Job e2e-nc6cc in namespace job-8738 with labels: map[e2e-job-label:e2e-nc6cc e2e-nc6cc:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Jan 30 12:48:33.528: INFO: Event MODIFIED observed for Job e2e-nc6cc in namespace job-8738 with labels: map[e2e-job-label:e2e-nc6cc e2e-nc6cc:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Jan 30 12:48:33.528: INFO: Event DELETED found for Job e2e-nc6cc in namespace job-8738 with labels: map[e2e-job-label:e2e-nc6cc e2e-nc6cc:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    STEP: Relist jobs to confirm deletion 01/30/23 12:48:33.528
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Jan 30 12:48:33.530: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-8738" for this suite. 01/30/23 12:48:33.532
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:217
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 12:48:33.535
Jan 30 12:48:33.535: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename emptydir 01/30/23 12:48:33.535
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:48:33.542
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:48:33.544
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:217
STEP: Creating a pod to test emptydir 0777 on node default medium 01/30/23 12:48:33.546
Jan 30 12:48:33.550: INFO: Waiting up to 5m0s for pod "pod-1a0cbd28-a93f-466e-a6b6-5558825f7d6d" in namespace "emptydir-7646" to be "Succeeded or Failed"
Jan 30 12:48:33.551: INFO: Pod "pod-1a0cbd28-a93f-466e-a6b6-5558825f7d6d": Phase="Pending", Reason="", readiness=false. Elapsed: 1.624996ms
Jan 30 12:48:35.553: INFO: Pod "pod-1a0cbd28-a93f-466e-a6b6-5558825f7d6d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003725899s
Jan 30 12:48:37.555: INFO: Pod "pod-1a0cbd28-a93f-466e-a6b6-5558825f7d6d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005536977s
STEP: Saw pod success 01/30/23 12:48:37.555
Jan 30 12:48:37.555: INFO: Pod "pod-1a0cbd28-a93f-466e-a6b6-5558825f7d6d" satisfied condition "Succeeded or Failed"
Jan 30 12:48:37.557: INFO: Trying to get logs from node pubt2-nks-for-dev1.dg.163.org pod pod-1a0cbd28-a93f-466e-a6b6-5558825f7d6d container test-container: <nil>
STEP: delete the pod 01/30/23 12:48:37.569
Jan 30 12:48:37.573: INFO: Waiting for pod pod-1a0cbd28-a93f-466e-a6b6-5558825f7d6d to disappear
Jan 30 12:48:37.575: INFO: Pod pod-1a0cbd28-a93f-466e-a6b6-5558825f7d6d no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jan 30 12:48:37.575: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-7646" for this suite. 01/30/23 12:48:37.577
------------------------------
• [4.044 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:217

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 12:48:33.535
    Jan 30 12:48:33.535: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename emptydir 01/30/23 12:48:33.535
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:48:33.542
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:48:33.544
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:217
    STEP: Creating a pod to test emptydir 0777 on node default medium 01/30/23 12:48:33.546
    Jan 30 12:48:33.550: INFO: Waiting up to 5m0s for pod "pod-1a0cbd28-a93f-466e-a6b6-5558825f7d6d" in namespace "emptydir-7646" to be "Succeeded or Failed"
    Jan 30 12:48:33.551: INFO: Pod "pod-1a0cbd28-a93f-466e-a6b6-5558825f7d6d": Phase="Pending", Reason="", readiness=false. Elapsed: 1.624996ms
    Jan 30 12:48:35.553: INFO: Pod "pod-1a0cbd28-a93f-466e-a6b6-5558825f7d6d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003725899s
    Jan 30 12:48:37.555: INFO: Pod "pod-1a0cbd28-a93f-466e-a6b6-5558825f7d6d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005536977s
    STEP: Saw pod success 01/30/23 12:48:37.555
    Jan 30 12:48:37.555: INFO: Pod "pod-1a0cbd28-a93f-466e-a6b6-5558825f7d6d" satisfied condition "Succeeded or Failed"
    Jan 30 12:48:37.557: INFO: Trying to get logs from node pubt2-nks-for-dev1.dg.163.org pod pod-1a0cbd28-a93f-466e-a6b6-5558825f7d6d container test-container: <nil>
    STEP: delete the pod 01/30/23 12:48:37.569
    Jan 30 12:48:37.573: INFO: Waiting for pod pod-1a0cbd28-a93f-466e-a6b6-5558825f7d6d to disappear
    Jan 30 12:48:37.575: INFO: Pod pod-1a0cbd28-a93f-466e-a6b6-5558825f7d6d no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jan 30 12:48:37.575: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-7646" for this suite. 01/30/23 12:48:37.577
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:331
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 12:48:37.58
Jan 30 12:48:37.580: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename sched-pred 01/30/23 12:48:37.58
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:48:37.586
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:48:37.588
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:97
Jan 30 12:48:37.590: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jan 30 12:48:37.594: INFO: Waiting for terminating namespaces to be deleted...
Jan 30 12:48:37.596: INFO: 
Logging pods the apiserver thinks is on node pubt2-nks-for-dev1.dg.163.org before test
Jan 30 12:48:37.601: INFO: calico-node-5xgvr from kube-system started at 2023-01-30 11:14:53 +0000 UTC (1 container statuses recorded)
Jan 30 12:48:37.601: INFO: 	Container calico-node ready: true, restart count 0
Jan 30 12:48:37.601: INFO: cleanlog-w9znh from kube-system started at 2023-01-30 11:02:22 +0000 UTC (1 container statuses recorded)
Jan 30 12:48:37.601: INFO: 	Container cleanlog ready: true, restart count 0
Jan 30 12:48:37.601: INFO: kube-proxy-fvt68 from kube-system started at 2023-01-30 11:02:22 +0000 UTC (1 container statuses recorded)
Jan 30 12:48:37.601: INFO: 	Container kube-proxy ready: true, restart count 0
Jan 30 12:48:37.601: INFO: node-local-dns-8jh9v from kube-system started at 2023-01-30 11:02:22 +0000 UTC (1 container statuses recorded)
Jan 30 12:48:37.601: INFO: 	Container node-cache ready: true, restart count 5
Jan 30 12:48:37.601: INFO: sonobuoy-systemd-logs-daemon-set-6d9e6fe3a8a74fcc-st9q5 from sonobuoy started at 2023-01-30 11:17:19 +0000 UTC (2 container statuses recorded)
Jan 30 12:48:37.601: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 30 12:48:37.601: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 30 12:48:37.601: INFO: 
Logging pods the apiserver thinks is on node pubt2-nks-for-dev3.dg.163.org before test
Jan 30 12:48:37.606: INFO: calico-kube-controllers-568668f974-5gtkq from kube-system started at 2023-01-30 09:59:07 +0000 UTC (1 container statuses recorded)
Jan 30 12:48:37.606: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Jan 30 12:48:37.606: INFO: calico-node-h7jwg from kube-system started at 2023-01-30 11:14:53 +0000 UTC (1 container statuses recorded)
Jan 30 12:48:37.606: INFO: 	Container calico-node ready: true, restart count 0
Jan 30 12:48:37.606: INFO: cleanlog-crksz from kube-system started at 2023-01-30 09:56:18 +0000 UTC (1 container statuses recorded)
Jan 30 12:48:37.606: INFO: 	Container cleanlog ready: true, restart count 0
Jan 30 12:48:37.606: INFO: coredns-5bf7dfc67-2gl8f from kube-system started at 2023-01-30 10:08:32 +0000 UTC (1 container statuses recorded)
Jan 30 12:48:37.606: INFO: 	Container coredns ready: true, restart count 0
Jan 30 12:48:37.606: INFO: coredns-5bf7dfc67-92sjx from kube-system started at 2023-01-30 12:46:12 +0000 UTC (1 container statuses recorded)
Jan 30 12:48:37.606: INFO: 	Container coredns ready: true, restart count 0
Jan 30 12:48:37.606: INFO: etcd-main-pubt2-nks-for-dev3.dg.163.org from kube-system started at 2023-01-30 09:56:17 +0000 UTC (1 container statuses recorded)
Jan 30 12:48:37.606: INFO: 	Container etcd ready: true, restart count 1
Jan 30 12:48:37.606: INFO: kube-apiserver-pubt2-nks-for-dev3.dg.163.org from kube-system started at 2023-01-30 09:56:18 +0000 UTC (1 container statuses recorded)
Jan 30 12:48:37.606: INFO: 	Container kube-apiserver ready: true, restart count 1
Jan 30 12:48:37.606: INFO: kube-controller-manager-pubt2-nks-for-dev3.dg.163.org from kube-system started at 2023-01-30 09:56:18 +0000 UTC (1 container statuses recorded)
Jan 30 12:48:37.606: INFO: 	Container kube-controller-manager ready: true, restart count 1
Jan 30 12:48:37.606: INFO: kube-proxy-br2mf from kube-system started at 2023-01-30 09:58:40 +0000 UTC (1 container statuses recorded)
Jan 30 12:48:37.606: INFO: 	Container kube-proxy ready: true, restart count 0
Jan 30 12:48:37.606: INFO: kube-scheduler-pubt2-nks-for-dev3.dg.163.org from kube-system started at 2023-01-30 09:56:17 +0000 UTC (1 container statuses recorded)
Jan 30 12:48:37.606: INFO: 	Container kube-scheduler ready: true, restart count 1
Jan 30 12:48:37.606: INFO: node-local-dns-txwnd from kube-system started at 2023-01-30 10:09:12 +0000 UTC (1 container statuses recorded)
Jan 30 12:48:37.606: INFO: 	Container node-cache ready: true, restart count 0
Jan 30 12:48:37.606: INFO: sonobuoy from sonobuoy started at 2023-01-30 11:17:15 +0000 UTC (1 container statuses recorded)
Jan 30 12:48:37.606: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jan 30 12:48:37.606: INFO: sonobuoy-e2e-job-a0572830995342a7 from sonobuoy started at 2023-01-30 11:17:19 +0000 UTC (2 container statuses recorded)
Jan 30 12:48:37.606: INFO: 	Container e2e ready: true, restart count 0
Jan 30 12:48:37.606: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 30 12:48:37.606: INFO: sonobuoy-systemd-logs-daemon-set-6d9e6fe3a8a74fcc-nkw5p from sonobuoy started at 2023-01-30 11:17:19 +0000 UTC (2 container statuses recorded)
Jan 30 12:48:37.606: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 30 12:48:37.606: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:331
STEP: verifying the node has the label node pubt2-nks-for-dev1.dg.163.org 01/30/23 12:48:37.621
STEP: verifying the node has the label node pubt2-nks-for-dev3.dg.163.org 01/30/23 12:48:37.631
Jan 30 12:48:37.652: INFO: Pod calico-kube-controllers-568668f974-5gtkq requesting resource cpu=0m on Node pubt2-nks-for-dev3.dg.163.org
Jan 30 12:48:37.652: INFO: Pod calico-node-5xgvr requesting resource cpu=250m on Node pubt2-nks-for-dev1.dg.163.org
Jan 30 12:48:37.652: INFO: Pod calico-node-h7jwg requesting resource cpu=250m on Node pubt2-nks-for-dev3.dg.163.org
Jan 30 12:48:37.652: INFO: Pod cleanlog-crksz requesting resource cpu=100m on Node pubt2-nks-for-dev3.dg.163.org
Jan 30 12:48:37.652: INFO: Pod cleanlog-w9znh requesting resource cpu=100m on Node pubt2-nks-for-dev1.dg.163.org
Jan 30 12:48:37.652: INFO: Pod coredns-5bf7dfc67-2gl8f requesting resource cpu=100m on Node pubt2-nks-for-dev3.dg.163.org
Jan 30 12:48:37.652: INFO: Pod coredns-5bf7dfc67-92sjx requesting resource cpu=100m on Node pubt2-nks-for-dev3.dg.163.org
Jan 30 12:48:37.652: INFO: Pod etcd-main-pubt2-nks-for-dev3.dg.163.org requesting resource cpu=0m on Node pubt2-nks-for-dev3.dg.163.org
Jan 30 12:48:37.652: INFO: Pod kube-apiserver-pubt2-nks-for-dev3.dg.163.org requesting resource cpu=0m on Node pubt2-nks-for-dev3.dg.163.org
Jan 30 12:48:37.652: INFO: Pod kube-controller-manager-pubt2-nks-for-dev3.dg.163.org requesting resource cpu=0m on Node pubt2-nks-for-dev3.dg.163.org
Jan 30 12:48:37.652: INFO: Pod kube-proxy-br2mf requesting resource cpu=200m on Node pubt2-nks-for-dev3.dg.163.org
Jan 30 12:48:37.652: INFO: Pod kube-proxy-fvt68 requesting resource cpu=200m on Node pubt2-nks-for-dev1.dg.163.org
Jan 30 12:48:37.652: INFO: Pod kube-scheduler-pubt2-nks-for-dev3.dg.163.org requesting resource cpu=0m on Node pubt2-nks-for-dev3.dg.163.org
Jan 30 12:48:37.652: INFO: Pod node-local-dns-8jh9v requesting resource cpu=25m on Node pubt2-nks-for-dev1.dg.163.org
Jan 30 12:48:37.652: INFO: Pod node-local-dns-txwnd requesting resource cpu=25m on Node pubt2-nks-for-dev3.dg.163.org
Jan 30 12:48:37.652: INFO: Pod sonobuoy requesting resource cpu=0m on Node pubt2-nks-for-dev3.dg.163.org
Jan 30 12:48:37.652: INFO: Pod sonobuoy-e2e-job-a0572830995342a7 requesting resource cpu=0m on Node pubt2-nks-for-dev3.dg.163.org
Jan 30 12:48:37.652: INFO: Pod sonobuoy-systemd-logs-daemon-set-6d9e6fe3a8a74fcc-nkw5p requesting resource cpu=0m on Node pubt2-nks-for-dev3.dg.163.org
Jan 30 12:48:37.652: INFO: Pod sonobuoy-systemd-logs-daemon-set-6d9e6fe3a8a74fcc-st9q5 requesting resource cpu=0m on Node pubt2-nks-for-dev1.dg.163.org
STEP: Starting Pods to consume most of the cluster CPU. 01/30/23 12:48:37.652
Jan 30 12:48:37.652: INFO: Creating a pod which consumes cpu=38097m on Node pubt2-nks-for-dev1.dg.163.org
Jan 30 12:48:37.657: INFO: Creating a pod which consumes cpu=37957m on Node pubt2-nks-for-dev3.dg.163.org
Jan 30 12:48:37.661: INFO: Waiting up to 5m0s for pod "filler-pod-1d7bf8f4-ab83-48bd-90ce-5c3304656615" in namespace "sched-pred-1026" to be "running"
Jan 30 12:48:37.663: INFO: Pod "filler-pod-1d7bf8f4-ab83-48bd-90ce-5c3304656615": Phase="Pending", Reason="", readiness=false. Elapsed: 1.809575ms
Jan 30 12:48:39.666: INFO: Pod "filler-pod-1d7bf8f4-ab83-48bd-90ce-5c3304656615": Phase="Running", Reason="", readiness=true. Elapsed: 2.004927639s
Jan 30 12:48:39.666: INFO: Pod "filler-pod-1d7bf8f4-ab83-48bd-90ce-5c3304656615" satisfied condition "running"
Jan 30 12:48:39.666: INFO: Waiting up to 5m0s for pod "filler-pod-cf71ab00-4b0f-4581-94c1-64d5077d8d60" in namespace "sched-pred-1026" to be "running"
Jan 30 12:48:39.667: INFO: Pod "filler-pod-cf71ab00-4b0f-4581-94c1-64d5077d8d60": Phase="Pending", Reason="", readiness=false. Elapsed: 1.642509ms
Jan 30 12:48:41.670: INFO: Pod "filler-pod-cf71ab00-4b0f-4581-94c1-64d5077d8d60": Phase="Running", Reason="", readiness=true. Elapsed: 2.00470222s
Jan 30 12:48:41.670: INFO: Pod "filler-pod-cf71ab00-4b0f-4581-94c1-64d5077d8d60" satisfied condition "running"
STEP: Creating another pod that requires unavailable amount of CPU. 01/30/23 12:48:41.67
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-1d7bf8f4-ab83-48bd-90ce-5c3304656615.173f1709e4815838], Reason = [Scheduled], Message = [Successfully assigned sched-pred-1026/filler-pod-1d7bf8f4-ab83-48bd-90ce-5c3304656615 to pubt2-nks-for-dev1.dg.163.org] 01/30/23 12:48:41.673
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-1d7bf8f4-ab83-48bd-90ce-5c3304656615.173f170a179cc77f], Reason = [Pulled], Message = [Container image "harbor.cloud.netease.com/qzprod-k8s/e2e/pause:3.9" already present on machine] 01/30/23 12:48:41.673
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-1d7bf8f4-ab83-48bd-90ce-5c3304656615.173f170a1ae28426], Reason = [Created], Message = [Created container filler-pod-1d7bf8f4-ab83-48bd-90ce-5c3304656615] 01/30/23 12:48:41.673
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-1d7bf8f4-ab83-48bd-90ce-5c3304656615.173f170a24dbea25], Reason = [Started], Message = [Started container filler-pod-1d7bf8f4-ab83-48bd-90ce-5c3304656615] 01/30/23 12:48:41.673
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-cf71ab00-4b0f-4581-94c1-64d5077d8d60.173f1709e4adc015], Reason = [Scheduled], Message = [Successfully assigned sched-pred-1026/filler-pod-cf71ab00-4b0f-4581-94c1-64d5077d8d60 to pubt2-nks-for-dev3.dg.163.org] 01/30/23 12:48:41.673
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-cf71ab00-4b0f-4581-94c1-64d5077d8d60.173f170a18877751], Reason = [Pulled], Message = [Container image "harbor.cloud.netease.com/qzprod-k8s/e2e/pause:3.9" already present on machine] 01/30/23 12:48:41.673
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-cf71ab00-4b0f-4581-94c1-64d5077d8d60.173f170a1e306348], Reason = [Created], Message = [Created container filler-pod-cf71ab00-4b0f-4581-94c1-64d5077d8d60] 01/30/23 12:48:41.673
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-cf71ab00-4b0f-4581-94c1-64d5077d8d60.173f170a29f2535e], Reason = [Started], Message = [Started container filler-pod-cf71ab00-4b0f-4581-94c1-64d5077d8d60] 01/30/23 12:48:41.673
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.173f170ad3e8196e], Reason = [FailedScheduling], Message = [0/2 nodes are available: 2 Insufficient cpu. preemption: 0/2 nodes are available: 2 No preemption victims found for incoming pod..] 01/30/23 12:48:41.679
STEP: removing the label node off the node pubt2-nks-for-dev1.dg.163.org 01/30/23 12:48:42.68
STEP: verifying the node doesn't have the label node 01/30/23 12:48:42.691
STEP: removing the label node off the node pubt2-nks-for-dev3.dg.163.org 01/30/23 12:48:42.693
STEP: verifying the node doesn't have the label node 01/30/23 12:48:42.701
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/node/init/init.go:32
Jan 30 12:48:42.703: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:88
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-pred-1026" for this suite. 01/30/23 12:48:42.706
------------------------------
• [SLOW TEST] [5.128 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:331

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 12:48:37.58
    Jan 30 12:48:37.580: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename sched-pred 01/30/23 12:48:37.58
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:48:37.586
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:48:37.588
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:97
    Jan 30 12:48:37.590: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Jan 30 12:48:37.594: INFO: Waiting for terminating namespaces to be deleted...
    Jan 30 12:48:37.596: INFO: 
    Logging pods the apiserver thinks is on node pubt2-nks-for-dev1.dg.163.org before test
    Jan 30 12:48:37.601: INFO: calico-node-5xgvr from kube-system started at 2023-01-30 11:14:53 +0000 UTC (1 container statuses recorded)
    Jan 30 12:48:37.601: INFO: 	Container calico-node ready: true, restart count 0
    Jan 30 12:48:37.601: INFO: cleanlog-w9znh from kube-system started at 2023-01-30 11:02:22 +0000 UTC (1 container statuses recorded)
    Jan 30 12:48:37.601: INFO: 	Container cleanlog ready: true, restart count 0
    Jan 30 12:48:37.601: INFO: kube-proxy-fvt68 from kube-system started at 2023-01-30 11:02:22 +0000 UTC (1 container statuses recorded)
    Jan 30 12:48:37.601: INFO: 	Container kube-proxy ready: true, restart count 0
    Jan 30 12:48:37.601: INFO: node-local-dns-8jh9v from kube-system started at 2023-01-30 11:02:22 +0000 UTC (1 container statuses recorded)
    Jan 30 12:48:37.601: INFO: 	Container node-cache ready: true, restart count 5
    Jan 30 12:48:37.601: INFO: sonobuoy-systemd-logs-daemon-set-6d9e6fe3a8a74fcc-st9q5 from sonobuoy started at 2023-01-30 11:17:19 +0000 UTC (2 container statuses recorded)
    Jan 30 12:48:37.601: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 30 12:48:37.601: INFO: 	Container systemd-logs ready: true, restart count 0
    Jan 30 12:48:37.601: INFO: 
    Logging pods the apiserver thinks is on node pubt2-nks-for-dev3.dg.163.org before test
    Jan 30 12:48:37.606: INFO: calico-kube-controllers-568668f974-5gtkq from kube-system started at 2023-01-30 09:59:07 +0000 UTC (1 container statuses recorded)
    Jan 30 12:48:37.606: INFO: 	Container calico-kube-controllers ready: true, restart count 0
    Jan 30 12:48:37.606: INFO: calico-node-h7jwg from kube-system started at 2023-01-30 11:14:53 +0000 UTC (1 container statuses recorded)
    Jan 30 12:48:37.606: INFO: 	Container calico-node ready: true, restart count 0
    Jan 30 12:48:37.606: INFO: cleanlog-crksz from kube-system started at 2023-01-30 09:56:18 +0000 UTC (1 container statuses recorded)
    Jan 30 12:48:37.606: INFO: 	Container cleanlog ready: true, restart count 0
    Jan 30 12:48:37.606: INFO: coredns-5bf7dfc67-2gl8f from kube-system started at 2023-01-30 10:08:32 +0000 UTC (1 container statuses recorded)
    Jan 30 12:48:37.606: INFO: 	Container coredns ready: true, restart count 0
    Jan 30 12:48:37.606: INFO: coredns-5bf7dfc67-92sjx from kube-system started at 2023-01-30 12:46:12 +0000 UTC (1 container statuses recorded)
    Jan 30 12:48:37.606: INFO: 	Container coredns ready: true, restart count 0
    Jan 30 12:48:37.606: INFO: etcd-main-pubt2-nks-for-dev3.dg.163.org from kube-system started at 2023-01-30 09:56:17 +0000 UTC (1 container statuses recorded)
    Jan 30 12:48:37.606: INFO: 	Container etcd ready: true, restart count 1
    Jan 30 12:48:37.606: INFO: kube-apiserver-pubt2-nks-for-dev3.dg.163.org from kube-system started at 2023-01-30 09:56:18 +0000 UTC (1 container statuses recorded)
    Jan 30 12:48:37.606: INFO: 	Container kube-apiserver ready: true, restart count 1
    Jan 30 12:48:37.606: INFO: kube-controller-manager-pubt2-nks-for-dev3.dg.163.org from kube-system started at 2023-01-30 09:56:18 +0000 UTC (1 container statuses recorded)
    Jan 30 12:48:37.606: INFO: 	Container kube-controller-manager ready: true, restart count 1
    Jan 30 12:48:37.606: INFO: kube-proxy-br2mf from kube-system started at 2023-01-30 09:58:40 +0000 UTC (1 container statuses recorded)
    Jan 30 12:48:37.606: INFO: 	Container kube-proxy ready: true, restart count 0
    Jan 30 12:48:37.606: INFO: kube-scheduler-pubt2-nks-for-dev3.dg.163.org from kube-system started at 2023-01-30 09:56:17 +0000 UTC (1 container statuses recorded)
    Jan 30 12:48:37.606: INFO: 	Container kube-scheduler ready: true, restart count 1
    Jan 30 12:48:37.606: INFO: node-local-dns-txwnd from kube-system started at 2023-01-30 10:09:12 +0000 UTC (1 container statuses recorded)
    Jan 30 12:48:37.606: INFO: 	Container node-cache ready: true, restart count 0
    Jan 30 12:48:37.606: INFO: sonobuoy from sonobuoy started at 2023-01-30 11:17:15 +0000 UTC (1 container statuses recorded)
    Jan 30 12:48:37.606: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Jan 30 12:48:37.606: INFO: sonobuoy-e2e-job-a0572830995342a7 from sonobuoy started at 2023-01-30 11:17:19 +0000 UTC (2 container statuses recorded)
    Jan 30 12:48:37.606: INFO: 	Container e2e ready: true, restart count 0
    Jan 30 12:48:37.606: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 30 12:48:37.606: INFO: sonobuoy-systemd-logs-daemon-set-6d9e6fe3a8a74fcc-nkw5p from sonobuoy started at 2023-01-30 11:17:19 +0000 UTC (2 container statuses recorded)
    Jan 30 12:48:37.606: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 30 12:48:37.606: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates resource limits of pods that are allowed to run  [Conformance]
      test/e2e/scheduling/predicates.go:331
    STEP: verifying the node has the label node pubt2-nks-for-dev1.dg.163.org 01/30/23 12:48:37.621
    STEP: verifying the node has the label node pubt2-nks-for-dev3.dg.163.org 01/30/23 12:48:37.631
    Jan 30 12:48:37.652: INFO: Pod calico-kube-controllers-568668f974-5gtkq requesting resource cpu=0m on Node pubt2-nks-for-dev3.dg.163.org
    Jan 30 12:48:37.652: INFO: Pod calico-node-5xgvr requesting resource cpu=250m on Node pubt2-nks-for-dev1.dg.163.org
    Jan 30 12:48:37.652: INFO: Pod calico-node-h7jwg requesting resource cpu=250m on Node pubt2-nks-for-dev3.dg.163.org
    Jan 30 12:48:37.652: INFO: Pod cleanlog-crksz requesting resource cpu=100m on Node pubt2-nks-for-dev3.dg.163.org
    Jan 30 12:48:37.652: INFO: Pod cleanlog-w9znh requesting resource cpu=100m on Node pubt2-nks-for-dev1.dg.163.org
    Jan 30 12:48:37.652: INFO: Pod coredns-5bf7dfc67-2gl8f requesting resource cpu=100m on Node pubt2-nks-for-dev3.dg.163.org
    Jan 30 12:48:37.652: INFO: Pod coredns-5bf7dfc67-92sjx requesting resource cpu=100m on Node pubt2-nks-for-dev3.dg.163.org
    Jan 30 12:48:37.652: INFO: Pod etcd-main-pubt2-nks-for-dev3.dg.163.org requesting resource cpu=0m on Node pubt2-nks-for-dev3.dg.163.org
    Jan 30 12:48:37.652: INFO: Pod kube-apiserver-pubt2-nks-for-dev3.dg.163.org requesting resource cpu=0m on Node pubt2-nks-for-dev3.dg.163.org
    Jan 30 12:48:37.652: INFO: Pod kube-controller-manager-pubt2-nks-for-dev3.dg.163.org requesting resource cpu=0m on Node pubt2-nks-for-dev3.dg.163.org
    Jan 30 12:48:37.652: INFO: Pod kube-proxy-br2mf requesting resource cpu=200m on Node pubt2-nks-for-dev3.dg.163.org
    Jan 30 12:48:37.652: INFO: Pod kube-proxy-fvt68 requesting resource cpu=200m on Node pubt2-nks-for-dev1.dg.163.org
    Jan 30 12:48:37.652: INFO: Pod kube-scheduler-pubt2-nks-for-dev3.dg.163.org requesting resource cpu=0m on Node pubt2-nks-for-dev3.dg.163.org
    Jan 30 12:48:37.652: INFO: Pod node-local-dns-8jh9v requesting resource cpu=25m on Node pubt2-nks-for-dev1.dg.163.org
    Jan 30 12:48:37.652: INFO: Pod node-local-dns-txwnd requesting resource cpu=25m on Node pubt2-nks-for-dev3.dg.163.org
    Jan 30 12:48:37.652: INFO: Pod sonobuoy requesting resource cpu=0m on Node pubt2-nks-for-dev3.dg.163.org
    Jan 30 12:48:37.652: INFO: Pod sonobuoy-e2e-job-a0572830995342a7 requesting resource cpu=0m on Node pubt2-nks-for-dev3.dg.163.org
    Jan 30 12:48:37.652: INFO: Pod sonobuoy-systemd-logs-daemon-set-6d9e6fe3a8a74fcc-nkw5p requesting resource cpu=0m on Node pubt2-nks-for-dev3.dg.163.org
    Jan 30 12:48:37.652: INFO: Pod sonobuoy-systemd-logs-daemon-set-6d9e6fe3a8a74fcc-st9q5 requesting resource cpu=0m on Node pubt2-nks-for-dev1.dg.163.org
    STEP: Starting Pods to consume most of the cluster CPU. 01/30/23 12:48:37.652
    Jan 30 12:48:37.652: INFO: Creating a pod which consumes cpu=38097m on Node pubt2-nks-for-dev1.dg.163.org
    Jan 30 12:48:37.657: INFO: Creating a pod which consumes cpu=37957m on Node pubt2-nks-for-dev3.dg.163.org
    Jan 30 12:48:37.661: INFO: Waiting up to 5m0s for pod "filler-pod-1d7bf8f4-ab83-48bd-90ce-5c3304656615" in namespace "sched-pred-1026" to be "running"
    Jan 30 12:48:37.663: INFO: Pod "filler-pod-1d7bf8f4-ab83-48bd-90ce-5c3304656615": Phase="Pending", Reason="", readiness=false. Elapsed: 1.809575ms
    Jan 30 12:48:39.666: INFO: Pod "filler-pod-1d7bf8f4-ab83-48bd-90ce-5c3304656615": Phase="Running", Reason="", readiness=true. Elapsed: 2.004927639s
    Jan 30 12:48:39.666: INFO: Pod "filler-pod-1d7bf8f4-ab83-48bd-90ce-5c3304656615" satisfied condition "running"
    Jan 30 12:48:39.666: INFO: Waiting up to 5m0s for pod "filler-pod-cf71ab00-4b0f-4581-94c1-64d5077d8d60" in namespace "sched-pred-1026" to be "running"
    Jan 30 12:48:39.667: INFO: Pod "filler-pod-cf71ab00-4b0f-4581-94c1-64d5077d8d60": Phase="Pending", Reason="", readiness=false. Elapsed: 1.642509ms
    Jan 30 12:48:41.670: INFO: Pod "filler-pod-cf71ab00-4b0f-4581-94c1-64d5077d8d60": Phase="Running", Reason="", readiness=true. Elapsed: 2.00470222s
    Jan 30 12:48:41.670: INFO: Pod "filler-pod-cf71ab00-4b0f-4581-94c1-64d5077d8d60" satisfied condition "running"
    STEP: Creating another pod that requires unavailable amount of CPU. 01/30/23 12:48:41.67
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-1d7bf8f4-ab83-48bd-90ce-5c3304656615.173f1709e4815838], Reason = [Scheduled], Message = [Successfully assigned sched-pred-1026/filler-pod-1d7bf8f4-ab83-48bd-90ce-5c3304656615 to pubt2-nks-for-dev1.dg.163.org] 01/30/23 12:48:41.673
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-1d7bf8f4-ab83-48bd-90ce-5c3304656615.173f170a179cc77f], Reason = [Pulled], Message = [Container image "harbor.cloud.netease.com/qzprod-k8s/e2e/pause:3.9" already present on machine] 01/30/23 12:48:41.673
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-1d7bf8f4-ab83-48bd-90ce-5c3304656615.173f170a1ae28426], Reason = [Created], Message = [Created container filler-pod-1d7bf8f4-ab83-48bd-90ce-5c3304656615] 01/30/23 12:48:41.673
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-1d7bf8f4-ab83-48bd-90ce-5c3304656615.173f170a24dbea25], Reason = [Started], Message = [Started container filler-pod-1d7bf8f4-ab83-48bd-90ce-5c3304656615] 01/30/23 12:48:41.673
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-cf71ab00-4b0f-4581-94c1-64d5077d8d60.173f1709e4adc015], Reason = [Scheduled], Message = [Successfully assigned sched-pred-1026/filler-pod-cf71ab00-4b0f-4581-94c1-64d5077d8d60 to pubt2-nks-for-dev3.dg.163.org] 01/30/23 12:48:41.673
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-cf71ab00-4b0f-4581-94c1-64d5077d8d60.173f170a18877751], Reason = [Pulled], Message = [Container image "harbor.cloud.netease.com/qzprod-k8s/e2e/pause:3.9" already present on machine] 01/30/23 12:48:41.673
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-cf71ab00-4b0f-4581-94c1-64d5077d8d60.173f170a1e306348], Reason = [Created], Message = [Created container filler-pod-cf71ab00-4b0f-4581-94c1-64d5077d8d60] 01/30/23 12:48:41.673
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-cf71ab00-4b0f-4581-94c1-64d5077d8d60.173f170a29f2535e], Reason = [Started], Message = [Started container filler-pod-cf71ab00-4b0f-4581-94c1-64d5077d8d60] 01/30/23 12:48:41.673
    STEP: Considering event: 
    Type = [Warning], Name = [additional-pod.173f170ad3e8196e], Reason = [FailedScheduling], Message = [0/2 nodes are available: 2 Insufficient cpu. preemption: 0/2 nodes are available: 2 No preemption victims found for incoming pod..] 01/30/23 12:48:41.679
    STEP: removing the label node off the node pubt2-nks-for-dev1.dg.163.org 01/30/23 12:48:42.68
    STEP: verifying the node doesn't have the label node 01/30/23 12:48:42.691
    STEP: removing the label node off the node pubt2-nks-for-dev3.dg.163.org 01/30/23 12:48:42.693
    STEP: verifying the node doesn't have the label node 01/30/23 12:48:42.701
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan 30 12:48:42.703: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:88
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-pred-1026" for this suite. 01/30/23 12:48:42.706
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 12:48:42.708
Jan 30 12:48:42.709: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename gc 01/30/23 12:48:42.709
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:48:42.716
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:48:42.718
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491
STEP: create the deployment 01/30/23 12:48:42.72
STEP: Wait for the Deployment to create new ReplicaSet 01/30/23 12:48:42.722
STEP: delete the deployment 01/30/23 12:48:43.227
STEP: wait for all rs to be garbage collected 01/30/23 12:48:43.231
STEP: expected 0 rs, got 1 rs 01/30/23 12:48:43.234
STEP: expected 0 pods, got 2 pods 01/30/23 12:48:43.236
STEP: Gathering metrics 01/30/23 12:48:43.742
Jan 30 12:48:43.762: INFO: Waiting up to 5m0s for pod "kube-controller-manager-pubt2-nks-for-dev3.dg.163.org" in namespace "kube-system" to be "running and ready"
Jan 30 12:48:43.764: INFO: Pod "kube-controller-manager-pubt2-nks-for-dev3.dg.163.org": Phase="Running", Reason="", readiness=true. Elapsed: 1.76773ms
Jan 30 12:48:43.764: INFO: The phase of Pod kube-controller-manager-pubt2-nks-for-dev3.dg.163.org is Running (Ready = true)
Jan 30 12:48:43.764: INFO: Pod "kube-controller-manager-pubt2-nks-for-dev3.dg.163.org" satisfied condition "running and ready"
Jan 30 12:48:43.841: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Jan 30 12:48:43.841: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-2337" for this suite. 01/30/23 12:48:43.844
------------------------------
• [1.138 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 12:48:42.708
    Jan 30 12:48:42.709: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename gc 01/30/23 12:48:42.709
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:48:42.716
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:48:42.718
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete RS created by deployment when not orphaning [Conformance]
      test/e2e/apimachinery/garbage_collector.go:491
    STEP: create the deployment 01/30/23 12:48:42.72
    STEP: Wait for the Deployment to create new ReplicaSet 01/30/23 12:48:42.722
    STEP: delete the deployment 01/30/23 12:48:43.227
    STEP: wait for all rs to be garbage collected 01/30/23 12:48:43.231
    STEP: expected 0 rs, got 1 rs 01/30/23 12:48:43.234
    STEP: expected 0 pods, got 2 pods 01/30/23 12:48:43.236
    STEP: Gathering metrics 01/30/23 12:48:43.742
    Jan 30 12:48:43.762: INFO: Waiting up to 5m0s for pod "kube-controller-manager-pubt2-nks-for-dev3.dg.163.org" in namespace "kube-system" to be "running and ready"
    Jan 30 12:48:43.764: INFO: Pod "kube-controller-manager-pubt2-nks-for-dev3.dg.163.org": Phase="Running", Reason="", readiness=true. Elapsed: 1.76773ms
    Jan 30 12:48:43.764: INFO: The phase of Pod kube-controller-manager-pubt2-nks-for-dev3.dg.163.org is Running (Ready = true)
    Jan 30 12:48:43.764: INFO: Pod "kube-controller-manager-pubt2-nks-for-dev3.dg.163.org" satisfied condition "running and ready"
    Jan 30 12:48:43.841: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Jan 30 12:48:43.841: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-2337" for this suite. 01/30/23 12:48:43.844
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] Watchers
  should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 12:48:43.847
Jan 30 12:48:43.847: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename watch 01/30/23 12:48:43.848
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:48:43.854
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:48:43.857
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334
STEP: getting a starting resourceVersion 01/30/23 12:48:43.859
STEP: starting a background goroutine to produce watch events 01/30/23 12:48:43.861
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order 01/30/23 12:48:43.861
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Jan 30 12:48:46.650: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-6979" for this suite. 01/30/23 12:48:46.7
------------------------------
• [2.904 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 12:48:43.847
    Jan 30 12:48:43.847: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename watch 01/30/23 12:48:43.848
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:48:43.854
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:48:43.857
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should receive events on concurrent watches in same order [Conformance]
      test/e2e/apimachinery/watch.go:334
    STEP: getting a starting resourceVersion 01/30/23 12:48:43.859
    STEP: starting a background goroutine to produce watch events 01/30/23 12:48:43.861
    STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order 01/30/23 12:48:43.861
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Jan 30 12:48:46.650: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-6979" for this suite. 01/30/23 12:48:46.7
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo
  should create and stop a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:339
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 12:48:46.751
Jan 30 12:48:46.751: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename kubectl 01/30/23 12:48:46.752
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:48:46.758
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:48:46.76
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Update Demo
  test/e2e/kubectl/kubectl.go:326
[It] should create and stop a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:339
STEP: creating a replication controller 01/30/23 12:48:46.762
Jan 30 12:48:46.762: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=kubectl-2657 create -f -'
Jan 30 12:48:47.754: INFO: stderr: ""
Jan 30 12:48:47.754: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up. 01/30/23 12:48:47.754
Jan 30 12:48:47.754: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=kubectl-2657 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan 30 12:48:47.822: INFO: stderr: ""
Jan 30 12:48:47.822: INFO: stdout: "update-demo-nautilus-hmr7d update-demo-nautilus-jdshn "
Jan 30 12:48:47.822: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=kubectl-2657 get pods update-demo-nautilus-hmr7d -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 30 12:48:47.882: INFO: stderr: ""
Jan 30 12:48:47.882: INFO: stdout: ""
Jan 30 12:48:47.882: INFO: update-demo-nautilus-hmr7d is created but not running
Jan 30 12:48:52.883: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=kubectl-2657 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan 30 12:48:52.948: INFO: stderr: ""
Jan 30 12:48:52.948: INFO: stdout: "update-demo-nautilus-hmr7d update-demo-nautilus-jdshn "
Jan 30 12:48:52.948: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=kubectl-2657 get pods update-demo-nautilus-hmr7d -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 30 12:48:53.006: INFO: stderr: ""
Jan 30 12:48:53.006: INFO: stdout: "true"
Jan 30 12:48:53.006: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=kubectl-2657 get pods update-demo-nautilus-hmr7d -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan 30 12:48:53.064: INFO: stderr: ""
Jan 30 12:48:53.064: INFO: stdout: "harbor.cloud.netease.com/qzprod-k8s/e2e/nautilus:1.7"
Jan 30 12:48:53.064: INFO: validating pod update-demo-nautilus-hmr7d
Jan 30 12:48:53.067: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 30 12:48:53.067: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 30 12:48:53.067: INFO: update-demo-nautilus-hmr7d is verified up and running
Jan 30 12:48:53.067: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=kubectl-2657 get pods update-demo-nautilus-jdshn -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 30 12:48:53.122: INFO: stderr: ""
Jan 30 12:48:53.122: INFO: stdout: "true"
Jan 30 12:48:53.122: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=kubectl-2657 get pods update-demo-nautilus-jdshn -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan 30 12:48:53.179: INFO: stderr: ""
Jan 30 12:48:53.179: INFO: stdout: "harbor.cloud.netease.com/qzprod-k8s/e2e/nautilus:1.7"
Jan 30 12:48:53.179: INFO: validating pod update-demo-nautilus-jdshn
Jan 30 12:48:53.182: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 30 12:48:53.182: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 30 12:48:53.182: INFO: update-demo-nautilus-jdshn is verified up and running
STEP: using delete to clean up resources 01/30/23 12:48:53.182
Jan 30 12:48:53.182: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=kubectl-2657 delete --grace-period=0 --force -f -'
Jan 30 12:48:53.237: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 30 12:48:53.237: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Jan 30 12:48:53.237: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=kubectl-2657 get rc,svc -l name=update-demo --no-headers'
Jan 30 12:48:53.300: INFO: stderr: "No resources found in kubectl-2657 namespace.\n"
Jan 30 12:48:53.300: INFO: stdout: ""
Jan 30 12:48:53.300: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=kubectl-2657 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jan 30 12:48:53.359: INFO: stderr: ""
Jan 30 12:48:53.359: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jan 30 12:48:53.359: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-2657" for this suite. 01/30/23 12:48:53.362
------------------------------
• [SLOW TEST] [6.613 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Update Demo
  test/e2e/kubectl/kubectl.go:324
    should create and stop a replication controller  [Conformance]
    test/e2e/kubectl/kubectl.go:339

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 12:48:46.751
    Jan 30 12:48:46.751: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename kubectl 01/30/23 12:48:46.752
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:48:46.758
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:48:46.76
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Update Demo
      test/e2e/kubectl/kubectl.go:326
    [It] should create and stop a replication controller  [Conformance]
      test/e2e/kubectl/kubectl.go:339
    STEP: creating a replication controller 01/30/23 12:48:46.762
    Jan 30 12:48:46.762: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=kubectl-2657 create -f -'
    Jan 30 12:48:47.754: INFO: stderr: ""
    Jan 30 12:48:47.754: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 01/30/23 12:48:47.754
    Jan 30 12:48:47.754: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=kubectl-2657 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jan 30 12:48:47.822: INFO: stderr: ""
    Jan 30 12:48:47.822: INFO: stdout: "update-demo-nautilus-hmr7d update-demo-nautilus-jdshn "
    Jan 30 12:48:47.822: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=kubectl-2657 get pods update-demo-nautilus-hmr7d -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan 30 12:48:47.882: INFO: stderr: ""
    Jan 30 12:48:47.882: INFO: stdout: ""
    Jan 30 12:48:47.882: INFO: update-demo-nautilus-hmr7d is created but not running
    Jan 30 12:48:52.883: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=kubectl-2657 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jan 30 12:48:52.948: INFO: stderr: ""
    Jan 30 12:48:52.948: INFO: stdout: "update-demo-nautilus-hmr7d update-demo-nautilus-jdshn "
    Jan 30 12:48:52.948: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=kubectl-2657 get pods update-demo-nautilus-hmr7d -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan 30 12:48:53.006: INFO: stderr: ""
    Jan 30 12:48:53.006: INFO: stdout: "true"
    Jan 30 12:48:53.006: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=kubectl-2657 get pods update-demo-nautilus-hmr7d -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jan 30 12:48:53.064: INFO: stderr: ""
    Jan 30 12:48:53.064: INFO: stdout: "harbor.cloud.netease.com/qzprod-k8s/e2e/nautilus:1.7"
    Jan 30 12:48:53.064: INFO: validating pod update-demo-nautilus-hmr7d
    Jan 30 12:48:53.067: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jan 30 12:48:53.067: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jan 30 12:48:53.067: INFO: update-demo-nautilus-hmr7d is verified up and running
    Jan 30 12:48:53.067: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=kubectl-2657 get pods update-demo-nautilus-jdshn -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan 30 12:48:53.122: INFO: stderr: ""
    Jan 30 12:48:53.122: INFO: stdout: "true"
    Jan 30 12:48:53.122: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=kubectl-2657 get pods update-demo-nautilus-jdshn -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jan 30 12:48:53.179: INFO: stderr: ""
    Jan 30 12:48:53.179: INFO: stdout: "harbor.cloud.netease.com/qzprod-k8s/e2e/nautilus:1.7"
    Jan 30 12:48:53.179: INFO: validating pod update-demo-nautilus-jdshn
    Jan 30 12:48:53.182: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jan 30 12:48:53.182: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jan 30 12:48:53.182: INFO: update-demo-nautilus-jdshn is verified up and running
    STEP: using delete to clean up resources 01/30/23 12:48:53.182
    Jan 30 12:48:53.182: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=kubectl-2657 delete --grace-period=0 --force -f -'
    Jan 30 12:48:53.237: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jan 30 12:48:53.237: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
    Jan 30 12:48:53.237: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=kubectl-2657 get rc,svc -l name=update-demo --no-headers'
    Jan 30 12:48:53.300: INFO: stderr: "No resources found in kubectl-2657 namespace.\n"
    Jan 30 12:48:53.300: INFO: stdout: ""
    Jan 30 12:48:53.300: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=kubectl-2657 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Jan 30 12:48:53.359: INFO: stderr: ""
    Jan 30 12:48:53.359: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jan 30 12:48:53.359: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-2657" for this suite. 01/30/23 12:48:53.362
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1477
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 12:48:53.365
Jan 30 12:48:53.365: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename services 01/30/23 12:48:53.366
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:48:53.372
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:48:53.374
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1477
STEP: creating a service externalname-service with the type=ExternalName in namespace services-8456 01/30/23 12:48:53.376
STEP: changing the ExternalName service to type=NodePort 01/30/23 12:48:53.378
STEP: creating replication controller externalname-service in namespace services-8456 01/30/23 12:48:53.385
I0130 12:48:53.388590      23 runners.go:193] Created replication controller with name: externalname-service, namespace: services-8456, replica count: 2
I0130 12:48:56.439510      23 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 30 12:48:56.439: INFO: Creating new exec pod
Jan 30 12:48:56.442: INFO: Waiting up to 5m0s for pod "execpodsx2k2" in namespace "services-8456" to be "running"
Jan 30 12:48:56.443: INFO: Pod "execpodsx2k2": Phase="Pending", Reason="", readiness=false. Elapsed: 1.700234ms
Jan 30 12:48:58.446: INFO: Pod "execpodsx2k2": Phase="Running", Reason="", readiness=true. Elapsed: 2.004487882s
Jan 30 12:48:58.446: INFO: Pod "execpodsx2k2" satisfied condition "running"
Jan 30 12:48:59.449: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=services-8456 exec execpodsx2k2 -- /bin/sh -x -c nc -v -z -w 2 externalname-service 80'
Jan 30 12:48:59.597: INFO: stderr: "+ nc -v -z -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Jan 30 12:48:59.597: INFO: stdout: ""
Jan 30 12:48:59.597: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=services-8456 exec execpodsx2k2 -- /bin/sh -x -c nc -v -z -w 2 10.178.82.155 80'
Jan 30 12:48:59.723: INFO: stderr: "+ nc -v -z -w 2 10.178.82.155 80\nConnection to 10.178.82.155 80 port [tcp/http] succeeded!\n"
Jan 30 12:48:59.723: INFO: stdout: ""
Jan 30 12:48:59.723: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=services-8456 exec execpodsx2k2 -- /bin/sh -x -c nc -v -z -w 2 10.182.0.82 30402'
Jan 30 12:48:59.851: INFO: stderr: "+ nc -v -z -w 2 10.182.0.82 30402\nConnection to 10.182.0.82 30402 port [tcp/*] succeeded!\n"
Jan 30 12:48:59.851: INFO: stdout: ""
Jan 30 12:48:59.851: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=services-8456 exec execpodsx2k2 -- /bin/sh -x -c nc -v -z -w 2 10.182.0.84 30402'
Jan 30 12:48:59.979: INFO: stderr: "+ nc -v -z -w 2 10.182.0.84 30402\nConnection to 10.182.0.84 30402 port [tcp/*] succeeded!\n"
Jan 30 12:48:59.979: INFO: stdout: ""
Jan 30 12:48:59.979: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jan 30 12:48:59.986: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-8456" for this suite. 01/30/23 12:48:59.988
------------------------------
• [SLOW TEST] [6.625 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1477

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 12:48:53.365
    Jan 30 12:48:53.365: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename services 01/30/23 12:48:53.366
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:48:53.372
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:48:53.374
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to change the type from ExternalName to NodePort [Conformance]
      test/e2e/network/service.go:1477
    STEP: creating a service externalname-service with the type=ExternalName in namespace services-8456 01/30/23 12:48:53.376
    STEP: changing the ExternalName service to type=NodePort 01/30/23 12:48:53.378
    STEP: creating replication controller externalname-service in namespace services-8456 01/30/23 12:48:53.385
    I0130 12:48:53.388590      23 runners.go:193] Created replication controller with name: externalname-service, namespace: services-8456, replica count: 2
    I0130 12:48:56.439510      23 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan 30 12:48:56.439: INFO: Creating new exec pod
    Jan 30 12:48:56.442: INFO: Waiting up to 5m0s for pod "execpodsx2k2" in namespace "services-8456" to be "running"
    Jan 30 12:48:56.443: INFO: Pod "execpodsx2k2": Phase="Pending", Reason="", readiness=false. Elapsed: 1.700234ms
    Jan 30 12:48:58.446: INFO: Pod "execpodsx2k2": Phase="Running", Reason="", readiness=true. Elapsed: 2.004487882s
    Jan 30 12:48:58.446: INFO: Pod "execpodsx2k2" satisfied condition "running"
    Jan 30 12:48:59.449: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=services-8456 exec execpodsx2k2 -- /bin/sh -x -c nc -v -z -w 2 externalname-service 80'
    Jan 30 12:48:59.597: INFO: stderr: "+ nc -v -z -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Jan 30 12:48:59.597: INFO: stdout: ""
    Jan 30 12:48:59.597: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=services-8456 exec execpodsx2k2 -- /bin/sh -x -c nc -v -z -w 2 10.178.82.155 80'
    Jan 30 12:48:59.723: INFO: stderr: "+ nc -v -z -w 2 10.178.82.155 80\nConnection to 10.178.82.155 80 port [tcp/http] succeeded!\n"
    Jan 30 12:48:59.723: INFO: stdout: ""
    Jan 30 12:48:59.723: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=services-8456 exec execpodsx2k2 -- /bin/sh -x -c nc -v -z -w 2 10.182.0.82 30402'
    Jan 30 12:48:59.851: INFO: stderr: "+ nc -v -z -w 2 10.182.0.82 30402\nConnection to 10.182.0.82 30402 port [tcp/*] succeeded!\n"
    Jan 30 12:48:59.851: INFO: stdout: ""
    Jan 30 12:48:59.851: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=services-8456 exec execpodsx2k2 -- /bin/sh -x -c nc -v -z -w 2 10.182.0.84 30402'
    Jan 30 12:48:59.979: INFO: stderr: "+ nc -v -z -w 2 10.182.0.84 30402\nConnection to 10.182.0.84 30402 port [tcp/*] succeeded!\n"
    Jan 30 12:48:59.979: INFO: stdout: ""
    Jan 30 12:48:59.979: INFO: Cleaning up the ExternalName to NodePort test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jan 30 12:48:59.986: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-8456" for this suite. 01/30/23 12:48:59.988
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:177
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 12:48:59.992
Jan 30 12:48:59.992: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename init-container 01/30/23 12:48:59.993
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:49:00
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:49:00.002
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:165
[It] should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:177
STEP: creating the pod 01/30/23 12:49:00.005
Jan 30 12:49:00.005: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Jan 30 12:49:05.389: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "init-container-2941" for this suite. 01/30/23 12:49:05.391
------------------------------
• [SLOW TEST] [5.402 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:177

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 12:48:59.992
    Jan 30 12:48:59.992: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename init-container 01/30/23 12:48:59.993
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:49:00
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:49:00.002
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:165
    [It] should invoke init containers on a RestartNever pod [Conformance]
      test/e2e/common/node/init_container.go:177
    STEP: creating the pod 01/30/23 12:49:00.005
    Jan 30 12:49:00.005: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Jan 30 12:49:05.389: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "init-container-2941" for this suite. 01/30/23 12:49:05.391
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should get and update a ReplicationController scale [Conformance]
  test/e2e/apps/rc.go:402
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 12:49:05.395
Jan 30 12:49:05.395: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename replication-controller 01/30/23 12:49:05.395
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:49:05.401
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:49:05.403
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should get and update a ReplicationController scale [Conformance]
  test/e2e/apps/rc.go:402
STEP: Creating ReplicationController "e2e-rc-npm2l" 01/30/23 12:49:05.405
Jan 30 12:49:05.407: INFO: Get Replication Controller "e2e-rc-npm2l" to confirm replicas
Jan 30 12:49:06.409: INFO: Get Replication Controller "e2e-rc-npm2l" to confirm replicas
Jan 30 12:49:06.410: INFO: Found 1 replicas for "e2e-rc-npm2l" replication controller
STEP: Getting scale subresource for ReplicationController "e2e-rc-npm2l" 01/30/23 12:49:06.41
STEP: Updating a scale subresource 01/30/23 12:49:06.412
STEP: Verifying replicas where modified for replication controller "e2e-rc-npm2l" 01/30/23 12:49:06.415
Jan 30 12:49:06.415: INFO: Get Replication Controller "e2e-rc-npm2l" to confirm replicas
Jan 30 12:49:07.418: INFO: Get Replication Controller "e2e-rc-npm2l" to confirm replicas
Jan 30 12:49:07.420: INFO: Found 2 replicas for "e2e-rc-npm2l" replication controller
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Jan 30 12:49:07.420: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-9050" for this suite. 01/30/23 12:49:07.422
------------------------------
• [2.030 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should get and update a ReplicationController scale [Conformance]
  test/e2e/apps/rc.go:402

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 12:49:05.395
    Jan 30 12:49:05.395: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename replication-controller 01/30/23 12:49:05.395
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:49:05.401
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:49:05.403
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should get and update a ReplicationController scale [Conformance]
      test/e2e/apps/rc.go:402
    STEP: Creating ReplicationController "e2e-rc-npm2l" 01/30/23 12:49:05.405
    Jan 30 12:49:05.407: INFO: Get Replication Controller "e2e-rc-npm2l" to confirm replicas
    Jan 30 12:49:06.409: INFO: Get Replication Controller "e2e-rc-npm2l" to confirm replicas
    Jan 30 12:49:06.410: INFO: Found 1 replicas for "e2e-rc-npm2l" replication controller
    STEP: Getting scale subresource for ReplicationController "e2e-rc-npm2l" 01/30/23 12:49:06.41
    STEP: Updating a scale subresource 01/30/23 12:49:06.412
    STEP: Verifying replicas where modified for replication controller "e2e-rc-npm2l" 01/30/23 12:49:06.415
    Jan 30 12:49:06.415: INFO: Get Replication Controller "e2e-rc-npm2l" to confirm replicas
    Jan 30 12:49:07.418: INFO: Get Replication Controller "e2e-rc-npm2l" to confirm replicas
    Jan 30 12:49:07.420: INFO: Found 2 replicas for "e2e-rc-npm2l" replication controller
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Jan 30 12:49:07.420: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-9050" for this suite. 01/30/23 12:49:07.422
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with downward pod [Conformance]
  test/e2e/storage/subpath.go:92
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 12:49:07.425
Jan 30 12:49:07.425: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename subpath 01/30/23 12:49:07.426
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:49:07.432
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:49:07.434
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 01/30/23 12:49:07.436
[It] should support subpaths with downward pod [Conformance]
  test/e2e/storage/subpath.go:92
STEP: Creating pod pod-subpath-test-downwardapi-98nd 01/30/23 12:49:07.44
STEP: Creating a pod to test atomic-volume-subpath 01/30/23 12:49:07.44
Jan 30 12:49:07.444: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-98nd" in namespace "subpath-6046" to be "Succeeded or Failed"
Jan 30 12:49:07.446: INFO: Pod "pod-subpath-test-downwardapi-98nd": Phase="Pending", Reason="", readiness=false. Elapsed: 1.556036ms
Jan 30 12:49:09.449: INFO: Pod "pod-subpath-test-downwardapi-98nd": Phase="Running", Reason="", readiness=true. Elapsed: 2.004233506s
Jan 30 12:49:11.449: INFO: Pod "pod-subpath-test-downwardapi-98nd": Phase="Running", Reason="", readiness=true. Elapsed: 4.004065799s
Jan 30 12:49:13.449: INFO: Pod "pod-subpath-test-downwardapi-98nd": Phase="Running", Reason="", readiness=true. Elapsed: 6.004087086s
Jan 30 12:49:15.449: INFO: Pod "pod-subpath-test-downwardapi-98nd": Phase="Running", Reason="", readiness=true. Elapsed: 8.004334222s
Jan 30 12:49:17.449: INFO: Pod "pod-subpath-test-downwardapi-98nd": Phase="Running", Reason="", readiness=true. Elapsed: 10.005006775s
Jan 30 12:49:19.449: INFO: Pod "pod-subpath-test-downwardapi-98nd": Phase="Running", Reason="", readiness=true. Elapsed: 12.004541383s
Jan 30 12:49:21.450: INFO: Pod "pod-subpath-test-downwardapi-98nd": Phase="Running", Reason="", readiness=true. Elapsed: 14.005508708s
Jan 30 12:49:23.450: INFO: Pod "pod-subpath-test-downwardapi-98nd": Phase="Running", Reason="", readiness=true. Elapsed: 16.005288566s
Jan 30 12:49:25.449: INFO: Pod "pod-subpath-test-downwardapi-98nd": Phase="Running", Reason="", readiness=true. Elapsed: 18.004748705s
Jan 30 12:49:27.449: INFO: Pod "pod-subpath-test-downwardapi-98nd": Phase="Running", Reason="", readiness=true. Elapsed: 20.004136712s
Jan 30 12:49:29.448: INFO: Pod "pod-subpath-test-downwardapi-98nd": Phase="Running", Reason="", readiness=true. Elapsed: 22.004017571s
Jan 30 12:49:31.449: INFO: Pod "pod-subpath-test-downwardapi-98nd": Phase="Running", Reason="", readiness=false. Elapsed: 24.005018324s
Jan 30 12:49:33.449: INFO: Pod "pod-subpath-test-downwardapi-98nd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.004295876s
STEP: Saw pod success 01/30/23 12:49:33.449
Jan 30 12:49:33.449: INFO: Pod "pod-subpath-test-downwardapi-98nd" satisfied condition "Succeeded or Failed"
Jan 30 12:49:33.451: INFO: Trying to get logs from node pubt2-nks-for-dev1.dg.163.org pod pod-subpath-test-downwardapi-98nd container test-container-subpath-downwardapi-98nd: <nil>
STEP: delete the pod 01/30/23 12:49:33.457
Jan 30 12:49:33.462: INFO: Waiting for pod pod-subpath-test-downwardapi-98nd to disappear
Jan 30 12:49:33.463: INFO: Pod pod-subpath-test-downwardapi-98nd no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-98nd 01/30/23 12:49:33.463
Jan 30 12:49:33.463: INFO: Deleting pod "pod-subpath-test-downwardapi-98nd" in namespace "subpath-6046"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Jan 30 12:49:33.465: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-6046" for this suite. 01/30/23 12:49:33.467
------------------------------
• [SLOW TEST] [26.044 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with downward pod [Conformance]
    test/e2e/storage/subpath.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 12:49:07.425
    Jan 30 12:49:07.425: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename subpath 01/30/23 12:49:07.426
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:49:07.432
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:49:07.434
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 01/30/23 12:49:07.436
    [It] should support subpaths with downward pod [Conformance]
      test/e2e/storage/subpath.go:92
    STEP: Creating pod pod-subpath-test-downwardapi-98nd 01/30/23 12:49:07.44
    STEP: Creating a pod to test atomic-volume-subpath 01/30/23 12:49:07.44
    Jan 30 12:49:07.444: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-98nd" in namespace "subpath-6046" to be "Succeeded or Failed"
    Jan 30 12:49:07.446: INFO: Pod "pod-subpath-test-downwardapi-98nd": Phase="Pending", Reason="", readiness=false. Elapsed: 1.556036ms
    Jan 30 12:49:09.449: INFO: Pod "pod-subpath-test-downwardapi-98nd": Phase="Running", Reason="", readiness=true. Elapsed: 2.004233506s
    Jan 30 12:49:11.449: INFO: Pod "pod-subpath-test-downwardapi-98nd": Phase="Running", Reason="", readiness=true. Elapsed: 4.004065799s
    Jan 30 12:49:13.449: INFO: Pod "pod-subpath-test-downwardapi-98nd": Phase="Running", Reason="", readiness=true. Elapsed: 6.004087086s
    Jan 30 12:49:15.449: INFO: Pod "pod-subpath-test-downwardapi-98nd": Phase="Running", Reason="", readiness=true. Elapsed: 8.004334222s
    Jan 30 12:49:17.449: INFO: Pod "pod-subpath-test-downwardapi-98nd": Phase="Running", Reason="", readiness=true. Elapsed: 10.005006775s
    Jan 30 12:49:19.449: INFO: Pod "pod-subpath-test-downwardapi-98nd": Phase="Running", Reason="", readiness=true. Elapsed: 12.004541383s
    Jan 30 12:49:21.450: INFO: Pod "pod-subpath-test-downwardapi-98nd": Phase="Running", Reason="", readiness=true. Elapsed: 14.005508708s
    Jan 30 12:49:23.450: INFO: Pod "pod-subpath-test-downwardapi-98nd": Phase="Running", Reason="", readiness=true. Elapsed: 16.005288566s
    Jan 30 12:49:25.449: INFO: Pod "pod-subpath-test-downwardapi-98nd": Phase="Running", Reason="", readiness=true. Elapsed: 18.004748705s
    Jan 30 12:49:27.449: INFO: Pod "pod-subpath-test-downwardapi-98nd": Phase="Running", Reason="", readiness=true. Elapsed: 20.004136712s
    Jan 30 12:49:29.448: INFO: Pod "pod-subpath-test-downwardapi-98nd": Phase="Running", Reason="", readiness=true. Elapsed: 22.004017571s
    Jan 30 12:49:31.449: INFO: Pod "pod-subpath-test-downwardapi-98nd": Phase="Running", Reason="", readiness=false. Elapsed: 24.005018324s
    Jan 30 12:49:33.449: INFO: Pod "pod-subpath-test-downwardapi-98nd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.004295876s
    STEP: Saw pod success 01/30/23 12:49:33.449
    Jan 30 12:49:33.449: INFO: Pod "pod-subpath-test-downwardapi-98nd" satisfied condition "Succeeded or Failed"
    Jan 30 12:49:33.451: INFO: Trying to get logs from node pubt2-nks-for-dev1.dg.163.org pod pod-subpath-test-downwardapi-98nd container test-container-subpath-downwardapi-98nd: <nil>
    STEP: delete the pod 01/30/23 12:49:33.457
    Jan 30 12:49:33.462: INFO: Waiting for pod pod-subpath-test-downwardapi-98nd to disappear
    Jan 30 12:49:33.463: INFO: Pod pod-subpath-test-downwardapi-98nd no longer exists
    STEP: Deleting pod pod-subpath-test-downwardapi-98nd 01/30/23 12:49:33.463
    Jan 30 12:49:33.463: INFO: Deleting pod "pod-subpath-test-downwardapi-98nd" in namespace "subpath-6046"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Jan 30 12:49:33.465: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-6046" for this suite. 01/30/23 12:49:33.467
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:803
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 12:49:33.469
Jan 30 12:49:33.469: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename resourcequota 01/30/23 12:49:33.47
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:49:33.476
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:49:33.478
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:803
STEP: Creating a ResourceQuota with best effort scope 01/30/23 12:49:33.48
STEP: Ensuring ResourceQuota status is calculated 01/30/23 12:49:33.482
STEP: Creating a ResourceQuota with not best effort scope 01/30/23 12:49:35.484
STEP: Ensuring ResourceQuota status is calculated 01/30/23 12:49:35.486
STEP: Creating a best-effort pod 01/30/23 12:49:37.489
STEP: Ensuring resource quota with best effort scope captures the pod usage 01/30/23 12:49:37.496
STEP: Ensuring resource quota with not best effort ignored the pod usage 01/30/23 12:49:39.499
STEP: Deleting the pod 01/30/23 12:49:41.501
STEP: Ensuring resource quota status released the pod usage 01/30/23 12:49:41.505
STEP: Creating a not best-effort pod 01/30/23 12:49:43.508
STEP: Ensuring resource quota with not best effort scope captures the pod usage 01/30/23 12:49:43.514
STEP: Ensuring resource quota with best effort scope ignored the pod usage 01/30/23 12:49:45.517
STEP: Deleting the pod 01/30/23 12:49:47.521
STEP: Ensuring resource quota status released the pod usage 01/30/23 12:49:47.525
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Jan 30 12:49:49.528: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-2944" for this suite. 01/30/23 12:49:49.53
------------------------------
• [SLOW TEST] [16.063 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:803

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 12:49:33.469
    Jan 30 12:49:33.469: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename resourcequota 01/30/23 12:49:33.47
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:49:33.476
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:49:33.478
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should verify ResourceQuota with best effort scope. [Conformance]
      test/e2e/apimachinery/resource_quota.go:803
    STEP: Creating a ResourceQuota with best effort scope 01/30/23 12:49:33.48
    STEP: Ensuring ResourceQuota status is calculated 01/30/23 12:49:33.482
    STEP: Creating a ResourceQuota with not best effort scope 01/30/23 12:49:35.484
    STEP: Ensuring ResourceQuota status is calculated 01/30/23 12:49:35.486
    STEP: Creating a best-effort pod 01/30/23 12:49:37.489
    STEP: Ensuring resource quota with best effort scope captures the pod usage 01/30/23 12:49:37.496
    STEP: Ensuring resource quota with not best effort ignored the pod usage 01/30/23 12:49:39.499
    STEP: Deleting the pod 01/30/23 12:49:41.501
    STEP: Ensuring resource quota status released the pod usage 01/30/23 12:49:41.505
    STEP: Creating a not best-effort pod 01/30/23 12:49:43.508
    STEP: Ensuring resource quota with not best effort scope captures the pod usage 01/30/23 12:49:43.514
    STEP: Ensuring resource quota with best effort scope ignored the pod usage 01/30/23 12:49:45.517
    STEP: Deleting the pod 01/30/23 12:49:47.521
    STEP: Ensuring resource quota status released the pod usage 01/30/23 12:49:47.525
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Jan 30 12:49:49.528: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-2944" for this suite. 01/30/23 12:49:49.53
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency
  should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59
[BeforeEach] [sig-network] Service endpoints latency
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 12:49:49.533
Jan 30 12:49:49.533: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename svc-latency 01/30/23 12:49:49.534
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:49:49.541
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:49:49.542
[BeforeEach] [sig-network] Service endpoints latency
  test/e2e/framework/metrics/init/init.go:31
[It] should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59
Jan 30 12:49:49.544: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: creating replication controller svc-latency-rc in namespace svc-latency-7939 01/30/23 12:49:49.545
I0130 12:49:49.548401      23 runners.go:193] Created replication controller with name: svc-latency-rc, namespace: svc-latency-7939, replica count: 1
I0130 12:49:50.599666      23 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0130 12:49:51.600779      23 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 30 12:49:51.705: INFO: Created: latency-svc-t8wzq
Jan 30 12:49:51.709: INFO: Got endpoints: latency-svc-t8wzq [7.6509ms]
Jan 30 12:49:51.713: INFO: Created: latency-svc-lb7xd
Jan 30 12:49:51.714: INFO: Created: latency-svc-75bxp
Jan 30 12:49:51.715: INFO: Got endpoints: latency-svc-lb7xd [6.767295ms]
Jan 30 12:49:51.716: INFO: Created: latency-svc-55v5m
Jan 30 12:49:51.717: INFO: Got endpoints: latency-svc-75bxp [8.090806ms]
Jan 30 12:49:51.717: INFO: Created: latency-svc-vzjcs
Jan 30 12:49:51.718: INFO: Got endpoints: latency-svc-55v5m [9.685268ms]
Jan 30 12:49:51.719: INFO: Created: latency-svc-lw9f8
Jan 30 12:49:51.719: INFO: Got endpoints: latency-svc-vzjcs [10.313103ms]
Jan 30 12:49:51.720: INFO: Created: latency-svc-2cdhz
Jan 30 12:49:51.720: INFO: Got endpoints: latency-svc-lw9f8 [11.633496ms]
Jan 30 12:49:51.721: INFO: Got endpoints: latency-svc-2cdhz [12.687279ms]
Jan 30 12:49:51.722: INFO: Created: latency-svc-shmkz
Jan 30 12:49:51.723: INFO: Created: latency-svc-nm79h
Jan 30 12:49:51.723: INFO: Got endpoints: latency-svc-shmkz [14.41055ms]
Jan 30 12:49:51.724: INFO: Created: latency-svc-nnkk6
Jan 30 12:49:51.725: INFO: Got endpoints: latency-svc-nm79h [16.001417ms]
Jan 30 12:49:51.725: INFO: Created: latency-svc-m9567
Jan 30 12:49:51.725: INFO: Got endpoints: latency-svc-nnkk6 [16.608688ms]
Jan 30 12:49:51.727: INFO: Created: latency-svc-sq984
Jan 30 12:49:51.727: INFO: Got endpoints: latency-svc-m9567 [18.402853ms]
Jan 30 12:49:51.728: INFO: Created: latency-svc-kgh88
Jan 30 12:49:51.729: INFO: Got endpoints: latency-svc-sq984 [20.088777ms]
Jan 30 12:49:51.729: INFO: Created: latency-svc-kcjld
Jan 30 12:49:51.729: INFO: Got endpoints: latency-svc-kgh88 [20.677963ms]
Jan 30 12:49:51.731: INFO: Created: latency-svc-hbfgk
Jan 30 12:49:51.732: INFO: Got endpoints: latency-svc-kcjld [22.800846ms]
Jan 30 12:49:51.732: INFO: Created: latency-svc-hljfd
Jan 30 12:49:51.733: INFO: Got endpoints: latency-svc-hbfgk [24.008317ms]
Jan 30 12:49:51.733: INFO: Created: latency-svc-6h66d
Jan 30 12:49:51.734: INFO: Got endpoints: latency-svc-hljfd [25.622332ms]
Jan 30 12:49:51.735: INFO: Created: latency-svc-99sh8
Jan 30 12:49:51.736: INFO: Got endpoints: latency-svc-6h66d [20.12793ms]
Jan 30 12:49:51.736: INFO: Created: latency-svc-cxvxn
Jan 30 12:49:51.737: INFO: Got endpoints: latency-svc-99sh8 [20.194472ms]
Jan 30 12:49:51.738: INFO: Got endpoints: latency-svc-cxvxn [19.639987ms]
Jan 30 12:49:51.738: INFO: Created: latency-svc-k2lfq
Jan 30 12:49:51.739: INFO: Created: latency-svc-hc68q
Jan 30 12:49:51.740: INFO: Got endpoints: latency-svc-k2lfq [20.625932ms]
Jan 30 12:49:51.741: INFO: Created: latency-svc-pmrp4
Jan 30 12:49:51.741: INFO: Got endpoints: latency-svc-hc68q [20.74081ms]
Jan 30 12:49:51.743: INFO: Got endpoints: latency-svc-pmrp4 [21.036972ms]
Jan 30 12:49:51.758: INFO: Created: latency-svc-vvtss
Jan 30 12:49:51.758: INFO: Created: latency-svc-v6hvm
Jan 30 12:49:51.759: INFO: Created: latency-svc-zp754
Jan 30 12:49:51.759: INFO: Created: latency-svc-8sfbg
Jan 30 12:49:51.759: INFO: Created: latency-svc-m2p4m
Jan 30 12:49:51.759: INFO: Created: latency-svc-4d4g6
Jan 30 12:49:51.759: INFO: Created: latency-svc-lcp2b
Jan 30 12:49:51.759: INFO: Created: latency-svc-6n4k4
Jan 30 12:49:51.759: INFO: Created: latency-svc-g7hzm
Jan 30 12:49:51.759: INFO: Created: latency-svc-pvq2f
Jan 30 12:49:51.759: INFO: Created: latency-svc-656rs
Jan 30 12:49:51.759: INFO: Created: latency-svc-klw9p
Jan 30 12:49:51.759: INFO: Created: latency-svc-zvxzf
Jan 30 12:49:51.759: INFO: Created: latency-svc-ng6hb
Jan 30 12:49:51.759: INFO: Created: latency-svc-ldgp4
Jan 30 12:49:51.761: INFO: Got endpoints: latency-svc-klw9p [33.749056ms]
Jan 30 12:49:51.761: INFO: Got endpoints: latency-svc-zp754 [19.866747ms]
Jan 30 12:49:51.761: INFO: Got endpoints: latency-svc-zvxzf [35.521553ms]
Jan 30 12:49:51.762: INFO: Got endpoints: latency-svc-vvtss [19.019803ms]
Jan 30 12:49:51.762: INFO: Got endpoints: latency-svc-4d4g6 [27.137701ms]
Jan 30 12:49:51.763: INFO: Got endpoints: latency-svc-m2p4m [27.353093ms]
Jan 30 12:49:51.763: INFO: Got endpoints: latency-svc-656rs [34.344455ms]
Jan 30 12:49:51.763: INFO: Got endpoints: latency-svc-8sfbg [31.78146ms]
Jan 30 12:49:51.764: INFO: Got endpoints: latency-svc-v6hvm [26.036895ms]
Jan 30 12:49:51.764: INFO: Got endpoints: latency-svc-ldgp4 [39.606512ms]
Jan 30 12:49:51.765: INFO: Created: latency-svc-zlqjp
Jan 30 12:49:51.766: INFO: Got endpoints: latency-svc-lcp2b [26.240729ms]
Jan 30 12:49:51.766: INFO: Got endpoints: latency-svc-6n4k4 [29.044354ms]
Jan 30 12:49:51.766: INFO: Created: latency-svc-995q5
Jan 30 12:49:51.766: INFO: Got endpoints: latency-svc-g7hzm [33.229241ms]
Jan 30 12:49:51.767: INFO: Got endpoints: latency-svc-ng6hb [37.908391ms]
Jan 30 12:49:51.767: INFO: Got endpoints: latency-svc-pvq2f [44.255793ms]
Jan 30 12:49:51.767: INFO: Created: latency-svc-7lfw4
Jan 30 12:49:51.769: INFO: Got endpoints: latency-svc-995q5 [7.642498ms]
Jan 30 12:49:51.769: INFO: Got endpoints: latency-svc-zlqjp [7.824051ms]
Jan 30 12:49:51.769: INFO: Created: latency-svc-dqbfk
Jan 30 12:49:51.769: INFO: Got endpoints: latency-svc-7lfw4 [8.51065ms]
Jan 30 12:49:51.770: INFO: Created: latency-svc-bblfw
Jan 30 12:49:51.771: INFO: Got endpoints: latency-svc-dqbfk [9.604173ms]
Jan 30 12:49:51.772: INFO: Created: latency-svc-d7qwb
Jan 30 12:49:51.773: INFO: Got endpoints: latency-svc-bblfw [10.981748ms]
Jan 30 12:49:51.773: INFO: Created: latency-svc-2hk6w
Jan 30 12:49:51.774: INFO: Got endpoints: latency-svc-d7qwb [10.837951ms]
Jan 30 12:49:51.774: INFO: Created: latency-svc-gnhdp
Jan 30 12:49:51.775: INFO: Got endpoints: latency-svc-2hk6w [11.783094ms]
Jan 30 12:49:51.776: INFO: Created: latency-svc-rxg2l
Jan 30 12:49:51.776: INFO: Got endpoints: latency-svc-gnhdp [12.947927ms]
Jan 30 12:49:51.777: INFO: Created: latency-svc-gc4qs
Jan 30 12:49:51.778: INFO: Got endpoints: latency-svc-rxg2l [13.07085ms]
Jan 30 12:49:51.779: INFO: Got endpoints: latency-svc-gc4qs [14.807269ms]
Jan 30 12:49:51.779: INFO: Created: latency-svc-f48bh
Jan 30 12:49:51.780: INFO: Created: latency-svc-ps4f2
Jan 30 12:49:51.781: INFO: Got endpoints: latency-svc-f48bh [14.786583ms]
Jan 30 12:49:51.781: INFO: Created: latency-svc-cg5q7
Jan 30 12:49:51.782: INFO: Got endpoints: latency-svc-ps4f2 [16.031893ms]
Jan 30 12:49:51.783: INFO: Got endpoints: latency-svc-cg5q7 [16.982684ms]
Jan 30 12:49:51.798: INFO: Created: latency-svc-zd46s
Jan 30 12:49:51.799: INFO: Created: latency-svc-wjkx6
Jan 30 12:49:51.799: INFO: Created: latency-svc-59l6x
Jan 30 12:49:51.799: INFO: Created: latency-svc-h5v5m
Jan 30 12:49:51.799: INFO: Created: latency-svc-72lsv
Jan 30 12:49:51.799: INFO: Created: latency-svc-wvr25
Jan 30 12:49:51.799: INFO: Created: latency-svc-8j4wp
Jan 30 12:49:51.799: INFO: Created: latency-svc-cvfh5
Jan 30 12:49:51.799: INFO: Created: latency-svc-gffcs
Jan 30 12:49:51.800: INFO: Created: latency-svc-582zh
Jan 30 12:49:51.800: INFO: Created: latency-svc-pwfps
Jan 30 12:49:51.800: INFO: Created: latency-svc-tn574
Jan 30 12:49:51.800: INFO: Created: latency-svc-txc8p
Jan 30 12:49:51.800: INFO: Created: latency-svc-g68nf
Jan 30 12:49:51.800: INFO: Created: latency-svc-f596m
Jan 30 12:49:51.801: INFO: Got endpoints: latency-svc-zd46s [33.120009ms]
Jan 30 12:49:51.801: INFO: Got endpoints: latency-svc-txc8p [32.074729ms]
Jan 30 12:49:51.801: INFO: Got endpoints: latency-svc-pwfps [29.686767ms]
Jan 30 12:49:51.802: INFO: Got endpoints: latency-svc-72lsv [18.82482ms]
Jan 30 12:49:51.802: INFO: Got endpoints: latency-svc-59l6x [19.892722ms]
Jan 30 12:49:51.803: INFO: Got endpoints: latency-svc-wjkx6 [33.874019ms]
Jan 30 12:49:51.804: INFO: Got endpoints: latency-svc-wvr25 [25.95768ms]
Jan 30 12:49:51.804: INFO: Got endpoints: latency-svc-h5v5m [22.879867ms]
Jan 30 12:49:51.804: INFO: Got endpoints: latency-svc-582zh [37.073306ms]
Jan 30 12:49:51.805: INFO: Created: latency-svc-gs48r
Jan 30 12:49:51.805: INFO: Got endpoints: latency-svc-f596m [29.785908ms]
Jan 30 12:49:51.806: INFO: Got endpoints: latency-svc-tn574 [37.258105ms]
Jan 30 12:49:51.806: INFO: Created: latency-svc-pntk6
Jan 30 12:49:51.806: INFO: Got endpoints: latency-svc-8j4wp [32.588782ms]
Jan 30 12:49:51.807: INFO: Got endpoints: latency-svc-cvfh5 [30.426993ms]
Jan 30 12:49:51.808: INFO: Created: latency-svc-4zvnx
Jan 30 12:49:51.808: INFO: Got endpoints: latency-svc-g68nf [28.761236ms]
Jan 30 12:49:51.809: INFO: Created: latency-svc-s8lhp
Jan 30 12:49:51.810: INFO: Created: latency-svc-kn5fk
Jan 30 12:49:51.811: INFO: Created: latency-svc-ldn7j
Jan 30 12:49:51.813: INFO: Created: latency-svc-st4hl
Jan 30 12:49:51.814: INFO: Created: latency-svc-tnvjx
Jan 30 12:49:51.815: INFO: Created: latency-svc-6hgsx
Jan 30 12:49:51.816: INFO: Created: latency-svc-tgbjr
Jan 30 12:49:51.817: INFO: Created: latency-svc-7rtkc
Jan 30 12:49:51.819: INFO: Created: latency-svc-ts9kt
Jan 30 12:49:51.820: INFO: Created: latency-svc-gxkp5
Jan 30 12:49:51.821: INFO: Created: latency-svc-tlr6p
Jan 30 12:49:51.833: INFO: Got endpoints: latency-svc-gffcs [60.772261ms]
Jan 30 12:49:51.837: INFO: Created: latency-svc-8wp5s
Jan 30 12:49:51.858: INFO: Got endpoints: latency-svc-gs48r [57.722346ms]
Jan 30 12:49:51.862: INFO: Created: latency-svc-gzrpk
Jan 30 12:49:51.883: INFO: Got endpoints: latency-svc-pntk6 [82.181651ms]
Jan 30 12:49:51.886: INFO: Created: latency-svc-75bzn
Jan 30 12:49:51.908: INFO: Got endpoints: latency-svc-4zvnx [106.918137ms]
Jan 30 12:49:51.911: INFO: Created: latency-svc-v7hsc
Jan 30 12:49:51.934: INFO: Got endpoints: latency-svc-s8lhp [131.816354ms]
Jan 30 12:49:51.937: INFO: Created: latency-svc-lhzp5
Jan 30 12:49:51.958: INFO: Got endpoints: latency-svc-kn5fk [156.187681ms]
Jan 30 12:49:51.962: INFO: Created: latency-svc-88c9m
Jan 30 12:49:51.983: INFO: Got endpoints: latency-svc-ldn7j [179.641291ms]
Jan 30 12:49:51.986: INFO: Created: latency-svc-tbblr
Jan 30 12:49:52.009: INFO: Got endpoints: latency-svc-st4hl [204.984569ms]
Jan 30 12:49:52.012: INFO: Created: latency-svc-mlw5t
Jan 30 12:49:52.033: INFO: Got endpoints: latency-svc-tnvjx [229.228836ms]
Jan 30 12:49:52.036: INFO: Created: latency-svc-rt2w6
Jan 30 12:49:52.058: INFO: Got endpoints: latency-svc-6hgsx [253.61922ms]
Jan 30 12:49:52.062: INFO: Created: latency-svc-ppkt9
Jan 30 12:49:52.083: INFO: Got endpoints: latency-svc-tgbjr [278.697307ms]
Jan 30 12:49:52.087: INFO: Created: latency-svc-fdlt8
Jan 30 12:49:52.108: INFO: Got endpoints: latency-svc-7rtkc [302.097767ms]
Jan 30 12:49:52.112: INFO: Created: latency-svc-hmsd4
Jan 30 12:49:52.133: INFO: Got endpoints: latency-svc-ts9kt [326.880424ms]
Jan 30 12:49:52.137: INFO: Created: latency-svc-rxvt7
Jan 30 12:49:52.158: INFO: Got endpoints: latency-svc-gxkp5 [351.134862ms]
Jan 30 12:49:52.161: INFO: Created: latency-svc-mn7dc
Jan 30 12:49:52.184: INFO: Got endpoints: latency-svc-tlr6p [375.78916ms]
Jan 30 12:49:52.187: INFO: Created: latency-svc-fv5mh
Jan 30 12:49:52.208: INFO: Got endpoints: latency-svc-8wp5s [375.026576ms]
Jan 30 12:49:52.212: INFO: Created: latency-svc-wlf6n
Jan 30 12:49:52.234: INFO: Got endpoints: latency-svc-gzrpk [375.438525ms]
Jan 30 12:49:52.237: INFO: Created: latency-svc-4zvcd
Jan 30 12:49:52.258: INFO: Got endpoints: latency-svc-75bzn [375.396286ms]
Jan 30 12:49:52.262: INFO: Created: latency-svc-9lbpm
Jan 30 12:49:52.283: INFO: Got endpoints: latency-svc-v7hsc [375.240337ms]
Jan 30 12:49:52.287: INFO: Created: latency-svc-tlzpf
Jan 30 12:49:52.308: INFO: Got endpoints: latency-svc-lhzp5 [374.278547ms]
Jan 30 12:49:52.312: INFO: Created: latency-svc-k7b29
Jan 30 12:49:52.333: INFO: Got endpoints: latency-svc-88c9m [374.708933ms]
Jan 30 12:49:52.336: INFO: Created: latency-svc-btl56
Jan 30 12:49:52.358: INFO: Got endpoints: latency-svc-tbblr [374.555068ms]
Jan 30 12:49:52.361: INFO: Created: latency-svc-q2b2p
Jan 30 12:49:52.383: INFO: Got endpoints: latency-svc-mlw5t [374.756868ms]
Jan 30 12:49:52.387: INFO: Created: latency-svc-whqfd
Jan 30 12:49:52.408: INFO: Got endpoints: latency-svc-rt2w6 [375.292427ms]
Jan 30 12:49:52.412: INFO: Created: latency-svc-7872r
Jan 30 12:49:52.433: INFO: Got endpoints: latency-svc-ppkt9 [374.143306ms]
Jan 30 12:49:52.436: INFO: Created: latency-svc-b6jwl
Jan 30 12:49:52.458: INFO: Got endpoints: latency-svc-fdlt8 [374.850819ms]
Jan 30 12:49:52.461: INFO: Created: latency-svc-gjpsw
Jan 30 12:49:52.484: INFO: Got endpoints: latency-svc-hmsd4 [375.720598ms]
Jan 30 12:49:52.487: INFO: Created: latency-svc-bdcxc
Jan 30 12:49:52.508: INFO: Got endpoints: latency-svc-rxvt7 [374.904442ms]
Jan 30 12:49:52.512: INFO: Created: latency-svc-vqwrq
Jan 30 12:49:52.533: INFO: Got endpoints: latency-svc-mn7dc [375.468678ms]
Jan 30 12:49:52.537: INFO: Created: latency-svc-8c2sw
Jan 30 12:49:52.558: INFO: Got endpoints: latency-svc-fv5mh [374.408197ms]
Jan 30 12:49:52.561: INFO: Created: latency-svc-nd9tl
Jan 30 12:49:52.583: INFO: Got endpoints: latency-svc-wlf6n [374.229655ms]
Jan 30 12:49:52.586: INFO: Created: latency-svc-84gvg
Jan 30 12:49:52.608: INFO: Got endpoints: latency-svc-4zvcd [374.519985ms]
Jan 30 12:49:52.612: INFO: Created: latency-svc-m4m2m
Jan 30 12:49:52.634: INFO: Got endpoints: latency-svc-9lbpm [375.198125ms]
Jan 30 12:49:52.637: INFO: Created: latency-svc-pzjtx
Jan 30 12:49:52.658: INFO: Got endpoints: latency-svc-tlzpf [374.671749ms]
Jan 30 12:49:52.661: INFO: Created: latency-svc-kz7vj
Jan 30 12:49:52.683: INFO: Got endpoints: latency-svc-k7b29 [375.039441ms]
Jan 30 12:49:52.686: INFO: Created: latency-svc-vvdzs
Jan 30 12:49:52.708: INFO: Got endpoints: latency-svc-btl56 [374.934985ms]
Jan 30 12:49:52.711: INFO: Created: latency-svc-lcn6h
Jan 30 12:49:52.734: INFO: Got endpoints: latency-svc-q2b2p [375.954587ms]
Jan 30 12:49:52.737: INFO: Created: latency-svc-gngwn
Jan 30 12:49:52.758: INFO: Got endpoints: latency-svc-whqfd [374.306879ms]
Jan 30 12:49:52.761: INFO: Created: latency-svc-bwpvj
Jan 30 12:49:52.783: INFO: Got endpoints: latency-svc-7872r [374.960512ms]
Jan 30 12:49:52.787: INFO: Created: latency-svc-szlgj
Jan 30 12:49:52.808: INFO: Got endpoints: latency-svc-b6jwl [375.538309ms]
Jan 30 12:49:52.811: INFO: Created: latency-svc-jnzwm
Jan 30 12:49:52.833: INFO: Got endpoints: latency-svc-gjpsw [374.71498ms]
Jan 30 12:49:52.836: INFO: Created: latency-svc-mlmq9
Jan 30 12:49:52.858: INFO: Got endpoints: latency-svc-bdcxc [373.918958ms]
Jan 30 12:49:52.861: INFO: Created: latency-svc-t6z44
Jan 30 12:49:52.884: INFO: Got endpoints: latency-svc-vqwrq [375.566804ms]
Jan 30 12:49:52.887: INFO: Created: latency-svc-fn7mj
Jan 30 12:49:52.908: INFO: Got endpoints: latency-svc-8c2sw [374.340103ms]
Jan 30 12:49:52.911: INFO: Created: latency-svc-hbv6l
Jan 30 12:49:52.933: INFO: Got endpoints: latency-svc-nd9tl [375.244093ms]
Jan 30 12:49:52.937: INFO: Created: latency-svc-qrc4x
Jan 30 12:49:52.959: INFO: Got endpoints: latency-svc-84gvg [375.861659ms]
Jan 30 12:49:52.962: INFO: Created: latency-svc-htsmp
Jan 30 12:49:52.983: INFO: Got endpoints: latency-svc-m4m2m [374.878759ms]
Jan 30 12:49:52.987: INFO: Created: latency-svc-nhxns
Jan 30 12:49:53.008: INFO: Got endpoints: latency-svc-pzjtx [374.484043ms]
Jan 30 12:49:53.011: INFO: Created: latency-svc-8bvgq
Jan 30 12:49:53.034: INFO: Got endpoints: latency-svc-kz7vj [375.853709ms]
Jan 30 12:49:53.037: INFO: Created: latency-svc-9zv4z
Jan 30 12:49:53.058: INFO: Got endpoints: latency-svc-vvdzs [374.931864ms]
Jan 30 12:49:53.061: INFO: Created: latency-svc-nr4dc
Jan 30 12:49:53.083: INFO: Got endpoints: latency-svc-lcn6h [375.267907ms]
Jan 30 12:49:53.087: INFO: Created: latency-svc-sngns
Jan 30 12:49:53.108: INFO: Got endpoints: latency-svc-gngwn [374.135682ms]
Jan 30 12:49:53.111: INFO: Created: latency-svc-97cwr
Jan 30 12:49:53.134: INFO: Got endpoints: latency-svc-bwpvj [375.893814ms]
Jan 30 12:49:53.137: INFO: Created: latency-svc-bm7vb
Jan 30 12:49:53.158: INFO: Got endpoints: latency-svc-szlgj [374.637431ms]
Jan 30 12:49:53.162: INFO: Created: latency-svc-wqbk6
Jan 30 12:49:53.183: INFO: Got endpoints: latency-svc-jnzwm [375.127276ms]
Jan 30 12:49:53.187: INFO: Created: latency-svc-2jrxn
Jan 30 12:49:53.208: INFO: Got endpoints: latency-svc-mlmq9 [375.252439ms]
Jan 30 12:49:53.211: INFO: Created: latency-svc-dw4q2
Jan 30 12:49:53.233: INFO: Got endpoints: latency-svc-t6z44 [374.832215ms]
Jan 30 12:49:53.236: INFO: Created: latency-svc-dkwm2
Jan 30 12:49:53.259: INFO: Got endpoints: latency-svc-fn7mj [374.803228ms]
Jan 30 12:49:53.262: INFO: Created: latency-svc-8nfrz
Jan 30 12:49:53.284: INFO: Got endpoints: latency-svc-hbv6l [376.131162ms]
Jan 30 12:49:53.287: INFO: Created: latency-svc-9gqw2
Jan 30 12:49:53.308: INFO: Got endpoints: latency-svc-qrc4x [374.537436ms]
Jan 30 12:49:53.311: INFO: Created: latency-svc-n9lr8
Jan 30 12:49:53.334: INFO: Got endpoints: latency-svc-htsmp [375.027914ms]
Jan 30 12:49:53.337: INFO: Created: latency-svc-d56xp
Jan 30 12:49:53.358: INFO: Got endpoints: latency-svc-nhxns [374.63538ms]
Jan 30 12:49:53.361: INFO: Created: latency-svc-wmjck
Jan 30 12:49:53.384: INFO: Got endpoints: latency-svc-8bvgq [375.527608ms]
Jan 30 12:49:53.387: INFO: Created: latency-svc-nc6jl
Jan 30 12:49:53.408: INFO: Got endpoints: latency-svc-9zv4z [373.946192ms]
Jan 30 12:49:53.411: INFO: Created: latency-svc-kb6s4
Jan 30 12:49:53.433: INFO: Got endpoints: latency-svc-nr4dc [375.075167ms]
Jan 30 12:49:53.439: INFO: Created: latency-svc-7rt5n
Jan 30 12:49:53.458: INFO: Got endpoints: latency-svc-sngns [375.048899ms]
Jan 30 12:49:53.462: INFO: Created: latency-svc-zhnlz
Jan 30 12:49:53.483: INFO: Got endpoints: latency-svc-97cwr [374.869645ms]
Jan 30 12:49:53.486: INFO: Created: latency-svc-qddf6
Jan 30 12:49:53.508: INFO: Got endpoints: latency-svc-bm7vb [374.752608ms]
Jan 30 12:49:53.512: INFO: Created: latency-svc-ksljz
Jan 30 12:49:53.533: INFO: Got endpoints: latency-svc-wqbk6 [375.355159ms]
Jan 30 12:49:53.537: INFO: Created: latency-svc-f7z5d
Jan 30 12:49:53.558: INFO: Got endpoints: latency-svc-2jrxn [374.196617ms]
Jan 30 12:49:53.561: INFO: Created: latency-svc-7n4tf
Jan 30 12:49:53.583: INFO: Got endpoints: latency-svc-dw4q2 [375.260444ms]
Jan 30 12:49:53.587: INFO: Created: latency-svc-4zdwz
Jan 30 12:49:53.608: INFO: Got endpoints: latency-svc-dkwm2 [375.307346ms]
Jan 30 12:49:53.611: INFO: Created: latency-svc-ftbqk
Jan 30 12:49:53.633: INFO: Got endpoints: latency-svc-8nfrz [374.462421ms]
Jan 30 12:49:53.636: INFO: Created: latency-svc-ktcxt
Jan 30 12:49:53.658: INFO: Got endpoints: latency-svc-9gqw2 [374.250949ms]
Jan 30 12:49:53.662: INFO: Created: latency-svc-bmhd9
Jan 30 12:49:53.683: INFO: Got endpoints: latency-svc-n9lr8 [375.514922ms]
Jan 30 12:49:53.687: INFO: Created: latency-svc-htpsp
Jan 30 12:49:53.708: INFO: Got endpoints: latency-svc-d56xp [374.257747ms]
Jan 30 12:49:53.711: INFO: Created: latency-svc-q5gxc
Jan 30 12:49:53.733: INFO: Got endpoints: latency-svc-wmjck [375.056333ms]
Jan 30 12:49:53.736: INFO: Created: latency-svc-5v9nz
Jan 30 12:49:53.758: INFO: Got endpoints: latency-svc-nc6jl [374.053915ms]
Jan 30 12:49:53.761: INFO: Created: latency-svc-nbm2p
Jan 30 12:49:53.783: INFO: Got endpoints: latency-svc-kb6s4 [375.532702ms]
Jan 30 12:49:53.787: INFO: Created: latency-svc-rv9h8
Jan 30 12:49:53.808: INFO: Got endpoints: latency-svc-7rt5n [374.790902ms]
Jan 30 12:49:53.811: INFO: Created: latency-svc-6s528
Jan 30 12:49:53.833: INFO: Got endpoints: latency-svc-zhnlz [374.491747ms]
Jan 30 12:49:53.836: INFO: Created: latency-svc-6hbpq
Jan 30 12:49:53.858: INFO: Got endpoints: latency-svc-qddf6 [375.224692ms]
Jan 30 12:49:53.861: INFO: Created: latency-svc-dvn49
Jan 30 12:49:53.883: INFO: Got endpoints: latency-svc-ksljz [374.44087ms]
Jan 30 12:49:53.886: INFO: Created: latency-svc-c78wt
Jan 30 12:49:53.908: INFO: Got endpoints: latency-svc-f7z5d [375.060179ms]
Jan 30 12:49:53.913: INFO: Created: latency-svc-5gz9g
Jan 30 12:49:53.933: INFO: Got endpoints: latency-svc-7n4tf [375.755165ms]
Jan 30 12:49:53.937: INFO: Created: latency-svc-x9zhp
Jan 30 12:49:53.958: INFO: Got endpoints: latency-svc-4zdwz [375.13931ms]
Jan 30 12:49:53.962: INFO: Created: latency-svc-mkl8n
Jan 30 12:49:53.984: INFO: Got endpoints: latency-svc-ftbqk [375.55157ms]
Jan 30 12:49:53.987: INFO: Created: latency-svc-jnffp
Jan 30 12:49:54.009: INFO: Got endpoints: latency-svc-ktcxt [375.624878ms]
Jan 30 12:49:54.013: INFO: Created: latency-svc-97k8s
Jan 30 12:49:54.033: INFO: Got endpoints: latency-svc-bmhd9 [374.806948ms]
Jan 30 12:49:54.037: INFO: Created: latency-svc-fp7x5
Jan 30 12:49:54.058: INFO: Got endpoints: latency-svc-htpsp [374.206575ms]
Jan 30 12:49:54.061: INFO: Created: latency-svc-kf5tm
Jan 30 12:49:54.083: INFO: Got endpoints: latency-svc-q5gxc [375.058383ms]
Jan 30 12:49:54.087: INFO: Created: latency-svc-mz4xn
Jan 30 12:49:54.108: INFO: Got endpoints: latency-svc-5v9nz [374.584988ms]
Jan 30 12:49:54.111: INFO: Created: latency-svc-r46nx
Jan 30 12:49:54.133: INFO: Got endpoints: latency-svc-nbm2p [374.99491ms]
Jan 30 12:49:54.136: INFO: Created: latency-svc-w8wm9
Jan 30 12:49:54.158: INFO: Got endpoints: latency-svc-rv9h8 [375.097056ms]
Jan 30 12:49:54.162: INFO: Created: latency-svc-d94vd
Jan 30 12:49:54.183: INFO: Got endpoints: latency-svc-6s528 [375.4244ms]
Jan 30 12:49:54.187: INFO: Created: latency-svc-vt2kl
Jan 30 12:49:54.208: INFO: Got endpoints: latency-svc-6hbpq [375.379799ms]
Jan 30 12:49:54.212: INFO: Created: latency-svc-8gl5n
Jan 30 12:49:54.233: INFO: Got endpoints: latency-svc-dvn49 [375.291316ms]
Jan 30 12:49:54.237: INFO: Created: latency-svc-8lv6q
Jan 30 12:49:54.258: INFO: Got endpoints: latency-svc-c78wt [375.47599ms]
Jan 30 12:49:54.262: INFO: Created: latency-svc-znnkb
Jan 30 12:49:54.283: INFO: Got endpoints: latency-svc-5gz9g [374.416662ms]
Jan 30 12:49:54.286: INFO: Created: latency-svc-xfjr5
Jan 30 12:49:54.308: INFO: Got endpoints: latency-svc-x9zhp [374.343815ms]
Jan 30 12:49:54.311: INFO: Created: latency-svc-9qt8c
Jan 30 12:49:54.334: INFO: Got endpoints: latency-svc-mkl8n [375.093566ms]
Jan 30 12:49:54.337: INFO: Created: latency-svc-829ml
Jan 30 12:49:54.358: INFO: Got endpoints: latency-svc-jnffp [374.911615ms]
Jan 30 12:49:54.362: INFO: Created: latency-svc-fg68l
Jan 30 12:49:54.383: INFO: Got endpoints: latency-svc-97k8s [374.367576ms]
Jan 30 12:49:54.387: INFO: Created: latency-svc-ckll9
Jan 30 12:49:54.408: INFO: Got endpoints: latency-svc-fp7x5 [375.174167ms]
Jan 30 12:49:54.412: INFO: Created: latency-svc-5rv9j
Jan 30 12:49:54.433: INFO: Got endpoints: latency-svc-kf5tm [375.572121ms]
Jan 30 12:49:54.437: INFO: Created: latency-svc-2djtc
Jan 30 12:49:54.458: INFO: Got endpoints: latency-svc-mz4xn [375.429535ms]
Jan 30 12:49:54.462: INFO: Created: latency-svc-bkhlf
Jan 30 12:49:54.483: INFO: Got endpoints: latency-svc-r46nx [375.610382ms]
Jan 30 12:49:54.487: INFO: Created: latency-svc-k4hzv
Jan 30 12:49:54.508: INFO: Got endpoints: latency-svc-w8wm9 [375.128974ms]
Jan 30 12:49:54.511: INFO: Created: latency-svc-9hk2x
Jan 30 12:49:54.533: INFO: Got endpoints: latency-svc-d94vd [374.617698ms]
Jan 30 12:49:54.537: INFO: Created: latency-svc-q8qqm
Jan 30 12:49:54.558: INFO: Got endpoints: latency-svc-vt2kl [375.007078ms]
Jan 30 12:49:54.562: INFO: Created: latency-svc-zrnd8
Jan 30 12:49:54.583: INFO: Got endpoints: latency-svc-8gl5n [374.599464ms]
Jan 30 12:49:54.586: INFO: Created: latency-svc-pdptj
Jan 30 12:49:54.608: INFO: Got endpoints: latency-svc-8lv6q [374.783447ms]
Jan 30 12:49:54.611: INFO: Created: latency-svc-q6l78
Jan 30 12:49:54.633: INFO: Got endpoints: latency-svc-znnkb [374.638261ms]
Jan 30 12:49:54.636: INFO: Created: latency-svc-wmdqz
Jan 30 12:49:54.658: INFO: Got endpoints: latency-svc-xfjr5 [375.023776ms]
Jan 30 12:49:54.662: INFO: Created: latency-svc-nskfh
Jan 30 12:49:54.683: INFO: Got endpoints: latency-svc-9qt8c [375.107197ms]
Jan 30 12:49:54.687: INFO: Created: latency-svc-6r8tc
Jan 30 12:49:54.708: INFO: Got endpoints: latency-svc-829ml [374.13777ms]
Jan 30 12:49:54.711: INFO: Created: latency-svc-jzvd8
Jan 30 12:49:54.733: INFO: Got endpoints: latency-svc-fg68l [374.709745ms]
Jan 30 12:49:54.737: INFO: Created: latency-svc-stmvj
Jan 30 12:49:54.758: INFO: Got endpoints: latency-svc-ckll9 [374.678638ms]
Jan 30 12:49:54.761: INFO: Created: latency-svc-4kx7p
Jan 30 12:49:54.783: INFO: Got endpoints: latency-svc-5rv9j [374.564614ms]
Jan 30 12:49:54.786: INFO: Created: latency-svc-dnp4h
Jan 30 12:49:54.809: INFO: Got endpoints: latency-svc-2djtc [375.411144ms]
Jan 30 12:49:54.812: INFO: Created: latency-svc-rkq9l
Jan 30 12:49:54.833: INFO: Got endpoints: latency-svc-bkhlf [374.929528ms]
Jan 30 12:49:54.837: INFO: Created: latency-svc-sh7hr
Jan 30 12:49:54.858: INFO: Got endpoints: latency-svc-k4hzv [375.120532ms]
Jan 30 12:49:54.862: INFO: Created: latency-svc-rj4c7
Jan 30 12:49:54.883: INFO: Got endpoints: latency-svc-9hk2x [375.115901ms]
Jan 30 12:49:54.908: INFO: Got endpoints: latency-svc-q8qqm [374.68853ms]
Jan 30 12:49:54.933: INFO: Got endpoints: latency-svc-zrnd8 [374.166209ms]
Jan 30 12:49:54.958: INFO: Got endpoints: latency-svc-pdptj [375.02405ms]
Jan 30 12:49:54.983: INFO: Got endpoints: latency-svc-q6l78 [375.250417ms]
Jan 30 12:49:55.008: INFO: Got endpoints: latency-svc-wmdqz [375.314092ms]
Jan 30 12:49:55.034: INFO: Got endpoints: latency-svc-nskfh [375.790233ms]
Jan 30 12:49:55.058: INFO: Got endpoints: latency-svc-6r8tc [375.076983ms]
Jan 30 12:49:55.083: INFO: Got endpoints: latency-svc-jzvd8 [375.642576ms]
Jan 30 12:49:55.108: INFO: Got endpoints: latency-svc-stmvj [374.331878ms]
Jan 30 12:49:55.133: INFO: Got endpoints: latency-svc-4kx7p [375.249382ms]
Jan 30 12:49:55.158: INFO: Got endpoints: latency-svc-dnp4h [375.180435ms]
Jan 30 12:49:55.184: INFO: Got endpoints: latency-svc-rkq9l [374.906822ms]
Jan 30 12:49:55.208: INFO: Got endpoints: latency-svc-sh7hr [374.915738ms]
Jan 30 12:49:55.233: INFO: Got endpoints: latency-svc-rj4c7 [374.997319ms]
Jan 30 12:49:55.233: INFO: Latencies: [6.767295ms 7.642498ms 7.824051ms 8.090806ms 8.51065ms 9.604173ms 9.685268ms 10.313103ms 10.837951ms 10.981748ms 11.633496ms 11.783094ms 12.687279ms 12.947927ms 13.07085ms 14.41055ms 14.786583ms 14.807269ms 16.001417ms 16.031893ms 16.608688ms 16.982684ms 18.402853ms 18.82482ms 19.019803ms 19.639987ms 19.866747ms 19.892722ms 20.088777ms 20.12793ms 20.194472ms 20.625932ms 20.677963ms 20.74081ms 21.036972ms 22.800846ms 22.879867ms 24.008317ms 25.622332ms 25.95768ms 26.036895ms 26.240729ms 27.137701ms 27.353093ms 28.761236ms 29.044354ms 29.686767ms 29.785908ms 30.426993ms 31.78146ms 32.074729ms 32.588782ms 33.120009ms 33.229241ms 33.749056ms 33.874019ms 34.344455ms 35.521553ms 37.073306ms 37.258105ms 37.908391ms 39.606512ms 44.255793ms 57.722346ms 60.772261ms 82.181651ms 106.918137ms 131.816354ms 156.187681ms 179.641291ms 204.984569ms 229.228836ms 253.61922ms 278.697307ms 302.097767ms 326.880424ms 351.134862ms 373.918958ms 373.946192ms 374.053915ms 374.135682ms 374.13777ms 374.143306ms 374.166209ms 374.196617ms 374.206575ms 374.229655ms 374.250949ms 374.257747ms 374.278547ms 374.306879ms 374.331878ms 374.340103ms 374.343815ms 374.367576ms 374.408197ms 374.416662ms 374.44087ms 374.462421ms 374.484043ms 374.491747ms 374.519985ms 374.537436ms 374.555068ms 374.564614ms 374.584988ms 374.599464ms 374.617698ms 374.63538ms 374.637431ms 374.638261ms 374.671749ms 374.678638ms 374.68853ms 374.708933ms 374.709745ms 374.71498ms 374.752608ms 374.756868ms 374.783447ms 374.790902ms 374.803228ms 374.806948ms 374.832215ms 374.850819ms 374.869645ms 374.878759ms 374.904442ms 374.906822ms 374.911615ms 374.915738ms 374.929528ms 374.931864ms 374.934985ms 374.960512ms 374.99491ms 374.997319ms 375.007078ms 375.023776ms 375.02405ms 375.026576ms 375.027914ms 375.039441ms 375.048899ms 375.056333ms 375.058383ms 375.060179ms 375.075167ms 375.076983ms 375.093566ms 375.097056ms 375.107197ms 375.115901ms 375.120532ms 375.127276ms 375.128974ms 375.13931ms 375.174167ms 375.180435ms 375.198125ms 375.224692ms 375.240337ms 375.244093ms 375.249382ms 375.250417ms 375.252439ms 375.260444ms 375.267907ms 375.291316ms 375.292427ms 375.307346ms 375.314092ms 375.355159ms 375.379799ms 375.396286ms 375.411144ms 375.4244ms 375.429535ms 375.438525ms 375.468678ms 375.47599ms 375.514922ms 375.527608ms 375.532702ms 375.538309ms 375.55157ms 375.566804ms 375.572121ms 375.610382ms 375.624878ms 375.642576ms 375.720598ms 375.755165ms 375.78916ms 375.790233ms 375.853709ms 375.861659ms 375.893814ms 375.954587ms 376.131162ms]
Jan 30 12:49:55.233: INFO: 50 %ile: 374.491747ms
Jan 30 12:49:55.233: INFO: 90 %ile: 375.47599ms
Jan 30 12:49:55.234: INFO: 99 %ile: 375.954587ms
Jan 30 12:49:55.234: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  test/e2e/framework/node/init/init.go:32
Jan 30 12:49:55.234: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Service endpoints latency
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Service endpoints latency
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Service endpoints latency
  tear down framework | framework.go:193
STEP: Destroying namespace "svc-latency-7939" for this suite. 01/30/23 12:49:55.237
------------------------------
• [SLOW TEST] [5.706 seconds]
[sig-network] Service endpoints latency
test/e2e/network/common/framework.go:23
  should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Service endpoints latency
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 12:49:49.533
    Jan 30 12:49:49.533: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename svc-latency 01/30/23 12:49:49.534
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:49:49.541
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:49:49.542
    [BeforeEach] [sig-network] Service endpoints latency
      test/e2e/framework/metrics/init/init.go:31
    [It] should not be very high  [Conformance]
      test/e2e/network/service_latency.go:59
    Jan 30 12:49:49.544: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: creating replication controller svc-latency-rc in namespace svc-latency-7939 01/30/23 12:49:49.545
    I0130 12:49:49.548401      23 runners.go:193] Created replication controller with name: svc-latency-rc, namespace: svc-latency-7939, replica count: 1
    I0130 12:49:50.599666      23 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0130 12:49:51.600779      23 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan 30 12:49:51.705: INFO: Created: latency-svc-t8wzq
    Jan 30 12:49:51.709: INFO: Got endpoints: latency-svc-t8wzq [7.6509ms]
    Jan 30 12:49:51.713: INFO: Created: latency-svc-lb7xd
    Jan 30 12:49:51.714: INFO: Created: latency-svc-75bxp
    Jan 30 12:49:51.715: INFO: Got endpoints: latency-svc-lb7xd [6.767295ms]
    Jan 30 12:49:51.716: INFO: Created: latency-svc-55v5m
    Jan 30 12:49:51.717: INFO: Got endpoints: latency-svc-75bxp [8.090806ms]
    Jan 30 12:49:51.717: INFO: Created: latency-svc-vzjcs
    Jan 30 12:49:51.718: INFO: Got endpoints: latency-svc-55v5m [9.685268ms]
    Jan 30 12:49:51.719: INFO: Created: latency-svc-lw9f8
    Jan 30 12:49:51.719: INFO: Got endpoints: latency-svc-vzjcs [10.313103ms]
    Jan 30 12:49:51.720: INFO: Created: latency-svc-2cdhz
    Jan 30 12:49:51.720: INFO: Got endpoints: latency-svc-lw9f8 [11.633496ms]
    Jan 30 12:49:51.721: INFO: Got endpoints: latency-svc-2cdhz [12.687279ms]
    Jan 30 12:49:51.722: INFO: Created: latency-svc-shmkz
    Jan 30 12:49:51.723: INFO: Created: latency-svc-nm79h
    Jan 30 12:49:51.723: INFO: Got endpoints: latency-svc-shmkz [14.41055ms]
    Jan 30 12:49:51.724: INFO: Created: latency-svc-nnkk6
    Jan 30 12:49:51.725: INFO: Got endpoints: latency-svc-nm79h [16.001417ms]
    Jan 30 12:49:51.725: INFO: Created: latency-svc-m9567
    Jan 30 12:49:51.725: INFO: Got endpoints: latency-svc-nnkk6 [16.608688ms]
    Jan 30 12:49:51.727: INFO: Created: latency-svc-sq984
    Jan 30 12:49:51.727: INFO: Got endpoints: latency-svc-m9567 [18.402853ms]
    Jan 30 12:49:51.728: INFO: Created: latency-svc-kgh88
    Jan 30 12:49:51.729: INFO: Got endpoints: latency-svc-sq984 [20.088777ms]
    Jan 30 12:49:51.729: INFO: Created: latency-svc-kcjld
    Jan 30 12:49:51.729: INFO: Got endpoints: latency-svc-kgh88 [20.677963ms]
    Jan 30 12:49:51.731: INFO: Created: latency-svc-hbfgk
    Jan 30 12:49:51.732: INFO: Got endpoints: latency-svc-kcjld [22.800846ms]
    Jan 30 12:49:51.732: INFO: Created: latency-svc-hljfd
    Jan 30 12:49:51.733: INFO: Got endpoints: latency-svc-hbfgk [24.008317ms]
    Jan 30 12:49:51.733: INFO: Created: latency-svc-6h66d
    Jan 30 12:49:51.734: INFO: Got endpoints: latency-svc-hljfd [25.622332ms]
    Jan 30 12:49:51.735: INFO: Created: latency-svc-99sh8
    Jan 30 12:49:51.736: INFO: Got endpoints: latency-svc-6h66d [20.12793ms]
    Jan 30 12:49:51.736: INFO: Created: latency-svc-cxvxn
    Jan 30 12:49:51.737: INFO: Got endpoints: latency-svc-99sh8 [20.194472ms]
    Jan 30 12:49:51.738: INFO: Got endpoints: latency-svc-cxvxn [19.639987ms]
    Jan 30 12:49:51.738: INFO: Created: latency-svc-k2lfq
    Jan 30 12:49:51.739: INFO: Created: latency-svc-hc68q
    Jan 30 12:49:51.740: INFO: Got endpoints: latency-svc-k2lfq [20.625932ms]
    Jan 30 12:49:51.741: INFO: Created: latency-svc-pmrp4
    Jan 30 12:49:51.741: INFO: Got endpoints: latency-svc-hc68q [20.74081ms]
    Jan 30 12:49:51.743: INFO: Got endpoints: latency-svc-pmrp4 [21.036972ms]
    Jan 30 12:49:51.758: INFO: Created: latency-svc-vvtss
    Jan 30 12:49:51.758: INFO: Created: latency-svc-v6hvm
    Jan 30 12:49:51.759: INFO: Created: latency-svc-zp754
    Jan 30 12:49:51.759: INFO: Created: latency-svc-8sfbg
    Jan 30 12:49:51.759: INFO: Created: latency-svc-m2p4m
    Jan 30 12:49:51.759: INFO: Created: latency-svc-4d4g6
    Jan 30 12:49:51.759: INFO: Created: latency-svc-lcp2b
    Jan 30 12:49:51.759: INFO: Created: latency-svc-6n4k4
    Jan 30 12:49:51.759: INFO: Created: latency-svc-g7hzm
    Jan 30 12:49:51.759: INFO: Created: latency-svc-pvq2f
    Jan 30 12:49:51.759: INFO: Created: latency-svc-656rs
    Jan 30 12:49:51.759: INFO: Created: latency-svc-klw9p
    Jan 30 12:49:51.759: INFO: Created: latency-svc-zvxzf
    Jan 30 12:49:51.759: INFO: Created: latency-svc-ng6hb
    Jan 30 12:49:51.759: INFO: Created: latency-svc-ldgp4
    Jan 30 12:49:51.761: INFO: Got endpoints: latency-svc-klw9p [33.749056ms]
    Jan 30 12:49:51.761: INFO: Got endpoints: latency-svc-zp754 [19.866747ms]
    Jan 30 12:49:51.761: INFO: Got endpoints: latency-svc-zvxzf [35.521553ms]
    Jan 30 12:49:51.762: INFO: Got endpoints: latency-svc-vvtss [19.019803ms]
    Jan 30 12:49:51.762: INFO: Got endpoints: latency-svc-4d4g6 [27.137701ms]
    Jan 30 12:49:51.763: INFO: Got endpoints: latency-svc-m2p4m [27.353093ms]
    Jan 30 12:49:51.763: INFO: Got endpoints: latency-svc-656rs [34.344455ms]
    Jan 30 12:49:51.763: INFO: Got endpoints: latency-svc-8sfbg [31.78146ms]
    Jan 30 12:49:51.764: INFO: Got endpoints: latency-svc-v6hvm [26.036895ms]
    Jan 30 12:49:51.764: INFO: Got endpoints: latency-svc-ldgp4 [39.606512ms]
    Jan 30 12:49:51.765: INFO: Created: latency-svc-zlqjp
    Jan 30 12:49:51.766: INFO: Got endpoints: latency-svc-lcp2b [26.240729ms]
    Jan 30 12:49:51.766: INFO: Got endpoints: latency-svc-6n4k4 [29.044354ms]
    Jan 30 12:49:51.766: INFO: Created: latency-svc-995q5
    Jan 30 12:49:51.766: INFO: Got endpoints: latency-svc-g7hzm [33.229241ms]
    Jan 30 12:49:51.767: INFO: Got endpoints: latency-svc-ng6hb [37.908391ms]
    Jan 30 12:49:51.767: INFO: Got endpoints: latency-svc-pvq2f [44.255793ms]
    Jan 30 12:49:51.767: INFO: Created: latency-svc-7lfw4
    Jan 30 12:49:51.769: INFO: Got endpoints: latency-svc-995q5 [7.642498ms]
    Jan 30 12:49:51.769: INFO: Got endpoints: latency-svc-zlqjp [7.824051ms]
    Jan 30 12:49:51.769: INFO: Created: latency-svc-dqbfk
    Jan 30 12:49:51.769: INFO: Got endpoints: latency-svc-7lfw4 [8.51065ms]
    Jan 30 12:49:51.770: INFO: Created: latency-svc-bblfw
    Jan 30 12:49:51.771: INFO: Got endpoints: latency-svc-dqbfk [9.604173ms]
    Jan 30 12:49:51.772: INFO: Created: latency-svc-d7qwb
    Jan 30 12:49:51.773: INFO: Got endpoints: latency-svc-bblfw [10.981748ms]
    Jan 30 12:49:51.773: INFO: Created: latency-svc-2hk6w
    Jan 30 12:49:51.774: INFO: Got endpoints: latency-svc-d7qwb [10.837951ms]
    Jan 30 12:49:51.774: INFO: Created: latency-svc-gnhdp
    Jan 30 12:49:51.775: INFO: Got endpoints: latency-svc-2hk6w [11.783094ms]
    Jan 30 12:49:51.776: INFO: Created: latency-svc-rxg2l
    Jan 30 12:49:51.776: INFO: Got endpoints: latency-svc-gnhdp [12.947927ms]
    Jan 30 12:49:51.777: INFO: Created: latency-svc-gc4qs
    Jan 30 12:49:51.778: INFO: Got endpoints: latency-svc-rxg2l [13.07085ms]
    Jan 30 12:49:51.779: INFO: Got endpoints: latency-svc-gc4qs [14.807269ms]
    Jan 30 12:49:51.779: INFO: Created: latency-svc-f48bh
    Jan 30 12:49:51.780: INFO: Created: latency-svc-ps4f2
    Jan 30 12:49:51.781: INFO: Got endpoints: latency-svc-f48bh [14.786583ms]
    Jan 30 12:49:51.781: INFO: Created: latency-svc-cg5q7
    Jan 30 12:49:51.782: INFO: Got endpoints: latency-svc-ps4f2 [16.031893ms]
    Jan 30 12:49:51.783: INFO: Got endpoints: latency-svc-cg5q7 [16.982684ms]
    Jan 30 12:49:51.798: INFO: Created: latency-svc-zd46s
    Jan 30 12:49:51.799: INFO: Created: latency-svc-wjkx6
    Jan 30 12:49:51.799: INFO: Created: latency-svc-59l6x
    Jan 30 12:49:51.799: INFO: Created: latency-svc-h5v5m
    Jan 30 12:49:51.799: INFO: Created: latency-svc-72lsv
    Jan 30 12:49:51.799: INFO: Created: latency-svc-wvr25
    Jan 30 12:49:51.799: INFO: Created: latency-svc-8j4wp
    Jan 30 12:49:51.799: INFO: Created: latency-svc-cvfh5
    Jan 30 12:49:51.799: INFO: Created: latency-svc-gffcs
    Jan 30 12:49:51.800: INFO: Created: latency-svc-582zh
    Jan 30 12:49:51.800: INFO: Created: latency-svc-pwfps
    Jan 30 12:49:51.800: INFO: Created: latency-svc-tn574
    Jan 30 12:49:51.800: INFO: Created: latency-svc-txc8p
    Jan 30 12:49:51.800: INFO: Created: latency-svc-g68nf
    Jan 30 12:49:51.800: INFO: Created: latency-svc-f596m
    Jan 30 12:49:51.801: INFO: Got endpoints: latency-svc-zd46s [33.120009ms]
    Jan 30 12:49:51.801: INFO: Got endpoints: latency-svc-txc8p [32.074729ms]
    Jan 30 12:49:51.801: INFO: Got endpoints: latency-svc-pwfps [29.686767ms]
    Jan 30 12:49:51.802: INFO: Got endpoints: latency-svc-72lsv [18.82482ms]
    Jan 30 12:49:51.802: INFO: Got endpoints: latency-svc-59l6x [19.892722ms]
    Jan 30 12:49:51.803: INFO: Got endpoints: latency-svc-wjkx6 [33.874019ms]
    Jan 30 12:49:51.804: INFO: Got endpoints: latency-svc-wvr25 [25.95768ms]
    Jan 30 12:49:51.804: INFO: Got endpoints: latency-svc-h5v5m [22.879867ms]
    Jan 30 12:49:51.804: INFO: Got endpoints: latency-svc-582zh [37.073306ms]
    Jan 30 12:49:51.805: INFO: Created: latency-svc-gs48r
    Jan 30 12:49:51.805: INFO: Got endpoints: latency-svc-f596m [29.785908ms]
    Jan 30 12:49:51.806: INFO: Got endpoints: latency-svc-tn574 [37.258105ms]
    Jan 30 12:49:51.806: INFO: Created: latency-svc-pntk6
    Jan 30 12:49:51.806: INFO: Got endpoints: latency-svc-8j4wp [32.588782ms]
    Jan 30 12:49:51.807: INFO: Got endpoints: latency-svc-cvfh5 [30.426993ms]
    Jan 30 12:49:51.808: INFO: Created: latency-svc-4zvnx
    Jan 30 12:49:51.808: INFO: Got endpoints: latency-svc-g68nf [28.761236ms]
    Jan 30 12:49:51.809: INFO: Created: latency-svc-s8lhp
    Jan 30 12:49:51.810: INFO: Created: latency-svc-kn5fk
    Jan 30 12:49:51.811: INFO: Created: latency-svc-ldn7j
    Jan 30 12:49:51.813: INFO: Created: latency-svc-st4hl
    Jan 30 12:49:51.814: INFO: Created: latency-svc-tnvjx
    Jan 30 12:49:51.815: INFO: Created: latency-svc-6hgsx
    Jan 30 12:49:51.816: INFO: Created: latency-svc-tgbjr
    Jan 30 12:49:51.817: INFO: Created: latency-svc-7rtkc
    Jan 30 12:49:51.819: INFO: Created: latency-svc-ts9kt
    Jan 30 12:49:51.820: INFO: Created: latency-svc-gxkp5
    Jan 30 12:49:51.821: INFO: Created: latency-svc-tlr6p
    Jan 30 12:49:51.833: INFO: Got endpoints: latency-svc-gffcs [60.772261ms]
    Jan 30 12:49:51.837: INFO: Created: latency-svc-8wp5s
    Jan 30 12:49:51.858: INFO: Got endpoints: latency-svc-gs48r [57.722346ms]
    Jan 30 12:49:51.862: INFO: Created: latency-svc-gzrpk
    Jan 30 12:49:51.883: INFO: Got endpoints: latency-svc-pntk6 [82.181651ms]
    Jan 30 12:49:51.886: INFO: Created: latency-svc-75bzn
    Jan 30 12:49:51.908: INFO: Got endpoints: latency-svc-4zvnx [106.918137ms]
    Jan 30 12:49:51.911: INFO: Created: latency-svc-v7hsc
    Jan 30 12:49:51.934: INFO: Got endpoints: latency-svc-s8lhp [131.816354ms]
    Jan 30 12:49:51.937: INFO: Created: latency-svc-lhzp5
    Jan 30 12:49:51.958: INFO: Got endpoints: latency-svc-kn5fk [156.187681ms]
    Jan 30 12:49:51.962: INFO: Created: latency-svc-88c9m
    Jan 30 12:49:51.983: INFO: Got endpoints: latency-svc-ldn7j [179.641291ms]
    Jan 30 12:49:51.986: INFO: Created: latency-svc-tbblr
    Jan 30 12:49:52.009: INFO: Got endpoints: latency-svc-st4hl [204.984569ms]
    Jan 30 12:49:52.012: INFO: Created: latency-svc-mlw5t
    Jan 30 12:49:52.033: INFO: Got endpoints: latency-svc-tnvjx [229.228836ms]
    Jan 30 12:49:52.036: INFO: Created: latency-svc-rt2w6
    Jan 30 12:49:52.058: INFO: Got endpoints: latency-svc-6hgsx [253.61922ms]
    Jan 30 12:49:52.062: INFO: Created: latency-svc-ppkt9
    Jan 30 12:49:52.083: INFO: Got endpoints: latency-svc-tgbjr [278.697307ms]
    Jan 30 12:49:52.087: INFO: Created: latency-svc-fdlt8
    Jan 30 12:49:52.108: INFO: Got endpoints: latency-svc-7rtkc [302.097767ms]
    Jan 30 12:49:52.112: INFO: Created: latency-svc-hmsd4
    Jan 30 12:49:52.133: INFO: Got endpoints: latency-svc-ts9kt [326.880424ms]
    Jan 30 12:49:52.137: INFO: Created: latency-svc-rxvt7
    Jan 30 12:49:52.158: INFO: Got endpoints: latency-svc-gxkp5 [351.134862ms]
    Jan 30 12:49:52.161: INFO: Created: latency-svc-mn7dc
    Jan 30 12:49:52.184: INFO: Got endpoints: latency-svc-tlr6p [375.78916ms]
    Jan 30 12:49:52.187: INFO: Created: latency-svc-fv5mh
    Jan 30 12:49:52.208: INFO: Got endpoints: latency-svc-8wp5s [375.026576ms]
    Jan 30 12:49:52.212: INFO: Created: latency-svc-wlf6n
    Jan 30 12:49:52.234: INFO: Got endpoints: latency-svc-gzrpk [375.438525ms]
    Jan 30 12:49:52.237: INFO: Created: latency-svc-4zvcd
    Jan 30 12:49:52.258: INFO: Got endpoints: latency-svc-75bzn [375.396286ms]
    Jan 30 12:49:52.262: INFO: Created: latency-svc-9lbpm
    Jan 30 12:49:52.283: INFO: Got endpoints: latency-svc-v7hsc [375.240337ms]
    Jan 30 12:49:52.287: INFO: Created: latency-svc-tlzpf
    Jan 30 12:49:52.308: INFO: Got endpoints: latency-svc-lhzp5 [374.278547ms]
    Jan 30 12:49:52.312: INFO: Created: latency-svc-k7b29
    Jan 30 12:49:52.333: INFO: Got endpoints: latency-svc-88c9m [374.708933ms]
    Jan 30 12:49:52.336: INFO: Created: latency-svc-btl56
    Jan 30 12:49:52.358: INFO: Got endpoints: latency-svc-tbblr [374.555068ms]
    Jan 30 12:49:52.361: INFO: Created: latency-svc-q2b2p
    Jan 30 12:49:52.383: INFO: Got endpoints: latency-svc-mlw5t [374.756868ms]
    Jan 30 12:49:52.387: INFO: Created: latency-svc-whqfd
    Jan 30 12:49:52.408: INFO: Got endpoints: latency-svc-rt2w6 [375.292427ms]
    Jan 30 12:49:52.412: INFO: Created: latency-svc-7872r
    Jan 30 12:49:52.433: INFO: Got endpoints: latency-svc-ppkt9 [374.143306ms]
    Jan 30 12:49:52.436: INFO: Created: latency-svc-b6jwl
    Jan 30 12:49:52.458: INFO: Got endpoints: latency-svc-fdlt8 [374.850819ms]
    Jan 30 12:49:52.461: INFO: Created: latency-svc-gjpsw
    Jan 30 12:49:52.484: INFO: Got endpoints: latency-svc-hmsd4 [375.720598ms]
    Jan 30 12:49:52.487: INFO: Created: latency-svc-bdcxc
    Jan 30 12:49:52.508: INFO: Got endpoints: latency-svc-rxvt7 [374.904442ms]
    Jan 30 12:49:52.512: INFO: Created: latency-svc-vqwrq
    Jan 30 12:49:52.533: INFO: Got endpoints: latency-svc-mn7dc [375.468678ms]
    Jan 30 12:49:52.537: INFO: Created: latency-svc-8c2sw
    Jan 30 12:49:52.558: INFO: Got endpoints: latency-svc-fv5mh [374.408197ms]
    Jan 30 12:49:52.561: INFO: Created: latency-svc-nd9tl
    Jan 30 12:49:52.583: INFO: Got endpoints: latency-svc-wlf6n [374.229655ms]
    Jan 30 12:49:52.586: INFO: Created: latency-svc-84gvg
    Jan 30 12:49:52.608: INFO: Got endpoints: latency-svc-4zvcd [374.519985ms]
    Jan 30 12:49:52.612: INFO: Created: latency-svc-m4m2m
    Jan 30 12:49:52.634: INFO: Got endpoints: latency-svc-9lbpm [375.198125ms]
    Jan 30 12:49:52.637: INFO: Created: latency-svc-pzjtx
    Jan 30 12:49:52.658: INFO: Got endpoints: latency-svc-tlzpf [374.671749ms]
    Jan 30 12:49:52.661: INFO: Created: latency-svc-kz7vj
    Jan 30 12:49:52.683: INFO: Got endpoints: latency-svc-k7b29 [375.039441ms]
    Jan 30 12:49:52.686: INFO: Created: latency-svc-vvdzs
    Jan 30 12:49:52.708: INFO: Got endpoints: latency-svc-btl56 [374.934985ms]
    Jan 30 12:49:52.711: INFO: Created: latency-svc-lcn6h
    Jan 30 12:49:52.734: INFO: Got endpoints: latency-svc-q2b2p [375.954587ms]
    Jan 30 12:49:52.737: INFO: Created: latency-svc-gngwn
    Jan 30 12:49:52.758: INFO: Got endpoints: latency-svc-whqfd [374.306879ms]
    Jan 30 12:49:52.761: INFO: Created: latency-svc-bwpvj
    Jan 30 12:49:52.783: INFO: Got endpoints: latency-svc-7872r [374.960512ms]
    Jan 30 12:49:52.787: INFO: Created: latency-svc-szlgj
    Jan 30 12:49:52.808: INFO: Got endpoints: latency-svc-b6jwl [375.538309ms]
    Jan 30 12:49:52.811: INFO: Created: latency-svc-jnzwm
    Jan 30 12:49:52.833: INFO: Got endpoints: latency-svc-gjpsw [374.71498ms]
    Jan 30 12:49:52.836: INFO: Created: latency-svc-mlmq9
    Jan 30 12:49:52.858: INFO: Got endpoints: latency-svc-bdcxc [373.918958ms]
    Jan 30 12:49:52.861: INFO: Created: latency-svc-t6z44
    Jan 30 12:49:52.884: INFO: Got endpoints: latency-svc-vqwrq [375.566804ms]
    Jan 30 12:49:52.887: INFO: Created: latency-svc-fn7mj
    Jan 30 12:49:52.908: INFO: Got endpoints: latency-svc-8c2sw [374.340103ms]
    Jan 30 12:49:52.911: INFO: Created: latency-svc-hbv6l
    Jan 30 12:49:52.933: INFO: Got endpoints: latency-svc-nd9tl [375.244093ms]
    Jan 30 12:49:52.937: INFO: Created: latency-svc-qrc4x
    Jan 30 12:49:52.959: INFO: Got endpoints: latency-svc-84gvg [375.861659ms]
    Jan 30 12:49:52.962: INFO: Created: latency-svc-htsmp
    Jan 30 12:49:52.983: INFO: Got endpoints: latency-svc-m4m2m [374.878759ms]
    Jan 30 12:49:52.987: INFO: Created: latency-svc-nhxns
    Jan 30 12:49:53.008: INFO: Got endpoints: latency-svc-pzjtx [374.484043ms]
    Jan 30 12:49:53.011: INFO: Created: latency-svc-8bvgq
    Jan 30 12:49:53.034: INFO: Got endpoints: latency-svc-kz7vj [375.853709ms]
    Jan 30 12:49:53.037: INFO: Created: latency-svc-9zv4z
    Jan 30 12:49:53.058: INFO: Got endpoints: latency-svc-vvdzs [374.931864ms]
    Jan 30 12:49:53.061: INFO: Created: latency-svc-nr4dc
    Jan 30 12:49:53.083: INFO: Got endpoints: latency-svc-lcn6h [375.267907ms]
    Jan 30 12:49:53.087: INFO: Created: latency-svc-sngns
    Jan 30 12:49:53.108: INFO: Got endpoints: latency-svc-gngwn [374.135682ms]
    Jan 30 12:49:53.111: INFO: Created: latency-svc-97cwr
    Jan 30 12:49:53.134: INFO: Got endpoints: latency-svc-bwpvj [375.893814ms]
    Jan 30 12:49:53.137: INFO: Created: latency-svc-bm7vb
    Jan 30 12:49:53.158: INFO: Got endpoints: latency-svc-szlgj [374.637431ms]
    Jan 30 12:49:53.162: INFO: Created: latency-svc-wqbk6
    Jan 30 12:49:53.183: INFO: Got endpoints: latency-svc-jnzwm [375.127276ms]
    Jan 30 12:49:53.187: INFO: Created: latency-svc-2jrxn
    Jan 30 12:49:53.208: INFO: Got endpoints: latency-svc-mlmq9 [375.252439ms]
    Jan 30 12:49:53.211: INFO: Created: latency-svc-dw4q2
    Jan 30 12:49:53.233: INFO: Got endpoints: latency-svc-t6z44 [374.832215ms]
    Jan 30 12:49:53.236: INFO: Created: latency-svc-dkwm2
    Jan 30 12:49:53.259: INFO: Got endpoints: latency-svc-fn7mj [374.803228ms]
    Jan 30 12:49:53.262: INFO: Created: latency-svc-8nfrz
    Jan 30 12:49:53.284: INFO: Got endpoints: latency-svc-hbv6l [376.131162ms]
    Jan 30 12:49:53.287: INFO: Created: latency-svc-9gqw2
    Jan 30 12:49:53.308: INFO: Got endpoints: latency-svc-qrc4x [374.537436ms]
    Jan 30 12:49:53.311: INFO: Created: latency-svc-n9lr8
    Jan 30 12:49:53.334: INFO: Got endpoints: latency-svc-htsmp [375.027914ms]
    Jan 30 12:49:53.337: INFO: Created: latency-svc-d56xp
    Jan 30 12:49:53.358: INFO: Got endpoints: latency-svc-nhxns [374.63538ms]
    Jan 30 12:49:53.361: INFO: Created: latency-svc-wmjck
    Jan 30 12:49:53.384: INFO: Got endpoints: latency-svc-8bvgq [375.527608ms]
    Jan 30 12:49:53.387: INFO: Created: latency-svc-nc6jl
    Jan 30 12:49:53.408: INFO: Got endpoints: latency-svc-9zv4z [373.946192ms]
    Jan 30 12:49:53.411: INFO: Created: latency-svc-kb6s4
    Jan 30 12:49:53.433: INFO: Got endpoints: latency-svc-nr4dc [375.075167ms]
    Jan 30 12:49:53.439: INFO: Created: latency-svc-7rt5n
    Jan 30 12:49:53.458: INFO: Got endpoints: latency-svc-sngns [375.048899ms]
    Jan 30 12:49:53.462: INFO: Created: latency-svc-zhnlz
    Jan 30 12:49:53.483: INFO: Got endpoints: latency-svc-97cwr [374.869645ms]
    Jan 30 12:49:53.486: INFO: Created: latency-svc-qddf6
    Jan 30 12:49:53.508: INFO: Got endpoints: latency-svc-bm7vb [374.752608ms]
    Jan 30 12:49:53.512: INFO: Created: latency-svc-ksljz
    Jan 30 12:49:53.533: INFO: Got endpoints: latency-svc-wqbk6 [375.355159ms]
    Jan 30 12:49:53.537: INFO: Created: latency-svc-f7z5d
    Jan 30 12:49:53.558: INFO: Got endpoints: latency-svc-2jrxn [374.196617ms]
    Jan 30 12:49:53.561: INFO: Created: latency-svc-7n4tf
    Jan 30 12:49:53.583: INFO: Got endpoints: latency-svc-dw4q2 [375.260444ms]
    Jan 30 12:49:53.587: INFO: Created: latency-svc-4zdwz
    Jan 30 12:49:53.608: INFO: Got endpoints: latency-svc-dkwm2 [375.307346ms]
    Jan 30 12:49:53.611: INFO: Created: latency-svc-ftbqk
    Jan 30 12:49:53.633: INFO: Got endpoints: latency-svc-8nfrz [374.462421ms]
    Jan 30 12:49:53.636: INFO: Created: latency-svc-ktcxt
    Jan 30 12:49:53.658: INFO: Got endpoints: latency-svc-9gqw2 [374.250949ms]
    Jan 30 12:49:53.662: INFO: Created: latency-svc-bmhd9
    Jan 30 12:49:53.683: INFO: Got endpoints: latency-svc-n9lr8 [375.514922ms]
    Jan 30 12:49:53.687: INFO: Created: latency-svc-htpsp
    Jan 30 12:49:53.708: INFO: Got endpoints: latency-svc-d56xp [374.257747ms]
    Jan 30 12:49:53.711: INFO: Created: latency-svc-q5gxc
    Jan 30 12:49:53.733: INFO: Got endpoints: latency-svc-wmjck [375.056333ms]
    Jan 30 12:49:53.736: INFO: Created: latency-svc-5v9nz
    Jan 30 12:49:53.758: INFO: Got endpoints: latency-svc-nc6jl [374.053915ms]
    Jan 30 12:49:53.761: INFO: Created: latency-svc-nbm2p
    Jan 30 12:49:53.783: INFO: Got endpoints: latency-svc-kb6s4 [375.532702ms]
    Jan 30 12:49:53.787: INFO: Created: latency-svc-rv9h8
    Jan 30 12:49:53.808: INFO: Got endpoints: latency-svc-7rt5n [374.790902ms]
    Jan 30 12:49:53.811: INFO: Created: latency-svc-6s528
    Jan 30 12:49:53.833: INFO: Got endpoints: latency-svc-zhnlz [374.491747ms]
    Jan 30 12:49:53.836: INFO: Created: latency-svc-6hbpq
    Jan 30 12:49:53.858: INFO: Got endpoints: latency-svc-qddf6 [375.224692ms]
    Jan 30 12:49:53.861: INFO: Created: latency-svc-dvn49
    Jan 30 12:49:53.883: INFO: Got endpoints: latency-svc-ksljz [374.44087ms]
    Jan 30 12:49:53.886: INFO: Created: latency-svc-c78wt
    Jan 30 12:49:53.908: INFO: Got endpoints: latency-svc-f7z5d [375.060179ms]
    Jan 30 12:49:53.913: INFO: Created: latency-svc-5gz9g
    Jan 30 12:49:53.933: INFO: Got endpoints: latency-svc-7n4tf [375.755165ms]
    Jan 30 12:49:53.937: INFO: Created: latency-svc-x9zhp
    Jan 30 12:49:53.958: INFO: Got endpoints: latency-svc-4zdwz [375.13931ms]
    Jan 30 12:49:53.962: INFO: Created: latency-svc-mkl8n
    Jan 30 12:49:53.984: INFO: Got endpoints: latency-svc-ftbqk [375.55157ms]
    Jan 30 12:49:53.987: INFO: Created: latency-svc-jnffp
    Jan 30 12:49:54.009: INFO: Got endpoints: latency-svc-ktcxt [375.624878ms]
    Jan 30 12:49:54.013: INFO: Created: latency-svc-97k8s
    Jan 30 12:49:54.033: INFO: Got endpoints: latency-svc-bmhd9 [374.806948ms]
    Jan 30 12:49:54.037: INFO: Created: latency-svc-fp7x5
    Jan 30 12:49:54.058: INFO: Got endpoints: latency-svc-htpsp [374.206575ms]
    Jan 30 12:49:54.061: INFO: Created: latency-svc-kf5tm
    Jan 30 12:49:54.083: INFO: Got endpoints: latency-svc-q5gxc [375.058383ms]
    Jan 30 12:49:54.087: INFO: Created: latency-svc-mz4xn
    Jan 30 12:49:54.108: INFO: Got endpoints: latency-svc-5v9nz [374.584988ms]
    Jan 30 12:49:54.111: INFO: Created: latency-svc-r46nx
    Jan 30 12:49:54.133: INFO: Got endpoints: latency-svc-nbm2p [374.99491ms]
    Jan 30 12:49:54.136: INFO: Created: latency-svc-w8wm9
    Jan 30 12:49:54.158: INFO: Got endpoints: latency-svc-rv9h8 [375.097056ms]
    Jan 30 12:49:54.162: INFO: Created: latency-svc-d94vd
    Jan 30 12:49:54.183: INFO: Got endpoints: latency-svc-6s528 [375.4244ms]
    Jan 30 12:49:54.187: INFO: Created: latency-svc-vt2kl
    Jan 30 12:49:54.208: INFO: Got endpoints: latency-svc-6hbpq [375.379799ms]
    Jan 30 12:49:54.212: INFO: Created: latency-svc-8gl5n
    Jan 30 12:49:54.233: INFO: Got endpoints: latency-svc-dvn49 [375.291316ms]
    Jan 30 12:49:54.237: INFO: Created: latency-svc-8lv6q
    Jan 30 12:49:54.258: INFO: Got endpoints: latency-svc-c78wt [375.47599ms]
    Jan 30 12:49:54.262: INFO: Created: latency-svc-znnkb
    Jan 30 12:49:54.283: INFO: Got endpoints: latency-svc-5gz9g [374.416662ms]
    Jan 30 12:49:54.286: INFO: Created: latency-svc-xfjr5
    Jan 30 12:49:54.308: INFO: Got endpoints: latency-svc-x9zhp [374.343815ms]
    Jan 30 12:49:54.311: INFO: Created: latency-svc-9qt8c
    Jan 30 12:49:54.334: INFO: Got endpoints: latency-svc-mkl8n [375.093566ms]
    Jan 30 12:49:54.337: INFO: Created: latency-svc-829ml
    Jan 30 12:49:54.358: INFO: Got endpoints: latency-svc-jnffp [374.911615ms]
    Jan 30 12:49:54.362: INFO: Created: latency-svc-fg68l
    Jan 30 12:49:54.383: INFO: Got endpoints: latency-svc-97k8s [374.367576ms]
    Jan 30 12:49:54.387: INFO: Created: latency-svc-ckll9
    Jan 30 12:49:54.408: INFO: Got endpoints: latency-svc-fp7x5 [375.174167ms]
    Jan 30 12:49:54.412: INFO: Created: latency-svc-5rv9j
    Jan 30 12:49:54.433: INFO: Got endpoints: latency-svc-kf5tm [375.572121ms]
    Jan 30 12:49:54.437: INFO: Created: latency-svc-2djtc
    Jan 30 12:49:54.458: INFO: Got endpoints: latency-svc-mz4xn [375.429535ms]
    Jan 30 12:49:54.462: INFO: Created: latency-svc-bkhlf
    Jan 30 12:49:54.483: INFO: Got endpoints: latency-svc-r46nx [375.610382ms]
    Jan 30 12:49:54.487: INFO: Created: latency-svc-k4hzv
    Jan 30 12:49:54.508: INFO: Got endpoints: latency-svc-w8wm9 [375.128974ms]
    Jan 30 12:49:54.511: INFO: Created: latency-svc-9hk2x
    Jan 30 12:49:54.533: INFO: Got endpoints: latency-svc-d94vd [374.617698ms]
    Jan 30 12:49:54.537: INFO: Created: latency-svc-q8qqm
    Jan 30 12:49:54.558: INFO: Got endpoints: latency-svc-vt2kl [375.007078ms]
    Jan 30 12:49:54.562: INFO: Created: latency-svc-zrnd8
    Jan 30 12:49:54.583: INFO: Got endpoints: latency-svc-8gl5n [374.599464ms]
    Jan 30 12:49:54.586: INFO: Created: latency-svc-pdptj
    Jan 30 12:49:54.608: INFO: Got endpoints: latency-svc-8lv6q [374.783447ms]
    Jan 30 12:49:54.611: INFO: Created: latency-svc-q6l78
    Jan 30 12:49:54.633: INFO: Got endpoints: latency-svc-znnkb [374.638261ms]
    Jan 30 12:49:54.636: INFO: Created: latency-svc-wmdqz
    Jan 30 12:49:54.658: INFO: Got endpoints: latency-svc-xfjr5 [375.023776ms]
    Jan 30 12:49:54.662: INFO: Created: latency-svc-nskfh
    Jan 30 12:49:54.683: INFO: Got endpoints: latency-svc-9qt8c [375.107197ms]
    Jan 30 12:49:54.687: INFO: Created: latency-svc-6r8tc
    Jan 30 12:49:54.708: INFO: Got endpoints: latency-svc-829ml [374.13777ms]
    Jan 30 12:49:54.711: INFO: Created: latency-svc-jzvd8
    Jan 30 12:49:54.733: INFO: Got endpoints: latency-svc-fg68l [374.709745ms]
    Jan 30 12:49:54.737: INFO: Created: latency-svc-stmvj
    Jan 30 12:49:54.758: INFO: Got endpoints: latency-svc-ckll9 [374.678638ms]
    Jan 30 12:49:54.761: INFO: Created: latency-svc-4kx7p
    Jan 30 12:49:54.783: INFO: Got endpoints: latency-svc-5rv9j [374.564614ms]
    Jan 30 12:49:54.786: INFO: Created: latency-svc-dnp4h
    Jan 30 12:49:54.809: INFO: Got endpoints: latency-svc-2djtc [375.411144ms]
    Jan 30 12:49:54.812: INFO: Created: latency-svc-rkq9l
    Jan 30 12:49:54.833: INFO: Got endpoints: latency-svc-bkhlf [374.929528ms]
    Jan 30 12:49:54.837: INFO: Created: latency-svc-sh7hr
    Jan 30 12:49:54.858: INFO: Got endpoints: latency-svc-k4hzv [375.120532ms]
    Jan 30 12:49:54.862: INFO: Created: latency-svc-rj4c7
    Jan 30 12:49:54.883: INFO: Got endpoints: latency-svc-9hk2x [375.115901ms]
    Jan 30 12:49:54.908: INFO: Got endpoints: latency-svc-q8qqm [374.68853ms]
    Jan 30 12:49:54.933: INFO: Got endpoints: latency-svc-zrnd8 [374.166209ms]
    Jan 30 12:49:54.958: INFO: Got endpoints: latency-svc-pdptj [375.02405ms]
    Jan 30 12:49:54.983: INFO: Got endpoints: latency-svc-q6l78 [375.250417ms]
    Jan 30 12:49:55.008: INFO: Got endpoints: latency-svc-wmdqz [375.314092ms]
    Jan 30 12:49:55.034: INFO: Got endpoints: latency-svc-nskfh [375.790233ms]
    Jan 30 12:49:55.058: INFO: Got endpoints: latency-svc-6r8tc [375.076983ms]
    Jan 30 12:49:55.083: INFO: Got endpoints: latency-svc-jzvd8 [375.642576ms]
    Jan 30 12:49:55.108: INFO: Got endpoints: latency-svc-stmvj [374.331878ms]
    Jan 30 12:49:55.133: INFO: Got endpoints: latency-svc-4kx7p [375.249382ms]
    Jan 30 12:49:55.158: INFO: Got endpoints: latency-svc-dnp4h [375.180435ms]
    Jan 30 12:49:55.184: INFO: Got endpoints: latency-svc-rkq9l [374.906822ms]
    Jan 30 12:49:55.208: INFO: Got endpoints: latency-svc-sh7hr [374.915738ms]
    Jan 30 12:49:55.233: INFO: Got endpoints: latency-svc-rj4c7 [374.997319ms]
    Jan 30 12:49:55.233: INFO: Latencies: [6.767295ms 7.642498ms 7.824051ms 8.090806ms 8.51065ms 9.604173ms 9.685268ms 10.313103ms 10.837951ms 10.981748ms 11.633496ms 11.783094ms 12.687279ms 12.947927ms 13.07085ms 14.41055ms 14.786583ms 14.807269ms 16.001417ms 16.031893ms 16.608688ms 16.982684ms 18.402853ms 18.82482ms 19.019803ms 19.639987ms 19.866747ms 19.892722ms 20.088777ms 20.12793ms 20.194472ms 20.625932ms 20.677963ms 20.74081ms 21.036972ms 22.800846ms 22.879867ms 24.008317ms 25.622332ms 25.95768ms 26.036895ms 26.240729ms 27.137701ms 27.353093ms 28.761236ms 29.044354ms 29.686767ms 29.785908ms 30.426993ms 31.78146ms 32.074729ms 32.588782ms 33.120009ms 33.229241ms 33.749056ms 33.874019ms 34.344455ms 35.521553ms 37.073306ms 37.258105ms 37.908391ms 39.606512ms 44.255793ms 57.722346ms 60.772261ms 82.181651ms 106.918137ms 131.816354ms 156.187681ms 179.641291ms 204.984569ms 229.228836ms 253.61922ms 278.697307ms 302.097767ms 326.880424ms 351.134862ms 373.918958ms 373.946192ms 374.053915ms 374.135682ms 374.13777ms 374.143306ms 374.166209ms 374.196617ms 374.206575ms 374.229655ms 374.250949ms 374.257747ms 374.278547ms 374.306879ms 374.331878ms 374.340103ms 374.343815ms 374.367576ms 374.408197ms 374.416662ms 374.44087ms 374.462421ms 374.484043ms 374.491747ms 374.519985ms 374.537436ms 374.555068ms 374.564614ms 374.584988ms 374.599464ms 374.617698ms 374.63538ms 374.637431ms 374.638261ms 374.671749ms 374.678638ms 374.68853ms 374.708933ms 374.709745ms 374.71498ms 374.752608ms 374.756868ms 374.783447ms 374.790902ms 374.803228ms 374.806948ms 374.832215ms 374.850819ms 374.869645ms 374.878759ms 374.904442ms 374.906822ms 374.911615ms 374.915738ms 374.929528ms 374.931864ms 374.934985ms 374.960512ms 374.99491ms 374.997319ms 375.007078ms 375.023776ms 375.02405ms 375.026576ms 375.027914ms 375.039441ms 375.048899ms 375.056333ms 375.058383ms 375.060179ms 375.075167ms 375.076983ms 375.093566ms 375.097056ms 375.107197ms 375.115901ms 375.120532ms 375.127276ms 375.128974ms 375.13931ms 375.174167ms 375.180435ms 375.198125ms 375.224692ms 375.240337ms 375.244093ms 375.249382ms 375.250417ms 375.252439ms 375.260444ms 375.267907ms 375.291316ms 375.292427ms 375.307346ms 375.314092ms 375.355159ms 375.379799ms 375.396286ms 375.411144ms 375.4244ms 375.429535ms 375.438525ms 375.468678ms 375.47599ms 375.514922ms 375.527608ms 375.532702ms 375.538309ms 375.55157ms 375.566804ms 375.572121ms 375.610382ms 375.624878ms 375.642576ms 375.720598ms 375.755165ms 375.78916ms 375.790233ms 375.853709ms 375.861659ms 375.893814ms 375.954587ms 376.131162ms]
    Jan 30 12:49:55.233: INFO: 50 %ile: 374.491747ms
    Jan 30 12:49:55.233: INFO: 90 %ile: 375.47599ms
    Jan 30 12:49:55.234: INFO: 99 %ile: 375.954587ms
    Jan 30 12:49:55.234: INFO: Total sample count: 200
    [AfterEach] [sig-network] Service endpoints latency
      test/e2e/framework/node/init/init.go:32
    Jan 30 12:49:55.234: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Service endpoints latency
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Service endpoints latency
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Service endpoints latency
      tear down framework | framework.go:193
    STEP: Destroying namespace "svc-latency-7939" for this suite. 01/30/23 12:49:55.237
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:152
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 12:49:55.241
Jan 30 12:49:55.241: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename var-expansion 01/30/23 12:49:55.242
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:49:55.247
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:49:55.249
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:152
Jan 30 12:49:55.255: INFO: Waiting up to 2m0s for pod "var-expansion-ae72a39d-e903-4099-9820-62ee97d64508" in namespace "var-expansion-4479" to be "container 0 failed with reason CreateContainerConfigError"
Jan 30 12:49:55.257: INFO: Pod "var-expansion-ae72a39d-e903-4099-9820-62ee97d64508": Phase="Pending", Reason="", readiness=false. Elapsed: 1.50071ms
Jan 30 12:49:57.259: INFO: Pod "var-expansion-ae72a39d-e903-4099-9820-62ee97d64508": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004069273s
Jan 30 12:49:57.259: INFO: Pod "var-expansion-ae72a39d-e903-4099-9820-62ee97d64508" satisfied condition "container 0 failed with reason CreateContainerConfigError"
Jan 30 12:49:57.259: INFO: Deleting pod "var-expansion-ae72a39d-e903-4099-9820-62ee97d64508" in namespace "var-expansion-4479"
Jan 30 12:49:57.262: INFO: Wait up to 5m0s for pod "var-expansion-ae72a39d-e903-4099-9820-62ee97d64508" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Jan 30 12:49:59.266: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-4479" for this suite. 01/30/23 12:49:59.269
------------------------------
• [4.030 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:152

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 12:49:55.241
    Jan 30 12:49:55.241: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename var-expansion 01/30/23 12:49:55.242
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:49:55.247
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:49:55.249
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
      test/e2e/common/node/expansion.go:152
    Jan 30 12:49:55.255: INFO: Waiting up to 2m0s for pod "var-expansion-ae72a39d-e903-4099-9820-62ee97d64508" in namespace "var-expansion-4479" to be "container 0 failed with reason CreateContainerConfigError"
    Jan 30 12:49:55.257: INFO: Pod "var-expansion-ae72a39d-e903-4099-9820-62ee97d64508": Phase="Pending", Reason="", readiness=false. Elapsed: 1.50071ms
    Jan 30 12:49:57.259: INFO: Pod "var-expansion-ae72a39d-e903-4099-9820-62ee97d64508": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004069273s
    Jan 30 12:49:57.259: INFO: Pod "var-expansion-ae72a39d-e903-4099-9820-62ee97d64508" satisfied condition "container 0 failed with reason CreateContainerConfigError"
    Jan 30 12:49:57.259: INFO: Deleting pod "var-expansion-ae72a39d-e903-4099-9820-62ee97d64508" in namespace "var-expansion-4479"
    Jan 30 12:49:57.262: INFO: Wait up to 5m0s for pod "var-expansion-ae72a39d-e903-4099-9820-62ee97d64508" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Jan 30 12:49:59.266: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-4479" for this suite. 01/30/23 12:49:59.269
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:392
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 12:49:59.271
Jan 30 12:49:59.271: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename resourcequota 01/30/23 12:49:59.272
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:49:59.278
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:49:59.28
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:392
STEP: Counting existing ResourceQuota 01/30/23 12:49:59.282
STEP: Creating a ResourceQuota 01/30/23 12:50:04.284
STEP: Ensuring resource quota status is calculated 01/30/23 12:50:04.287
STEP: Creating a ReplicationController 01/30/23 12:50:06.29
STEP: Ensuring resource quota status captures replication controller creation 01/30/23 12:50:06.297
STEP: Deleting a ReplicationController 01/30/23 12:50:08.299
STEP: Ensuring resource quota status released usage 01/30/23 12:50:08.302
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Jan 30 12:50:10.305: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-891" for this suite. 01/30/23 12:50:10.308
------------------------------
• [SLOW TEST] [11.040 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:392

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 12:49:59.271
    Jan 30 12:49:59.271: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename resourcequota 01/30/23 12:49:59.272
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:49:59.278
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:49:59.28
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
      test/e2e/apimachinery/resource_quota.go:392
    STEP: Counting existing ResourceQuota 01/30/23 12:49:59.282
    STEP: Creating a ResourceQuota 01/30/23 12:50:04.284
    STEP: Ensuring resource quota status is calculated 01/30/23 12:50:04.287
    STEP: Creating a ReplicationController 01/30/23 12:50:06.29
    STEP: Ensuring resource quota status captures replication controller creation 01/30/23 12:50:06.297
    STEP: Deleting a ReplicationController 01/30/23 12:50:08.299
    STEP: Ensuring resource quota status released usage 01/30/23 12:50:08.302
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Jan 30 12:50:10.305: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-891" for this suite. 01/30/23 12:50:10.308
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:308
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 12:50:10.311
Jan 30 12:50:10.312: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename webhook 01/30/23 12:50:10.312
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:50:10.319
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:50:10.321
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 01/30/23 12:50:10.329
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/30/23 12:50:10.63
STEP: Deploying the webhook pod 01/30/23 12:50:10.633
STEP: Wait for the deployment to be ready 01/30/23 12:50:10.639
Jan 30 12:50:10.642: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 01/30/23 12:50:12.648
STEP: Verifying the service has paired with the endpoint 01/30/23 12:50:12.651
Jan 30 12:50:13.652: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:308
STEP: Registering the crd webhook via the AdmissionRegistration API 01/30/23 12:50:13.653
STEP: Creating a custom resource definition that should be denied by the webhook 01/30/23 12:50:13.663
Jan 30 12:50:13.663: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 30 12:50:13.674: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-6501" for this suite. 01/30/23 12:50:13.688
STEP: Destroying namespace "webhook-6501-markers" for this suite. 01/30/23 12:50:13.691
------------------------------
• [3.381 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:308

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 12:50:10.311
    Jan 30 12:50:10.312: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename webhook 01/30/23 12:50:10.312
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:50:10.319
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:50:10.321
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 01/30/23 12:50:10.329
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/30/23 12:50:10.63
    STEP: Deploying the webhook pod 01/30/23 12:50:10.633
    STEP: Wait for the deployment to be ready 01/30/23 12:50:10.639
    Jan 30 12:50:10.642: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 01/30/23 12:50:12.648
    STEP: Verifying the service has paired with the endpoint 01/30/23 12:50:12.651
    Jan 30 12:50:13.652: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should deny crd creation [Conformance]
      test/e2e/apimachinery/webhook.go:308
    STEP: Registering the crd webhook via the AdmissionRegistration API 01/30/23 12:50:13.653
    STEP: Creating a custom resource definition that should be denied by the webhook 01/30/23 12:50:13.663
    Jan 30 12:50:13.663: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 30 12:50:13.674: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-6501" for this suite. 01/30/23 12:50:13.688
    STEP: Destroying namespace "webhook-6501-markers" for this suite. 01/30/23 12:50:13.691
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets
  should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:154
[BeforeEach] [sig-node] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 12:50:13.695
Jan 30 12:50:13.695: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename secrets 01/30/23 12:50:13.695
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:50:13.701
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:50:13.703
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:154
STEP: creating a secret 01/30/23 12:50:13.705
STEP: listing secrets in all namespaces to ensure that there are more than zero 01/30/23 12:50:13.707
STEP: patching the secret 01/30/23 12:50:13.708
STEP: deleting the secret using a LabelSelector 01/30/23 12:50:13.714
STEP: listing secrets in all namespaces, searching for label name and value in patch 01/30/23 12:50:13.717
[AfterEach] [sig-node] Secrets
  test/e2e/framework/node/init/init.go:32
Jan 30 12:50:13.718: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-5328" for this suite. 01/30/23 12:50:13.721
------------------------------
• [0.028 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 12:50:13.695
    Jan 30 12:50:13.695: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename secrets 01/30/23 12:50:13.695
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:50:13.701
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:50:13.703
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should patch a secret [Conformance]
      test/e2e/common/node/secrets.go:154
    STEP: creating a secret 01/30/23 12:50:13.705
    STEP: listing secrets in all namespaces to ensure that there are more than zero 01/30/23 12:50:13.707
    STEP: patching the secret 01/30/23 12:50:13.708
    STEP: deleting the secret using a LabelSelector 01/30/23 12:50:13.714
    STEP: listing secrets in all namespaces, searching for label name and value in patch 01/30/23 12:50:13.717
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/node/init/init.go:32
    Jan 30 12:50:13.718: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-5328" for this suite. 01/30/23 12:50:13.721
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-network] DNS
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 12:50:13.723
Jan 30 12:50:13.723: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename dns 01/30/23 12:50:13.724
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:50:13.73
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:50:13.732
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193
STEP: Creating a test headless service 01/30/23 12:50:13.733
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7553 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-7553;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7553 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-7553;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7553.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-7553.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7553.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-7553.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-7553.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-7553.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-7553.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-7553.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-7553.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-7553.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-7553.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-7553.svc;check="$$(dig +notcp +noall +answer +search 43.73.178.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.178.73.43_udp@PTR;check="$$(dig +tcp +noall +answer +search 43.73.178.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.178.73.43_tcp@PTR;sleep 1; done
 01/30/23 12:50:13.74
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7553 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-7553;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7553 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-7553;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7553.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-7553.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7553.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-7553.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-7553.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-7553.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-7553.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-7553.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-7553.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-7553.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-7553.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-7553.svc;check="$$(dig +notcp +noall +answer +search 43.73.178.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.178.73.43_udp@PTR;check="$$(dig +tcp +noall +answer +search 43.73.178.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.178.73.43_tcp@PTR;sleep 1; done
 01/30/23 12:50:13.74
STEP: creating a pod to probe DNS 01/30/23 12:50:13.74
STEP: submitting the pod to kubernetes 01/30/23 12:50:13.74
Jan 30 12:50:13.744: INFO: Waiting up to 15m0s for pod "dns-test-e6bc71c1-28a7-48d2-8f2f-74b97894f86e" in namespace "dns-7553" to be "running"
Jan 30 12:50:13.746: INFO: Pod "dns-test-e6bc71c1-28a7-48d2-8f2f-74b97894f86e": Phase="Pending", Reason="", readiness=false. Elapsed: 1.827536ms
Jan 30 12:50:15.750: INFO: Pod "dns-test-e6bc71c1-28a7-48d2-8f2f-74b97894f86e": Phase="Running", Reason="", readiness=true. Elapsed: 2.005022506s
Jan 30 12:50:15.750: INFO: Pod "dns-test-e6bc71c1-28a7-48d2-8f2f-74b97894f86e" satisfied condition "running"
STEP: retrieving the pod 01/30/23 12:50:15.75
STEP: looking for the results for each expected name from probers 01/30/23 12:50:15.751
Jan 30 12:50:15.754: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-7553/dns-test-e6bc71c1-28a7-48d2-8f2f-74b97894f86e: the server could not find the requested resource (get pods dns-test-e6bc71c1-28a7-48d2-8f2f-74b97894f86e)
Jan 30 12:50:15.756: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-7553/dns-test-e6bc71c1-28a7-48d2-8f2f-74b97894f86e: the server could not find the requested resource (get pods dns-test-e6bc71c1-28a7-48d2-8f2f-74b97894f86e)
Jan 30 12:50:15.758: INFO: Unable to read wheezy_udp@dns-test-service.dns-7553 from pod dns-7553/dns-test-e6bc71c1-28a7-48d2-8f2f-74b97894f86e: the server could not find the requested resource (get pods dns-test-e6bc71c1-28a7-48d2-8f2f-74b97894f86e)
Jan 30 12:50:15.759: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7553 from pod dns-7553/dns-test-e6bc71c1-28a7-48d2-8f2f-74b97894f86e: the server could not find the requested resource (get pods dns-test-e6bc71c1-28a7-48d2-8f2f-74b97894f86e)
Jan 30 12:50:15.761: INFO: Unable to read wheezy_udp@dns-test-service.dns-7553.svc from pod dns-7553/dns-test-e6bc71c1-28a7-48d2-8f2f-74b97894f86e: the server could not find the requested resource (get pods dns-test-e6bc71c1-28a7-48d2-8f2f-74b97894f86e)
Jan 30 12:50:15.763: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7553.svc from pod dns-7553/dns-test-e6bc71c1-28a7-48d2-8f2f-74b97894f86e: the server could not find the requested resource (get pods dns-test-e6bc71c1-28a7-48d2-8f2f-74b97894f86e)
Jan 30 12:50:15.764: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-7553.svc from pod dns-7553/dns-test-e6bc71c1-28a7-48d2-8f2f-74b97894f86e: the server could not find the requested resource (get pods dns-test-e6bc71c1-28a7-48d2-8f2f-74b97894f86e)
Jan 30 12:50:15.766: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-7553.svc from pod dns-7553/dns-test-e6bc71c1-28a7-48d2-8f2f-74b97894f86e: the server could not find the requested resource (get pods dns-test-e6bc71c1-28a7-48d2-8f2f-74b97894f86e)
Jan 30 12:50:15.775: INFO: Unable to read jessie_udp@dns-test-service from pod dns-7553/dns-test-e6bc71c1-28a7-48d2-8f2f-74b97894f86e: the server could not find the requested resource (get pods dns-test-e6bc71c1-28a7-48d2-8f2f-74b97894f86e)
Jan 30 12:50:15.777: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-7553/dns-test-e6bc71c1-28a7-48d2-8f2f-74b97894f86e: the server could not find the requested resource (get pods dns-test-e6bc71c1-28a7-48d2-8f2f-74b97894f86e)
Jan 30 12:50:15.778: INFO: Unable to read jessie_udp@dns-test-service.dns-7553 from pod dns-7553/dns-test-e6bc71c1-28a7-48d2-8f2f-74b97894f86e: the server could not find the requested resource (get pods dns-test-e6bc71c1-28a7-48d2-8f2f-74b97894f86e)
Jan 30 12:50:15.780: INFO: Unable to read jessie_tcp@dns-test-service.dns-7553 from pod dns-7553/dns-test-e6bc71c1-28a7-48d2-8f2f-74b97894f86e: the server could not find the requested resource (get pods dns-test-e6bc71c1-28a7-48d2-8f2f-74b97894f86e)
Jan 30 12:50:15.782: INFO: Unable to read jessie_udp@dns-test-service.dns-7553.svc from pod dns-7553/dns-test-e6bc71c1-28a7-48d2-8f2f-74b97894f86e: the server could not find the requested resource (get pods dns-test-e6bc71c1-28a7-48d2-8f2f-74b97894f86e)
Jan 30 12:50:15.783: INFO: Unable to read jessie_tcp@dns-test-service.dns-7553.svc from pod dns-7553/dns-test-e6bc71c1-28a7-48d2-8f2f-74b97894f86e: the server could not find the requested resource (get pods dns-test-e6bc71c1-28a7-48d2-8f2f-74b97894f86e)
Jan 30 12:50:15.785: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-7553.svc from pod dns-7553/dns-test-e6bc71c1-28a7-48d2-8f2f-74b97894f86e: the server could not find the requested resource (get pods dns-test-e6bc71c1-28a7-48d2-8f2f-74b97894f86e)
Jan 30 12:50:15.787: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-7553.svc from pod dns-7553/dns-test-e6bc71c1-28a7-48d2-8f2f-74b97894f86e: the server could not find the requested resource (get pods dns-test-e6bc71c1-28a7-48d2-8f2f-74b97894f86e)
Jan 30 12:50:15.794: INFO: Lookups using dns-7553/dns-test-e6bc71c1-28a7-48d2-8f2f-74b97894f86e failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-7553 wheezy_tcp@dns-test-service.dns-7553 wheezy_udp@dns-test-service.dns-7553.svc wheezy_tcp@dns-test-service.dns-7553.svc wheezy_udp@_http._tcp.dns-test-service.dns-7553.svc wheezy_tcp@_http._tcp.dns-test-service.dns-7553.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-7553 jessie_tcp@dns-test-service.dns-7553 jessie_udp@dns-test-service.dns-7553.svc jessie_tcp@dns-test-service.dns-7553.svc jessie_udp@_http._tcp.dns-test-service.dns-7553.svc jessie_tcp@_http._tcp.dns-test-service.dns-7553.svc]

Jan 30 12:50:20.808: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-7553.svc from pod dns-7553/dns-test-e6bc71c1-28a7-48d2-8f2f-74b97894f86e: the server could not find the requested resource (get pods dns-test-e6bc71c1-28a7-48d2-8f2f-74b97894f86e)
Jan 30 12:50:20.809: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-7553.svc from pod dns-7553/dns-test-e6bc71c1-28a7-48d2-8f2f-74b97894f86e: the server could not find the requested resource (get pods dns-test-e6bc71c1-28a7-48d2-8f2f-74b97894f86e)
Jan 30 12:50:20.818: INFO: Unable to read jessie_udp@dns-test-service from pod dns-7553/dns-test-e6bc71c1-28a7-48d2-8f2f-74b97894f86e: the server could not find the requested resource (get pods dns-test-e6bc71c1-28a7-48d2-8f2f-74b97894f86e)
Jan 30 12:50:20.820: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-7553/dns-test-e6bc71c1-28a7-48d2-8f2f-74b97894f86e: the server could not find the requested resource (get pods dns-test-e6bc71c1-28a7-48d2-8f2f-74b97894f86e)
Jan 30 12:50:20.821: INFO: Unable to read jessie_udp@dns-test-service.dns-7553 from pod dns-7553/dns-test-e6bc71c1-28a7-48d2-8f2f-74b97894f86e: the server could not find the requested resource (get pods dns-test-e6bc71c1-28a7-48d2-8f2f-74b97894f86e)
Jan 30 12:50:20.823: INFO: Unable to read jessie_tcp@dns-test-service.dns-7553 from pod dns-7553/dns-test-e6bc71c1-28a7-48d2-8f2f-74b97894f86e: the server could not find the requested resource (get pods dns-test-e6bc71c1-28a7-48d2-8f2f-74b97894f86e)
Jan 30 12:50:20.825: INFO: Unable to read jessie_udp@dns-test-service.dns-7553.svc from pod dns-7553/dns-test-e6bc71c1-28a7-48d2-8f2f-74b97894f86e: the server could not find the requested resource (get pods dns-test-e6bc71c1-28a7-48d2-8f2f-74b97894f86e)
Jan 30 12:50:20.827: INFO: Unable to read jessie_tcp@dns-test-service.dns-7553.svc from pod dns-7553/dns-test-e6bc71c1-28a7-48d2-8f2f-74b97894f86e: the server could not find the requested resource (get pods dns-test-e6bc71c1-28a7-48d2-8f2f-74b97894f86e)
Jan 30 12:50:20.828: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-7553.svc from pod dns-7553/dns-test-e6bc71c1-28a7-48d2-8f2f-74b97894f86e: the server could not find the requested resource (get pods dns-test-e6bc71c1-28a7-48d2-8f2f-74b97894f86e)
Jan 30 12:50:20.830: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-7553.svc from pod dns-7553/dns-test-e6bc71c1-28a7-48d2-8f2f-74b97894f86e: the server could not find the requested resource (get pods dns-test-e6bc71c1-28a7-48d2-8f2f-74b97894f86e)
Jan 30 12:50:20.837: INFO: Lookups using dns-7553/dns-test-e6bc71c1-28a7-48d2-8f2f-74b97894f86e failed for: [wheezy_udp@_http._tcp.dns-test-service.dns-7553.svc wheezy_tcp@_http._tcp.dns-test-service.dns-7553.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-7553 jessie_tcp@dns-test-service.dns-7553 jessie_udp@dns-test-service.dns-7553.svc jessie_tcp@dns-test-service.dns-7553.svc jessie_udp@_http._tcp.dns-test-service.dns-7553.svc jessie_tcp@_http._tcp.dns-test-service.dns-7553.svc]

Jan 30 12:50:25.838: INFO: DNS probes using dns-7553/dns-test-e6bc71c1-28a7-48d2-8f2f-74b97894f86e succeeded

STEP: deleting the pod 01/30/23 12:50:25.838
STEP: deleting the test service 01/30/23 12:50:25.843
STEP: deleting the test headless service 01/30/23 12:50:25.848
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Jan 30 12:50:25.852: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-7553" for this suite. 01/30/23 12:50:25.854
------------------------------
• [SLOW TEST] [12.133 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 12:50:13.723
    Jan 30 12:50:13.723: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename dns 01/30/23 12:50:13.724
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:50:13.73
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:50:13.732
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
      test/e2e/network/dns.go:193
    STEP: Creating a test headless service 01/30/23 12:50:13.733
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7553 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-7553;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7553 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-7553;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7553.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-7553.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7553.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-7553.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-7553.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-7553.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-7553.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-7553.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-7553.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-7553.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-7553.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-7553.svc;check="$$(dig +notcp +noall +answer +search 43.73.178.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.178.73.43_udp@PTR;check="$$(dig +tcp +noall +answer +search 43.73.178.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.178.73.43_tcp@PTR;sleep 1; done
     01/30/23 12:50:13.74
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7553 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-7553;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7553 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-7553;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7553.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-7553.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7553.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-7553.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-7553.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-7553.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-7553.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-7553.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-7553.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-7553.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-7553.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-7553.svc;check="$$(dig +notcp +noall +answer +search 43.73.178.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.178.73.43_udp@PTR;check="$$(dig +tcp +noall +answer +search 43.73.178.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.178.73.43_tcp@PTR;sleep 1; done
     01/30/23 12:50:13.74
    STEP: creating a pod to probe DNS 01/30/23 12:50:13.74
    STEP: submitting the pod to kubernetes 01/30/23 12:50:13.74
    Jan 30 12:50:13.744: INFO: Waiting up to 15m0s for pod "dns-test-e6bc71c1-28a7-48d2-8f2f-74b97894f86e" in namespace "dns-7553" to be "running"
    Jan 30 12:50:13.746: INFO: Pod "dns-test-e6bc71c1-28a7-48d2-8f2f-74b97894f86e": Phase="Pending", Reason="", readiness=false. Elapsed: 1.827536ms
    Jan 30 12:50:15.750: INFO: Pod "dns-test-e6bc71c1-28a7-48d2-8f2f-74b97894f86e": Phase="Running", Reason="", readiness=true. Elapsed: 2.005022506s
    Jan 30 12:50:15.750: INFO: Pod "dns-test-e6bc71c1-28a7-48d2-8f2f-74b97894f86e" satisfied condition "running"
    STEP: retrieving the pod 01/30/23 12:50:15.75
    STEP: looking for the results for each expected name from probers 01/30/23 12:50:15.751
    Jan 30 12:50:15.754: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-7553/dns-test-e6bc71c1-28a7-48d2-8f2f-74b97894f86e: the server could not find the requested resource (get pods dns-test-e6bc71c1-28a7-48d2-8f2f-74b97894f86e)
    Jan 30 12:50:15.756: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-7553/dns-test-e6bc71c1-28a7-48d2-8f2f-74b97894f86e: the server could not find the requested resource (get pods dns-test-e6bc71c1-28a7-48d2-8f2f-74b97894f86e)
    Jan 30 12:50:15.758: INFO: Unable to read wheezy_udp@dns-test-service.dns-7553 from pod dns-7553/dns-test-e6bc71c1-28a7-48d2-8f2f-74b97894f86e: the server could not find the requested resource (get pods dns-test-e6bc71c1-28a7-48d2-8f2f-74b97894f86e)
    Jan 30 12:50:15.759: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7553 from pod dns-7553/dns-test-e6bc71c1-28a7-48d2-8f2f-74b97894f86e: the server could not find the requested resource (get pods dns-test-e6bc71c1-28a7-48d2-8f2f-74b97894f86e)
    Jan 30 12:50:15.761: INFO: Unable to read wheezy_udp@dns-test-service.dns-7553.svc from pod dns-7553/dns-test-e6bc71c1-28a7-48d2-8f2f-74b97894f86e: the server could not find the requested resource (get pods dns-test-e6bc71c1-28a7-48d2-8f2f-74b97894f86e)
    Jan 30 12:50:15.763: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7553.svc from pod dns-7553/dns-test-e6bc71c1-28a7-48d2-8f2f-74b97894f86e: the server could not find the requested resource (get pods dns-test-e6bc71c1-28a7-48d2-8f2f-74b97894f86e)
    Jan 30 12:50:15.764: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-7553.svc from pod dns-7553/dns-test-e6bc71c1-28a7-48d2-8f2f-74b97894f86e: the server could not find the requested resource (get pods dns-test-e6bc71c1-28a7-48d2-8f2f-74b97894f86e)
    Jan 30 12:50:15.766: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-7553.svc from pod dns-7553/dns-test-e6bc71c1-28a7-48d2-8f2f-74b97894f86e: the server could not find the requested resource (get pods dns-test-e6bc71c1-28a7-48d2-8f2f-74b97894f86e)
    Jan 30 12:50:15.775: INFO: Unable to read jessie_udp@dns-test-service from pod dns-7553/dns-test-e6bc71c1-28a7-48d2-8f2f-74b97894f86e: the server could not find the requested resource (get pods dns-test-e6bc71c1-28a7-48d2-8f2f-74b97894f86e)
    Jan 30 12:50:15.777: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-7553/dns-test-e6bc71c1-28a7-48d2-8f2f-74b97894f86e: the server could not find the requested resource (get pods dns-test-e6bc71c1-28a7-48d2-8f2f-74b97894f86e)
    Jan 30 12:50:15.778: INFO: Unable to read jessie_udp@dns-test-service.dns-7553 from pod dns-7553/dns-test-e6bc71c1-28a7-48d2-8f2f-74b97894f86e: the server could not find the requested resource (get pods dns-test-e6bc71c1-28a7-48d2-8f2f-74b97894f86e)
    Jan 30 12:50:15.780: INFO: Unable to read jessie_tcp@dns-test-service.dns-7553 from pod dns-7553/dns-test-e6bc71c1-28a7-48d2-8f2f-74b97894f86e: the server could not find the requested resource (get pods dns-test-e6bc71c1-28a7-48d2-8f2f-74b97894f86e)
    Jan 30 12:50:15.782: INFO: Unable to read jessie_udp@dns-test-service.dns-7553.svc from pod dns-7553/dns-test-e6bc71c1-28a7-48d2-8f2f-74b97894f86e: the server could not find the requested resource (get pods dns-test-e6bc71c1-28a7-48d2-8f2f-74b97894f86e)
    Jan 30 12:50:15.783: INFO: Unable to read jessie_tcp@dns-test-service.dns-7553.svc from pod dns-7553/dns-test-e6bc71c1-28a7-48d2-8f2f-74b97894f86e: the server could not find the requested resource (get pods dns-test-e6bc71c1-28a7-48d2-8f2f-74b97894f86e)
    Jan 30 12:50:15.785: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-7553.svc from pod dns-7553/dns-test-e6bc71c1-28a7-48d2-8f2f-74b97894f86e: the server could not find the requested resource (get pods dns-test-e6bc71c1-28a7-48d2-8f2f-74b97894f86e)
    Jan 30 12:50:15.787: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-7553.svc from pod dns-7553/dns-test-e6bc71c1-28a7-48d2-8f2f-74b97894f86e: the server could not find the requested resource (get pods dns-test-e6bc71c1-28a7-48d2-8f2f-74b97894f86e)
    Jan 30 12:50:15.794: INFO: Lookups using dns-7553/dns-test-e6bc71c1-28a7-48d2-8f2f-74b97894f86e failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-7553 wheezy_tcp@dns-test-service.dns-7553 wheezy_udp@dns-test-service.dns-7553.svc wheezy_tcp@dns-test-service.dns-7553.svc wheezy_udp@_http._tcp.dns-test-service.dns-7553.svc wheezy_tcp@_http._tcp.dns-test-service.dns-7553.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-7553 jessie_tcp@dns-test-service.dns-7553 jessie_udp@dns-test-service.dns-7553.svc jessie_tcp@dns-test-service.dns-7553.svc jessie_udp@_http._tcp.dns-test-service.dns-7553.svc jessie_tcp@_http._tcp.dns-test-service.dns-7553.svc]

    Jan 30 12:50:20.808: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-7553.svc from pod dns-7553/dns-test-e6bc71c1-28a7-48d2-8f2f-74b97894f86e: the server could not find the requested resource (get pods dns-test-e6bc71c1-28a7-48d2-8f2f-74b97894f86e)
    Jan 30 12:50:20.809: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-7553.svc from pod dns-7553/dns-test-e6bc71c1-28a7-48d2-8f2f-74b97894f86e: the server could not find the requested resource (get pods dns-test-e6bc71c1-28a7-48d2-8f2f-74b97894f86e)
    Jan 30 12:50:20.818: INFO: Unable to read jessie_udp@dns-test-service from pod dns-7553/dns-test-e6bc71c1-28a7-48d2-8f2f-74b97894f86e: the server could not find the requested resource (get pods dns-test-e6bc71c1-28a7-48d2-8f2f-74b97894f86e)
    Jan 30 12:50:20.820: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-7553/dns-test-e6bc71c1-28a7-48d2-8f2f-74b97894f86e: the server could not find the requested resource (get pods dns-test-e6bc71c1-28a7-48d2-8f2f-74b97894f86e)
    Jan 30 12:50:20.821: INFO: Unable to read jessie_udp@dns-test-service.dns-7553 from pod dns-7553/dns-test-e6bc71c1-28a7-48d2-8f2f-74b97894f86e: the server could not find the requested resource (get pods dns-test-e6bc71c1-28a7-48d2-8f2f-74b97894f86e)
    Jan 30 12:50:20.823: INFO: Unable to read jessie_tcp@dns-test-service.dns-7553 from pod dns-7553/dns-test-e6bc71c1-28a7-48d2-8f2f-74b97894f86e: the server could not find the requested resource (get pods dns-test-e6bc71c1-28a7-48d2-8f2f-74b97894f86e)
    Jan 30 12:50:20.825: INFO: Unable to read jessie_udp@dns-test-service.dns-7553.svc from pod dns-7553/dns-test-e6bc71c1-28a7-48d2-8f2f-74b97894f86e: the server could not find the requested resource (get pods dns-test-e6bc71c1-28a7-48d2-8f2f-74b97894f86e)
    Jan 30 12:50:20.827: INFO: Unable to read jessie_tcp@dns-test-service.dns-7553.svc from pod dns-7553/dns-test-e6bc71c1-28a7-48d2-8f2f-74b97894f86e: the server could not find the requested resource (get pods dns-test-e6bc71c1-28a7-48d2-8f2f-74b97894f86e)
    Jan 30 12:50:20.828: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-7553.svc from pod dns-7553/dns-test-e6bc71c1-28a7-48d2-8f2f-74b97894f86e: the server could not find the requested resource (get pods dns-test-e6bc71c1-28a7-48d2-8f2f-74b97894f86e)
    Jan 30 12:50:20.830: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-7553.svc from pod dns-7553/dns-test-e6bc71c1-28a7-48d2-8f2f-74b97894f86e: the server could not find the requested resource (get pods dns-test-e6bc71c1-28a7-48d2-8f2f-74b97894f86e)
    Jan 30 12:50:20.837: INFO: Lookups using dns-7553/dns-test-e6bc71c1-28a7-48d2-8f2f-74b97894f86e failed for: [wheezy_udp@_http._tcp.dns-test-service.dns-7553.svc wheezy_tcp@_http._tcp.dns-test-service.dns-7553.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-7553 jessie_tcp@dns-test-service.dns-7553 jessie_udp@dns-test-service.dns-7553.svc jessie_tcp@dns-test-service.dns-7553.svc jessie_udp@_http._tcp.dns-test-service.dns-7553.svc jessie_tcp@_http._tcp.dns-test-service.dns-7553.svc]

    Jan 30 12:50:25.838: INFO: DNS probes using dns-7553/dns-test-e6bc71c1-28a7-48d2-8f2f-74b97894f86e succeeded

    STEP: deleting the pod 01/30/23 12:50:25.838
    STEP: deleting the test service 01/30/23 12:50:25.843
    STEP: deleting the test headless service 01/30/23 12:50:25.848
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Jan 30 12:50:25.852: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-7553" for this suite. 01/30/23 12:50:25.854
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-apps] Job
  should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:366
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 12:50:25.857
Jan 30 12:50:25.857: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename job 01/30/23 12:50:25.857
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:50:25.864
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:50:25.866
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:366
STEP: Creating Indexed job 01/30/23 12:50:25.868
STEP: Ensuring job reaches completions 01/30/23 12:50:25.871
STEP: Ensuring pods with index for job exist 01/30/23 12:50:37.875
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Jan 30 12:50:37.877: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-7980" for this suite. 01/30/23 12:50:37.88
------------------------------
• [SLOW TEST] [12.025 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:366

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 12:50:25.857
    Jan 30 12:50:25.857: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename job 01/30/23 12:50:25.857
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:50:25.864
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:50:25.866
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
      test/e2e/apps/job.go:366
    STEP: Creating Indexed job 01/30/23 12:50:25.868
    STEP: Ensuring job reaches completions 01/30/23 12:50:25.871
    STEP: Ensuring pods with index for job exist 01/30/23 12:50:37.875
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Jan 30 12:50:37.877: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-7980" for this suite. 01/30/23 12:50:37.88
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:221
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 12:50:37.883
Jan 30 12:50:37.883: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename webhook 01/30/23 12:50:37.884
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:50:37.89
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:50:37.892
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 01/30/23 12:50:37.9
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/30/23 12:50:38.343
STEP: Deploying the webhook pod 01/30/23 12:50:38.346
STEP: Wait for the deployment to be ready 01/30/23 12:50:38.352
Jan 30 12:50:38.355: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 01/30/23 12:50:40.361
STEP: Verifying the service has paired with the endpoint 01/30/23 12:50:40.364
Jan 30 12:50:41.364: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:221
Jan 30 12:50:41.367: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Registering the custom resource webhook via the AdmissionRegistration API 01/30/23 12:50:41.872
STEP: Creating a custom resource that should be denied by the webhook 01/30/23 12:50:41.882
STEP: Creating a custom resource whose deletion would be denied by the webhook 01/30/23 12:50:43.934
STEP: Updating the custom resource with disallowed data should be denied 01/30/23 12:50:43.939
STEP: Deleting the custom resource should be denied 01/30/23 12:50:43.946
STEP: Remove the offending key and value from the custom resource data 01/30/23 12:50:43.95
STEP: Deleting the updated custom resource should be successful 01/30/23 12:50:43.956
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 30 12:50:44.469: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-6010" for this suite. 01/30/23 12:50:44.483
STEP: Destroying namespace "webhook-6010-markers" for this suite. 01/30/23 12:50:44.486
------------------------------
• [SLOW TEST] [6.605 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:221

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 12:50:37.883
    Jan 30 12:50:37.883: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename webhook 01/30/23 12:50:37.884
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:50:37.89
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:50:37.892
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 01/30/23 12:50:37.9
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/30/23 12:50:38.343
    STEP: Deploying the webhook pod 01/30/23 12:50:38.346
    STEP: Wait for the deployment to be ready 01/30/23 12:50:38.352
    Jan 30 12:50:38.355: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 01/30/23 12:50:40.361
    STEP: Verifying the service has paired with the endpoint 01/30/23 12:50:40.364
    Jan 30 12:50:41.364: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny custom resource creation, update and deletion [Conformance]
      test/e2e/apimachinery/webhook.go:221
    Jan 30 12:50:41.367: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Registering the custom resource webhook via the AdmissionRegistration API 01/30/23 12:50:41.872
    STEP: Creating a custom resource that should be denied by the webhook 01/30/23 12:50:41.882
    STEP: Creating a custom resource whose deletion would be denied by the webhook 01/30/23 12:50:43.934
    STEP: Updating the custom resource with disallowed data should be denied 01/30/23 12:50:43.939
    STEP: Deleting the custom resource should be denied 01/30/23 12:50:43.946
    STEP: Remove the offending key and value from the custom resource data 01/30/23 12:50:43.95
    STEP: Deleting the updated custom resource should be successful 01/30/23 12:50:43.956
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 30 12:50:44.469: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-6010" for this suite. 01/30/23 12:50:44.483
    STEP: Destroying namespace "webhook-6010-markers" for this suite. 01/30/23 12:50:44.486
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175
[BeforeEach] [sig-instrumentation] Events
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 12:50:44.489
Jan 30 12:50:44.489: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename events 01/30/23 12:50:44.49
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:50:44.496
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:50:44.498
[BeforeEach] [sig-instrumentation] Events
  test/e2e/framework/metrics/init/init.go:31
[It] should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175
STEP: Create set of events 01/30/23 12:50:44.499
Jan 30 12:50:44.501: INFO: created test-event-1
Jan 30 12:50:44.503: INFO: created test-event-2
Jan 30 12:50:44.505: INFO: created test-event-3
STEP: get a list of Events with a label in the current namespace 01/30/23 12:50:44.505
STEP: delete collection of events 01/30/23 12:50:44.506
Jan 30 12:50:44.507: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity 01/30/23 12:50:44.513
Jan 30 12:50:44.513: INFO: requesting list of events to confirm quantity
[AfterEach] [sig-instrumentation] Events
  test/e2e/framework/node/init/init.go:32
Jan 30 12:50:44.514: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-instrumentation] Events
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-instrumentation] Events
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-instrumentation] Events
  tear down framework | framework.go:193
STEP: Destroying namespace "events-5698" for this suite. 01/30/23 12:50:44.516
------------------------------
• [0.030 seconds]
[sig-instrumentation] Events
test/e2e/instrumentation/common/framework.go:23
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 12:50:44.489
    Jan 30 12:50:44.489: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename events 01/30/23 12:50:44.49
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:50:44.496
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:50:44.498
    [BeforeEach] [sig-instrumentation] Events
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete a collection of events [Conformance]
      test/e2e/instrumentation/core_events.go:175
    STEP: Create set of events 01/30/23 12:50:44.499
    Jan 30 12:50:44.501: INFO: created test-event-1
    Jan 30 12:50:44.503: INFO: created test-event-2
    Jan 30 12:50:44.505: INFO: created test-event-3
    STEP: get a list of Events with a label in the current namespace 01/30/23 12:50:44.505
    STEP: delete collection of events 01/30/23 12:50:44.506
    Jan 30 12:50:44.507: INFO: requesting DeleteCollection of events
    STEP: check that the list of events matches the requested quantity 01/30/23 12:50:44.513
    Jan 30 12:50:44.513: INFO: requesting list of events to confirm quantity
    [AfterEach] [sig-instrumentation] Events
      test/e2e/framework/node/init/init.go:32
    Jan 30 12:50:44.514: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-instrumentation] Events
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-instrumentation] Events
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-instrumentation] Events
      tear down framework | framework.go:193
    STEP: Destroying namespace "events-5698" for this suite. 01/30/23 12:50:44.516
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:88
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 12:50:44.518
Jan 30 12:50:44.519: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename projected 01/30/23 12:50:44.519
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:50:44.524
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:50:44.526
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:88
STEP: Creating projection with secret that has name projected-secret-test-map-098ec06d-d216-4ebb-b8d6-1aeb9eb89c99 01/30/23 12:50:44.528
STEP: Creating a pod to test consume secrets 01/30/23 12:50:44.53
Jan 30 12:50:44.534: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-9497bd92-1405-4a17-9424-153c8777f6f0" in namespace "projected-1934" to be "Succeeded or Failed"
Jan 30 12:50:44.535: INFO: Pod "pod-projected-secrets-9497bd92-1405-4a17-9424-153c8777f6f0": Phase="Pending", Reason="", readiness=false. Elapsed: 1.454512ms
Jan 30 12:50:46.538: INFO: Pod "pod-projected-secrets-9497bd92-1405-4a17-9424-153c8777f6f0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0047296s
Jan 30 12:50:48.537: INFO: Pod "pod-projected-secrets-9497bd92-1405-4a17-9424-153c8777f6f0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003880985s
STEP: Saw pod success 01/30/23 12:50:48.537
Jan 30 12:50:48.538: INFO: Pod "pod-projected-secrets-9497bd92-1405-4a17-9424-153c8777f6f0" satisfied condition "Succeeded or Failed"
Jan 30 12:50:48.539: INFO: Trying to get logs from node pubt2-nks-for-dev1.dg.163.org pod pod-projected-secrets-9497bd92-1405-4a17-9424-153c8777f6f0 container projected-secret-volume-test: <nil>
STEP: delete the pod 01/30/23 12:50:48.546
Jan 30 12:50:48.551: INFO: Waiting for pod pod-projected-secrets-9497bd92-1405-4a17-9424-153c8777f6f0 to disappear
Jan 30 12:50:48.552: INFO: Pod pod-projected-secrets-9497bd92-1405-4a17-9424-153c8777f6f0 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Jan 30 12:50:48.552: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-1934" for this suite. 01/30/23 12:50:48.554
------------------------------
• [4.038 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:88

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 12:50:44.518
    Jan 30 12:50:44.519: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename projected 01/30/23 12:50:44.519
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:50:44.524
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:50:44.526
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:88
    STEP: Creating projection with secret that has name projected-secret-test-map-098ec06d-d216-4ebb-b8d6-1aeb9eb89c99 01/30/23 12:50:44.528
    STEP: Creating a pod to test consume secrets 01/30/23 12:50:44.53
    Jan 30 12:50:44.534: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-9497bd92-1405-4a17-9424-153c8777f6f0" in namespace "projected-1934" to be "Succeeded or Failed"
    Jan 30 12:50:44.535: INFO: Pod "pod-projected-secrets-9497bd92-1405-4a17-9424-153c8777f6f0": Phase="Pending", Reason="", readiness=false. Elapsed: 1.454512ms
    Jan 30 12:50:46.538: INFO: Pod "pod-projected-secrets-9497bd92-1405-4a17-9424-153c8777f6f0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0047296s
    Jan 30 12:50:48.537: INFO: Pod "pod-projected-secrets-9497bd92-1405-4a17-9424-153c8777f6f0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003880985s
    STEP: Saw pod success 01/30/23 12:50:48.537
    Jan 30 12:50:48.538: INFO: Pod "pod-projected-secrets-9497bd92-1405-4a17-9424-153c8777f6f0" satisfied condition "Succeeded or Failed"
    Jan 30 12:50:48.539: INFO: Trying to get logs from node pubt2-nks-for-dev1.dg.163.org pod pod-projected-secrets-9497bd92-1405-4a17-9424-153c8777f6f0 container projected-secret-volume-test: <nil>
    STEP: delete the pod 01/30/23 12:50:48.546
    Jan 30 12:50:48.551: INFO: Waiting for pod pod-projected-secrets-9497bd92-1405-4a17-9424-153c8777f6f0 to disappear
    Jan 30 12:50:48.552: INFO: Pod pod-projected-secrets-9497bd92-1405-4a17-9424-153c8777f6f0 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Jan 30 12:50:48.552: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-1934" for this suite. 01/30/23 12:50:48.554
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Security Context When creating a pod with readOnlyRootFilesystem
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:486
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 12:50:48.557
Jan 30 12:50:48.557: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename security-context-test 01/30/23 12:50:48.558
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:50:48.564
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:50:48.566
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:50
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:486
Jan 30 12:50:48.571: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-11165e47-1a3a-4fca-914f-9b54c9a99fc9" in namespace "security-context-test-973" to be "Succeeded or Failed"
Jan 30 12:50:48.574: INFO: Pod "busybox-readonly-false-11165e47-1a3a-4fca-914f-9b54c9a99fc9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009606ms
Jan 30 12:50:50.576: INFO: Pod "busybox-readonly-false-11165e47-1a3a-4fca-914f-9b54c9a99fc9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004355471s
Jan 30 12:50:52.577: INFO: Pod "busybox-readonly-false-11165e47-1a3a-4fca-914f-9b54c9a99fc9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005183906s
Jan 30 12:50:52.577: INFO: Pod "busybox-readonly-false-11165e47-1a3a-4fca-914f-9b54c9a99fc9" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Jan 30 12:50:52.577: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-test-973" for this suite. 01/30/23 12:50:52.579
------------------------------
• [4.025 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a pod with readOnlyRootFilesystem
  test/e2e/common/node/security_context.go:430
    should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:486

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 12:50:48.557
    Jan 30 12:50:48.557: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename security-context-test 01/30/23 12:50:48.558
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:50:48.564
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:50:48.566
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:50
    [It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:486
    Jan 30 12:50:48.571: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-11165e47-1a3a-4fca-914f-9b54c9a99fc9" in namespace "security-context-test-973" to be "Succeeded or Failed"
    Jan 30 12:50:48.574: INFO: Pod "busybox-readonly-false-11165e47-1a3a-4fca-914f-9b54c9a99fc9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009606ms
    Jan 30 12:50:50.576: INFO: Pod "busybox-readonly-false-11165e47-1a3a-4fca-914f-9b54c9a99fc9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004355471s
    Jan 30 12:50:52.577: INFO: Pod "busybox-readonly-false-11165e47-1a3a-4fca-914f-9b54c9a99fc9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005183906s
    Jan 30 12:50:52.577: INFO: Pod "busybox-readonly-false-11165e47-1a3a-4fca-914f-9b54c9a99fc9" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Jan 30 12:50:52.577: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-test-973" for this suite. 01/30/23 12:50:52.579
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:160
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 12:50:52.582
Jan 30 12:50:52.582: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename resourcequota 01/30/23 12:50:52.583
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:50:52.604
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:50:52.606
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:160
STEP: Discovering how many secrets are in namespace by default 01/30/23 12:50:52.608
STEP: Counting existing ResourceQuota 01/30/23 12:50:57.611
STEP: Creating a ResourceQuota 01/30/23 12:51:02.613
STEP: Ensuring resource quota status is calculated 01/30/23 12:51:02.616
STEP: Creating a Secret 01/30/23 12:51:04.618
STEP: Ensuring resource quota status captures secret creation 01/30/23 12:51:04.624
STEP: Deleting a secret 01/30/23 12:51:06.628
STEP: Ensuring resource quota status released usage 01/30/23 12:51:06.631
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Jan 30 12:51:08.634: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-2473" for this suite. 01/30/23 12:51:08.637
------------------------------
• [SLOW TEST] [16.057 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 12:50:52.582
    Jan 30 12:50:52.582: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename resourcequota 01/30/23 12:50:52.583
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:50:52.604
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:50:52.606
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a secret. [Conformance]
      test/e2e/apimachinery/resource_quota.go:160
    STEP: Discovering how many secrets are in namespace by default 01/30/23 12:50:52.608
    STEP: Counting existing ResourceQuota 01/30/23 12:50:57.611
    STEP: Creating a ResourceQuota 01/30/23 12:51:02.613
    STEP: Ensuring resource quota status is calculated 01/30/23 12:51:02.616
    STEP: Creating a Secret 01/30/23 12:51:04.618
    STEP: Ensuring resource quota status captures secret creation 01/30/23 12:51:04.624
    STEP: Deleting a secret 01/30/23 12:51:06.628
    STEP: Ensuring resource quota status released usage 01/30/23 12:51:06.631
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Jan 30 12:51:08.634: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-2473" for this suite. 01/30/23 12:51:08.637
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:166
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 12:51:08.64
Jan 30 12:51:08.640: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename downward-api 01/30/23 12:51:08.641
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:51:08.647
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:51:08.649
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:166
STEP: Creating a pod to test downward api env vars 01/30/23 12:51:08.651
Jan 30 12:51:08.656: INFO: Waiting up to 5m0s for pod "downward-api-b849cc68-3cd2-4021-93ae-c469fb1cfd30" in namespace "downward-api-2699" to be "Succeeded or Failed"
Jan 30 12:51:08.658: INFO: Pod "downward-api-b849cc68-3cd2-4021-93ae-c469fb1cfd30": Phase="Pending", Reason="", readiness=false. Elapsed: 1.683142ms
Jan 30 12:51:10.661: INFO: Pod "downward-api-b849cc68-3cd2-4021-93ae-c469fb1cfd30": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004584633s
Jan 30 12:51:12.661: INFO: Pod "downward-api-b849cc68-3cd2-4021-93ae-c469fb1cfd30": Phase="Pending", Reason="", readiness=false. Elapsed: 4.005001482s
Jan 30 12:51:14.661: INFO: Pod "downward-api-b849cc68-3cd2-4021-93ae-c469fb1cfd30": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.004511686s
STEP: Saw pod success 01/30/23 12:51:14.661
Jan 30 12:51:14.661: INFO: Pod "downward-api-b849cc68-3cd2-4021-93ae-c469fb1cfd30" satisfied condition "Succeeded or Failed"
Jan 30 12:51:14.662: INFO: Trying to get logs from node pubt2-nks-for-dev1.dg.163.org pod downward-api-b849cc68-3cd2-4021-93ae-c469fb1cfd30 container dapi-container: <nil>
STEP: delete the pod 01/30/23 12:51:14.668
Jan 30 12:51:14.672: INFO: Waiting for pod downward-api-b849cc68-3cd2-4021-93ae-c469fb1cfd30 to disappear
Jan 30 12:51:14.674: INFO: Pod downward-api-b849cc68-3cd2-4021-93ae-c469fb1cfd30 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Jan 30 12:51:14.674: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-2699" for this suite. 01/30/23 12:51:14.676
------------------------------
• [SLOW TEST] [6.038 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:166

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 12:51:08.64
    Jan 30 12:51:08.640: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename downward-api 01/30/23 12:51:08.641
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:51:08.647
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:51:08.649
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:166
    STEP: Creating a pod to test downward api env vars 01/30/23 12:51:08.651
    Jan 30 12:51:08.656: INFO: Waiting up to 5m0s for pod "downward-api-b849cc68-3cd2-4021-93ae-c469fb1cfd30" in namespace "downward-api-2699" to be "Succeeded or Failed"
    Jan 30 12:51:08.658: INFO: Pod "downward-api-b849cc68-3cd2-4021-93ae-c469fb1cfd30": Phase="Pending", Reason="", readiness=false. Elapsed: 1.683142ms
    Jan 30 12:51:10.661: INFO: Pod "downward-api-b849cc68-3cd2-4021-93ae-c469fb1cfd30": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004584633s
    Jan 30 12:51:12.661: INFO: Pod "downward-api-b849cc68-3cd2-4021-93ae-c469fb1cfd30": Phase="Pending", Reason="", readiness=false. Elapsed: 4.005001482s
    Jan 30 12:51:14.661: INFO: Pod "downward-api-b849cc68-3cd2-4021-93ae-c469fb1cfd30": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.004511686s
    STEP: Saw pod success 01/30/23 12:51:14.661
    Jan 30 12:51:14.661: INFO: Pod "downward-api-b849cc68-3cd2-4021-93ae-c469fb1cfd30" satisfied condition "Succeeded or Failed"
    Jan 30 12:51:14.662: INFO: Trying to get logs from node pubt2-nks-for-dev1.dg.163.org pod downward-api-b849cc68-3cd2-4021-93ae-c469fb1cfd30 container dapi-container: <nil>
    STEP: delete the pod 01/30/23 12:51:14.668
    Jan 30 12:51:14.672: INFO: Waiting for pod downward-api-b849cc68-3cd2-4021-93ae-c469fb1cfd30 to disappear
    Jan 30 12:51:14.674: INFO: Pod downward-api-b849cc68-3cd2-4021-93ae-c469fb1cfd30 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Jan 30 12:51:14.674: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-2699" for this suite. 01/30/23 12:51:14.676
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 12:51:14.679
Jan 30 12:51:14.679: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename gc 01/30/23 12:51:14.68
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:51:14.686
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:51:14.688
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550
STEP: create the deployment 01/30/23 12:51:14.69
STEP: Wait for the Deployment to create new ReplicaSet 01/30/23 12:51:14.693
STEP: delete the deployment 01/30/23 12:51:15.199
STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs 01/30/23 12:51:15.202
STEP: Gathering metrics 01/30/23 12:51:15.713
Jan 30 12:51:15.731: INFO: Waiting up to 5m0s for pod "kube-controller-manager-pubt2-nks-for-dev3.dg.163.org" in namespace "kube-system" to be "running and ready"
Jan 30 12:51:15.733: INFO: Pod "kube-controller-manager-pubt2-nks-for-dev3.dg.163.org": Phase="Running", Reason="", readiness=true. Elapsed: 1.775176ms
Jan 30 12:51:15.733: INFO: The phase of Pod kube-controller-manager-pubt2-nks-for-dev3.dg.163.org is Running (Ready = true)
Jan 30 12:51:15.733: INFO: Pod "kube-controller-manager-pubt2-nks-for-dev3.dg.163.org" satisfied condition "running and ready"
Jan 30 12:51:15.806: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Jan 30 12:51:15.806: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-6923" for this suite. 01/30/23 12:51:15.808
------------------------------
• [1.132 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 12:51:14.679
    Jan 30 12:51:14.679: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename gc 01/30/23 12:51:14.68
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:51:14.686
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:51:14.688
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
      test/e2e/apimachinery/garbage_collector.go:550
    STEP: create the deployment 01/30/23 12:51:14.69
    STEP: Wait for the Deployment to create new ReplicaSet 01/30/23 12:51:14.693
    STEP: delete the deployment 01/30/23 12:51:15.199
    STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs 01/30/23 12:51:15.202
    STEP: Gathering metrics 01/30/23 12:51:15.713
    Jan 30 12:51:15.731: INFO: Waiting up to 5m0s for pod "kube-controller-manager-pubt2-nks-for-dev3.dg.163.org" in namespace "kube-system" to be "running and ready"
    Jan 30 12:51:15.733: INFO: Pod "kube-controller-manager-pubt2-nks-for-dev3.dg.163.org": Phase="Running", Reason="", readiness=true. Elapsed: 1.775176ms
    Jan 30 12:51:15.733: INFO: The phase of Pod kube-controller-manager-pubt2-nks-for-dev3.dg.163.org is Running (Ready = true)
    Jan 30 12:51:15.733: INFO: Pod "kube-controller-manager-pubt2-nks-for-dev3.dg.163.org" satisfied condition "running and ready"
    Jan 30 12:51:15.806: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Jan 30 12:51:15.806: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-6923" for this suite. 01/30/23 12:51:15.808
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-apps] Deployment
  Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 12:51:15.811
Jan 30 12:51:15.811: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename deployment 01/30/23 12:51:15.812
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:51:15.818
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:51:15.82
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150
Jan 30 12:51:15.822: INFO: Creating simple deployment test-new-deployment
Jan 30 12:51:15.828: INFO: new replicaset for deployment "test-new-deployment" is yet to be created
STEP: getting scale subresource 01/30/23 12:51:17.835
STEP: updating a scale subresource 01/30/23 12:51:17.837
STEP: verifying the deployment Spec.Replicas was modified 01/30/23 12:51:17.839
STEP: Patch a scale subresource 01/30/23 12:51:17.841
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan 30 12:51:17.847: INFO: Deployment "test-new-deployment":
&Deployment{ObjectMeta:{test-new-deployment  deployment-4659  a79c9983-6f17-4e14-b1f4-9877238f9b10 43959 3 2023-01-30 12:51:15 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2023-01-30 12:51:15 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-30 12:51:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd harbor.cloud.netease.com/qzprod-k8s/e2e/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0049ebc18 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-01-30 12:51:16 +0000 UTC,LastTransitionTime:2023-01-30 12:51:16 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-5fd5d6fdb9" has successfully progressed.,LastUpdateTime:2023-01-30 12:51:16 +0000 UTC,LastTransitionTime:2023-01-30 12:51:15 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jan 30 12:51:17.849: INFO: New ReplicaSet "test-new-deployment-5fd5d6fdb9" of Deployment "test-new-deployment":
&ReplicaSet{ObjectMeta:{test-new-deployment-5fd5d6fdb9  deployment-4659  d70f3e56-fc03-4c65-859f-a786c9413ef0 43958 2 2023-01-30 12:51:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5fd5d6fdb9] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment a79c9983-6f17-4e14-b1f4-9877238f9b10 0xc004aec267 0xc004aec268}] [] [{kube-controller-manager Update apps/v1 2023-01-30 12:51:16 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-01-30 12:51:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a79c9983-6f17-4e14-b1f4-9877238f9b10\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 5fd5d6fdb9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5fd5d6fdb9] map[] [] [] []} {[] [] [{httpd harbor.cloud.netease.com/qzprod-k8s/e2e/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004aec2f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jan 30 12:51:17.851: INFO: Pod "test-new-deployment-5fd5d6fdb9-mltzs" is available:
&Pod{ObjectMeta:{test-new-deployment-5fd5d6fdb9-mltzs test-new-deployment-5fd5d6fdb9- deployment-4659  4313eabf-8ebf-494b-af14-0fa1b057f2bd 43949 0 2023-01-30 12:51:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5fd5d6fdb9] map[cni.projectcalico.org/podIP:10.178.151.176/32 cni.projectcalico.org/podIPs:10.178.151.176/32] [{apps/v1 ReplicaSet test-new-deployment-5fd5d6fdb9 d70f3e56-fc03-4c65-859f-a786c9413ef0 0xc0068268c7 0xc0068268c8}] [] [{kube-controller-manager Update v1 2023-01-30 12:51:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d70f3e56-fc03-4c65-859f-a786c9413ef0\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-30 12:51:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-30 12:51:16 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.178.151.176\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xw76d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:harbor.cloud.netease.com/qzprod-k8s/e2e/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xw76d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pubt2-nks-for-dev1.dg.163.org,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 12:51:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 12:51:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 12:51:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 12:51:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.182.0.82,PodIP:10.178.151.176,StartTime:2023-01-30 12:51:15 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-30 12:51:16 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:harbor.cloud.netease.com/qzprod-k8s/e2e/httpd:2.4.38-4,ImageID:docker-pullable://harbor.cloud.netease.com/qzprod-k8s/e2e/httpd@sha256:8b745b6e9cb6a6184353110aabe28d8d6f6145d3fe7c0e1899aa70c8228c0fd3,ContainerID:docker://430401b30351438b7ed6ec657752ee9ad99f0a56264f0c6c15f3f02ee299af63,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.178.151.176,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 30 12:51:17.851: INFO: Pod "test-new-deployment-5fd5d6fdb9-x9vdt" is not available:
&Pod{ObjectMeta:{test-new-deployment-5fd5d6fdb9-x9vdt test-new-deployment-5fd5d6fdb9- deployment-4659  cf0fe08d-06a9-4a4b-bd07-8de59b3fcb21 43963 0 2023-01-30 12:51:17 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5fd5d6fdb9] map[] [{apps/v1 ReplicaSet test-new-deployment-5fd5d6fdb9 d70f3e56-fc03-4c65-859f-a786c9413ef0 0xc006826ac7 0xc006826ac8}] [] [{kube-controller-manager Update v1 2023-01-30 12:51:17 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d70f3e56-fc03-4c65-859f-a786c9413ef0\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wjxg4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:harbor.cloud.netease.com/qzprod-k8s/e2e/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wjxg4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pubt2-nks-for-dev3.dg.163.org,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 12:51:17 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Jan 30 12:51:17.851: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-4659" for this suite. 01/30/23 12:51:17.854
------------------------------
• [2.045 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 12:51:15.811
    Jan 30 12:51:15.811: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename deployment 01/30/23 12:51:15.812
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:51:15.818
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:51:15.82
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] Deployment should have a working scale subresource [Conformance]
      test/e2e/apps/deployment.go:150
    Jan 30 12:51:15.822: INFO: Creating simple deployment test-new-deployment
    Jan 30 12:51:15.828: INFO: new replicaset for deployment "test-new-deployment" is yet to be created
    STEP: getting scale subresource 01/30/23 12:51:17.835
    STEP: updating a scale subresource 01/30/23 12:51:17.837
    STEP: verifying the deployment Spec.Replicas was modified 01/30/23 12:51:17.839
    STEP: Patch a scale subresource 01/30/23 12:51:17.841
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jan 30 12:51:17.847: INFO: Deployment "test-new-deployment":
    &Deployment{ObjectMeta:{test-new-deployment  deployment-4659  a79c9983-6f17-4e14-b1f4-9877238f9b10 43959 3 2023-01-30 12:51:15 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2023-01-30 12:51:15 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-30 12:51:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd harbor.cloud.netease.com/qzprod-k8s/e2e/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0049ebc18 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-01-30 12:51:16 +0000 UTC,LastTransitionTime:2023-01-30 12:51:16 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-5fd5d6fdb9" has successfully progressed.,LastUpdateTime:2023-01-30 12:51:16 +0000 UTC,LastTransitionTime:2023-01-30 12:51:15 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Jan 30 12:51:17.849: INFO: New ReplicaSet "test-new-deployment-5fd5d6fdb9" of Deployment "test-new-deployment":
    &ReplicaSet{ObjectMeta:{test-new-deployment-5fd5d6fdb9  deployment-4659  d70f3e56-fc03-4c65-859f-a786c9413ef0 43958 2 2023-01-30 12:51:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5fd5d6fdb9] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment a79c9983-6f17-4e14-b1f4-9877238f9b10 0xc004aec267 0xc004aec268}] [] [{kube-controller-manager Update apps/v1 2023-01-30 12:51:16 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-01-30 12:51:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a79c9983-6f17-4e14-b1f4-9877238f9b10\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 5fd5d6fdb9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5fd5d6fdb9] map[] [] [] []} {[] [] [{httpd harbor.cloud.netease.com/qzprod-k8s/e2e/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004aec2f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Jan 30 12:51:17.851: INFO: Pod "test-new-deployment-5fd5d6fdb9-mltzs" is available:
    &Pod{ObjectMeta:{test-new-deployment-5fd5d6fdb9-mltzs test-new-deployment-5fd5d6fdb9- deployment-4659  4313eabf-8ebf-494b-af14-0fa1b057f2bd 43949 0 2023-01-30 12:51:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5fd5d6fdb9] map[cni.projectcalico.org/podIP:10.178.151.176/32 cni.projectcalico.org/podIPs:10.178.151.176/32] [{apps/v1 ReplicaSet test-new-deployment-5fd5d6fdb9 d70f3e56-fc03-4c65-859f-a786c9413ef0 0xc0068268c7 0xc0068268c8}] [] [{kube-controller-manager Update v1 2023-01-30 12:51:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d70f3e56-fc03-4c65-859f-a786c9413ef0\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-30 12:51:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-30 12:51:16 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.178.151.176\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xw76d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:harbor.cloud.netease.com/qzprod-k8s/e2e/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xw76d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pubt2-nks-for-dev1.dg.163.org,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 12:51:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 12:51:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 12:51:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 12:51:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.182.0.82,PodIP:10.178.151.176,StartTime:2023-01-30 12:51:15 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-30 12:51:16 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:harbor.cloud.netease.com/qzprod-k8s/e2e/httpd:2.4.38-4,ImageID:docker-pullable://harbor.cloud.netease.com/qzprod-k8s/e2e/httpd@sha256:8b745b6e9cb6a6184353110aabe28d8d6f6145d3fe7c0e1899aa70c8228c0fd3,ContainerID:docker://430401b30351438b7ed6ec657752ee9ad99f0a56264f0c6c15f3f02ee299af63,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.178.151.176,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 30 12:51:17.851: INFO: Pod "test-new-deployment-5fd5d6fdb9-x9vdt" is not available:
    &Pod{ObjectMeta:{test-new-deployment-5fd5d6fdb9-x9vdt test-new-deployment-5fd5d6fdb9- deployment-4659  cf0fe08d-06a9-4a4b-bd07-8de59b3fcb21 43963 0 2023-01-30 12:51:17 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5fd5d6fdb9] map[] [{apps/v1 ReplicaSet test-new-deployment-5fd5d6fdb9 d70f3e56-fc03-4c65-859f-a786c9413ef0 0xc006826ac7 0xc006826ac8}] [] [{kube-controller-manager Update v1 2023-01-30 12:51:17 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d70f3e56-fc03-4c65-859f-a786c9413ef0\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wjxg4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:harbor.cloud.netease.com/qzprod-k8s/e2e/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wjxg4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pubt2-nks-for-dev3.dg.163.org,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 12:51:17 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Jan 30 12:51:17.851: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-4659" for this suite. 01/30/23 12:51:17.854
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:84
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 12:51:17.858
Jan 30 12:51:17.858: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename downward-api 01/30/23 12:51:17.858
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:51:17.865
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:51:17.867
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:84
STEP: Creating a pod to test downward API volume plugin 01/30/23 12:51:17.869
Jan 30 12:51:17.873: INFO: Waiting up to 5m0s for pod "downwardapi-volume-064ea981-a6de-4d69-a5a3-cb625a804301" in namespace "downward-api-1802" to be "Succeeded or Failed"
Jan 30 12:51:17.875: INFO: Pod "downwardapi-volume-064ea981-a6de-4d69-a5a3-cb625a804301": Phase="Pending", Reason="", readiness=false. Elapsed: 1.731182ms
Jan 30 12:51:19.877: INFO: Pod "downwardapi-volume-064ea981-a6de-4d69-a5a3-cb625a804301": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004221042s
Jan 30 12:51:21.878: INFO: Pod "downwardapi-volume-064ea981-a6de-4d69-a5a3-cb625a804301": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005096854s
STEP: Saw pod success 01/30/23 12:51:21.878
Jan 30 12:51:21.878: INFO: Pod "downwardapi-volume-064ea981-a6de-4d69-a5a3-cb625a804301" satisfied condition "Succeeded or Failed"
Jan 30 12:51:21.880: INFO: Trying to get logs from node pubt2-nks-for-dev1.dg.163.org pod downwardapi-volume-064ea981-a6de-4d69-a5a3-cb625a804301 container client-container: <nil>
STEP: delete the pod 01/30/23 12:51:21.884
Jan 30 12:51:21.889: INFO: Waiting for pod downwardapi-volume-064ea981-a6de-4d69-a5a3-cb625a804301 to disappear
Jan 30 12:51:21.890: INFO: Pod downwardapi-volume-064ea981-a6de-4d69-a5a3-cb625a804301 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Jan 30 12:51:21.890: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-1802" for this suite. 01/30/23 12:51:21.892
------------------------------
• [4.037 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:84

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 12:51:17.858
    Jan 30 12:51:17.858: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename downward-api 01/30/23 12:51:17.858
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:51:17.865
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:51:17.867
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:84
    STEP: Creating a pod to test downward API volume plugin 01/30/23 12:51:17.869
    Jan 30 12:51:17.873: INFO: Waiting up to 5m0s for pod "downwardapi-volume-064ea981-a6de-4d69-a5a3-cb625a804301" in namespace "downward-api-1802" to be "Succeeded or Failed"
    Jan 30 12:51:17.875: INFO: Pod "downwardapi-volume-064ea981-a6de-4d69-a5a3-cb625a804301": Phase="Pending", Reason="", readiness=false. Elapsed: 1.731182ms
    Jan 30 12:51:19.877: INFO: Pod "downwardapi-volume-064ea981-a6de-4d69-a5a3-cb625a804301": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004221042s
    Jan 30 12:51:21.878: INFO: Pod "downwardapi-volume-064ea981-a6de-4d69-a5a3-cb625a804301": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005096854s
    STEP: Saw pod success 01/30/23 12:51:21.878
    Jan 30 12:51:21.878: INFO: Pod "downwardapi-volume-064ea981-a6de-4d69-a5a3-cb625a804301" satisfied condition "Succeeded or Failed"
    Jan 30 12:51:21.880: INFO: Trying to get logs from node pubt2-nks-for-dev1.dg.163.org pod downwardapi-volume-064ea981-a6de-4d69-a5a3-cb625a804301 container client-container: <nil>
    STEP: delete the pod 01/30/23 12:51:21.884
    Jan 30 12:51:21.889: INFO: Waiting for pod downwardapi-volume-064ea981-a6de-4d69-a5a3-cb625a804301 to disappear
    Jan 30 12:51:21.890: INFO: Pod downwardapi-volume-064ea981-a6de-4d69-a5a3-cb625a804301 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Jan 30 12:51:21.890: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-1802" for this suite. 01/30/23 12:51:21.892
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSliceMirroring
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53
[BeforeEach] [sig-network] EndpointSliceMirroring
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 12:51:21.895
Jan 30 12:51:21.895: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename endpointslicemirroring 01/30/23 12:51:21.896
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:51:21.902
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:51:21.904
[BeforeEach] [sig-network] EndpointSliceMirroring
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSliceMirroring
  test/e2e/network/endpointslicemirroring.go:41
[It] should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53
STEP: mirroring a new custom Endpoint 01/30/23 12:51:21.909
Jan 30 12:51:21.913: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
STEP: mirroring an update to a custom Endpoint 01/30/23 12:51:23.916
Jan 30 12:51:23.920: INFO: Expected EndpointSlice to have 10.2.3.4 as address, got 10.1.2.3
STEP: mirroring deletion of a custom Endpoint 01/30/23 12:51:25.923
Jan 30 12:51:25.928: INFO: Waiting for 0 EndpointSlices to exist, got 1
[AfterEach] [sig-network] EndpointSliceMirroring
  test/e2e/framework/node/init/init.go:32
Jan 30 12:51:27.930: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslicemirroring-355" for this suite. 01/30/23 12:51:27.932
------------------------------
• [SLOW TEST] [6.039 seconds]
[sig-network] EndpointSliceMirroring
test/e2e/network/common/framework.go:23
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSliceMirroring
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 12:51:21.895
    Jan 30 12:51:21.895: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename endpointslicemirroring 01/30/23 12:51:21.896
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:51:21.902
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:51:21.904
    [BeforeEach] [sig-network] EndpointSliceMirroring
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSliceMirroring
      test/e2e/network/endpointslicemirroring.go:41
    [It] should mirror a custom Endpoints resource through create update and delete [Conformance]
      test/e2e/network/endpointslicemirroring.go:53
    STEP: mirroring a new custom Endpoint 01/30/23 12:51:21.909
    Jan 30 12:51:21.913: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
    STEP: mirroring an update to a custom Endpoint 01/30/23 12:51:23.916
    Jan 30 12:51:23.920: INFO: Expected EndpointSlice to have 10.2.3.4 as address, got 10.1.2.3
    STEP: mirroring deletion of a custom Endpoint 01/30/23 12:51:25.923
    Jan 30 12:51:25.928: INFO: Waiting for 0 EndpointSlices to exist, got 1
    [AfterEach] [sig-network] EndpointSliceMirroring
      test/e2e/framework/node/init/init.go:32
    Jan 30 12:51:27.930: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslicemirroring-355" for this suite. 01/30/23 12:51:27.932
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-network] EndpointSlice
  should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:353
[BeforeEach] [sig-network] EndpointSlice
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 12:51:27.935
Jan 30 12:51:27.935: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename endpointslice 01/30/23 12:51:27.936
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:51:27.942
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:51:27.943
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:52
[It] should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:353
STEP: getting /apis 01/30/23 12:51:27.945
STEP: getting /apis/discovery.k8s.io 01/30/23 12:51:27.947
STEP: getting /apis/discovery.k8s.iov1 01/30/23 12:51:27.948
STEP: creating 01/30/23 12:51:27.949
STEP: getting 01/30/23 12:51:27.954
STEP: listing 01/30/23 12:51:27.956
STEP: watching 01/30/23 12:51:27.957
Jan 30 12:51:27.957: INFO: starting watch
STEP: cluster-wide listing 01/30/23 12:51:27.958
STEP: cluster-wide watching 01/30/23 12:51:27.96
Jan 30 12:51:27.960: INFO: starting watch
STEP: patching 01/30/23 12:51:27.961
STEP: updating 01/30/23 12:51:27.964
Jan 30 12:51:27.967: INFO: waiting for watch events with expected annotations
Jan 30 12:51:27.967: INFO: saw patched and updated annotations
STEP: deleting 01/30/23 12:51:27.968
STEP: deleting a collection 01/30/23 12:51:27.972
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/node/init/init.go:32
Jan 30 12:51:27.978: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSlice
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSlice
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslice-3982" for this suite. 01/30/23 12:51:27.98
------------------------------
• [0.047 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:353

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 12:51:27.935
    Jan 30 12:51:27.935: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename endpointslice 01/30/23 12:51:27.936
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:51:27.942
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:51:27.943
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:52
    [It] should support creating EndpointSlice API operations [Conformance]
      test/e2e/network/endpointslice.go:353
    STEP: getting /apis 01/30/23 12:51:27.945
    STEP: getting /apis/discovery.k8s.io 01/30/23 12:51:27.947
    STEP: getting /apis/discovery.k8s.iov1 01/30/23 12:51:27.948
    STEP: creating 01/30/23 12:51:27.949
    STEP: getting 01/30/23 12:51:27.954
    STEP: listing 01/30/23 12:51:27.956
    STEP: watching 01/30/23 12:51:27.957
    Jan 30 12:51:27.957: INFO: starting watch
    STEP: cluster-wide listing 01/30/23 12:51:27.958
    STEP: cluster-wide watching 01/30/23 12:51:27.96
    Jan 30 12:51:27.960: INFO: starting watch
    STEP: patching 01/30/23 12:51:27.961
    STEP: updating 01/30/23 12:51:27.964
    Jan 30 12:51:27.967: INFO: waiting for watch events with expected annotations
    Jan 30 12:51:27.967: INFO: saw patched and updated annotations
    STEP: deleting 01/30/23 12:51:27.968
    STEP: deleting a collection 01/30/23 12:51:27.972
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/node/init/init.go:32
    Jan 30 12:51:27.978: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslice-3982" for this suite. 01/30/23 12:51:27.98
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Discovery
  should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122
[BeforeEach] [sig-api-machinery] Discovery
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 12:51:27.983
Jan 30 12:51:27.983: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename discovery 01/30/23 12:51:27.983
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:51:27.989
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:51:27.99
[BeforeEach] [sig-api-machinery] Discovery
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] Discovery
  test/e2e/apimachinery/discovery.go:43
STEP: Setting up server cert 01/30/23 12:51:27.993
[It] should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122
Jan 30 12:51:29.053: INFO: Checking APIGroup: apiregistration.k8s.io
Jan 30 12:51:29.055: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
Jan 30 12:51:29.055: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
Jan 30 12:51:29.055: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
Jan 30 12:51:29.055: INFO: Checking APIGroup: apps
Jan 30 12:51:29.055: INFO: PreferredVersion.GroupVersion: apps/v1
Jan 30 12:51:29.055: INFO: Versions found [{apps/v1 v1}]
Jan 30 12:51:29.055: INFO: apps/v1 matches apps/v1
Jan 30 12:51:29.055: INFO: Checking APIGroup: events.k8s.io
Jan 30 12:51:29.056: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
Jan 30 12:51:29.056: INFO: Versions found [{events.k8s.io/v1 v1}]
Jan 30 12:51:29.056: INFO: events.k8s.io/v1 matches events.k8s.io/v1
Jan 30 12:51:29.056: INFO: Checking APIGroup: authentication.k8s.io
Jan 30 12:51:29.057: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
Jan 30 12:51:29.057: INFO: Versions found [{authentication.k8s.io/v1 v1}]
Jan 30 12:51:29.057: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
Jan 30 12:51:29.057: INFO: Checking APIGroup: authorization.k8s.io
Jan 30 12:51:29.058: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
Jan 30 12:51:29.058: INFO: Versions found [{authorization.k8s.io/v1 v1}]
Jan 30 12:51:29.058: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
Jan 30 12:51:29.058: INFO: Checking APIGroup: autoscaling
Jan 30 12:51:29.058: INFO: PreferredVersion.GroupVersion: autoscaling/v2
Jan 30 12:51:29.058: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1}]
Jan 30 12:51:29.058: INFO: autoscaling/v2 matches autoscaling/v2
Jan 30 12:51:29.058: INFO: Checking APIGroup: batch
Jan 30 12:51:29.059: INFO: PreferredVersion.GroupVersion: batch/v1
Jan 30 12:51:29.059: INFO: Versions found [{batch/v1 v1}]
Jan 30 12:51:29.059: INFO: batch/v1 matches batch/v1
Jan 30 12:51:29.059: INFO: Checking APIGroup: certificates.k8s.io
Jan 30 12:51:29.060: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
Jan 30 12:51:29.060: INFO: Versions found [{certificates.k8s.io/v1 v1}]
Jan 30 12:51:29.060: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
Jan 30 12:51:29.060: INFO: Checking APIGroup: networking.k8s.io
Jan 30 12:51:29.060: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
Jan 30 12:51:29.060: INFO: Versions found [{networking.k8s.io/v1 v1}]
Jan 30 12:51:29.060: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
Jan 30 12:51:29.060: INFO: Checking APIGroup: policy
Jan 30 12:51:29.061: INFO: PreferredVersion.GroupVersion: policy/v1
Jan 30 12:51:29.061: INFO: Versions found [{policy/v1 v1}]
Jan 30 12:51:29.061: INFO: policy/v1 matches policy/v1
Jan 30 12:51:29.061: INFO: Checking APIGroup: rbac.authorization.k8s.io
Jan 30 12:51:29.062: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
Jan 30 12:51:29.062: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
Jan 30 12:51:29.062: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
Jan 30 12:51:29.062: INFO: Checking APIGroup: storage.k8s.io
Jan 30 12:51:29.063: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
Jan 30 12:51:29.063: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
Jan 30 12:51:29.063: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
Jan 30 12:51:29.063: INFO: Checking APIGroup: admissionregistration.k8s.io
Jan 30 12:51:29.063: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
Jan 30 12:51:29.063: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
Jan 30 12:51:29.063: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
Jan 30 12:51:29.063: INFO: Checking APIGroup: apiextensions.k8s.io
Jan 30 12:51:29.064: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
Jan 30 12:51:29.064: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
Jan 30 12:51:29.064: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
Jan 30 12:51:29.064: INFO: Checking APIGroup: scheduling.k8s.io
Jan 30 12:51:29.065: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
Jan 30 12:51:29.065: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
Jan 30 12:51:29.065: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
Jan 30 12:51:29.065: INFO: Checking APIGroup: coordination.k8s.io
Jan 30 12:51:29.065: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
Jan 30 12:51:29.065: INFO: Versions found [{coordination.k8s.io/v1 v1}]
Jan 30 12:51:29.065: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
Jan 30 12:51:29.065: INFO: Checking APIGroup: node.k8s.io
Jan 30 12:51:29.066: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
Jan 30 12:51:29.066: INFO: Versions found [{node.k8s.io/v1 v1}]
Jan 30 12:51:29.066: INFO: node.k8s.io/v1 matches node.k8s.io/v1
Jan 30 12:51:29.066: INFO: Checking APIGroup: discovery.k8s.io
Jan 30 12:51:29.067: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
Jan 30 12:51:29.067: INFO: Versions found [{discovery.k8s.io/v1 v1}]
Jan 30 12:51:29.067: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
Jan 30 12:51:29.067: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
Jan 30 12:51:29.067: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta3
Jan 30 12:51:29.067: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta3 v1beta3} {flowcontrol.apiserver.k8s.io/v1beta2 v1beta2}]
Jan 30 12:51:29.067: INFO: flowcontrol.apiserver.k8s.io/v1beta3 matches flowcontrol.apiserver.k8s.io/v1beta3
Jan 30 12:51:29.067: INFO: Checking APIGroup: crd.projectcalico.org
Jan 30 12:51:29.068: INFO: PreferredVersion.GroupVersion: crd.projectcalico.org/v1
Jan 30 12:51:29.068: INFO: Versions found [{crd.projectcalico.org/v1 v1}]
Jan 30 12:51:29.068: INFO: crd.projectcalico.org/v1 matches crd.projectcalico.org/v1
[AfterEach] [sig-api-machinery] Discovery
  test/e2e/framework/node/init/init.go:32
Jan 30 12:51:29.068: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Discovery
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Discovery
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Discovery
  tear down framework | framework.go:193
STEP: Destroying namespace "discovery-9560" for this suite. 01/30/23 12:51:29.071
------------------------------
• [1.091 seconds]
[sig-api-machinery] Discovery
test/e2e/apimachinery/framework.go:23
  should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Discovery
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 12:51:27.983
    Jan 30 12:51:27.983: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename discovery 01/30/23 12:51:27.983
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:51:27.989
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:51:27.99
    [BeforeEach] [sig-api-machinery] Discovery
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] Discovery
      test/e2e/apimachinery/discovery.go:43
    STEP: Setting up server cert 01/30/23 12:51:27.993
    [It] should validate PreferredVersion for each APIGroup [Conformance]
      test/e2e/apimachinery/discovery.go:122
    Jan 30 12:51:29.053: INFO: Checking APIGroup: apiregistration.k8s.io
    Jan 30 12:51:29.055: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
    Jan 30 12:51:29.055: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
    Jan 30 12:51:29.055: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
    Jan 30 12:51:29.055: INFO: Checking APIGroup: apps
    Jan 30 12:51:29.055: INFO: PreferredVersion.GroupVersion: apps/v1
    Jan 30 12:51:29.055: INFO: Versions found [{apps/v1 v1}]
    Jan 30 12:51:29.055: INFO: apps/v1 matches apps/v1
    Jan 30 12:51:29.055: INFO: Checking APIGroup: events.k8s.io
    Jan 30 12:51:29.056: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
    Jan 30 12:51:29.056: INFO: Versions found [{events.k8s.io/v1 v1}]
    Jan 30 12:51:29.056: INFO: events.k8s.io/v1 matches events.k8s.io/v1
    Jan 30 12:51:29.056: INFO: Checking APIGroup: authentication.k8s.io
    Jan 30 12:51:29.057: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
    Jan 30 12:51:29.057: INFO: Versions found [{authentication.k8s.io/v1 v1}]
    Jan 30 12:51:29.057: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
    Jan 30 12:51:29.057: INFO: Checking APIGroup: authorization.k8s.io
    Jan 30 12:51:29.058: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
    Jan 30 12:51:29.058: INFO: Versions found [{authorization.k8s.io/v1 v1}]
    Jan 30 12:51:29.058: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
    Jan 30 12:51:29.058: INFO: Checking APIGroup: autoscaling
    Jan 30 12:51:29.058: INFO: PreferredVersion.GroupVersion: autoscaling/v2
    Jan 30 12:51:29.058: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1}]
    Jan 30 12:51:29.058: INFO: autoscaling/v2 matches autoscaling/v2
    Jan 30 12:51:29.058: INFO: Checking APIGroup: batch
    Jan 30 12:51:29.059: INFO: PreferredVersion.GroupVersion: batch/v1
    Jan 30 12:51:29.059: INFO: Versions found [{batch/v1 v1}]
    Jan 30 12:51:29.059: INFO: batch/v1 matches batch/v1
    Jan 30 12:51:29.059: INFO: Checking APIGroup: certificates.k8s.io
    Jan 30 12:51:29.060: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
    Jan 30 12:51:29.060: INFO: Versions found [{certificates.k8s.io/v1 v1}]
    Jan 30 12:51:29.060: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
    Jan 30 12:51:29.060: INFO: Checking APIGroup: networking.k8s.io
    Jan 30 12:51:29.060: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
    Jan 30 12:51:29.060: INFO: Versions found [{networking.k8s.io/v1 v1}]
    Jan 30 12:51:29.060: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
    Jan 30 12:51:29.060: INFO: Checking APIGroup: policy
    Jan 30 12:51:29.061: INFO: PreferredVersion.GroupVersion: policy/v1
    Jan 30 12:51:29.061: INFO: Versions found [{policy/v1 v1}]
    Jan 30 12:51:29.061: INFO: policy/v1 matches policy/v1
    Jan 30 12:51:29.061: INFO: Checking APIGroup: rbac.authorization.k8s.io
    Jan 30 12:51:29.062: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
    Jan 30 12:51:29.062: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
    Jan 30 12:51:29.062: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
    Jan 30 12:51:29.062: INFO: Checking APIGroup: storage.k8s.io
    Jan 30 12:51:29.063: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
    Jan 30 12:51:29.063: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
    Jan 30 12:51:29.063: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
    Jan 30 12:51:29.063: INFO: Checking APIGroup: admissionregistration.k8s.io
    Jan 30 12:51:29.063: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
    Jan 30 12:51:29.063: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
    Jan 30 12:51:29.063: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
    Jan 30 12:51:29.063: INFO: Checking APIGroup: apiextensions.k8s.io
    Jan 30 12:51:29.064: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
    Jan 30 12:51:29.064: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
    Jan 30 12:51:29.064: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
    Jan 30 12:51:29.064: INFO: Checking APIGroup: scheduling.k8s.io
    Jan 30 12:51:29.065: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
    Jan 30 12:51:29.065: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
    Jan 30 12:51:29.065: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
    Jan 30 12:51:29.065: INFO: Checking APIGroup: coordination.k8s.io
    Jan 30 12:51:29.065: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
    Jan 30 12:51:29.065: INFO: Versions found [{coordination.k8s.io/v1 v1}]
    Jan 30 12:51:29.065: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
    Jan 30 12:51:29.065: INFO: Checking APIGroup: node.k8s.io
    Jan 30 12:51:29.066: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
    Jan 30 12:51:29.066: INFO: Versions found [{node.k8s.io/v1 v1}]
    Jan 30 12:51:29.066: INFO: node.k8s.io/v1 matches node.k8s.io/v1
    Jan 30 12:51:29.066: INFO: Checking APIGroup: discovery.k8s.io
    Jan 30 12:51:29.067: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
    Jan 30 12:51:29.067: INFO: Versions found [{discovery.k8s.io/v1 v1}]
    Jan 30 12:51:29.067: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
    Jan 30 12:51:29.067: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
    Jan 30 12:51:29.067: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta3
    Jan 30 12:51:29.067: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta3 v1beta3} {flowcontrol.apiserver.k8s.io/v1beta2 v1beta2}]
    Jan 30 12:51:29.067: INFO: flowcontrol.apiserver.k8s.io/v1beta3 matches flowcontrol.apiserver.k8s.io/v1beta3
    Jan 30 12:51:29.067: INFO: Checking APIGroup: crd.projectcalico.org
    Jan 30 12:51:29.068: INFO: PreferredVersion.GroupVersion: crd.projectcalico.org/v1
    Jan 30 12:51:29.068: INFO: Versions found [{crd.projectcalico.org/v1 v1}]
    Jan 30 12:51:29.068: INFO: crd.projectcalico.org/v1 matches crd.projectcalico.org/v1
    [AfterEach] [sig-api-machinery] Discovery
      test/e2e/framework/node/init/init.go:32
    Jan 30 12:51:29.068: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Discovery
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Discovery
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Discovery
      tear down framework | framework.go:193
    STEP: Destroying namespace "discovery-9560" for this suite. 01/30/23 12:51:29.071
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-apps] CronJob
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 12:51:29.074
Jan 30 12:51:29.074: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename cronjob 01/30/23 12:51:29.075
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:51:29.081
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:51:29.083
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124
STEP: Creating a ForbidConcurrent cronjob 01/30/23 12:51:29.085
STEP: Ensuring a job is scheduled 01/30/23 12:51:29.088
STEP: Ensuring exactly one is scheduled 01/30/23 12:52:01.092
STEP: Ensuring exactly one running job exists by listing jobs explicitly 01/30/23 12:52:01.093
STEP: Ensuring no more jobs are scheduled 01/30/23 12:52:01.095
STEP: Removing cronjob 01/30/23 12:57:01.1
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Jan 30 12:57:01.103: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-3136" for this suite. 01/30/23 12:57:01.105
------------------------------
• [SLOW TEST] [332.033 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 12:51:29.074
    Jan 30 12:51:29.074: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename cronjob 01/30/23 12:51:29.075
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:51:29.081
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:51:29.083
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
      test/e2e/apps/cronjob.go:124
    STEP: Creating a ForbidConcurrent cronjob 01/30/23 12:51:29.085
    STEP: Ensuring a job is scheduled 01/30/23 12:51:29.088
    STEP: Ensuring exactly one is scheduled 01/30/23 12:52:01.092
    STEP: Ensuring exactly one running job exists by listing jobs explicitly 01/30/23 12:52:01.093
    STEP: Ensuring no more jobs are scheduled 01/30/23 12:52:01.095
    STEP: Removing cronjob 01/30/23 12:57:01.1
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Jan 30 12:57:01.103: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-3136" for this suite. 01/30/23 12:57:01.105
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch
  should add annotations for pods in rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1652
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 12:57:01.109
Jan 30 12:57:01.109: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename kubectl 01/30/23 12:57:01.11
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:57:01.123
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:57:01.125
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should add annotations for pods in rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1652
STEP: creating Agnhost RC 01/30/23 12:57:01.127
Jan 30 12:57:01.127: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=kubectl-2934 create -f -'
Jan 30 12:57:02.673: INFO: stderr: ""
Jan 30 12:57:02.673: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 01/30/23 12:57:02.673
Jan 30 12:57:03.677: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 30 12:57:03.677: INFO: Found 0 / 1
Jan 30 12:57:04.677: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 30 12:57:04.677: INFO: Found 1 / 1
Jan 30 12:57:04.677: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods 01/30/23 12:57:04.677
Jan 30 12:57:04.679: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 30 12:57:04.679: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jan 30 12:57:04.679: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=kubectl-2934 patch pod agnhost-primary-vhkxg -p {"metadata":{"annotations":{"x":"y"}}}'
Jan 30 12:57:04.744: INFO: stderr: ""
Jan 30 12:57:04.744: INFO: stdout: "pod/agnhost-primary-vhkxg patched\n"
STEP: checking annotations 01/30/23 12:57:04.744
Jan 30 12:57:04.746: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 30 12:57:04.746: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jan 30 12:57:04.747: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-2934" for this suite. 01/30/23 12:57:04.749
------------------------------
• [3.642 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl patch
  test/e2e/kubectl/kubectl.go:1646
    should add annotations for pods in rc  [Conformance]
    test/e2e/kubectl/kubectl.go:1652

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 12:57:01.109
    Jan 30 12:57:01.109: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename kubectl 01/30/23 12:57:01.11
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:57:01.123
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:57:01.125
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should add annotations for pods in rc  [Conformance]
      test/e2e/kubectl/kubectl.go:1652
    STEP: creating Agnhost RC 01/30/23 12:57:01.127
    Jan 30 12:57:01.127: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=kubectl-2934 create -f -'
    Jan 30 12:57:02.673: INFO: stderr: ""
    Jan 30 12:57:02.673: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 01/30/23 12:57:02.673
    Jan 30 12:57:03.677: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan 30 12:57:03.677: INFO: Found 0 / 1
    Jan 30 12:57:04.677: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan 30 12:57:04.677: INFO: Found 1 / 1
    Jan 30 12:57:04.677: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    STEP: patching all pods 01/30/23 12:57:04.677
    Jan 30 12:57:04.679: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan 30 12:57:04.679: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Jan 30 12:57:04.679: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=kubectl-2934 patch pod agnhost-primary-vhkxg -p {"metadata":{"annotations":{"x":"y"}}}'
    Jan 30 12:57:04.744: INFO: stderr: ""
    Jan 30 12:57:04.744: INFO: stdout: "pod/agnhost-primary-vhkxg patched\n"
    STEP: checking annotations 01/30/23 12:57:04.744
    Jan 30 12:57:04.746: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan 30 12:57:04.746: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jan 30 12:57:04.747: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-2934" for this suite. 01/30/23 12:57:04.749
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:84
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 12:57:04.752
Jan 30 12:57:04.752: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename projected 01/30/23 12:57:04.753
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:57:04.759
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:57:04.761
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:84
STEP: Creating a pod to test downward API volume plugin 01/30/23 12:57:04.763
Jan 30 12:57:04.768: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7e613cd0-907f-430f-a900-375da99e7ca8" in namespace "projected-1916" to be "Succeeded or Failed"
Jan 30 12:57:04.769: INFO: Pod "downwardapi-volume-7e613cd0-907f-430f-a900-375da99e7ca8": Phase="Pending", Reason="", readiness=false. Elapsed: 1.556692ms
Jan 30 12:57:06.772: INFO: Pod "downwardapi-volume-7e613cd0-907f-430f-a900-375da99e7ca8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004539132s
Jan 30 12:57:08.772: INFO: Pod "downwardapi-volume-7e613cd0-907f-430f-a900-375da99e7ca8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.004506684s
Jan 30 12:57:10.773: INFO: Pod "downwardapi-volume-7e613cd0-907f-430f-a900-375da99e7ca8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.005442414s
STEP: Saw pod success 01/30/23 12:57:10.773
Jan 30 12:57:10.773: INFO: Pod "downwardapi-volume-7e613cd0-907f-430f-a900-375da99e7ca8" satisfied condition "Succeeded or Failed"
Jan 30 12:57:10.775: INFO: Trying to get logs from node pubt2-nks-for-dev1.dg.163.org pod downwardapi-volume-7e613cd0-907f-430f-a900-375da99e7ca8 container client-container: <nil>
STEP: delete the pod 01/30/23 12:57:10.79
Jan 30 12:57:10.794: INFO: Waiting for pod downwardapi-volume-7e613cd0-907f-430f-a900-375da99e7ca8 to disappear
Jan 30 12:57:10.796: INFO: Pod downwardapi-volume-7e613cd0-907f-430f-a900-375da99e7ca8 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Jan 30 12:57:10.796: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-1916" for this suite. 01/30/23 12:57:10.798
------------------------------
• [SLOW TEST] [6.048 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:84

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 12:57:04.752
    Jan 30 12:57:04.752: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename projected 01/30/23 12:57:04.753
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:57:04.759
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:57:04.761
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:84
    STEP: Creating a pod to test downward API volume plugin 01/30/23 12:57:04.763
    Jan 30 12:57:04.768: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7e613cd0-907f-430f-a900-375da99e7ca8" in namespace "projected-1916" to be "Succeeded or Failed"
    Jan 30 12:57:04.769: INFO: Pod "downwardapi-volume-7e613cd0-907f-430f-a900-375da99e7ca8": Phase="Pending", Reason="", readiness=false. Elapsed: 1.556692ms
    Jan 30 12:57:06.772: INFO: Pod "downwardapi-volume-7e613cd0-907f-430f-a900-375da99e7ca8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004539132s
    Jan 30 12:57:08.772: INFO: Pod "downwardapi-volume-7e613cd0-907f-430f-a900-375da99e7ca8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.004506684s
    Jan 30 12:57:10.773: INFO: Pod "downwardapi-volume-7e613cd0-907f-430f-a900-375da99e7ca8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.005442414s
    STEP: Saw pod success 01/30/23 12:57:10.773
    Jan 30 12:57:10.773: INFO: Pod "downwardapi-volume-7e613cd0-907f-430f-a900-375da99e7ca8" satisfied condition "Succeeded or Failed"
    Jan 30 12:57:10.775: INFO: Trying to get logs from node pubt2-nks-for-dev1.dg.163.org pod downwardapi-volume-7e613cd0-907f-430f-a900-375da99e7ca8 container client-container: <nil>
    STEP: delete the pod 01/30/23 12:57:10.79
    Jan 30 12:57:10.794: INFO: Waiting for pod downwardapi-volume-7e613cd0-907f-430f-a900-375da99e7ca8 to disappear
    Jan 30 12:57:10.796: INFO: Pod downwardapi-volume-7e613cd0-907f-430f-a900-375da99e7ca8 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Jan 30 12:57:10.796: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-1916" for this suite. 01/30/23 12:57:10.798
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:186
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 12:57:10.801
Jan 30 12:57:10.801: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename var-expansion 01/30/23 12:57:10.802
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:57:10.807
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:57:10.809
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:186
Jan 30 12:57:10.815: INFO: Waiting up to 2m0s for pod "var-expansion-976151c8-b3f4-473c-9d2d-45124d058e36" in namespace "var-expansion-3263" to be "container 0 failed with reason CreateContainerConfigError"
Jan 30 12:57:10.817: INFO: Pod "var-expansion-976151c8-b3f4-473c-9d2d-45124d058e36": Phase="Pending", Reason="", readiness=false. Elapsed: 1.570586ms
Jan 30 12:57:12.819: INFO: Pod "var-expansion-976151c8-b3f4-473c-9d2d-45124d058e36": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00371962s
Jan 30 12:57:12.819: INFO: Pod "var-expansion-976151c8-b3f4-473c-9d2d-45124d058e36" satisfied condition "container 0 failed with reason CreateContainerConfigError"
Jan 30 12:57:12.819: INFO: Deleting pod "var-expansion-976151c8-b3f4-473c-9d2d-45124d058e36" in namespace "var-expansion-3263"
Jan 30 12:57:12.821: INFO: Wait up to 5m0s for pod "var-expansion-976151c8-b3f4-473c-9d2d-45124d058e36" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Jan 30 12:57:16.826: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-3263" for this suite. 01/30/23 12:57:16.829
------------------------------
• [SLOW TEST] [6.030 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:186

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 12:57:10.801
    Jan 30 12:57:10.801: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename var-expansion 01/30/23 12:57:10.802
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:57:10.807
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:57:10.809
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
      test/e2e/common/node/expansion.go:186
    Jan 30 12:57:10.815: INFO: Waiting up to 2m0s for pod "var-expansion-976151c8-b3f4-473c-9d2d-45124d058e36" in namespace "var-expansion-3263" to be "container 0 failed with reason CreateContainerConfigError"
    Jan 30 12:57:10.817: INFO: Pod "var-expansion-976151c8-b3f4-473c-9d2d-45124d058e36": Phase="Pending", Reason="", readiness=false. Elapsed: 1.570586ms
    Jan 30 12:57:12.819: INFO: Pod "var-expansion-976151c8-b3f4-473c-9d2d-45124d058e36": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00371962s
    Jan 30 12:57:12.819: INFO: Pod "var-expansion-976151c8-b3f4-473c-9d2d-45124d058e36" satisfied condition "container 0 failed with reason CreateContainerConfigError"
    Jan 30 12:57:12.819: INFO: Deleting pod "var-expansion-976151c8-b3f4-473c-9d2d-45124d058e36" in namespace "var-expansion-3263"
    Jan 30 12:57:12.821: INFO: Wait up to 5m0s for pod "var-expansion-976151c8-b3f4-473c-9d2d-45124d058e36" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Jan 30 12:57:16.826: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-3263" for this suite. 01/30/23 12:57:16.829
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl expose
  should create services for rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1415
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 12:57:16.833
Jan 30 12:57:16.833: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename kubectl 01/30/23 12:57:16.834
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:57:16.84
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:57:16.841
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should create services for rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1415
STEP: creating Agnhost RC 01/30/23 12:57:16.843
Jan 30 12:57:16.843: INFO: namespace kubectl-7836
Jan 30 12:57:16.844: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=kubectl-7836 create -f -'
Jan 30 12:57:17.006: INFO: stderr: ""
Jan 30 12:57:17.006: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 01/30/23 12:57:17.006
Jan 30 12:57:18.009: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 30 12:57:18.009: INFO: Found 0 / 1
Jan 30 12:57:19.010: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 30 12:57:19.010: INFO: Found 1 / 1
Jan 30 12:57:19.010: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jan 30 12:57:19.012: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 30 12:57:19.012: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jan 30 12:57:19.012: INFO: wait on agnhost-primary startup in kubectl-7836 
Jan 30 12:57:19.012: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=kubectl-7836 logs agnhost-primary-lqqzq agnhost-primary'
Jan 30 12:57:19.076: INFO: stderr: ""
Jan 30 12:57:19.076: INFO: stdout: "Paused\n"
STEP: exposing RC 01/30/23 12:57:19.076
Jan 30 12:57:19.077: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=kubectl-7836 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
Jan 30 12:57:19.140: INFO: stderr: ""
Jan 30 12:57:19.140: INFO: stdout: "service/rm2 exposed\n"
Jan 30 12:57:19.141: INFO: Service rm2 in namespace kubectl-7836 found.
STEP: exposing service 01/30/23 12:57:21.145
Jan 30 12:57:21.145: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=kubectl-7836 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
Jan 30 12:57:21.212: INFO: stderr: ""
Jan 30 12:57:21.212: INFO: stdout: "service/rm3 exposed\n"
Jan 30 12:57:21.214: INFO: Service rm3 in namespace kubectl-7836 found.
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jan 30 12:57:23.218: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-7836" for this suite. 01/30/23 12:57:23.22
------------------------------
• [SLOW TEST] [6.390 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl expose
  test/e2e/kubectl/kubectl.go:1409
    should create services for rc  [Conformance]
    test/e2e/kubectl/kubectl.go:1415

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 12:57:16.833
    Jan 30 12:57:16.833: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename kubectl 01/30/23 12:57:16.834
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:57:16.84
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:57:16.841
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should create services for rc  [Conformance]
      test/e2e/kubectl/kubectl.go:1415
    STEP: creating Agnhost RC 01/30/23 12:57:16.843
    Jan 30 12:57:16.843: INFO: namespace kubectl-7836
    Jan 30 12:57:16.844: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=kubectl-7836 create -f -'
    Jan 30 12:57:17.006: INFO: stderr: ""
    Jan 30 12:57:17.006: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 01/30/23 12:57:17.006
    Jan 30 12:57:18.009: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan 30 12:57:18.009: INFO: Found 0 / 1
    Jan 30 12:57:19.010: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan 30 12:57:19.010: INFO: Found 1 / 1
    Jan 30 12:57:19.010: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    Jan 30 12:57:19.012: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan 30 12:57:19.012: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Jan 30 12:57:19.012: INFO: wait on agnhost-primary startup in kubectl-7836 
    Jan 30 12:57:19.012: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=kubectl-7836 logs agnhost-primary-lqqzq agnhost-primary'
    Jan 30 12:57:19.076: INFO: stderr: ""
    Jan 30 12:57:19.076: INFO: stdout: "Paused\n"
    STEP: exposing RC 01/30/23 12:57:19.076
    Jan 30 12:57:19.077: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=kubectl-7836 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
    Jan 30 12:57:19.140: INFO: stderr: ""
    Jan 30 12:57:19.140: INFO: stdout: "service/rm2 exposed\n"
    Jan 30 12:57:19.141: INFO: Service rm2 in namespace kubectl-7836 found.
    STEP: exposing service 01/30/23 12:57:21.145
    Jan 30 12:57:21.145: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1067244644 --namespace=kubectl-7836 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
    Jan 30 12:57:21.212: INFO: stderr: ""
    Jan 30 12:57:21.212: INFO: stdout: "service/rm3 exposed\n"
    Jan 30 12:57:21.214: INFO: Service rm3 in namespace kubectl-7836 found.
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jan 30 12:57:23.218: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-7836" for this suite. 01/30/23 12:57:23.22
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:137
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 12:57:23.223
Jan 30 12:57:23.223: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename emptydir 01/30/23 12:57:23.224
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:57:23.23
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:57:23.232
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:137
STEP: Creating a pod to test emptydir 0666 on tmpfs 01/30/23 12:57:23.234
Jan 30 12:57:23.239: INFO: Waiting up to 5m0s for pod "pod-6994a693-d182-40c1-933d-d57d83cd5920" in namespace "emptydir-8992" to be "Succeeded or Failed"
Jan 30 12:57:23.241: INFO: Pod "pod-6994a693-d182-40c1-933d-d57d83cd5920": Phase="Pending", Reason="", readiness=false. Elapsed: 1.63119ms
Jan 30 12:57:25.243: INFO: Pod "pod-6994a693-d182-40c1-933d-d57d83cd5920": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0037025s
Jan 30 12:57:27.243: INFO: Pod "pod-6994a693-d182-40c1-933d-d57d83cd5920": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004306713s
STEP: Saw pod success 01/30/23 12:57:27.243
Jan 30 12:57:27.243: INFO: Pod "pod-6994a693-d182-40c1-933d-d57d83cd5920" satisfied condition "Succeeded or Failed"
Jan 30 12:57:27.245: INFO: Trying to get logs from node pubt2-nks-for-dev1.dg.163.org pod pod-6994a693-d182-40c1-933d-d57d83cd5920 container test-container: <nil>
STEP: delete the pod 01/30/23 12:57:27.249
Jan 30 12:57:27.254: INFO: Waiting for pod pod-6994a693-d182-40c1-933d-d57d83cd5920 to disappear
Jan 30 12:57:27.255: INFO: Pod pod-6994a693-d182-40c1-933d-d57d83cd5920 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jan 30 12:57:27.255: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-8992" for this suite. 01/30/23 12:57:27.258
------------------------------
• [4.037 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:137

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 12:57:23.223
    Jan 30 12:57:23.223: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename emptydir 01/30/23 12:57:23.224
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:57:23.23
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:57:23.232
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:137
    STEP: Creating a pod to test emptydir 0666 on tmpfs 01/30/23 12:57:23.234
    Jan 30 12:57:23.239: INFO: Waiting up to 5m0s for pod "pod-6994a693-d182-40c1-933d-d57d83cd5920" in namespace "emptydir-8992" to be "Succeeded or Failed"
    Jan 30 12:57:23.241: INFO: Pod "pod-6994a693-d182-40c1-933d-d57d83cd5920": Phase="Pending", Reason="", readiness=false. Elapsed: 1.63119ms
    Jan 30 12:57:25.243: INFO: Pod "pod-6994a693-d182-40c1-933d-d57d83cd5920": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0037025s
    Jan 30 12:57:27.243: INFO: Pod "pod-6994a693-d182-40c1-933d-d57d83cd5920": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004306713s
    STEP: Saw pod success 01/30/23 12:57:27.243
    Jan 30 12:57:27.243: INFO: Pod "pod-6994a693-d182-40c1-933d-d57d83cd5920" satisfied condition "Succeeded or Failed"
    Jan 30 12:57:27.245: INFO: Trying to get logs from node pubt2-nks-for-dev1.dg.163.org pod pod-6994a693-d182-40c1-933d-d57d83cd5920 container test-container: <nil>
    STEP: delete the pod 01/30/23 12:57:27.249
    Jan 30 12:57:27.254: INFO: Waiting for pod pod-6994a693-d182-40c1-933d-d57d83cd5920 to disappear
    Jan 30 12:57:27.255: INFO: Pod pod-6994a693-d182-40c1-933d-d57d83cd5920 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jan 30 12:57:27.255: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-8992" for this suite. 01/30/23 12:57:27.258
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:82
[BeforeEach] [sig-network] Networking
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 12:57:27.261
Jan 30 12:57:27.261: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename pod-network-test 01/30/23 12:57:27.261
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:57:27.267
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:57:27.269
[BeforeEach] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:31
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:82
STEP: Performing setup for networking test in namespace pod-network-test-7154 01/30/23 12:57:27.271
STEP: creating a selector 01/30/23 12:57:27.271
STEP: Creating the service pods in kubernetes 01/30/23 12:57:27.271
Jan 30 12:57:27.271: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jan 30 12:57:27.282: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-7154" to be "running and ready"
Jan 30 12:57:27.283: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 1.659558ms
Jan 30 12:57:27.283: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jan 30 12:57:29.286: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.004641409s
Jan 30 12:57:29.286: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 30 12:57:31.287: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.005613054s
Jan 30 12:57:31.287: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 30 12:57:33.286: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.004421863s
Jan 30 12:57:33.286: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 30 12:57:35.286: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.004388002s
Jan 30 12:57:35.286: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 30 12:57:37.286: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.004362129s
Jan 30 12:57:37.286: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 30 12:57:39.286: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.004234668s
Jan 30 12:57:39.286: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 30 12:57:41.287: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.004879844s
Jan 30 12:57:41.287: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 30 12:57:43.286: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.003904892s
Jan 30 12:57:43.286: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 30 12:57:45.287: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.004816445s
Jan 30 12:57:45.287: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 30 12:57:47.287: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.005716022s
Jan 30 12:57:47.287: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 30 12:57:49.286: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.004735132s
Jan 30 12:57:49.286: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Jan 30 12:57:49.287: INFO: Pod "netserver-0" satisfied condition "running and ready"
Jan 30 12:57:49.289: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-7154" to be "running and ready"
Jan 30 12:57:49.290: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 1.605645ms
Jan 30 12:57:49.290: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Jan 30 12:57:49.290: INFO: Pod "netserver-1" satisfied condition "running and ready"
STEP: Creating test pods 01/30/23 12:57:49.292
Jan 30 12:57:49.295: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-7154" to be "running"
Jan 30 12:57:49.296: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 1.578871ms
Jan 30 12:57:51.298: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003829788s
Jan 30 12:57:53.298: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.003581737s
Jan 30 12:57:53.298: INFO: Pod "test-container-pod" satisfied condition "running"
Jan 30 12:57:53.300: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
Jan 30 12:57:53.300: INFO: Breadth first check of 10.178.151.224 on host 10.182.0.82...
Jan 30 12:57:53.301: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.178.151.227:9080/dial?request=hostname&protocol=http&host=10.178.151.224&port=8083&tries=1'] Namespace:pod-network-test-7154 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 30 12:57:53.301: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
Jan 30 12:57:53.302: INFO: ExecWithOptions: Clientset creation
Jan 30 12:57:53.302: INFO: ExecWithOptions: execute(POST https://10.178.64.1:443/api/v1/namespaces/pod-network-test-7154/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.178.151.227%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.178.151.224%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Jan 30 12:57:53.395: INFO: Waiting for responses: map[]
Jan 30 12:57:53.395: INFO: reached 10.178.151.224 after 0/1 tries
Jan 30 12:57:53.395: INFO: Breadth first check of 10.178.197.206 on host 10.182.0.84...
Jan 30 12:57:53.396: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.178.151.227:9080/dial?request=hostname&protocol=http&host=10.178.197.206&port=8083&tries=1'] Namespace:pod-network-test-7154 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 30 12:57:53.396: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
Jan 30 12:57:53.397: INFO: ExecWithOptions: Clientset creation
Jan 30 12:57:53.397: INFO: ExecWithOptions: execute(POST https://10.178.64.1:443/api/v1/namespaces/pod-network-test-7154/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.178.151.227%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.178.197.206%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Jan 30 12:57:53.478: INFO: Waiting for responses: map[]
Jan 30 12:57:53.478: INFO: reached 10.178.197.206 after 0/1 tries
Jan 30 12:57:53.478: INFO: Going to retry 0 out of 2 pods....
[AfterEach] [sig-network] Networking
  test/e2e/framework/node/init/init.go:32
Jan 30 12:57:53.478: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Networking
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Networking
  tear down framework | framework.go:193
STEP: Destroying namespace "pod-network-test-7154" for this suite. 01/30/23 12:57:53.48
------------------------------
• [SLOW TEST] [26.222 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:82

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 12:57:27.261
    Jan 30 12:57:27.261: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename pod-network-test 01/30/23 12:57:27.261
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:57:27.267
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:57:27.269
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:31
    [It] should function for intra-pod communication: http [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:82
    STEP: Performing setup for networking test in namespace pod-network-test-7154 01/30/23 12:57:27.271
    STEP: creating a selector 01/30/23 12:57:27.271
    STEP: Creating the service pods in kubernetes 01/30/23 12:57:27.271
    Jan 30 12:57:27.271: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Jan 30 12:57:27.282: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-7154" to be "running and ready"
    Jan 30 12:57:27.283: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 1.659558ms
    Jan 30 12:57:27.283: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Jan 30 12:57:29.286: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.004641409s
    Jan 30 12:57:29.286: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 30 12:57:31.287: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.005613054s
    Jan 30 12:57:31.287: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 30 12:57:33.286: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.004421863s
    Jan 30 12:57:33.286: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 30 12:57:35.286: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.004388002s
    Jan 30 12:57:35.286: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 30 12:57:37.286: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.004362129s
    Jan 30 12:57:37.286: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 30 12:57:39.286: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.004234668s
    Jan 30 12:57:39.286: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 30 12:57:41.287: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.004879844s
    Jan 30 12:57:41.287: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 30 12:57:43.286: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.003904892s
    Jan 30 12:57:43.286: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 30 12:57:45.287: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.004816445s
    Jan 30 12:57:45.287: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 30 12:57:47.287: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.005716022s
    Jan 30 12:57:47.287: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 30 12:57:49.286: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.004735132s
    Jan 30 12:57:49.286: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Jan 30 12:57:49.287: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Jan 30 12:57:49.289: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-7154" to be "running and ready"
    Jan 30 12:57:49.290: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 1.605645ms
    Jan 30 12:57:49.290: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Jan 30 12:57:49.290: INFO: Pod "netserver-1" satisfied condition "running and ready"
    STEP: Creating test pods 01/30/23 12:57:49.292
    Jan 30 12:57:49.295: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-7154" to be "running"
    Jan 30 12:57:49.296: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 1.578871ms
    Jan 30 12:57:51.298: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003829788s
    Jan 30 12:57:53.298: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.003581737s
    Jan 30 12:57:53.298: INFO: Pod "test-container-pod" satisfied condition "running"
    Jan 30 12:57:53.300: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
    Jan 30 12:57:53.300: INFO: Breadth first check of 10.178.151.224 on host 10.182.0.82...
    Jan 30 12:57:53.301: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.178.151.227:9080/dial?request=hostname&protocol=http&host=10.178.151.224&port=8083&tries=1'] Namespace:pod-network-test-7154 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 30 12:57:53.301: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    Jan 30 12:57:53.302: INFO: ExecWithOptions: Clientset creation
    Jan 30 12:57:53.302: INFO: ExecWithOptions: execute(POST https://10.178.64.1:443/api/v1/namespaces/pod-network-test-7154/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.178.151.227%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.178.151.224%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Jan 30 12:57:53.395: INFO: Waiting for responses: map[]
    Jan 30 12:57:53.395: INFO: reached 10.178.151.224 after 0/1 tries
    Jan 30 12:57:53.395: INFO: Breadth first check of 10.178.197.206 on host 10.182.0.84...
    Jan 30 12:57:53.396: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.178.151.227:9080/dial?request=hostname&protocol=http&host=10.178.197.206&port=8083&tries=1'] Namespace:pod-network-test-7154 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 30 12:57:53.396: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    Jan 30 12:57:53.397: INFO: ExecWithOptions: Clientset creation
    Jan 30 12:57:53.397: INFO: ExecWithOptions: execute(POST https://10.178.64.1:443/api/v1/namespaces/pod-network-test-7154/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.178.151.227%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.178.197.206%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Jan 30 12:57:53.478: INFO: Waiting for responses: map[]
    Jan 30 12:57:53.478: INFO: reached 10.178.197.206 after 0/1 tries
    Jan 30 12:57:53.478: INFO: Going to retry 0 out of 2 pods....
    [AfterEach] [sig-network] Networking
      test/e2e/framework/node/init/init.go:32
    Jan 30 12:57:53.478: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Networking
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Networking
      tear down framework | framework.go:193
    STEP: Destroying namespace "pod-network-test-7154" for this suite. 01/30/23 12:57:53.48
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] Garbage collector
  should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 12:57:53.483
Jan 30 12:57:53.483: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename gc 01/30/23 12:57:53.484
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:57:53.49
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:57:53.492
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849
Jan 30 12:57:53.507: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"f53282b7-19e4-4e5b-af79-0fb9bddad247", Controller:(*bool)(0xc0015fde56), BlockOwnerDeletion:(*bool)(0xc0015fde57)}}
Jan 30 12:57:53.510: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"4ef659c6-1e1f-4d97-bb06-2f9d2028a0a5", Controller:(*bool)(0xc005592336), BlockOwnerDeletion:(*bool)(0xc005592337)}}
Jan 30 12:57:53.514: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"dd49fad7-bbf1-430f-8d9b-e03523df17f5", Controller:(*bool)(0xc006fead6e), BlockOwnerDeletion:(*bool)(0xc006fead6f)}}
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Jan 30 12:57:58.519: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-8902" for this suite. 01/30/23 12:57:58.521
------------------------------
• [SLOW TEST] [5.040 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 12:57:53.483
    Jan 30 12:57:53.483: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename gc 01/30/23 12:57:53.484
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:57:53.49
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:57:53.492
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should not be blocked by dependency circle [Conformance]
      test/e2e/apimachinery/garbage_collector.go:849
    Jan 30 12:57:53.507: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"f53282b7-19e4-4e5b-af79-0fb9bddad247", Controller:(*bool)(0xc0015fde56), BlockOwnerDeletion:(*bool)(0xc0015fde57)}}
    Jan 30 12:57:53.510: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"4ef659c6-1e1f-4d97-bb06-2f9d2028a0a5", Controller:(*bool)(0xc005592336), BlockOwnerDeletion:(*bool)(0xc005592337)}}
    Jan 30 12:57:53.514: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"dd49fad7-bbf1-430f-8d9b-e03523df17f5", Controller:(*bool)(0xc006fead6e), BlockOwnerDeletion:(*bool)(0xc006fead6f)}}
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Jan 30 12:57:58.519: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-8902" for this suite. 01/30/23 12:57:58.521
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:466
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 12:57:58.523
Jan 30 12:57:58.523: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename sched-pred 01/30/23 12:57:58.524
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:57:58.53
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:57:58.532
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:97
Jan 30 12:57:58.534: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jan 30 12:57:58.538: INFO: Waiting for terminating namespaces to be deleted...
Jan 30 12:57:58.539: INFO: 
Logging pods the apiserver thinks is on node pubt2-nks-for-dev1.dg.163.org before test
Jan 30 12:57:58.544: INFO: calico-node-5xgvr from kube-system started at 2023-01-30 11:14:53 +0000 UTC (1 container statuses recorded)
Jan 30 12:57:58.544: INFO: 	Container calico-node ready: true, restart count 0
Jan 30 12:57:58.544: INFO: cleanlog-w9znh from kube-system started at 2023-01-30 11:02:22 +0000 UTC (1 container statuses recorded)
Jan 30 12:57:58.544: INFO: 	Container cleanlog ready: true, restart count 0
Jan 30 12:57:58.544: INFO: kube-proxy-fvt68 from kube-system started at 2023-01-30 11:02:22 +0000 UTC (1 container statuses recorded)
Jan 30 12:57:58.544: INFO: 	Container kube-proxy ready: true, restart count 0
Jan 30 12:57:58.544: INFO: node-local-dns-8jh9v from kube-system started at 2023-01-30 11:02:22 +0000 UTC (1 container statuses recorded)
Jan 30 12:57:58.544: INFO: 	Container node-cache ready: true, restart count 5
Jan 30 12:57:58.544: INFO: netserver-0 from pod-network-test-7154 started at 2023-01-30 12:57:27 +0000 UTC (1 container statuses recorded)
Jan 30 12:57:58.544: INFO: 	Container webserver ready: true, restart count 0
Jan 30 12:57:58.544: INFO: test-container-pod from pod-network-test-7154 started at 2023-01-30 12:57:49 +0000 UTC (1 container statuses recorded)
Jan 30 12:57:58.544: INFO: 	Container webserver ready: true, restart count 0
Jan 30 12:57:58.544: INFO: sonobuoy-systemd-logs-daemon-set-6d9e6fe3a8a74fcc-st9q5 from sonobuoy started at 2023-01-30 11:17:19 +0000 UTC (2 container statuses recorded)
Jan 30 12:57:58.544: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 30 12:57:58.544: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 30 12:57:58.544: INFO: 
Logging pods the apiserver thinks is on node pubt2-nks-for-dev3.dg.163.org before test
Jan 30 12:57:58.550: INFO: calico-kube-controllers-568668f974-5gtkq from kube-system started at 2023-01-30 09:59:07 +0000 UTC (1 container statuses recorded)
Jan 30 12:57:58.550: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Jan 30 12:57:58.550: INFO: calico-node-h7jwg from kube-system started at 2023-01-30 11:14:53 +0000 UTC (1 container statuses recorded)
Jan 30 12:57:58.550: INFO: 	Container calico-node ready: true, restart count 0
Jan 30 12:57:58.550: INFO: cleanlog-crksz from kube-system started at 2023-01-30 09:56:18 +0000 UTC (1 container statuses recorded)
Jan 30 12:57:58.550: INFO: 	Container cleanlog ready: true, restart count 0
Jan 30 12:57:58.550: INFO: coredns-5bf7dfc67-2gl8f from kube-system started at 2023-01-30 10:08:32 +0000 UTC (1 container statuses recorded)
Jan 30 12:57:58.550: INFO: 	Container coredns ready: true, restart count 0
Jan 30 12:57:58.550: INFO: coredns-5bf7dfc67-92sjx from kube-system started at 2023-01-30 12:46:12 +0000 UTC (1 container statuses recorded)
Jan 30 12:57:58.550: INFO: 	Container coredns ready: true, restart count 0
Jan 30 12:57:58.550: INFO: etcd-main-pubt2-nks-for-dev3.dg.163.org from kube-system started at 2023-01-30 09:56:17 +0000 UTC (1 container statuses recorded)
Jan 30 12:57:58.550: INFO: 	Container etcd ready: true, restart count 1
Jan 30 12:57:58.550: INFO: kube-apiserver-pubt2-nks-for-dev3.dg.163.org from kube-system started at 2023-01-30 09:56:18 +0000 UTC (1 container statuses recorded)
Jan 30 12:57:58.550: INFO: 	Container kube-apiserver ready: true, restart count 1
Jan 30 12:57:58.550: INFO: kube-controller-manager-pubt2-nks-for-dev3.dg.163.org from kube-system started at 2023-01-30 09:56:18 +0000 UTC (1 container statuses recorded)
Jan 30 12:57:58.550: INFO: 	Container kube-controller-manager ready: true, restart count 1
Jan 30 12:57:58.550: INFO: kube-proxy-br2mf from kube-system started at 2023-01-30 09:58:40 +0000 UTC (1 container statuses recorded)
Jan 30 12:57:58.550: INFO: 	Container kube-proxy ready: true, restart count 0
Jan 30 12:57:58.550: INFO: kube-scheduler-pubt2-nks-for-dev3.dg.163.org from kube-system started at 2023-01-30 09:56:17 +0000 UTC (1 container statuses recorded)
Jan 30 12:57:58.550: INFO: 	Container kube-scheduler ready: true, restart count 1
Jan 30 12:57:58.550: INFO: node-local-dns-txwnd from kube-system started at 2023-01-30 10:09:12 +0000 UTC (1 container statuses recorded)
Jan 30 12:57:58.550: INFO: 	Container node-cache ready: true, restart count 0
Jan 30 12:57:58.550: INFO: netserver-1 from pod-network-test-7154 started at 2023-01-30 12:57:27 +0000 UTC (1 container statuses recorded)
Jan 30 12:57:58.550: INFO: 	Container webserver ready: true, restart count 0
Jan 30 12:57:58.550: INFO: sonobuoy from sonobuoy started at 2023-01-30 11:17:15 +0000 UTC (1 container statuses recorded)
Jan 30 12:57:58.550: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jan 30 12:57:58.550: INFO: sonobuoy-e2e-job-a0572830995342a7 from sonobuoy started at 2023-01-30 11:17:19 +0000 UTC (2 container statuses recorded)
Jan 30 12:57:58.550: INFO: 	Container e2e ready: true, restart count 0
Jan 30 12:57:58.550: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 30 12:57:58.550: INFO: sonobuoy-systemd-logs-daemon-set-6d9e6fe3a8a74fcc-nkw5p from sonobuoy started at 2023-01-30 11:17:19 +0000 UTC (2 container statuses recorded)
Jan 30 12:57:58.550: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 30 12:57:58.550: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:466
STEP: Trying to launch a pod without a label to get a node which can launch it. 01/30/23 12:57:58.55
Jan 30 12:57:58.554: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-3241" to be "running"
Jan 30 12:57:58.556: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 1.548664ms
Jan 30 12:58:00.558: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003622931s
Jan 30 12:58:02.558: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 4.004388565s
Jan 30 12:58:04.559: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 6.004577273s
Jan 30 12:58:06.559: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 8.005292225s
Jan 30 12:58:08.558: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 10.00439587s
Jan 30 12:58:10.559: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 12.005377126s
Jan 30 12:58:12.559: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 14.004868565s
Jan 30 12:58:14.559: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 16.004860998s
Jan 30 12:58:16.559: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 18.004874447s
Jan 30 12:58:18.559: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 20.004853223s
Jan 30 12:58:20.559: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 22.004773176s
Jan 30 12:58:22.559: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 24.004609219s
Jan 30 12:58:24.559: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 26.004557113s
Jan 30 12:58:26.560: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 28.005739971s
Jan 30 12:58:28.558: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 30.004459484s
Jan 30 12:58:30.559: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 32.005487723s
Jan 30 12:58:32.558: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 34.003615123s
Jan 30 12:58:32.558: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 01/30/23 12:58:32.559
STEP: Trying to apply a random label on the found node. 01/30/23 12:58:32.564
STEP: verifying the node has the label kubernetes.io/e2e-6b0cc27a-825b-48d2-a930-0c115592b31f 42 01/30/23 12:58:32.573
STEP: Trying to relaunch the pod, now with labels. 01/30/23 12:58:32.575
Jan 30 12:58:32.581: INFO: Waiting up to 5m0s for pod "with-labels" in namespace "sched-pred-3241" to be "not pending"
Jan 30 12:58:32.583: INFO: Pod "with-labels": Phase="Pending", Reason="", readiness=false. Elapsed: 1.755708ms
Jan 30 12:58:34.586: INFO: Pod "with-labels": Phase="Running", Reason="", readiness=true. Elapsed: 2.004394648s
Jan 30 12:58:34.586: INFO: Pod "with-labels" satisfied condition "not pending"
STEP: removing the label kubernetes.io/e2e-6b0cc27a-825b-48d2-a930-0c115592b31f off the node pubt2-nks-for-dev1.dg.163.org 01/30/23 12:58:34.587
STEP: verifying the node doesn't have the label kubernetes.io/e2e-6b0cc27a-825b-48d2-a930-0c115592b31f 01/30/23 12:58:34.597
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/node/init/init.go:32
Jan 30 12:58:34.599: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:88
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-pred-3241" for this suite. 01/30/23 12:58:34.602
------------------------------
• [SLOW TEST] [36.081 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:466

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 12:57:58.523
    Jan 30 12:57:58.523: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename sched-pred 01/30/23 12:57:58.524
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:57:58.53
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:57:58.532
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:97
    Jan 30 12:57:58.534: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Jan 30 12:57:58.538: INFO: Waiting for terminating namespaces to be deleted...
    Jan 30 12:57:58.539: INFO: 
    Logging pods the apiserver thinks is on node pubt2-nks-for-dev1.dg.163.org before test
    Jan 30 12:57:58.544: INFO: calico-node-5xgvr from kube-system started at 2023-01-30 11:14:53 +0000 UTC (1 container statuses recorded)
    Jan 30 12:57:58.544: INFO: 	Container calico-node ready: true, restart count 0
    Jan 30 12:57:58.544: INFO: cleanlog-w9znh from kube-system started at 2023-01-30 11:02:22 +0000 UTC (1 container statuses recorded)
    Jan 30 12:57:58.544: INFO: 	Container cleanlog ready: true, restart count 0
    Jan 30 12:57:58.544: INFO: kube-proxy-fvt68 from kube-system started at 2023-01-30 11:02:22 +0000 UTC (1 container statuses recorded)
    Jan 30 12:57:58.544: INFO: 	Container kube-proxy ready: true, restart count 0
    Jan 30 12:57:58.544: INFO: node-local-dns-8jh9v from kube-system started at 2023-01-30 11:02:22 +0000 UTC (1 container statuses recorded)
    Jan 30 12:57:58.544: INFO: 	Container node-cache ready: true, restart count 5
    Jan 30 12:57:58.544: INFO: netserver-0 from pod-network-test-7154 started at 2023-01-30 12:57:27 +0000 UTC (1 container statuses recorded)
    Jan 30 12:57:58.544: INFO: 	Container webserver ready: true, restart count 0
    Jan 30 12:57:58.544: INFO: test-container-pod from pod-network-test-7154 started at 2023-01-30 12:57:49 +0000 UTC (1 container statuses recorded)
    Jan 30 12:57:58.544: INFO: 	Container webserver ready: true, restart count 0
    Jan 30 12:57:58.544: INFO: sonobuoy-systemd-logs-daemon-set-6d9e6fe3a8a74fcc-st9q5 from sonobuoy started at 2023-01-30 11:17:19 +0000 UTC (2 container statuses recorded)
    Jan 30 12:57:58.544: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 30 12:57:58.544: INFO: 	Container systemd-logs ready: true, restart count 0
    Jan 30 12:57:58.544: INFO: 
    Logging pods the apiserver thinks is on node pubt2-nks-for-dev3.dg.163.org before test
    Jan 30 12:57:58.550: INFO: calico-kube-controllers-568668f974-5gtkq from kube-system started at 2023-01-30 09:59:07 +0000 UTC (1 container statuses recorded)
    Jan 30 12:57:58.550: INFO: 	Container calico-kube-controllers ready: true, restart count 0
    Jan 30 12:57:58.550: INFO: calico-node-h7jwg from kube-system started at 2023-01-30 11:14:53 +0000 UTC (1 container statuses recorded)
    Jan 30 12:57:58.550: INFO: 	Container calico-node ready: true, restart count 0
    Jan 30 12:57:58.550: INFO: cleanlog-crksz from kube-system started at 2023-01-30 09:56:18 +0000 UTC (1 container statuses recorded)
    Jan 30 12:57:58.550: INFO: 	Container cleanlog ready: true, restart count 0
    Jan 30 12:57:58.550: INFO: coredns-5bf7dfc67-2gl8f from kube-system started at 2023-01-30 10:08:32 +0000 UTC (1 container statuses recorded)
    Jan 30 12:57:58.550: INFO: 	Container coredns ready: true, restart count 0
    Jan 30 12:57:58.550: INFO: coredns-5bf7dfc67-92sjx from kube-system started at 2023-01-30 12:46:12 +0000 UTC (1 container statuses recorded)
    Jan 30 12:57:58.550: INFO: 	Container coredns ready: true, restart count 0
    Jan 30 12:57:58.550: INFO: etcd-main-pubt2-nks-for-dev3.dg.163.org from kube-system started at 2023-01-30 09:56:17 +0000 UTC (1 container statuses recorded)
    Jan 30 12:57:58.550: INFO: 	Container etcd ready: true, restart count 1
    Jan 30 12:57:58.550: INFO: kube-apiserver-pubt2-nks-for-dev3.dg.163.org from kube-system started at 2023-01-30 09:56:18 +0000 UTC (1 container statuses recorded)
    Jan 30 12:57:58.550: INFO: 	Container kube-apiserver ready: true, restart count 1
    Jan 30 12:57:58.550: INFO: kube-controller-manager-pubt2-nks-for-dev3.dg.163.org from kube-system started at 2023-01-30 09:56:18 +0000 UTC (1 container statuses recorded)
    Jan 30 12:57:58.550: INFO: 	Container kube-controller-manager ready: true, restart count 1
    Jan 30 12:57:58.550: INFO: kube-proxy-br2mf from kube-system started at 2023-01-30 09:58:40 +0000 UTC (1 container statuses recorded)
    Jan 30 12:57:58.550: INFO: 	Container kube-proxy ready: true, restart count 0
    Jan 30 12:57:58.550: INFO: kube-scheduler-pubt2-nks-for-dev3.dg.163.org from kube-system started at 2023-01-30 09:56:17 +0000 UTC (1 container statuses recorded)
    Jan 30 12:57:58.550: INFO: 	Container kube-scheduler ready: true, restart count 1
    Jan 30 12:57:58.550: INFO: node-local-dns-txwnd from kube-system started at 2023-01-30 10:09:12 +0000 UTC (1 container statuses recorded)
    Jan 30 12:57:58.550: INFO: 	Container node-cache ready: true, restart count 0
    Jan 30 12:57:58.550: INFO: netserver-1 from pod-network-test-7154 started at 2023-01-30 12:57:27 +0000 UTC (1 container statuses recorded)
    Jan 30 12:57:58.550: INFO: 	Container webserver ready: true, restart count 0
    Jan 30 12:57:58.550: INFO: sonobuoy from sonobuoy started at 2023-01-30 11:17:15 +0000 UTC (1 container statuses recorded)
    Jan 30 12:57:58.550: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Jan 30 12:57:58.550: INFO: sonobuoy-e2e-job-a0572830995342a7 from sonobuoy started at 2023-01-30 11:17:19 +0000 UTC (2 container statuses recorded)
    Jan 30 12:57:58.550: INFO: 	Container e2e ready: true, restart count 0
    Jan 30 12:57:58.550: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 30 12:57:58.550: INFO: sonobuoy-systemd-logs-daemon-set-6d9e6fe3a8a74fcc-nkw5p from sonobuoy started at 2023-01-30 11:17:19 +0000 UTC (2 container statuses recorded)
    Jan 30 12:57:58.550: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 30 12:57:58.550: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates that NodeSelector is respected if matching  [Conformance]
      test/e2e/scheduling/predicates.go:466
    STEP: Trying to launch a pod without a label to get a node which can launch it. 01/30/23 12:57:58.55
    Jan 30 12:57:58.554: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-3241" to be "running"
    Jan 30 12:57:58.556: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 1.548664ms
    Jan 30 12:58:00.558: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003622931s
    Jan 30 12:58:02.558: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 4.004388565s
    Jan 30 12:58:04.559: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 6.004577273s
    Jan 30 12:58:06.559: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 8.005292225s
    Jan 30 12:58:08.558: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 10.00439587s
    Jan 30 12:58:10.559: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 12.005377126s
    Jan 30 12:58:12.559: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 14.004868565s
    Jan 30 12:58:14.559: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 16.004860998s
    Jan 30 12:58:16.559: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 18.004874447s
    Jan 30 12:58:18.559: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 20.004853223s
    Jan 30 12:58:20.559: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 22.004773176s
    Jan 30 12:58:22.559: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 24.004609219s
    Jan 30 12:58:24.559: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 26.004557113s
    Jan 30 12:58:26.560: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 28.005739971s
    Jan 30 12:58:28.558: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 30.004459484s
    Jan 30 12:58:30.559: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 32.005487723s
    Jan 30 12:58:32.558: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 34.003615123s
    Jan 30 12:58:32.558: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 01/30/23 12:58:32.559
    STEP: Trying to apply a random label on the found node. 01/30/23 12:58:32.564
    STEP: verifying the node has the label kubernetes.io/e2e-6b0cc27a-825b-48d2-a930-0c115592b31f 42 01/30/23 12:58:32.573
    STEP: Trying to relaunch the pod, now with labels. 01/30/23 12:58:32.575
    Jan 30 12:58:32.581: INFO: Waiting up to 5m0s for pod "with-labels" in namespace "sched-pred-3241" to be "not pending"
    Jan 30 12:58:32.583: INFO: Pod "with-labels": Phase="Pending", Reason="", readiness=false. Elapsed: 1.755708ms
    Jan 30 12:58:34.586: INFO: Pod "with-labels": Phase="Running", Reason="", readiness=true. Elapsed: 2.004394648s
    Jan 30 12:58:34.586: INFO: Pod "with-labels" satisfied condition "not pending"
    STEP: removing the label kubernetes.io/e2e-6b0cc27a-825b-48d2-a930-0c115592b31f off the node pubt2-nks-for-dev1.dg.163.org 01/30/23 12:58:34.587
    STEP: verifying the node doesn't have the label kubernetes.io/e2e-6b0cc27a-825b-48d2-a930-0c115592b31f 01/30/23 12:58:34.597
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan 30 12:58:34.599: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:88
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-pred-3241" for this suite. 01/30/23 12:58:34.602
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:187
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 12:58:34.604
Jan 30 12:58:34.604: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename emptydir 01/30/23 12:58:34.605
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:58:34.611
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:58:34.613
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:187
STEP: Creating a pod to test emptydir 0777 on node default medium 01/30/23 12:58:34.615
Jan 30 12:58:34.619: INFO: Waiting up to 5m0s for pod "pod-09bd3e1c-740d-4523-867a-4dc018aaca21" in namespace "emptydir-6138" to be "Succeeded or Failed"
Jan 30 12:58:34.620: INFO: Pod "pod-09bd3e1c-740d-4523-867a-4dc018aaca21": Phase="Pending", Reason="", readiness=false. Elapsed: 1.489392ms
Jan 30 12:58:36.623: INFO: Pod "pod-09bd3e1c-740d-4523-867a-4dc018aaca21": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004223374s
Jan 30 12:58:38.623: INFO: Pod "pod-09bd3e1c-740d-4523-867a-4dc018aaca21": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004100564s
STEP: Saw pod success 01/30/23 12:58:38.623
Jan 30 12:58:38.623: INFO: Pod "pod-09bd3e1c-740d-4523-867a-4dc018aaca21" satisfied condition "Succeeded or Failed"
Jan 30 12:58:38.625: INFO: Trying to get logs from node pubt2-nks-for-dev1.dg.163.org pod pod-09bd3e1c-740d-4523-867a-4dc018aaca21 container test-container: <nil>
STEP: delete the pod 01/30/23 12:58:38.63
Jan 30 12:58:38.637: INFO: Waiting for pod pod-09bd3e1c-740d-4523-867a-4dc018aaca21 to disappear
Jan 30 12:58:38.639: INFO: Pod pod-09bd3e1c-740d-4523-867a-4dc018aaca21 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jan 30 12:58:38.639: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-6138" for this suite. 01/30/23 12:58:38.642
------------------------------
• [4.040 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:187

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 12:58:34.604
    Jan 30 12:58:34.604: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename emptydir 01/30/23 12:58:34.605
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:58:34.611
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:58:34.613
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:187
    STEP: Creating a pod to test emptydir 0777 on node default medium 01/30/23 12:58:34.615
    Jan 30 12:58:34.619: INFO: Waiting up to 5m0s for pod "pod-09bd3e1c-740d-4523-867a-4dc018aaca21" in namespace "emptydir-6138" to be "Succeeded or Failed"
    Jan 30 12:58:34.620: INFO: Pod "pod-09bd3e1c-740d-4523-867a-4dc018aaca21": Phase="Pending", Reason="", readiness=false. Elapsed: 1.489392ms
    Jan 30 12:58:36.623: INFO: Pod "pod-09bd3e1c-740d-4523-867a-4dc018aaca21": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004223374s
    Jan 30 12:58:38.623: INFO: Pod "pod-09bd3e1c-740d-4523-867a-4dc018aaca21": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004100564s
    STEP: Saw pod success 01/30/23 12:58:38.623
    Jan 30 12:58:38.623: INFO: Pod "pod-09bd3e1c-740d-4523-867a-4dc018aaca21" satisfied condition "Succeeded or Failed"
    Jan 30 12:58:38.625: INFO: Trying to get logs from node pubt2-nks-for-dev1.dg.163.org pod pod-09bd3e1c-740d-4523-867a-4dc018aaca21 container test-container: <nil>
    STEP: delete the pod 01/30/23 12:58:38.63
    Jan 30 12:58:38.637: INFO: Waiting for pod pod-09bd3e1c-740d-4523-867a-4dc018aaca21 to disappear
    Jan 30 12:58:38.639: INFO: Pod pod-09bd3e1c-740d-4523-867a-4dc018aaca21 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jan 30 12:58:38.639: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-6138" for this suite. 01/30/23 12:58:38.642
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:943
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 12:58:38.645
Jan 30 12:58:38.645: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename resourcequota 01/30/23 12:58:38.646
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:58:38.651
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:58:38.653
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:943
STEP: Creating a ResourceQuota 01/30/23 12:58:38.655
STEP: Getting a ResourceQuota 01/30/23 12:58:38.657
STEP: Listing all ResourceQuotas with LabelSelector 01/30/23 12:58:38.659
STEP: Patching the ResourceQuota 01/30/23 12:58:38.66
STEP: Deleting a Collection of ResourceQuotas 01/30/23 12:58:38.664
STEP: Verifying the deleted ResourceQuota 01/30/23 12:58:38.668
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Jan 30 12:58:38.669: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-8781" for this suite. 01/30/23 12:58:38.671
------------------------------
• [0.028 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:943

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 12:58:38.645
    Jan 30 12:58:38.645: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename resourcequota 01/30/23 12:58:38.646
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:58:38.651
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:58:38.653
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should manage the lifecycle of a ResourceQuota [Conformance]
      test/e2e/apimachinery/resource_quota.go:943
    STEP: Creating a ResourceQuota 01/30/23 12:58:38.655
    STEP: Getting a ResourceQuota 01/30/23 12:58:38.657
    STEP: Listing all ResourceQuotas with LabelSelector 01/30/23 12:58:38.659
    STEP: Patching the ResourceQuota 01/30/23 12:58:38.66
    STEP: Deleting a Collection of ResourceQuotas 01/30/23 12:58:38.664
    STEP: Verifying the deleted ResourceQuota 01/30/23 12:58:38.668
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Jan 30 12:58:38.669: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-8781" for this suite. 01/30/23 12:58:38.671
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:56
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 12:58:38.674
Jan 30 12:58:38.674: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename projected 01/30/23 12:58:38.675
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:58:38.68
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:58:38.682
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:56
STEP: Creating projection with secret that has name projected-secret-test-22e42b92-744a-4ee5-ac5f-11f23dff43ee 01/30/23 12:58:38.684
STEP: Creating a pod to test consume secrets 01/30/23 12:58:38.686
Jan 30 12:58:38.690: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-e6a1a159-6370-4719-b489-42eec7a90a75" in namespace "projected-5877" to be "Succeeded or Failed"
Jan 30 12:58:38.691: INFO: Pod "pod-projected-secrets-e6a1a159-6370-4719-b489-42eec7a90a75": Phase="Pending", Reason="", readiness=false. Elapsed: 1.597471ms
Jan 30 12:58:40.693: INFO: Pod "pod-projected-secrets-e6a1a159-6370-4719-b489-42eec7a90a75": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003663732s
Jan 30 12:58:42.695: INFO: Pod "pod-projected-secrets-e6a1a159-6370-4719-b489-42eec7a90a75": Phase="Pending", Reason="", readiness=false. Elapsed: 4.004970739s
Jan 30 12:58:44.695: INFO: Pod "pod-projected-secrets-e6a1a159-6370-4719-b489-42eec7a90a75": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.005117508s
STEP: Saw pod success 01/30/23 12:58:44.695
Jan 30 12:58:44.695: INFO: Pod "pod-projected-secrets-e6a1a159-6370-4719-b489-42eec7a90a75" satisfied condition "Succeeded or Failed"
Jan 30 12:58:44.697: INFO: Trying to get logs from node pubt2-nks-for-dev1.dg.163.org pod pod-projected-secrets-e6a1a159-6370-4719-b489-42eec7a90a75 container projected-secret-volume-test: <nil>
STEP: delete the pod 01/30/23 12:58:44.702
Jan 30 12:58:44.706: INFO: Waiting for pod pod-projected-secrets-e6a1a159-6370-4719-b489-42eec7a90a75 to disappear
Jan 30 12:58:44.708: INFO: Pod pod-projected-secrets-e6a1a159-6370-4719-b489-42eec7a90a75 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Jan 30 12:58:44.708: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-5877" for this suite. 01/30/23 12:58:44.71
------------------------------
• [SLOW TEST] [6.039 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:56

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 12:58:38.674
    Jan 30 12:58:38.674: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename projected 01/30/23 12:58:38.675
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:58:38.68
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:58:38.682
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:56
    STEP: Creating projection with secret that has name projected-secret-test-22e42b92-744a-4ee5-ac5f-11f23dff43ee 01/30/23 12:58:38.684
    STEP: Creating a pod to test consume secrets 01/30/23 12:58:38.686
    Jan 30 12:58:38.690: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-e6a1a159-6370-4719-b489-42eec7a90a75" in namespace "projected-5877" to be "Succeeded or Failed"
    Jan 30 12:58:38.691: INFO: Pod "pod-projected-secrets-e6a1a159-6370-4719-b489-42eec7a90a75": Phase="Pending", Reason="", readiness=false. Elapsed: 1.597471ms
    Jan 30 12:58:40.693: INFO: Pod "pod-projected-secrets-e6a1a159-6370-4719-b489-42eec7a90a75": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003663732s
    Jan 30 12:58:42.695: INFO: Pod "pod-projected-secrets-e6a1a159-6370-4719-b489-42eec7a90a75": Phase="Pending", Reason="", readiness=false. Elapsed: 4.004970739s
    Jan 30 12:58:44.695: INFO: Pod "pod-projected-secrets-e6a1a159-6370-4719-b489-42eec7a90a75": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.005117508s
    STEP: Saw pod success 01/30/23 12:58:44.695
    Jan 30 12:58:44.695: INFO: Pod "pod-projected-secrets-e6a1a159-6370-4719-b489-42eec7a90a75" satisfied condition "Succeeded or Failed"
    Jan 30 12:58:44.697: INFO: Trying to get logs from node pubt2-nks-for-dev1.dg.163.org pod pod-projected-secrets-e6a1a159-6370-4719-b489-42eec7a90a75 container projected-secret-volume-test: <nil>
    STEP: delete the pod 01/30/23 12:58:44.702
    Jan 30 12:58:44.706: INFO: Waiting for pod pod-projected-secrets-e6a1a159-6370-4719-b489-42eec7a90a75 to disappear
    Jan 30 12:58:44.708: INFO: Pod pod-projected-secrets-e6a1a159-6370-4719-b489-42eec7a90a75 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Jan 30 12:58:44.708: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-5877" for this suite. 01/30/23 12:58:44.71
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:112
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 12:58:44.713
Jan 30 12:58:44.713: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename var-expansion 01/30/23 12:58:44.714
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:58:44.72
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:58:44.722
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:112
STEP: Creating a pod to test substitution in volume subpath 01/30/23 12:58:44.724
Jan 30 12:58:44.728: INFO: Waiting up to 5m0s for pod "var-expansion-805fb954-09b2-4716-a347-37a497946768" in namespace "var-expansion-9167" to be "Succeeded or Failed"
Jan 30 12:58:44.730: INFO: Pod "var-expansion-805fb954-09b2-4716-a347-37a497946768": Phase="Pending", Reason="", readiness=false. Elapsed: 1.536873ms
Jan 30 12:58:46.733: INFO: Pod "var-expansion-805fb954-09b2-4716-a347-37a497946768": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004527529s
Jan 30 12:58:48.732: INFO: Pod "var-expansion-805fb954-09b2-4716-a347-37a497946768": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003650254s
STEP: Saw pod success 01/30/23 12:58:48.732
Jan 30 12:58:48.732: INFO: Pod "var-expansion-805fb954-09b2-4716-a347-37a497946768" satisfied condition "Succeeded or Failed"
Jan 30 12:58:48.734: INFO: Trying to get logs from node pubt2-nks-for-dev1.dg.163.org pod var-expansion-805fb954-09b2-4716-a347-37a497946768 container dapi-container: <nil>
STEP: delete the pod 01/30/23 12:58:48.738
Jan 30 12:58:48.742: INFO: Waiting for pod var-expansion-805fb954-09b2-4716-a347-37a497946768 to disappear
Jan 30 12:58:48.744: INFO: Pod var-expansion-805fb954-09b2-4716-a347-37a497946768 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Jan 30 12:58:48.744: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-9167" for this suite. 01/30/23 12:58:48.746
------------------------------
• [4.035 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:112

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 12:58:44.713
    Jan 30 12:58:44.713: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename var-expansion 01/30/23 12:58:44.714
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:58:44.72
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:58:44.722
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow substituting values in a volume subpath [Conformance]
      test/e2e/common/node/expansion.go:112
    STEP: Creating a pod to test substitution in volume subpath 01/30/23 12:58:44.724
    Jan 30 12:58:44.728: INFO: Waiting up to 5m0s for pod "var-expansion-805fb954-09b2-4716-a347-37a497946768" in namespace "var-expansion-9167" to be "Succeeded or Failed"
    Jan 30 12:58:44.730: INFO: Pod "var-expansion-805fb954-09b2-4716-a347-37a497946768": Phase="Pending", Reason="", readiness=false. Elapsed: 1.536873ms
    Jan 30 12:58:46.733: INFO: Pod "var-expansion-805fb954-09b2-4716-a347-37a497946768": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004527529s
    Jan 30 12:58:48.732: INFO: Pod "var-expansion-805fb954-09b2-4716-a347-37a497946768": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003650254s
    STEP: Saw pod success 01/30/23 12:58:48.732
    Jan 30 12:58:48.732: INFO: Pod "var-expansion-805fb954-09b2-4716-a347-37a497946768" satisfied condition "Succeeded or Failed"
    Jan 30 12:58:48.734: INFO: Trying to get logs from node pubt2-nks-for-dev1.dg.163.org pod var-expansion-805fb954-09b2-4716-a347-37a497946768 container dapi-container: <nil>
    STEP: delete the pod 01/30/23 12:58:48.738
    Jan 30 12:58:48.742: INFO: Waiting for pod var-expansion-805fb954-09b2-4716-a347-37a497946768 to disappear
    Jan 30 12:58:48.744: INFO: Pod var-expansion-805fb954-09b2-4716-a347-37a497946768 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Jan 30 12:58:48.744: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-9167" for this suite. 01/30/23 12:58:48.746
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:47
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 12:58:48.75
Jan 30 12:58:48.750: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename projected 01/30/23 12:58:48.75
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:58:48.756
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:58:48.758
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:47
STEP: Creating configMap with name projected-configmap-test-volume-5fc24eab-f239-4fa0-bd6f-8849a5bdf6d8 01/30/23 12:58:48.76
STEP: Creating a pod to test consume configMaps 01/30/23 12:58:48.762
Jan 30 12:58:48.766: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-86cbadce-f076-40da-93f2-9b06293b00a3" in namespace "projected-1602" to be "Succeeded or Failed"
Jan 30 12:58:48.767: INFO: Pod "pod-projected-configmaps-86cbadce-f076-40da-93f2-9b06293b00a3": Phase="Pending", Reason="", readiness=false. Elapsed: 1.529836ms
Jan 30 12:58:50.772: INFO: Pod "pod-projected-configmaps-86cbadce-f076-40da-93f2-9b06293b00a3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005928419s
Jan 30 12:58:52.770: INFO: Pod "pod-projected-configmaps-86cbadce-f076-40da-93f2-9b06293b00a3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003892867s
STEP: Saw pod success 01/30/23 12:58:52.77
Jan 30 12:58:52.770: INFO: Pod "pod-projected-configmaps-86cbadce-f076-40da-93f2-9b06293b00a3" satisfied condition "Succeeded or Failed"
Jan 30 12:58:52.772: INFO: Trying to get logs from node pubt2-nks-for-dev1.dg.163.org pod pod-projected-configmaps-86cbadce-f076-40da-93f2-9b06293b00a3 container agnhost-container: <nil>
STEP: delete the pod 01/30/23 12:58:52.776
Jan 30 12:58:52.781: INFO: Waiting for pod pod-projected-configmaps-86cbadce-f076-40da-93f2-9b06293b00a3 to disappear
Jan 30 12:58:52.782: INFO: Pod pod-projected-configmaps-86cbadce-f076-40da-93f2-9b06293b00a3 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Jan 30 12:58:52.782: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-1602" for this suite. 01/30/23 12:58:52.784
------------------------------
• [4.037 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:47

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 12:58:48.75
    Jan 30 12:58:48.750: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename projected 01/30/23 12:58:48.75
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:58:48.756
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:58:48.758
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:47
    STEP: Creating configMap with name projected-configmap-test-volume-5fc24eab-f239-4fa0-bd6f-8849a5bdf6d8 01/30/23 12:58:48.76
    STEP: Creating a pod to test consume configMaps 01/30/23 12:58:48.762
    Jan 30 12:58:48.766: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-86cbadce-f076-40da-93f2-9b06293b00a3" in namespace "projected-1602" to be "Succeeded or Failed"
    Jan 30 12:58:48.767: INFO: Pod "pod-projected-configmaps-86cbadce-f076-40da-93f2-9b06293b00a3": Phase="Pending", Reason="", readiness=false. Elapsed: 1.529836ms
    Jan 30 12:58:50.772: INFO: Pod "pod-projected-configmaps-86cbadce-f076-40da-93f2-9b06293b00a3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005928419s
    Jan 30 12:58:52.770: INFO: Pod "pod-projected-configmaps-86cbadce-f076-40da-93f2-9b06293b00a3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003892867s
    STEP: Saw pod success 01/30/23 12:58:52.77
    Jan 30 12:58:52.770: INFO: Pod "pod-projected-configmaps-86cbadce-f076-40da-93f2-9b06293b00a3" satisfied condition "Succeeded or Failed"
    Jan 30 12:58:52.772: INFO: Trying to get logs from node pubt2-nks-for-dev1.dg.163.org pod pod-projected-configmaps-86cbadce-f076-40da-93f2-9b06293b00a3 container agnhost-container: <nil>
    STEP: delete the pod 01/30/23 12:58:52.776
    Jan 30 12:58:52.781: INFO: Waiting for pod pod-projected-configmaps-86cbadce-f076-40da-93f2-9b06293b00a3 to disappear
    Jan 30 12:58:52.782: INFO: Pod pod-projected-configmaps-86cbadce-f076-40da-93f2-9b06293b00a3 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Jan 30 12:58:52.782: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-1602" for this suite. 01/30/23 12:58:52.784
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:261
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 12:58:52.788
Jan 30 12:58:52.788: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename projected 01/30/23 12:58:52.788
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:58:52.794
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:58:52.796
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:261
STEP: Creating a pod to test downward API volume plugin 01/30/23 12:58:52.798
Jan 30 12:58:52.803: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c12f4baf-7148-44ea-be6f-67eefece6ebf" in namespace "projected-7607" to be "Succeeded or Failed"
Jan 30 12:58:52.804: INFO: Pod "downwardapi-volume-c12f4baf-7148-44ea-be6f-67eefece6ebf": Phase="Pending", Reason="", readiness=false. Elapsed: 1.551703ms
Jan 30 12:58:54.807: INFO: Pod "downwardapi-volume-c12f4baf-7148-44ea-be6f-67eefece6ebf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004546852s
Jan 30 12:58:56.806: INFO: Pod "downwardapi-volume-c12f4baf-7148-44ea-be6f-67eefece6ebf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003662108s
STEP: Saw pod success 01/30/23 12:58:56.806
Jan 30 12:58:56.806: INFO: Pod "downwardapi-volume-c12f4baf-7148-44ea-be6f-67eefece6ebf" satisfied condition "Succeeded or Failed"
Jan 30 12:58:56.808: INFO: Trying to get logs from node pubt2-nks-for-dev1.dg.163.org pod downwardapi-volume-c12f4baf-7148-44ea-be6f-67eefece6ebf container client-container: <nil>
STEP: delete the pod 01/30/23 12:58:56.812
Jan 30 12:58:56.816: INFO: Waiting for pod downwardapi-volume-c12f4baf-7148-44ea-be6f-67eefece6ebf to disappear
Jan 30 12:58:56.818: INFO: Pod downwardapi-volume-c12f4baf-7148-44ea-be6f-67eefece6ebf no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Jan 30 12:58:56.818: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-7607" for this suite. 01/30/23 12:58:56.82
------------------------------
• [4.035 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:261

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 12:58:52.788
    Jan 30 12:58:52.788: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename projected 01/30/23 12:58:52.788
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:58:52.794
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:58:52.796
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:261
    STEP: Creating a pod to test downward API volume plugin 01/30/23 12:58:52.798
    Jan 30 12:58:52.803: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c12f4baf-7148-44ea-be6f-67eefece6ebf" in namespace "projected-7607" to be "Succeeded or Failed"
    Jan 30 12:58:52.804: INFO: Pod "downwardapi-volume-c12f4baf-7148-44ea-be6f-67eefece6ebf": Phase="Pending", Reason="", readiness=false. Elapsed: 1.551703ms
    Jan 30 12:58:54.807: INFO: Pod "downwardapi-volume-c12f4baf-7148-44ea-be6f-67eefece6ebf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004546852s
    Jan 30 12:58:56.806: INFO: Pod "downwardapi-volume-c12f4baf-7148-44ea-be6f-67eefece6ebf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003662108s
    STEP: Saw pod success 01/30/23 12:58:56.806
    Jan 30 12:58:56.806: INFO: Pod "downwardapi-volume-c12f4baf-7148-44ea-be6f-67eefece6ebf" satisfied condition "Succeeded or Failed"
    Jan 30 12:58:56.808: INFO: Trying to get logs from node pubt2-nks-for-dev1.dg.163.org pod downwardapi-volume-c12f4baf-7148-44ea-be6f-67eefece6ebf container client-container: <nil>
    STEP: delete the pod 01/30/23 12:58:56.812
    Jan 30 12:58:56.816: INFO: Waiting for pod downwardapi-volume-c12f4baf-7148-44ea-be6f-67eefece6ebf to disappear
    Jan 30 12:58:56.818: INFO: Pod downwardapi-volume-c12f4baf-7148-44ea-be6f-67eefece6ebf no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Jan 30 12:58:56.818: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-7607" for this suite. 01/30/23 12:58:56.82
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 12:58:56.824
Jan 30 12:58:56.824: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename cronjob 01/30/23 12:58:56.824
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:58:56.83
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:58:56.832
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69
STEP: Creating a cronjob 01/30/23 12:58:56.834
STEP: Ensuring more than one job is running at a time 01/30/23 12:58:56.837
STEP: Ensuring at least two running jobs exists by listing jobs explicitly 01/30/23 13:00:00.843
STEP: Removing cronjob 01/30/23 13:00:00.845
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Jan 30 13:00:00.847: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-4057" for this suite. 01/30/23 13:00:00.849
------------------------------
• [SLOW TEST] [64.029 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 12:58:56.824
    Jan 30 12:58:56.824: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename cronjob 01/30/23 12:58:56.824
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 12:58:56.83
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 12:58:56.832
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should schedule multiple jobs concurrently [Conformance]
      test/e2e/apps/cronjob.go:69
    STEP: Creating a cronjob 01/30/23 12:58:56.834
    STEP: Ensuring more than one job is running at a time 01/30/23 12:58:56.837
    STEP: Ensuring at least two running jobs exists by listing jobs explicitly 01/30/23 13:00:00.843
    STEP: Removing cronjob 01/30/23 13:00:00.845
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Jan 30 13:00:00.847: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-4057" for this suite. 01/30/23 13:00:00.849
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 13:00:00.853
Jan 30 13:00:00.853: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename dns 01/30/23 13:00:00.854
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 13:00:00.86
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 13:00:00.862
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig... 01/30/23 13:00:00.864
Jan 30 13:00:00.868: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-4909  4f968516-c749-40df-821c-1ed15cae16f5 45599 0 2023-01-30 13:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] [{e2e.test Update v1 2023-01-30 13:00:00 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-v5544,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:harbor.cloud.netease.com/qzprod-k8s/e2e/agnhost:2.43,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-v5544,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 30 13:00:00.869: INFO: Waiting up to 5m0s for pod "test-dns-nameservers" in namespace "dns-4909" to be "running and ready"
Jan 30 13:00:00.870: INFO: Pod "test-dns-nameservers": Phase="Pending", Reason="", readiness=false. Elapsed: 1.600814ms
Jan 30 13:00:00.870: INFO: The phase of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
Jan 30 13:00:02.873: INFO: Pod "test-dns-nameservers": Phase="Running", Reason="", readiness=true. Elapsed: 2.004593256s
Jan 30 13:00:02.873: INFO: The phase of Pod test-dns-nameservers is Running (Ready = true)
Jan 30 13:00:02.873: INFO: Pod "test-dns-nameservers" satisfied condition "running and ready"
STEP: Verifying customized DNS suffix list is configured on pod... 01/30/23 13:00:02.873
Jan 30 13:00:02.873: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-4909 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 30 13:00:02.873: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
Jan 30 13:00:02.874: INFO: ExecWithOptions: Clientset creation
Jan 30 13:00:02.874: INFO: ExecWithOptions: execute(POST https://10.178.64.1:443/api/v1/namespaces/dns-4909/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
STEP: Verifying customized DNS server is configured on pod... 01/30/23 13:00:02.959
Jan 30 13:00:02.960: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-4909 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 30 13:00:02.960: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
Jan 30 13:00:02.960: INFO: ExecWithOptions: Clientset creation
Jan 30 13:00:02.960: INFO: ExecWithOptions: execute(POST https://10.178.64.1:443/api/v1/namespaces/dns-4909/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jan 30 13:00:03.052: INFO: Deleting pod test-dns-nameservers...
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Jan 30 13:00:03.056: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-4909" for this suite. 01/30/23 13:00:03.059
------------------------------
• [2.209 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 13:00:00.853
    Jan 30 13:00:00.853: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename dns 01/30/23 13:00:00.854
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 13:00:00.86
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 13:00:00.862
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should support configurable pod DNS nameservers [Conformance]
      test/e2e/network/dns.go:411
    STEP: Creating a pod with dnsPolicy=None and customized dnsConfig... 01/30/23 13:00:00.864
    Jan 30 13:00:00.868: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-4909  4f968516-c749-40df-821c-1ed15cae16f5 45599 0 2023-01-30 13:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] [{e2e.test Update v1 2023-01-30 13:00:00 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-v5544,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:harbor.cloud.netease.com/qzprod-k8s/e2e/agnhost:2.43,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-v5544,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 30 13:00:00.869: INFO: Waiting up to 5m0s for pod "test-dns-nameservers" in namespace "dns-4909" to be "running and ready"
    Jan 30 13:00:00.870: INFO: Pod "test-dns-nameservers": Phase="Pending", Reason="", readiness=false. Elapsed: 1.600814ms
    Jan 30 13:00:00.870: INFO: The phase of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
    Jan 30 13:00:02.873: INFO: Pod "test-dns-nameservers": Phase="Running", Reason="", readiness=true. Elapsed: 2.004593256s
    Jan 30 13:00:02.873: INFO: The phase of Pod test-dns-nameservers is Running (Ready = true)
    Jan 30 13:00:02.873: INFO: Pod "test-dns-nameservers" satisfied condition "running and ready"
    STEP: Verifying customized DNS suffix list is configured on pod... 01/30/23 13:00:02.873
    Jan 30 13:00:02.873: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-4909 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 30 13:00:02.873: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    Jan 30 13:00:02.874: INFO: ExecWithOptions: Clientset creation
    Jan 30 13:00:02.874: INFO: ExecWithOptions: execute(POST https://10.178.64.1:443/api/v1/namespaces/dns-4909/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    STEP: Verifying customized DNS server is configured on pod... 01/30/23 13:00:02.959
    Jan 30 13:00:02.960: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-4909 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 30 13:00:02.960: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    Jan 30 13:00:02.960: INFO: ExecWithOptions: Clientset creation
    Jan 30 13:00:02.960: INFO: ExecWithOptions: execute(POST https://10.178.64.1:443/api/v1/namespaces/dns-4909/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Jan 30 13:00:03.052: INFO: Deleting pod test-dns-nameservers...
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Jan 30 13:00:03.056: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-4909" for this suite. 01/30/23 13:00:03.059
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  creating/deleting custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:58
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 13:00:03.062
Jan 30 13:00:03.062: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename custom-resource-definition 01/30/23 13:00:03.063
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 13:00:03.069
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 13:00:03.071
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] creating/deleting custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:58
Jan 30 13:00:03.073: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 30 13:00:04.084: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-287" for this suite. 01/30/23 13:00:04.086
------------------------------
• [1.027 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    creating/deleting custom resource definition objects works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:58

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 13:00:03.062
    Jan 30 13:00:03.062: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename custom-resource-definition 01/30/23 13:00:03.063
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 13:00:03.069
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 13:00:03.071
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] creating/deleting custom resource definition objects works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:58
    Jan 30 13:00:03.073: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 30 13:00:04.084: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-287" for this suite. 01/30/23 13:00:04.086
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1083
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 13:00:04.09
Jan 30 13:00:04.090: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename pods 01/30/23 13:00:04.09
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 13:00:04.097
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 13:00:04.098
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1083
STEP: Create a pod 01/30/23 13:00:04.1
Jan 30 13:00:04.105: INFO: Waiting up to 5m0s for pod "pod-6fbmw" in namespace "pods-4395" to be "running"
Jan 30 13:00:04.107: INFO: Pod "pod-6fbmw": Phase="Pending", Reason="", readiness=false. Elapsed: 1.519697ms
Jan 30 13:00:06.109: INFO: Pod "pod-6fbmw": Phase="Running", Reason="", readiness=true. Elapsed: 2.003853465s
Jan 30 13:00:06.109: INFO: Pod "pod-6fbmw" satisfied condition "running"
STEP: patching /status 01/30/23 13:00:06.109
Jan 30 13:00:06.115: INFO: Status Message: "Patched by e2e test" and Reason: "E2E"
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Jan 30 13:00:06.115: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-4395" for this suite. 01/30/23 13:00:06.118
------------------------------
• [2.030 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1083

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 13:00:04.09
    Jan 30 13:00:04.090: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename pods 01/30/23 13:00:04.09
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 13:00:04.097
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 13:00:04.098
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should patch a pod status [Conformance]
      test/e2e/common/node/pods.go:1083
    STEP: Create a pod 01/30/23 13:00:04.1
    Jan 30 13:00:04.105: INFO: Waiting up to 5m0s for pod "pod-6fbmw" in namespace "pods-4395" to be "running"
    Jan 30 13:00:04.107: INFO: Pod "pod-6fbmw": Phase="Pending", Reason="", readiness=false. Elapsed: 1.519697ms
    Jan 30 13:00:06.109: INFO: Pod "pod-6fbmw": Phase="Running", Reason="", readiness=true. Elapsed: 2.003853465s
    Jan 30 13:00:06.109: INFO: Pod "pod-6fbmw" satisfied condition "running"
    STEP: patching /status 01/30/23 13:00:06.109
    Jan 30 13:00:06.115: INFO: Status Message: "Patched by e2e test" and Reason: "E2E"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Jan 30 13:00:06.115: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-4395" for this suite. 01/30/23 13:00:06.118
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:108
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 13:00:06.121
Jan 30 13:00:06.121: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename container-probe 01/30/23 13:00:06.121
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 13:00:06.127
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 13:00:06.129
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:108
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Jan 30 13:01:06.137: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-1557" for this suite. 01/30/23 13:01:06.139
------------------------------
• [SLOW TEST] [60.021 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:108

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 13:00:06.121
    Jan 30 13:00:06.121: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename container-probe 01/30/23 13:00:06.121
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 13:00:06.127
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 13:00:06.129
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:108
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Jan 30 13:01:06.137: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-1557" for this suite. 01/30/23 13:01:06.139
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:357
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 13:01:06.142
Jan 30 13:01:06.142: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename crd-publish-openapi 01/30/23 13:01:06.142
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 13:01:06.149
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 13:01:06.151
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:357
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation 01/30/23 13:01:06.153
Jan 30 13:01:06.153: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
Jan 30 13:01:07.989: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 30 13:01:14.947: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-26" for this suite. 01/30/23 13:01:14.953
------------------------------
• [SLOW TEST] [8.814 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:357

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 13:01:06.142
    Jan 30 13:01:06.142: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename crd-publish-openapi 01/30/23 13:01:06.142
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 13:01:06.149
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 13:01:06.151
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for multiple CRDs of same group and version but different kinds [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:357
    STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation 01/30/23 13:01:06.153
    Jan 30 13:01:06.153: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    Jan 30 13:01:07.989: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 30 13:01:14.947: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-26" for this suite. 01/30/23 13:01:14.953
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:147
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 13:01:14.957
Jan 30 13:01:14.957: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename emptydir 01/30/23 13:01:14.958
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 13:01:14.964
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 13:01:14.966
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:147
STEP: Creating a pod to test emptydir 0777 on tmpfs 01/30/23 13:01:14.968
Jan 30 13:01:14.972: INFO: Waiting up to 5m0s for pod "pod-783ebbcd-5315-472b-933c-64d6cd2d1713" in namespace "emptydir-739" to be "Succeeded or Failed"
Jan 30 13:01:14.977: INFO: Pod "pod-783ebbcd-5315-472b-933c-64d6cd2d1713": Phase="Pending", Reason="", readiness=false. Elapsed: 4.701024ms
Jan 30 13:01:16.979: INFO: Pod "pod-783ebbcd-5315-472b-933c-64d6cd2d1713": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007050208s
Jan 30 13:01:18.980: INFO: Pod "pod-783ebbcd-5315-472b-933c-64d6cd2d1713": Phase="Pending", Reason="", readiness=false. Elapsed: 4.00741322s
Jan 30 13:01:20.979: INFO: Pod "pod-783ebbcd-5315-472b-933c-64d6cd2d1713": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.007239826s
STEP: Saw pod success 01/30/23 13:01:20.979
Jan 30 13:01:20.979: INFO: Pod "pod-783ebbcd-5315-472b-933c-64d6cd2d1713" satisfied condition "Succeeded or Failed"
Jan 30 13:01:20.981: INFO: Trying to get logs from node pubt2-nks-for-dev1.dg.163.org pod pod-783ebbcd-5315-472b-933c-64d6cd2d1713 container test-container: <nil>
STEP: delete the pod 01/30/23 13:01:20.993
Jan 30 13:01:20.997: INFO: Waiting for pod pod-783ebbcd-5315-472b-933c-64d6cd2d1713 to disappear
Jan 30 13:01:20.999: INFO: Pod pod-783ebbcd-5315-472b-933c-64d6cd2d1713 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jan 30 13:01:20.999: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-739" for this suite. 01/30/23 13:01:21.001
------------------------------
• [SLOW TEST] [6.046 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:147

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 13:01:14.957
    Jan 30 13:01:14.957: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename emptydir 01/30/23 13:01:14.958
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 13:01:14.964
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 13:01:14.966
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:147
    STEP: Creating a pod to test emptydir 0777 on tmpfs 01/30/23 13:01:14.968
    Jan 30 13:01:14.972: INFO: Waiting up to 5m0s for pod "pod-783ebbcd-5315-472b-933c-64d6cd2d1713" in namespace "emptydir-739" to be "Succeeded or Failed"
    Jan 30 13:01:14.977: INFO: Pod "pod-783ebbcd-5315-472b-933c-64d6cd2d1713": Phase="Pending", Reason="", readiness=false. Elapsed: 4.701024ms
    Jan 30 13:01:16.979: INFO: Pod "pod-783ebbcd-5315-472b-933c-64d6cd2d1713": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007050208s
    Jan 30 13:01:18.980: INFO: Pod "pod-783ebbcd-5315-472b-933c-64d6cd2d1713": Phase="Pending", Reason="", readiness=false. Elapsed: 4.00741322s
    Jan 30 13:01:20.979: INFO: Pod "pod-783ebbcd-5315-472b-933c-64d6cd2d1713": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.007239826s
    STEP: Saw pod success 01/30/23 13:01:20.979
    Jan 30 13:01:20.979: INFO: Pod "pod-783ebbcd-5315-472b-933c-64d6cd2d1713" satisfied condition "Succeeded or Failed"
    Jan 30 13:01:20.981: INFO: Trying to get logs from node pubt2-nks-for-dev1.dg.163.org pod pod-783ebbcd-5315-472b-933c-64d6cd2d1713 container test-container: <nil>
    STEP: delete the pod 01/30/23 13:01:20.993
    Jan 30 13:01:20.997: INFO: Waiting for pod pod-783ebbcd-5315-472b-933c-64d6cd2d1713 to disappear
    Jan 30 13:01:20.999: INFO: Pod pod-783ebbcd-5315-472b-933c-64d6cd2d1713 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jan 30 13:01:20.999: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-739" for this suite. 01/30/23 13:01:21.001
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with configmap pod with mountPath of existing file [Conformance]
  test/e2e/storage/subpath.go:80
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 13:01:21.004
Jan 30 13:01:21.004: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename subpath 01/30/23 13:01:21.005
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 13:01:21.011
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 13:01:21.013
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 01/30/23 13:01:21.014
[It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
  test/e2e/storage/subpath.go:80
STEP: Creating pod pod-subpath-test-configmap-zzs5 01/30/23 13:01:21.018
STEP: Creating a pod to test atomic-volume-subpath 01/30/23 13:01:21.018
Jan 30 13:01:21.022: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-zzs5" in namespace "subpath-7778" to be "Succeeded or Failed"
Jan 30 13:01:21.024: INFO: Pod "pod-subpath-test-configmap-zzs5": Phase="Pending", Reason="", readiness=false. Elapsed: 1.733227ms
Jan 30 13:01:23.027: INFO: Pod "pod-subpath-test-configmap-zzs5": Phase="Running", Reason="", readiness=true. Elapsed: 2.004270232s
Jan 30 13:01:25.027: INFO: Pod "pod-subpath-test-configmap-zzs5": Phase="Running", Reason="", readiness=true. Elapsed: 4.004383816s
Jan 30 13:01:27.028: INFO: Pod "pod-subpath-test-configmap-zzs5": Phase="Running", Reason="", readiness=true. Elapsed: 6.00540573s
Jan 30 13:01:29.027: INFO: Pod "pod-subpath-test-configmap-zzs5": Phase="Running", Reason="", readiness=true. Elapsed: 8.004899442s
Jan 30 13:01:31.026: INFO: Pod "pod-subpath-test-configmap-zzs5": Phase="Running", Reason="", readiness=true. Elapsed: 10.004004417s
Jan 30 13:01:33.027: INFO: Pod "pod-subpath-test-configmap-zzs5": Phase="Running", Reason="", readiness=true. Elapsed: 12.00414388s
Jan 30 13:01:35.028: INFO: Pod "pod-subpath-test-configmap-zzs5": Phase="Running", Reason="", readiness=true. Elapsed: 14.005436206s
Jan 30 13:01:37.028: INFO: Pod "pod-subpath-test-configmap-zzs5": Phase="Running", Reason="", readiness=true. Elapsed: 16.005176107s
Jan 30 13:01:39.027: INFO: Pod "pod-subpath-test-configmap-zzs5": Phase="Running", Reason="", readiness=true. Elapsed: 18.004569793s
Jan 30 13:01:41.027: INFO: Pod "pod-subpath-test-configmap-zzs5": Phase="Running", Reason="", readiness=true. Elapsed: 20.00444564s
Jan 30 13:01:43.027: INFO: Pod "pod-subpath-test-configmap-zzs5": Phase="Running", Reason="", readiness=false. Elapsed: 22.004101722s
Jan 30 13:01:45.028: INFO: Pod "pod-subpath-test-configmap-zzs5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.005670199s
STEP: Saw pod success 01/30/23 13:01:45.028
Jan 30 13:01:45.028: INFO: Pod "pod-subpath-test-configmap-zzs5" satisfied condition "Succeeded or Failed"
Jan 30 13:01:45.030: INFO: Trying to get logs from node pubt2-nks-for-dev1.dg.163.org pod pod-subpath-test-configmap-zzs5 container test-container-subpath-configmap-zzs5: <nil>
STEP: delete the pod 01/30/23 13:01:45.036
Jan 30 13:01:45.040: INFO: Waiting for pod pod-subpath-test-configmap-zzs5 to disappear
Jan 30 13:01:45.041: INFO: Pod pod-subpath-test-configmap-zzs5 no longer exists
STEP: Deleting pod pod-subpath-test-configmap-zzs5 01/30/23 13:01:45.041
Jan 30 13:01:45.041: INFO: Deleting pod "pod-subpath-test-configmap-zzs5" in namespace "subpath-7778"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Jan 30 13:01:45.043: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-7778" for this suite. 01/30/23 13:01:45.045
------------------------------
• [SLOW TEST] [24.044 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with configmap pod with mountPath of existing file [Conformance]
    test/e2e/storage/subpath.go:80

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 13:01:21.004
    Jan 30 13:01:21.004: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename subpath 01/30/23 13:01:21.005
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 13:01:21.011
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 13:01:21.013
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 01/30/23 13:01:21.014
    [It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
      test/e2e/storage/subpath.go:80
    STEP: Creating pod pod-subpath-test-configmap-zzs5 01/30/23 13:01:21.018
    STEP: Creating a pod to test atomic-volume-subpath 01/30/23 13:01:21.018
    Jan 30 13:01:21.022: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-zzs5" in namespace "subpath-7778" to be "Succeeded or Failed"
    Jan 30 13:01:21.024: INFO: Pod "pod-subpath-test-configmap-zzs5": Phase="Pending", Reason="", readiness=false. Elapsed: 1.733227ms
    Jan 30 13:01:23.027: INFO: Pod "pod-subpath-test-configmap-zzs5": Phase="Running", Reason="", readiness=true. Elapsed: 2.004270232s
    Jan 30 13:01:25.027: INFO: Pod "pod-subpath-test-configmap-zzs5": Phase="Running", Reason="", readiness=true. Elapsed: 4.004383816s
    Jan 30 13:01:27.028: INFO: Pod "pod-subpath-test-configmap-zzs5": Phase="Running", Reason="", readiness=true. Elapsed: 6.00540573s
    Jan 30 13:01:29.027: INFO: Pod "pod-subpath-test-configmap-zzs5": Phase="Running", Reason="", readiness=true. Elapsed: 8.004899442s
    Jan 30 13:01:31.026: INFO: Pod "pod-subpath-test-configmap-zzs5": Phase="Running", Reason="", readiness=true. Elapsed: 10.004004417s
    Jan 30 13:01:33.027: INFO: Pod "pod-subpath-test-configmap-zzs5": Phase="Running", Reason="", readiness=true. Elapsed: 12.00414388s
    Jan 30 13:01:35.028: INFO: Pod "pod-subpath-test-configmap-zzs5": Phase="Running", Reason="", readiness=true. Elapsed: 14.005436206s
    Jan 30 13:01:37.028: INFO: Pod "pod-subpath-test-configmap-zzs5": Phase="Running", Reason="", readiness=true. Elapsed: 16.005176107s
    Jan 30 13:01:39.027: INFO: Pod "pod-subpath-test-configmap-zzs5": Phase="Running", Reason="", readiness=true. Elapsed: 18.004569793s
    Jan 30 13:01:41.027: INFO: Pod "pod-subpath-test-configmap-zzs5": Phase="Running", Reason="", readiness=true. Elapsed: 20.00444564s
    Jan 30 13:01:43.027: INFO: Pod "pod-subpath-test-configmap-zzs5": Phase="Running", Reason="", readiness=false. Elapsed: 22.004101722s
    Jan 30 13:01:45.028: INFO: Pod "pod-subpath-test-configmap-zzs5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.005670199s
    STEP: Saw pod success 01/30/23 13:01:45.028
    Jan 30 13:01:45.028: INFO: Pod "pod-subpath-test-configmap-zzs5" satisfied condition "Succeeded or Failed"
    Jan 30 13:01:45.030: INFO: Trying to get logs from node pubt2-nks-for-dev1.dg.163.org pod pod-subpath-test-configmap-zzs5 container test-container-subpath-configmap-zzs5: <nil>
    STEP: delete the pod 01/30/23 13:01:45.036
    Jan 30 13:01:45.040: INFO: Waiting for pod pod-subpath-test-configmap-zzs5 to disappear
    Jan 30 13:01:45.041: INFO: Pod pod-subpath-test-configmap-zzs5 no longer exists
    STEP: Deleting pod pod-subpath-test-configmap-zzs5 01/30/23 13:01:45.041
    Jan 30 13:01:45.041: INFO: Deleting pod "pod-subpath-test-configmap-zzs5" in namespace "subpath-7778"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Jan 30 13:01:45.043: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-7778" for this suite. 01/30/23 13:01:45.045
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] EmptyDir volumes
  pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:227
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 13:01:45.048
Jan 30 13:01:45.048: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename emptydir 01/30/23 13:01:45.048
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 13:01:45.057
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 13:01:45.059
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:227
STEP: Creating Pod 01/30/23 13:01:45.061
Jan 30 13:01:45.065: INFO: Waiting up to 5m0s for pod "pod-sharedvolume-7c6d1bc6-91b9-42e9-8be2-fef253dac5ea" in namespace "emptydir-2698" to be "running"
Jan 30 13:01:45.067: INFO: Pod "pod-sharedvolume-7c6d1bc6-91b9-42e9-8be2-fef253dac5ea": Phase="Pending", Reason="", readiness=false. Elapsed: 1.515281ms
Jan 30 13:01:47.070: INFO: Pod "pod-sharedvolume-7c6d1bc6-91b9-42e9-8be2-fef253dac5ea": Phase="Running", Reason="", readiness=false. Elapsed: 2.004437456s
Jan 30 13:01:47.070: INFO: Pod "pod-sharedvolume-7c6d1bc6-91b9-42e9-8be2-fef253dac5ea" satisfied condition "running"
STEP: Reading file content from the nginx-container 01/30/23 13:01:47.07
Jan 30 13:01:47.070: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-2698 PodName:pod-sharedvolume-7c6d1bc6-91b9-42e9-8be2-fef253dac5ea ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 30 13:01:47.070: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
Jan 30 13:01:47.070: INFO: ExecWithOptions: Clientset creation
Jan 30 13:01:47.070: INFO: ExecWithOptions: execute(POST https://10.178.64.1:443/api/v1/namespaces/emptydir-2698/pods/pod-sharedvolume-7c6d1bc6-91b9-42e9-8be2-fef253dac5ea/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
Jan 30 13:01:47.144: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jan 30 13:01:47.144: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-2698" for this suite. 01/30/23 13:01:47.146
------------------------------
• [2.100 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:227

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 13:01:45.048
    Jan 30 13:01:45.048: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename emptydir 01/30/23 13:01:45.048
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 13:01:45.057
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 13:01:45.059
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] pod should support shared volumes between containers [Conformance]
      test/e2e/common/storage/empty_dir.go:227
    STEP: Creating Pod 01/30/23 13:01:45.061
    Jan 30 13:01:45.065: INFO: Waiting up to 5m0s for pod "pod-sharedvolume-7c6d1bc6-91b9-42e9-8be2-fef253dac5ea" in namespace "emptydir-2698" to be "running"
    Jan 30 13:01:45.067: INFO: Pod "pod-sharedvolume-7c6d1bc6-91b9-42e9-8be2-fef253dac5ea": Phase="Pending", Reason="", readiness=false. Elapsed: 1.515281ms
    Jan 30 13:01:47.070: INFO: Pod "pod-sharedvolume-7c6d1bc6-91b9-42e9-8be2-fef253dac5ea": Phase="Running", Reason="", readiness=false. Elapsed: 2.004437456s
    Jan 30 13:01:47.070: INFO: Pod "pod-sharedvolume-7c6d1bc6-91b9-42e9-8be2-fef253dac5ea" satisfied condition "running"
    STEP: Reading file content from the nginx-container 01/30/23 13:01:47.07
    Jan 30 13:01:47.070: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-2698 PodName:pod-sharedvolume-7c6d1bc6-91b9-42e9-8be2-fef253dac5ea ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 30 13:01:47.070: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    Jan 30 13:01:47.070: INFO: ExecWithOptions: Clientset creation
    Jan 30 13:01:47.070: INFO: ExecWithOptions: execute(POST https://10.178.64.1:443/api/v1/namespaces/emptydir-2698/pods/pod-sharedvolume-7c6d1bc6-91b9-42e9-8be2-fef253dac5ea/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
    Jan 30 13:01:47.144: INFO: Exec stderr: ""
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jan 30 13:01:47.144: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-2698" for this suite. 01/30/23 13:01:47.146
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Should recreate evicted statefulset [Conformance]
  test/e2e/apps/statefulset.go:739
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 13:01:47.149
Jan 30 13:01:47.149: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename statefulset 01/30/23 13:01:47.15
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 13:01:47.155
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 13:01:47.157
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-355 01/30/23 13:01:47.159
[It] Should recreate evicted statefulset [Conformance]
  test/e2e/apps/statefulset.go:739
STEP: Looking for a node to schedule stateful set and pod 01/30/23 13:01:47.161
STEP: Creating pod with conflicting port in namespace statefulset-355 01/30/23 13:01:47.164
STEP: Waiting until pod test-pod will start running in namespace statefulset-355 01/30/23 13:01:47.168
Jan 30 13:01:47.168: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "statefulset-355" to be "running"
Jan 30 13:01:47.169: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 1.524514ms
Jan 30 13:01:49.172: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.004460287s
Jan 30 13:01:49.172: INFO: Pod "test-pod" satisfied condition "running"
STEP: Creating statefulset with conflicting port in namespace statefulset-355 01/30/23 13:01:49.172
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-355 01/30/23 13:01:49.175
Jan 30 13:01:49.182: INFO: Observed stateful pod in namespace: statefulset-355, name: ss-0, uid: 71028d91-0117-4e54-b46d-7048aa15aee6, status phase: Pending. Waiting for statefulset controller to delete.
Jan 30 13:01:49.190: INFO: Observed stateful pod in namespace: statefulset-355, name: ss-0, uid: 71028d91-0117-4e54-b46d-7048aa15aee6, status phase: Failed. Waiting for statefulset controller to delete.
Jan 30 13:01:49.197: INFO: Observed stateful pod in namespace: statefulset-355, name: ss-0, uid: 71028d91-0117-4e54-b46d-7048aa15aee6, status phase: Failed. Waiting for statefulset controller to delete.
Jan 30 13:01:49.198: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-355
STEP: Removing pod with conflicting port in namespace statefulset-355 01/30/23 13:01:49.198
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-355 and will be in running state 01/30/23 13:01:49.202
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Jan 30 13:01:51.206: INFO: Deleting all statefulset in ns statefulset-355
Jan 30 13:01:51.208: INFO: Scaling statefulset ss to 0
Jan 30 13:02:01.217: INFO: Waiting for statefulset status.replicas updated to 0
Jan 30 13:02:01.219: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Jan 30 13:02:01.224: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-355" for this suite. 01/30/23 13:02:01.226
------------------------------
• [SLOW TEST] [14.080 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    Should recreate evicted statefulset [Conformance]
    test/e2e/apps/statefulset.go:739

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 13:01:47.149
    Jan 30 13:01:47.149: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename statefulset 01/30/23 13:01:47.15
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 13:01:47.155
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 13:01:47.157
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-355 01/30/23 13:01:47.159
    [It] Should recreate evicted statefulset [Conformance]
      test/e2e/apps/statefulset.go:739
    STEP: Looking for a node to schedule stateful set and pod 01/30/23 13:01:47.161
    STEP: Creating pod with conflicting port in namespace statefulset-355 01/30/23 13:01:47.164
    STEP: Waiting until pod test-pod will start running in namespace statefulset-355 01/30/23 13:01:47.168
    Jan 30 13:01:47.168: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "statefulset-355" to be "running"
    Jan 30 13:01:47.169: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 1.524514ms
    Jan 30 13:01:49.172: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.004460287s
    Jan 30 13:01:49.172: INFO: Pod "test-pod" satisfied condition "running"
    STEP: Creating statefulset with conflicting port in namespace statefulset-355 01/30/23 13:01:49.172
    STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-355 01/30/23 13:01:49.175
    Jan 30 13:01:49.182: INFO: Observed stateful pod in namespace: statefulset-355, name: ss-0, uid: 71028d91-0117-4e54-b46d-7048aa15aee6, status phase: Pending. Waiting for statefulset controller to delete.
    Jan 30 13:01:49.190: INFO: Observed stateful pod in namespace: statefulset-355, name: ss-0, uid: 71028d91-0117-4e54-b46d-7048aa15aee6, status phase: Failed. Waiting for statefulset controller to delete.
    Jan 30 13:01:49.197: INFO: Observed stateful pod in namespace: statefulset-355, name: ss-0, uid: 71028d91-0117-4e54-b46d-7048aa15aee6, status phase: Failed. Waiting for statefulset controller to delete.
    Jan 30 13:01:49.198: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-355
    STEP: Removing pod with conflicting port in namespace statefulset-355 01/30/23 13:01:49.198
    STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-355 and will be in running state 01/30/23 13:01:49.202
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Jan 30 13:01:51.206: INFO: Deleting all statefulset in ns statefulset-355
    Jan 30 13:01:51.208: INFO: Scaling statefulset ss to 0
    Jan 30 13:02:01.217: INFO: Waiting for statefulset status.replicas updated to 0
    Jan 30 13:02:01.219: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Jan 30 13:02:01.224: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-355" for this suite. 01/30/23 13:02:01.226
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:398
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 13:02:01.241
Jan 30 13:02:01.241: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename pods 01/30/23 13:02:01.242
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 13:02:01.249
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 13:02:01.251
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:398
STEP: creating the pod 01/30/23 13:02:01.252
STEP: submitting the pod to kubernetes 01/30/23 13:02:01.253
Jan 30 13:02:01.256: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-70004962-e302-4abe-a5fa-cd560a260bd8" in namespace "pods-1129" to be "running and ready"
Jan 30 13:02:01.258: INFO: Pod "pod-update-activedeadlineseconds-70004962-e302-4abe-a5fa-cd560a260bd8": Phase="Pending", Reason="", readiness=false. Elapsed: 1.668743ms
Jan 30 13:02:01.258: INFO: The phase of Pod pod-update-activedeadlineseconds-70004962-e302-4abe-a5fa-cd560a260bd8 is Pending, waiting for it to be Running (with Ready = true)
Jan 30 13:02:03.261: INFO: Pod "pod-update-activedeadlineseconds-70004962-e302-4abe-a5fa-cd560a260bd8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004591968s
Jan 30 13:02:03.261: INFO: The phase of Pod pod-update-activedeadlineseconds-70004962-e302-4abe-a5fa-cd560a260bd8 is Pending, waiting for it to be Running (with Ready = true)
Jan 30 13:02:05.262: INFO: Pod "pod-update-activedeadlineseconds-70004962-e302-4abe-a5fa-cd560a260bd8": Phase="Running", Reason="", readiness=true. Elapsed: 4.005819738s
Jan 30 13:02:05.262: INFO: The phase of Pod pod-update-activedeadlineseconds-70004962-e302-4abe-a5fa-cd560a260bd8 is Running (Ready = true)
Jan 30 13:02:05.262: INFO: Pod "pod-update-activedeadlineseconds-70004962-e302-4abe-a5fa-cd560a260bd8" satisfied condition "running and ready"
STEP: verifying the pod is in kubernetes 01/30/23 13:02:05.264
STEP: updating the pod 01/30/23 13:02:05.266
Jan 30 13:02:05.774: INFO: Successfully updated pod "pod-update-activedeadlineseconds-70004962-e302-4abe-a5fa-cd560a260bd8"
Jan 30 13:02:05.774: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-70004962-e302-4abe-a5fa-cd560a260bd8" in namespace "pods-1129" to be "terminated with reason DeadlineExceeded"
Jan 30 13:02:05.776: INFO: Pod "pod-update-activedeadlineseconds-70004962-e302-4abe-a5fa-cd560a260bd8": Phase="Running", Reason="", readiness=true. Elapsed: 1.908117ms
Jan 30 13:02:07.779: INFO: Pod "pod-update-activedeadlineseconds-70004962-e302-4abe-a5fa-cd560a260bd8": Phase="Running", Reason="", readiness=false. Elapsed: 2.004648299s
Jan 30 13:02:09.780: INFO: Pod "pod-update-activedeadlineseconds-70004962-e302-4abe-a5fa-cd560a260bd8": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 4.005926768s
Jan 30 13:02:09.780: INFO: Pod "pod-update-activedeadlineseconds-70004962-e302-4abe-a5fa-cd560a260bd8" satisfied condition "terminated with reason DeadlineExceeded"
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Jan 30 13:02:09.780: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-1129" for this suite. 01/30/23 13:02:09.782
------------------------------
• [SLOW TEST] [8.544 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:398

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 13:02:01.241
    Jan 30 13:02:01.241: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename pods 01/30/23 13:02:01.242
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 13:02:01.249
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 13:02:01.251
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:398
    STEP: creating the pod 01/30/23 13:02:01.252
    STEP: submitting the pod to kubernetes 01/30/23 13:02:01.253
    Jan 30 13:02:01.256: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-70004962-e302-4abe-a5fa-cd560a260bd8" in namespace "pods-1129" to be "running and ready"
    Jan 30 13:02:01.258: INFO: Pod "pod-update-activedeadlineseconds-70004962-e302-4abe-a5fa-cd560a260bd8": Phase="Pending", Reason="", readiness=false. Elapsed: 1.668743ms
    Jan 30 13:02:01.258: INFO: The phase of Pod pod-update-activedeadlineseconds-70004962-e302-4abe-a5fa-cd560a260bd8 is Pending, waiting for it to be Running (with Ready = true)
    Jan 30 13:02:03.261: INFO: Pod "pod-update-activedeadlineseconds-70004962-e302-4abe-a5fa-cd560a260bd8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004591968s
    Jan 30 13:02:03.261: INFO: The phase of Pod pod-update-activedeadlineseconds-70004962-e302-4abe-a5fa-cd560a260bd8 is Pending, waiting for it to be Running (with Ready = true)
    Jan 30 13:02:05.262: INFO: Pod "pod-update-activedeadlineseconds-70004962-e302-4abe-a5fa-cd560a260bd8": Phase="Running", Reason="", readiness=true. Elapsed: 4.005819738s
    Jan 30 13:02:05.262: INFO: The phase of Pod pod-update-activedeadlineseconds-70004962-e302-4abe-a5fa-cd560a260bd8 is Running (Ready = true)
    Jan 30 13:02:05.262: INFO: Pod "pod-update-activedeadlineseconds-70004962-e302-4abe-a5fa-cd560a260bd8" satisfied condition "running and ready"
    STEP: verifying the pod is in kubernetes 01/30/23 13:02:05.264
    STEP: updating the pod 01/30/23 13:02:05.266
    Jan 30 13:02:05.774: INFO: Successfully updated pod "pod-update-activedeadlineseconds-70004962-e302-4abe-a5fa-cd560a260bd8"
    Jan 30 13:02:05.774: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-70004962-e302-4abe-a5fa-cd560a260bd8" in namespace "pods-1129" to be "terminated with reason DeadlineExceeded"
    Jan 30 13:02:05.776: INFO: Pod "pod-update-activedeadlineseconds-70004962-e302-4abe-a5fa-cd560a260bd8": Phase="Running", Reason="", readiness=true. Elapsed: 1.908117ms
    Jan 30 13:02:07.779: INFO: Pod "pod-update-activedeadlineseconds-70004962-e302-4abe-a5fa-cd560a260bd8": Phase="Running", Reason="", readiness=false. Elapsed: 2.004648299s
    Jan 30 13:02:09.780: INFO: Pod "pod-update-activedeadlineseconds-70004962-e302-4abe-a5fa-cd560a260bd8": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 4.005926768s
    Jan 30 13:02:09.780: INFO: Pod "pod-update-activedeadlineseconds-70004962-e302-4abe-a5fa-cd560a260bd8" satisfied condition "terminated with reason DeadlineExceeded"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Jan 30 13:02:09.780: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-1129" for this suite. 01/30/23 13:02:09.782
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:89
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 13:02:09.786
Jan 30 13:02:09.786: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
STEP: Building a namespace api object, basename configmap 01/30/23 13:02:09.787
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 13:02:09.792
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 13:02:09.794
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:89
STEP: Creating configMap with name configmap-test-volume-map-a59a7bf9-11da-4585-b680-aa2d7ca1f020 01/30/23 13:02:09.796
STEP: Creating a pod to test consume configMaps 01/30/23 13:02:09.798
Jan 30 13:02:09.802: INFO: Waiting up to 5m0s for pod "pod-configmaps-8a39266b-a806-4ef7-b574-bfcf76e7f51c" in namespace "configmap-5156" to be "Succeeded or Failed"
Jan 30 13:02:09.803: INFO: Pod "pod-configmaps-8a39266b-a806-4ef7-b574-bfcf76e7f51c": Phase="Pending", Reason="", readiness=false. Elapsed: 1.440918ms
Jan 30 13:02:11.805: INFO: Pod "pod-configmaps-8a39266b-a806-4ef7-b574-bfcf76e7f51c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003411325s
Jan 30 13:02:13.805: INFO: Pod "pod-configmaps-8a39266b-a806-4ef7-b574-bfcf76e7f51c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.003386571s
Jan 30 13:02:15.808: INFO: Pod "pod-configmaps-8a39266b-a806-4ef7-b574-bfcf76e7f51c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.00618202s
STEP: Saw pod success 01/30/23 13:02:15.808
Jan 30 13:02:15.808: INFO: Pod "pod-configmaps-8a39266b-a806-4ef7-b574-bfcf76e7f51c" satisfied condition "Succeeded or Failed"
Jan 30 13:02:15.810: INFO: Trying to get logs from node pubt2-nks-for-dev1.dg.163.org pod pod-configmaps-8a39266b-a806-4ef7-b574-bfcf76e7f51c container agnhost-container: <nil>
STEP: delete the pod 01/30/23 13:02:15.817
Jan 30 13:02:15.821: INFO: Waiting for pod pod-configmaps-8a39266b-a806-4ef7-b574-bfcf76e7f51c to disappear
Jan 30 13:02:15.823: INFO: Pod pod-configmaps-8a39266b-a806-4ef7-b574-bfcf76e7f51c no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jan 30 13:02:15.823: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-5156" for this suite. 01/30/23 13:02:15.825
------------------------------
• [SLOW TEST] [6.041 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:89

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 13:02:09.786
    Jan 30 13:02:09.786: INFO: >>> kubeConfig: /tmp/kubeconfig-1067244644
    STEP: Building a namespace api object, basename configmap 01/30/23 13:02:09.787
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 13:02:09.792
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 13:02:09.794
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:89
    STEP: Creating configMap with name configmap-test-volume-map-a59a7bf9-11da-4585-b680-aa2d7ca1f020 01/30/23 13:02:09.796
    STEP: Creating a pod to test consume configMaps 01/30/23 13:02:09.798
    Jan 30 13:02:09.802: INFO: Waiting up to 5m0s for pod "pod-configmaps-8a39266b-a806-4ef7-b574-bfcf76e7f51c" in namespace "configmap-5156" to be "Succeeded or Failed"
    Jan 30 13:02:09.803: INFO: Pod "pod-configmaps-8a39266b-a806-4ef7-b574-bfcf76e7f51c": Phase="Pending", Reason="", readiness=false. Elapsed: 1.440918ms
    Jan 30 13:02:11.805: INFO: Pod "pod-configmaps-8a39266b-a806-4ef7-b574-bfcf76e7f51c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003411325s
    Jan 30 13:02:13.805: INFO: Pod "pod-configmaps-8a39266b-a806-4ef7-b574-bfcf76e7f51c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.003386571s
    Jan 30 13:02:15.808: INFO: Pod "pod-configmaps-8a39266b-a806-4ef7-b574-bfcf76e7f51c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.00618202s
    STEP: Saw pod success 01/30/23 13:02:15.808
    Jan 30 13:02:15.808: INFO: Pod "pod-configmaps-8a39266b-a806-4ef7-b574-bfcf76e7f51c" satisfied condition "Succeeded or Failed"
    Jan 30 13:02:15.810: INFO: Trying to get logs from node pubt2-nks-for-dev1.dg.163.org pod pod-configmaps-8a39266b-a806-4ef7-b574-bfcf76e7f51c container agnhost-container: <nil>
    STEP: delete the pod 01/30/23 13:02:15.817
    Jan 30 13:02:15.821: INFO: Waiting for pod pod-configmaps-8a39266b-a806-4ef7-b574-bfcf76e7f51c to disappear
    Jan 30 13:02:15.823: INFO: Pod pod-configmaps-8a39266b-a806-4ef7-b574-bfcf76e7f51c no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jan 30 13:02:15.823: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-5156" for this suite. 01/30/23 13:02:15.825
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[SynchronizedAfterSuite] 
test/e2e/e2e.go:88
[SynchronizedAfterSuite] TOP-LEVEL
  test/e2e/e2e.go:88
[SynchronizedAfterSuite] TOP-LEVEL
  test/e2e/e2e.go:88
Jan 30 13:02:15.829: INFO: Running AfterSuite actions on node 1
Jan 30 13:02:15.829: INFO: Skipping dumping logs from cluster
------------------------------
[SynchronizedAfterSuite] PASSED [0.000 seconds]
[SynchronizedAfterSuite] 
test/e2e/e2e.go:88

  Begin Captured GinkgoWriter Output >>
    [SynchronizedAfterSuite] TOP-LEVEL
      test/e2e/e2e.go:88
    [SynchronizedAfterSuite] TOP-LEVEL
      test/e2e/e2e.go:88
    Jan 30 13:02:15.829: INFO: Running AfterSuite actions on node 1
    Jan 30 13:02:15.829: INFO: Skipping dumping logs from cluster
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterSuite] Kubernetes e2e suite report
test/e2e/e2e_test.go:153
[ReportAfterSuite] TOP-LEVEL
  test/e2e/e2e_test.go:153
------------------------------
[ReportAfterSuite] PASSED [0.000 seconds]
[ReportAfterSuite] Kubernetes e2e suite report
test/e2e/e2e_test.go:153

  Begin Captured GinkgoWriter Output >>
    [ReportAfterSuite] TOP-LEVEL
      test/e2e/e2e_test.go:153
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterSuite] Kubernetes e2e JUnit report
test/e2e/framework/test_context.go:529
[ReportAfterSuite] TOP-LEVEL
  test/e2e/framework/test_context.go:529
------------------------------
[ReportAfterSuite] PASSED [0.070 seconds]
[ReportAfterSuite] Kubernetes e2e JUnit report
test/e2e/framework/test_context.go:529

  Begin Captured GinkgoWriter Output >>
    [ReportAfterSuite] TOP-LEVEL
      test/e2e/framework/test_context.go:529
  << End Captured GinkgoWriter Output
------------------------------

Ran 368 of 7069 Specs in 6282.760 seconds
SUCCESS! -- 368 Passed | 0 Failed | 0 Pending | 6701 Skipped
PASS

Ginkgo ran 1 suite in 1h44m43.033234415s
Test Suite Passed
[38;5;228mYou're using deprecated Ginkgo functionality:[0m
[38;5;228m=============================================[0m
  [38;5;11m--noColor is deprecated, use --no-color instead[0m
  [1mLearn more at:[0m [38;5;14m[4mhttps://onsi.github.io/ginkgo/MIGRATING_TO_V2#changed-command-line-flags[0m

[38;5;243mTo silence deprecations that can be silenced set the following environment variable:[0m
  [38;5;243mACK_GINKGO_DEPRECATIONS=2.4.0[0m

