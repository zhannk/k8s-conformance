I1209 06:58:35.672374      20 e2e.go:126] Starting e2e run "2287ad07-0247-4b9b-827b-66dd3791c4ed" on Ginkgo node 1
Dec  9 06:58:35.685: INFO: Enabling in-tree volume drivers
Running Suite: Kubernetes e2e suite - /usr/local/bin
====================================================
Random Seed: 1670569115 - will randomize all specs

Will run 368 of 7069 specs
------------------------------
[SynchronizedBeforeSuite] 
test/e2e/e2e.go:77
[SynchronizedBeforeSuite] TOP-LEVEL
  test/e2e/e2e.go:77
Dec  9 06:58:35.840: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
Dec  9 06:58:35.841: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Dec  9 06:58:35.862: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Dec  9 06:58:35.886: INFO: 11 / 11 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Dec  9 06:58:35.886: INFO: expected 2 pod replicas in namespace 'kube-system', 2 are Running and Ready.
Dec  9 06:58:35.886: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Dec  9 06:58:35.892: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'calico-node' (0 seconds elapsed)
Dec  9 06:58:35.892: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
Dec  9 06:58:35.892: INFO: e2e test version: v1.26.0
Dec  9 06:58:35.900: INFO: kube-apiserver version: v1.26.0
[SynchronizedBeforeSuite] TOP-LEVEL
  test/e2e/e2e.go:77
Dec  9 06:58:35.901: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
Dec  9 06:58:35.916: INFO: Cluster IP family: ipv4
------------------------------
[SynchronizedBeforeSuite] PASSED [0.077 seconds]
[SynchronizedBeforeSuite] 
test/e2e/e2e.go:77

  Begin Captured GinkgoWriter Output >>
    [SynchronizedBeforeSuite] TOP-LEVEL
      test/e2e/e2e.go:77
    Dec  9 06:58:35.840: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    Dec  9 06:58:35.841: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
    Dec  9 06:58:35.862: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
    Dec  9 06:58:35.886: INFO: 11 / 11 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
    Dec  9 06:58:35.886: INFO: expected 2 pod replicas in namespace 'kube-system', 2 are Running and Ready.
    Dec  9 06:58:35.886: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
    Dec  9 06:58:35.892: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'calico-node' (0 seconds elapsed)
    Dec  9 06:58:35.892: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
    Dec  9 06:58:35.892: INFO: e2e test version: v1.26.0
    Dec  9 06:58:35.900: INFO: kube-apiserver version: v1.26.0
    [SynchronizedBeforeSuite] TOP-LEVEL
      test/e2e/e2e.go:77
    Dec  9 06:58:35.901: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    Dec  9 06:58:35.916: INFO: Cluster IP family: ipv4
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers
  should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:73
[BeforeEach] [sig-node] Containers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 06:58:35.942
Dec  9 06:58:35.943: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename containers 12/09/22 06:58:35.943
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 06:58:36.018
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 06:58:36.028
[BeforeEach] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:73
STEP: Creating a pod to test override command 12/09/22 06:58:36.04
Dec  9 06:58:36.063: INFO: Waiting up to 5m0s for pod "client-containers-917f8d3b-bc89-4fdf-ad61-883932483ed9" in namespace "containers-7733" to be "Succeeded or Failed"
Dec  9 06:58:36.076: INFO: Pod "client-containers-917f8d3b-bc89-4fdf-ad61-883932483ed9": Phase="Pending", Reason="", readiness=false. Elapsed: 12.802689ms
Dec  9 06:58:38.080: INFO: Pod "client-containers-917f8d3b-bc89-4fdf-ad61-883932483ed9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016575203s
Dec  9 06:58:40.081: INFO: Pod "client-containers-917f8d3b-bc89-4fdf-ad61-883932483ed9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.017909358s
Dec  9 06:58:42.081: INFO: Pod "client-containers-917f8d3b-bc89-4fdf-ad61-883932483ed9": Phase="Pending", Reason="", readiness=false. Elapsed: 6.017743295s
Dec  9 06:58:44.081: INFO: Pod "client-containers-917f8d3b-bc89-4fdf-ad61-883932483ed9": Phase="Pending", Reason="", readiness=false. Elapsed: 8.018069285s
Dec  9 06:58:46.081: INFO: Pod "client-containers-917f8d3b-bc89-4fdf-ad61-883932483ed9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.017806985s
STEP: Saw pod success 12/09/22 06:58:46.081
Dec  9 06:58:46.081: INFO: Pod "client-containers-917f8d3b-bc89-4fdf-ad61-883932483ed9" satisfied condition "Succeeded or Failed"
Dec  9 06:58:46.085: INFO: Trying to get logs from node ip-10-0-10-179 pod client-containers-917f8d3b-bc89-4fdf-ad61-883932483ed9 container agnhost-container: <nil>
STEP: delete the pod 12/09/22 06:58:46.136
Dec  9 06:58:46.148: INFO: Waiting for pod client-containers-917f8d3b-bc89-4fdf-ad61-883932483ed9 to disappear
Dec  9 06:58:46.152: INFO: Pod client-containers-917f8d3b-bc89-4fdf-ad61-883932483ed9 no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/node/init/init.go:32
Dec  9 06:58:46.152: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Containers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Containers
  tear down framework | framework.go:193
STEP: Destroying namespace "containers-7733" for this suite. 12/09/22 06:58:46.155
------------------------------
â€¢ [SLOW TEST] [10.219 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:73

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 06:58:35.942
    Dec  9 06:58:35.943: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename containers 12/09/22 06:58:35.943
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 06:58:36.018
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 06:58:36.028
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:73
    STEP: Creating a pod to test override command 12/09/22 06:58:36.04
    Dec  9 06:58:36.063: INFO: Waiting up to 5m0s for pod "client-containers-917f8d3b-bc89-4fdf-ad61-883932483ed9" in namespace "containers-7733" to be "Succeeded or Failed"
    Dec  9 06:58:36.076: INFO: Pod "client-containers-917f8d3b-bc89-4fdf-ad61-883932483ed9": Phase="Pending", Reason="", readiness=false. Elapsed: 12.802689ms
    Dec  9 06:58:38.080: INFO: Pod "client-containers-917f8d3b-bc89-4fdf-ad61-883932483ed9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016575203s
    Dec  9 06:58:40.081: INFO: Pod "client-containers-917f8d3b-bc89-4fdf-ad61-883932483ed9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.017909358s
    Dec  9 06:58:42.081: INFO: Pod "client-containers-917f8d3b-bc89-4fdf-ad61-883932483ed9": Phase="Pending", Reason="", readiness=false. Elapsed: 6.017743295s
    Dec  9 06:58:44.081: INFO: Pod "client-containers-917f8d3b-bc89-4fdf-ad61-883932483ed9": Phase="Pending", Reason="", readiness=false. Elapsed: 8.018069285s
    Dec  9 06:58:46.081: INFO: Pod "client-containers-917f8d3b-bc89-4fdf-ad61-883932483ed9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.017806985s
    STEP: Saw pod success 12/09/22 06:58:46.081
    Dec  9 06:58:46.081: INFO: Pod "client-containers-917f8d3b-bc89-4fdf-ad61-883932483ed9" satisfied condition "Succeeded or Failed"
    Dec  9 06:58:46.085: INFO: Trying to get logs from node ip-10-0-10-179 pod client-containers-917f8d3b-bc89-4fdf-ad61-883932483ed9 container agnhost-container: <nil>
    STEP: delete the pod 12/09/22 06:58:46.136
    Dec  9 06:58:46.148: INFO: Waiting for pod client-containers-917f8d3b-bc89-4fdf-ad61-883932483ed9 to disappear
    Dec  9 06:58:46.152: INFO: Pod client-containers-917f8d3b-bc89-4fdf-ad61-883932483ed9 no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/node/init/init.go:32
    Dec  9 06:58:46.152: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Containers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Containers
      tear down framework | framework.go:193
    STEP: Destroying namespace "containers-7733" for this suite. 12/09/22 06:58:46.155
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-apps] Deployment
  should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 06:58:46.164
Dec  9 06:58:46.164: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename deployment 12/09/22 06:58:46.166
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 06:58:46.234
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 06:58:46.251
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479
STEP: creating a Deployment 12/09/22 06:58:46.375
Dec  9 06:58:46.375: INFO: Creating simple deployment test-deployment-v7x26
Dec  9 06:58:46.434: INFO: deployment "test-deployment-v7x26" doesn't have the required revision set
Dec  9 06:58:48.445: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.December, 9, 6, 58, 46, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 9, 6, 58, 46, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 9, 6, 58, 46, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 9, 6, 58, 46, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-deployment-v7x26-54bc444df\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  9 06:58:50.453: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.December, 9, 6, 58, 46, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 9, 6, 58, 46, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 9, 6, 58, 46, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 9, 6, 58, 46, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-deployment-v7x26-54bc444df\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Getting /status 12/09/22 06:58:52.452
Dec  9 06:58:52.457: INFO: Deployment test-deployment-v7x26 has Conditions: [{Available True 2022-12-09 06:58:50 +0000 UTC 2022-12-09 06:58:50 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2022-12-09 06:58:50 +0000 UTC 2022-12-09 06:58:46 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-v7x26-54bc444df" has successfully progressed.}]
STEP: updating Deployment Status 12/09/22 06:58:52.457
Dec  9 06:58:52.467: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.December, 9, 6, 58, 50, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 9, 6, 58, 50, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 9, 6, 58, 50, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 9, 6, 58, 46, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-v7x26-54bc444df\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Deployment status to be updated 12/09/22 06:58:52.467
Dec  9 06:58:52.472: INFO: Observed &Deployment event: ADDED
Dec  9 06:58:52.472: INFO: Observed Deployment test-deployment-v7x26 in namespace deployment-9618 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-12-09 06:58:46 +0000 UTC 2022-12-09 06:58:46 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-v7x26-54bc444df"}
Dec  9 06:58:52.473: INFO: Observed &Deployment event: MODIFIED
Dec  9 06:58:52.473: INFO: Observed Deployment test-deployment-v7x26 in namespace deployment-9618 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-12-09 06:58:46 +0000 UTC 2022-12-09 06:58:46 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-v7x26-54bc444df"}
Dec  9 06:58:52.473: INFO: Observed Deployment test-deployment-v7x26 in namespace deployment-9618 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2022-12-09 06:58:46 +0000 UTC 2022-12-09 06:58:46 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Dec  9 06:58:52.473: INFO: Observed &Deployment event: MODIFIED
Dec  9 06:58:52.473: INFO: Observed Deployment test-deployment-v7x26 in namespace deployment-9618 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2022-12-09 06:58:46 +0000 UTC 2022-12-09 06:58:46 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Dec  9 06:58:52.473: INFO: Observed Deployment test-deployment-v7x26 in namespace deployment-9618 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-12-09 06:58:46 +0000 UTC 2022-12-09 06:58:46 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-v7x26-54bc444df" is progressing.}
Dec  9 06:58:52.474: INFO: Observed &Deployment event: MODIFIED
Dec  9 06:58:52.474: INFO: Observed Deployment test-deployment-v7x26 in namespace deployment-9618 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2022-12-09 06:58:50 +0000 UTC 2022-12-09 06:58:50 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Dec  9 06:58:52.474: INFO: Observed Deployment test-deployment-v7x26 in namespace deployment-9618 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-12-09 06:58:50 +0000 UTC 2022-12-09 06:58:46 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-v7x26-54bc444df" has successfully progressed.}
Dec  9 06:58:52.474: INFO: Observed &Deployment event: MODIFIED
Dec  9 06:58:52.474: INFO: Observed Deployment test-deployment-v7x26 in namespace deployment-9618 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2022-12-09 06:58:50 +0000 UTC 2022-12-09 06:58:50 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Dec  9 06:58:52.474: INFO: Observed Deployment test-deployment-v7x26 in namespace deployment-9618 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-12-09 06:58:50 +0000 UTC 2022-12-09 06:58:46 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-v7x26-54bc444df" has successfully progressed.}
Dec  9 06:58:52.474: INFO: Found Deployment test-deployment-v7x26 in namespace deployment-9618 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Dec  9 06:58:52.474: INFO: Deployment test-deployment-v7x26 has an updated status
STEP: patching the Statefulset Status 12/09/22 06:58:52.474
Dec  9 06:58:52.474: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Dec  9 06:58:52.482: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Deployment status to be patched 12/09/22 06:58:52.482
Dec  9 06:58:52.488: INFO: Observed &Deployment event: ADDED
Dec  9 06:58:52.488: INFO: Observed deployment test-deployment-v7x26 in namespace deployment-9618 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-12-09 06:58:46 +0000 UTC 2022-12-09 06:58:46 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-v7x26-54bc444df"}
Dec  9 06:58:52.488: INFO: Observed &Deployment event: MODIFIED
Dec  9 06:58:52.488: INFO: Observed deployment test-deployment-v7x26 in namespace deployment-9618 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-12-09 06:58:46 +0000 UTC 2022-12-09 06:58:46 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-v7x26-54bc444df"}
Dec  9 06:58:52.488: INFO: Observed deployment test-deployment-v7x26 in namespace deployment-9618 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2022-12-09 06:58:46 +0000 UTC 2022-12-09 06:58:46 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Dec  9 06:58:52.488: INFO: Observed &Deployment event: MODIFIED
Dec  9 06:58:52.488: INFO: Observed deployment test-deployment-v7x26 in namespace deployment-9618 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2022-12-09 06:58:46 +0000 UTC 2022-12-09 06:58:46 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Dec  9 06:58:52.488: INFO: Observed deployment test-deployment-v7x26 in namespace deployment-9618 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-12-09 06:58:46 +0000 UTC 2022-12-09 06:58:46 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-v7x26-54bc444df" is progressing.}
Dec  9 06:58:52.489: INFO: Observed &Deployment event: MODIFIED
Dec  9 06:58:52.489: INFO: Observed deployment test-deployment-v7x26 in namespace deployment-9618 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2022-12-09 06:58:50 +0000 UTC 2022-12-09 06:58:50 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Dec  9 06:58:52.489: INFO: Observed deployment test-deployment-v7x26 in namespace deployment-9618 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-12-09 06:58:50 +0000 UTC 2022-12-09 06:58:46 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-v7x26-54bc444df" has successfully progressed.}
Dec  9 06:58:52.489: INFO: Observed &Deployment event: MODIFIED
Dec  9 06:58:52.489: INFO: Observed deployment test-deployment-v7x26 in namespace deployment-9618 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2022-12-09 06:58:50 +0000 UTC 2022-12-09 06:58:50 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Dec  9 06:58:52.489: INFO: Observed deployment test-deployment-v7x26 in namespace deployment-9618 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-12-09 06:58:50 +0000 UTC 2022-12-09 06:58:46 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-v7x26-54bc444df" has successfully progressed.}
Dec  9 06:58:52.489: INFO: Observed deployment test-deployment-v7x26 in namespace deployment-9618 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Dec  9 06:58:52.489: INFO: Observed &Deployment event: MODIFIED
Dec  9 06:58:52.489: INFO: Found deployment test-deployment-v7x26 in namespace deployment-9618 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
Dec  9 06:58:52.489: INFO: Deployment test-deployment-v7x26 has a patched status
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Dec  9 06:58:52.496: INFO: Deployment "test-deployment-v7x26":
&Deployment{ObjectMeta:{test-deployment-v7x26  deployment-9618  0ede56b0-1d6d-410d-93ce-338fbb36a23d 1033 1 2022-12-09 06:58:46 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2022-12-09 06:58:46 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {e2e.test Update apps/v1 2022-12-09 06:58:52 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status} {kube-controller-manager Update apps/v1 2022-12-09 06:58:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0030b75f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:FoundNewReplicaSet,Message:Found new replica set "test-deployment-v7x26-54bc444df",LastUpdateTime:2022-12-09 06:58:52 +0000 UTC,LastTransitionTime:2022-12-09 06:58:52 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Dec  9 06:58:52.499: INFO: New ReplicaSet "test-deployment-v7x26-54bc444df" of Deployment "test-deployment-v7x26":
&ReplicaSet{ObjectMeta:{test-deployment-v7x26-54bc444df  deployment-9618  b86762f4-0f8b-47f5-984f-442f4e4c9ef9 1018 1 2022-12-09 06:58:46 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-v7x26 0ede56b0-1d6d-410d-93ce-338fbb36a23d 0xc0030b7997 0xc0030b7998}] [] [{kube-controller-manager Update apps/v1 2022-12-09 06:58:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0ede56b0-1d6d-410d-93ce-338fbb36a23d\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-09 06:58:50 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 54bc444df,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0030b7a48 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Dec  9 06:58:52.509: INFO: Pod "test-deployment-v7x26-54bc444df-rdphq" is available:
&Pod{ObjectMeta:{test-deployment-v7x26-54bc444df-rdphq test-deployment-v7x26-54bc444df- deployment-9618  cf963bc1-0286-41d3-85bc-cf5b64b180b9 1017 0 2022-12-09 06:58:46 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[cni.projectcalico.org/containerID:87d896005cebbe57a093f0249e5283bb65075952eea42c778ce44e5480837fe6 cni.projectcalico.org/podIP:10.2.136.4/32 cni.projectcalico.org/podIPs:10.2.136.4/32] [{apps/v1 ReplicaSet test-deployment-v7x26-54bc444df b86762f4-0f8b-47f5-984f-442f4e4c9ef9 0xc001306dd7 0xc001306dd8}] [] [{kube-controller-manager Update v1 2022-12-09 06:58:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b86762f4-0f8b-47f5-984f-442f4e4c9ef9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-12-09 06:58:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-12-09 06:58:50 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.2.136.4\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rgtqx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rgtqx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-10-179,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 06:58:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 06:58:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 06:58:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 06:58:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.10.179,PodIP:10.2.136.4,StartTime:2022-12-09 06:58:46 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-12-09 06:58:50 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://8ba4b199da346ba02b6ededcadb56e7c6af825c2b9d56dca695922ac43ce3323,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.2.136.4,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Dec  9 06:58:52.509: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-9618" for this suite. 12/09/22 06:58:52.516
------------------------------
â€¢ [SLOW TEST] [6.366 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 06:58:46.164
    Dec  9 06:58:46.164: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename deployment 12/09/22 06:58:46.166
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 06:58:46.234
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 06:58:46.251
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] should validate Deployment Status endpoints [Conformance]
      test/e2e/apps/deployment.go:479
    STEP: creating a Deployment 12/09/22 06:58:46.375
    Dec  9 06:58:46.375: INFO: Creating simple deployment test-deployment-v7x26
    Dec  9 06:58:46.434: INFO: deployment "test-deployment-v7x26" doesn't have the required revision set
    Dec  9 06:58:48.445: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.December, 9, 6, 58, 46, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 9, 6, 58, 46, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 9, 6, 58, 46, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 9, 6, 58, 46, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-deployment-v7x26-54bc444df\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Dec  9 06:58:50.453: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.December, 9, 6, 58, 46, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 9, 6, 58, 46, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 9, 6, 58, 46, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 9, 6, 58, 46, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-deployment-v7x26-54bc444df\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Getting /status 12/09/22 06:58:52.452
    Dec  9 06:58:52.457: INFO: Deployment test-deployment-v7x26 has Conditions: [{Available True 2022-12-09 06:58:50 +0000 UTC 2022-12-09 06:58:50 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2022-12-09 06:58:50 +0000 UTC 2022-12-09 06:58:46 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-v7x26-54bc444df" has successfully progressed.}]
    STEP: updating Deployment Status 12/09/22 06:58:52.457
    Dec  9 06:58:52.467: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.December, 9, 6, 58, 50, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 9, 6, 58, 50, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 9, 6, 58, 50, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 9, 6, 58, 46, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-v7x26-54bc444df\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the Deployment status to be updated 12/09/22 06:58:52.467
    Dec  9 06:58:52.472: INFO: Observed &Deployment event: ADDED
    Dec  9 06:58:52.472: INFO: Observed Deployment test-deployment-v7x26 in namespace deployment-9618 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-12-09 06:58:46 +0000 UTC 2022-12-09 06:58:46 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-v7x26-54bc444df"}
    Dec  9 06:58:52.473: INFO: Observed &Deployment event: MODIFIED
    Dec  9 06:58:52.473: INFO: Observed Deployment test-deployment-v7x26 in namespace deployment-9618 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-12-09 06:58:46 +0000 UTC 2022-12-09 06:58:46 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-v7x26-54bc444df"}
    Dec  9 06:58:52.473: INFO: Observed Deployment test-deployment-v7x26 in namespace deployment-9618 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2022-12-09 06:58:46 +0000 UTC 2022-12-09 06:58:46 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Dec  9 06:58:52.473: INFO: Observed &Deployment event: MODIFIED
    Dec  9 06:58:52.473: INFO: Observed Deployment test-deployment-v7x26 in namespace deployment-9618 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2022-12-09 06:58:46 +0000 UTC 2022-12-09 06:58:46 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Dec  9 06:58:52.473: INFO: Observed Deployment test-deployment-v7x26 in namespace deployment-9618 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-12-09 06:58:46 +0000 UTC 2022-12-09 06:58:46 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-v7x26-54bc444df" is progressing.}
    Dec  9 06:58:52.474: INFO: Observed &Deployment event: MODIFIED
    Dec  9 06:58:52.474: INFO: Observed Deployment test-deployment-v7x26 in namespace deployment-9618 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2022-12-09 06:58:50 +0000 UTC 2022-12-09 06:58:50 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Dec  9 06:58:52.474: INFO: Observed Deployment test-deployment-v7x26 in namespace deployment-9618 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-12-09 06:58:50 +0000 UTC 2022-12-09 06:58:46 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-v7x26-54bc444df" has successfully progressed.}
    Dec  9 06:58:52.474: INFO: Observed &Deployment event: MODIFIED
    Dec  9 06:58:52.474: INFO: Observed Deployment test-deployment-v7x26 in namespace deployment-9618 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2022-12-09 06:58:50 +0000 UTC 2022-12-09 06:58:50 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Dec  9 06:58:52.474: INFO: Observed Deployment test-deployment-v7x26 in namespace deployment-9618 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-12-09 06:58:50 +0000 UTC 2022-12-09 06:58:46 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-v7x26-54bc444df" has successfully progressed.}
    Dec  9 06:58:52.474: INFO: Found Deployment test-deployment-v7x26 in namespace deployment-9618 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Dec  9 06:58:52.474: INFO: Deployment test-deployment-v7x26 has an updated status
    STEP: patching the Statefulset Status 12/09/22 06:58:52.474
    Dec  9 06:58:52.474: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Dec  9 06:58:52.482: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Deployment status to be patched 12/09/22 06:58:52.482
    Dec  9 06:58:52.488: INFO: Observed &Deployment event: ADDED
    Dec  9 06:58:52.488: INFO: Observed deployment test-deployment-v7x26 in namespace deployment-9618 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-12-09 06:58:46 +0000 UTC 2022-12-09 06:58:46 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-v7x26-54bc444df"}
    Dec  9 06:58:52.488: INFO: Observed &Deployment event: MODIFIED
    Dec  9 06:58:52.488: INFO: Observed deployment test-deployment-v7x26 in namespace deployment-9618 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-12-09 06:58:46 +0000 UTC 2022-12-09 06:58:46 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-v7x26-54bc444df"}
    Dec  9 06:58:52.488: INFO: Observed deployment test-deployment-v7x26 in namespace deployment-9618 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2022-12-09 06:58:46 +0000 UTC 2022-12-09 06:58:46 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Dec  9 06:58:52.488: INFO: Observed &Deployment event: MODIFIED
    Dec  9 06:58:52.488: INFO: Observed deployment test-deployment-v7x26 in namespace deployment-9618 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2022-12-09 06:58:46 +0000 UTC 2022-12-09 06:58:46 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Dec  9 06:58:52.488: INFO: Observed deployment test-deployment-v7x26 in namespace deployment-9618 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-12-09 06:58:46 +0000 UTC 2022-12-09 06:58:46 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-v7x26-54bc444df" is progressing.}
    Dec  9 06:58:52.489: INFO: Observed &Deployment event: MODIFIED
    Dec  9 06:58:52.489: INFO: Observed deployment test-deployment-v7x26 in namespace deployment-9618 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2022-12-09 06:58:50 +0000 UTC 2022-12-09 06:58:50 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Dec  9 06:58:52.489: INFO: Observed deployment test-deployment-v7x26 in namespace deployment-9618 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-12-09 06:58:50 +0000 UTC 2022-12-09 06:58:46 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-v7x26-54bc444df" has successfully progressed.}
    Dec  9 06:58:52.489: INFO: Observed &Deployment event: MODIFIED
    Dec  9 06:58:52.489: INFO: Observed deployment test-deployment-v7x26 in namespace deployment-9618 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2022-12-09 06:58:50 +0000 UTC 2022-12-09 06:58:50 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Dec  9 06:58:52.489: INFO: Observed deployment test-deployment-v7x26 in namespace deployment-9618 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-12-09 06:58:50 +0000 UTC 2022-12-09 06:58:46 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-v7x26-54bc444df" has successfully progressed.}
    Dec  9 06:58:52.489: INFO: Observed deployment test-deployment-v7x26 in namespace deployment-9618 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Dec  9 06:58:52.489: INFO: Observed &Deployment event: MODIFIED
    Dec  9 06:58:52.489: INFO: Found deployment test-deployment-v7x26 in namespace deployment-9618 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
    Dec  9 06:58:52.489: INFO: Deployment test-deployment-v7x26 has a patched status
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Dec  9 06:58:52.496: INFO: Deployment "test-deployment-v7x26":
    &Deployment{ObjectMeta:{test-deployment-v7x26  deployment-9618  0ede56b0-1d6d-410d-93ce-338fbb36a23d 1033 1 2022-12-09 06:58:46 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2022-12-09 06:58:46 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {e2e.test Update apps/v1 2022-12-09 06:58:52 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status} {kube-controller-manager Update apps/v1 2022-12-09 06:58:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0030b75f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:FoundNewReplicaSet,Message:Found new replica set "test-deployment-v7x26-54bc444df",LastUpdateTime:2022-12-09 06:58:52 +0000 UTC,LastTransitionTime:2022-12-09 06:58:52 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Dec  9 06:58:52.499: INFO: New ReplicaSet "test-deployment-v7x26-54bc444df" of Deployment "test-deployment-v7x26":
    &ReplicaSet{ObjectMeta:{test-deployment-v7x26-54bc444df  deployment-9618  b86762f4-0f8b-47f5-984f-442f4e4c9ef9 1018 1 2022-12-09 06:58:46 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-v7x26 0ede56b0-1d6d-410d-93ce-338fbb36a23d 0xc0030b7997 0xc0030b7998}] [] [{kube-controller-manager Update apps/v1 2022-12-09 06:58:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0ede56b0-1d6d-410d-93ce-338fbb36a23d\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-09 06:58:50 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 54bc444df,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0030b7a48 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Dec  9 06:58:52.509: INFO: Pod "test-deployment-v7x26-54bc444df-rdphq" is available:
    &Pod{ObjectMeta:{test-deployment-v7x26-54bc444df-rdphq test-deployment-v7x26-54bc444df- deployment-9618  cf963bc1-0286-41d3-85bc-cf5b64b180b9 1017 0 2022-12-09 06:58:46 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[cni.projectcalico.org/containerID:87d896005cebbe57a093f0249e5283bb65075952eea42c778ce44e5480837fe6 cni.projectcalico.org/podIP:10.2.136.4/32 cni.projectcalico.org/podIPs:10.2.136.4/32] [{apps/v1 ReplicaSet test-deployment-v7x26-54bc444df b86762f4-0f8b-47f5-984f-442f4e4c9ef9 0xc001306dd7 0xc001306dd8}] [] [{kube-controller-manager Update v1 2022-12-09 06:58:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b86762f4-0f8b-47f5-984f-442f4e4c9ef9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-12-09 06:58:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-12-09 06:58:50 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.2.136.4\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rgtqx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rgtqx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-10-179,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 06:58:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 06:58:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 06:58:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 06:58:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.10.179,PodIP:10.2.136.4,StartTime:2022-12-09 06:58:46 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-12-09 06:58:50 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://8ba4b199da346ba02b6ededcadb56e7c6af825c2b9d56dca695922ac43ce3323,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.2.136.4,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Dec  9 06:58:52.509: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-9618" for this suite. 12/09/22 06:58:52.516
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] EmptyDir wrapper volumes
  should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 06:58:52.531
Dec  9 06:58:52.531: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename emptydir-wrapper 12/09/22 06:58:52.532
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 06:58:52.56
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 06:58:52.567
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67
Dec  9 06:58:52.602: INFO: Waiting up to 5m0s for pod "pod-secrets-df926745-bf99-4b7d-9485-f9bfbeac543b" in namespace "emptydir-wrapper-7732" to be "running and ready"
Dec  9 06:58:52.606: INFO: Pod "pod-secrets-df926745-bf99-4b7d-9485-f9bfbeac543b": Phase="Pending", Reason="", readiness=false. Elapsed: 3.394928ms
Dec  9 06:58:52.606: INFO: The phase of Pod pod-secrets-df926745-bf99-4b7d-9485-f9bfbeac543b is Pending, waiting for it to be Running (with Ready = true)
Dec  9 06:58:54.609: INFO: Pod "pod-secrets-df926745-bf99-4b7d-9485-f9bfbeac543b": Phase="Running", Reason="", readiness=true. Elapsed: 2.006996824s
Dec  9 06:58:54.609: INFO: The phase of Pod pod-secrets-df926745-bf99-4b7d-9485-f9bfbeac543b is Running (Ready = true)
Dec  9 06:58:54.609: INFO: Pod "pod-secrets-df926745-bf99-4b7d-9485-f9bfbeac543b" satisfied condition "running and ready"
STEP: Cleaning up the secret 12/09/22 06:58:54.612
STEP: Cleaning up the configmap 12/09/22 06:58:54.619
STEP: Cleaning up the pod 12/09/22 06:58:54.626
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/node/init/init.go:32
Dec  9 06:58:54.638: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-wrapper-7732" for this suite. 12/09/22 06:58:54.643
------------------------------
â€¢ [2.117 seconds]
[sig-storage] EmptyDir wrapper volumes
test/e2e/storage/utils/framework.go:23
  should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 06:58:52.531
    Dec  9 06:58:52.531: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename emptydir-wrapper 12/09/22 06:58:52.532
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 06:58:52.56
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 06:58:52.567
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should not conflict [Conformance]
      test/e2e/storage/empty_dir_wrapper.go:67
    Dec  9 06:58:52.602: INFO: Waiting up to 5m0s for pod "pod-secrets-df926745-bf99-4b7d-9485-f9bfbeac543b" in namespace "emptydir-wrapper-7732" to be "running and ready"
    Dec  9 06:58:52.606: INFO: Pod "pod-secrets-df926745-bf99-4b7d-9485-f9bfbeac543b": Phase="Pending", Reason="", readiness=false. Elapsed: 3.394928ms
    Dec  9 06:58:52.606: INFO: The phase of Pod pod-secrets-df926745-bf99-4b7d-9485-f9bfbeac543b is Pending, waiting for it to be Running (with Ready = true)
    Dec  9 06:58:54.609: INFO: Pod "pod-secrets-df926745-bf99-4b7d-9485-f9bfbeac543b": Phase="Running", Reason="", readiness=true. Elapsed: 2.006996824s
    Dec  9 06:58:54.609: INFO: The phase of Pod pod-secrets-df926745-bf99-4b7d-9485-f9bfbeac543b is Running (Ready = true)
    Dec  9 06:58:54.609: INFO: Pod "pod-secrets-df926745-bf99-4b7d-9485-f9bfbeac543b" satisfied condition "running and ready"
    STEP: Cleaning up the secret 12/09/22 06:58:54.612
    STEP: Cleaning up the configmap 12/09/22 06:58:54.619
    STEP: Cleaning up the pod 12/09/22 06:58:54.626
    [AfterEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/node/init/init.go:32
    Dec  9 06:58:54.638: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-wrapper-7732" for this suite. 12/09/22 06:58:54.643
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:297
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 06:58:54.651
Dec  9 06:58:54.651: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename var-expansion 12/09/22 06:58:54.652
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 06:58:54.667
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 06:58:54.672
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:297
STEP: creating the pod 12/09/22 06:58:54.675
STEP: waiting for pod running 12/09/22 06:58:54.685
Dec  9 06:58:54.685: INFO: Waiting up to 2m0s for pod "var-expansion-c08c5c8e-29e3-4712-a554-7d56bfcb5d87" in namespace "var-expansion-2949" to be "running"
Dec  9 06:58:54.697: INFO: Pod "var-expansion-c08c5c8e-29e3-4712-a554-7d56bfcb5d87": Phase="Pending", Reason="", readiness=false. Elapsed: 12.225886ms
Dec  9 06:58:56.701: INFO: Pod "var-expansion-c08c5c8e-29e3-4712-a554-7d56bfcb5d87": Phase="Running", Reason="", readiness=true. Elapsed: 2.016434124s
Dec  9 06:58:56.701: INFO: Pod "var-expansion-c08c5c8e-29e3-4712-a554-7d56bfcb5d87" satisfied condition "running"
STEP: creating a file in subpath 12/09/22 06:58:56.701
Dec  9 06:58:56.704: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-2949 PodName:var-expansion-c08c5c8e-29e3-4712-a554-7d56bfcb5d87 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec  9 06:58:56.704: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
Dec  9 06:58:56.705: INFO: ExecWithOptions: Clientset creation
Dec  9 06:58:56.705: INFO: ExecWithOptions: execute(POST https://10.3.0.1:443/api/v1/namespaces/var-expansion-2949/pods/var-expansion-c08c5c8e-29e3-4712-a554-7d56bfcb5d87/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
STEP: test for file in mounted path 12/09/22 06:58:56.779
Dec  9 06:58:56.782: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-2949 PodName:var-expansion-c08c5c8e-29e3-4712-a554-7d56bfcb5d87 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec  9 06:58:56.782: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
Dec  9 06:58:56.783: INFO: ExecWithOptions: Clientset creation
Dec  9 06:58:56.783: INFO: ExecWithOptions: execute(POST https://10.3.0.1:443/api/v1/namespaces/var-expansion-2949/pods/var-expansion-c08c5c8e-29e3-4712-a554-7d56bfcb5d87/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
STEP: updating the annotation value 12/09/22 06:58:56.858
Dec  9 06:58:57.372: INFO: Successfully updated pod "var-expansion-c08c5c8e-29e3-4712-a554-7d56bfcb5d87"
STEP: waiting for annotated pod running 12/09/22 06:58:57.372
Dec  9 06:58:57.372: INFO: Waiting up to 2m0s for pod "var-expansion-c08c5c8e-29e3-4712-a554-7d56bfcb5d87" in namespace "var-expansion-2949" to be "running"
Dec  9 06:58:57.375: INFO: Pod "var-expansion-c08c5c8e-29e3-4712-a554-7d56bfcb5d87": Phase="Running", Reason="", readiness=true. Elapsed: 3.053892ms
Dec  9 06:58:57.375: INFO: Pod "var-expansion-c08c5c8e-29e3-4712-a554-7d56bfcb5d87" satisfied condition "running"
STEP: deleting the pod gracefully 12/09/22 06:58:57.375
Dec  9 06:58:57.375: INFO: Deleting pod "var-expansion-c08c5c8e-29e3-4712-a554-7d56bfcb5d87" in namespace "var-expansion-2949"
Dec  9 06:58:57.381: INFO: Wait up to 5m0s for pod "var-expansion-c08c5c8e-29e3-4712-a554-7d56bfcb5d87" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Dec  9 06:59:31.389: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-2949" for this suite. 12/09/22 06:59:31.393
------------------------------
â€¢ [SLOW TEST] [36.747 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:297

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 06:58:54.651
    Dec  9 06:58:54.651: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename var-expansion 12/09/22 06:58:54.652
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 06:58:54.667
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 06:58:54.672
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should succeed in writing subpaths in container [Slow] [Conformance]
      test/e2e/common/node/expansion.go:297
    STEP: creating the pod 12/09/22 06:58:54.675
    STEP: waiting for pod running 12/09/22 06:58:54.685
    Dec  9 06:58:54.685: INFO: Waiting up to 2m0s for pod "var-expansion-c08c5c8e-29e3-4712-a554-7d56bfcb5d87" in namespace "var-expansion-2949" to be "running"
    Dec  9 06:58:54.697: INFO: Pod "var-expansion-c08c5c8e-29e3-4712-a554-7d56bfcb5d87": Phase="Pending", Reason="", readiness=false. Elapsed: 12.225886ms
    Dec  9 06:58:56.701: INFO: Pod "var-expansion-c08c5c8e-29e3-4712-a554-7d56bfcb5d87": Phase="Running", Reason="", readiness=true. Elapsed: 2.016434124s
    Dec  9 06:58:56.701: INFO: Pod "var-expansion-c08c5c8e-29e3-4712-a554-7d56bfcb5d87" satisfied condition "running"
    STEP: creating a file in subpath 12/09/22 06:58:56.701
    Dec  9 06:58:56.704: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-2949 PodName:var-expansion-c08c5c8e-29e3-4712-a554-7d56bfcb5d87 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Dec  9 06:58:56.704: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    Dec  9 06:58:56.705: INFO: ExecWithOptions: Clientset creation
    Dec  9 06:58:56.705: INFO: ExecWithOptions: execute(POST https://10.3.0.1:443/api/v1/namespaces/var-expansion-2949/pods/var-expansion-c08c5c8e-29e3-4712-a554-7d56bfcb5d87/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
    STEP: test for file in mounted path 12/09/22 06:58:56.779
    Dec  9 06:58:56.782: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-2949 PodName:var-expansion-c08c5c8e-29e3-4712-a554-7d56bfcb5d87 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Dec  9 06:58:56.782: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    Dec  9 06:58:56.783: INFO: ExecWithOptions: Clientset creation
    Dec  9 06:58:56.783: INFO: ExecWithOptions: execute(POST https://10.3.0.1:443/api/v1/namespaces/var-expansion-2949/pods/var-expansion-c08c5c8e-29e3-4712-a554-7d56bfcb5d87/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
    STEP: updating the annotation value 12/09/22 06:58:56.858
    Dec  9 06:58:57.372: INFO: Successfully updated pod "var-expansion-c08c5c8e-29e3-4712-a554-7d56bfcb5d87"
    STEP: waiting for annotated pod running 12/09/22 06:58:57.372
    Dec  9 06:58:57.372: INFO: Waiting up to 2m0s for pod "var-expansion-c08c5c8e-29e3-4712-a554-7d56bfcb5d87" in namespace "var-expansion-2949" to be "running"
    Dec  9 06:58:57.375: INFO: Pod "var-expansion-c08c5c8e-29e3-4712-a554-7d56bfcb5d87": Phase="Running", Reason="", readiness=true. Elapsed: 3.053892ms
    Dec  9 06:58:57.375: INFO: Pod "var-expansion-c08c5c8e-29e3-4712-a554-7d56bfcb5d87" satisfied condition "running"
    STEP: deleting the pod gracefully 12/09/22 06:58:57.375
    Dec  9 06:58:57.375: INFO: Deleting pod "var-expansion-c08c5c8e-29e3-4712-a554-7d56bfcb5d87" in namespace "var-expansion-2949"
    Dec  9 06:58:57.381: INFO: Wait up to 5m0s for pod "var-expansion-c08c5c8e-29e3-4712-a554-7d56bfcb5d87" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Dec  9 06:59:31.389: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-2949" for this suite. 12/09/22 06:59:31.393
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 06:59:31.4
Dec  9 06:59:31.400: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename replicaset 12/09/22 06:59:31.401
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 06:59:31.427
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 06:59:31.435
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165
STEP: Create a ReplicaSet 12/09/22 06:59:31.439
STEP: Verify that the required pods have come up 12/09/22 06:59:31.444
Dec  9 06:59:31.449: INFO: Pod name sample-pod: Found 0 pods out of 3
Dec  9 06:59:36.454: INFO: Pod name sample-pod: Found 3 pods out of 3
STEP: ensuring each pod is running 12/09/22 06:59:36.454
Dec  9 06:59:36.454: INFO: Waiting up to 5m0s for pod "test-rs-nt42h" in namespace "replicaset-4719" to be "running"
Dec  9 06:59:36.458: INFO: Pod "test-rs-nt42h": Phase="Pending", Reason="", readiness=false. Elapsed: 3.127348ms
Dec  9 06:59:38.461: INFO: Pod "test-rs-nt42h": Phase="Running", Reason="", readiness=true. Elapsed: 2.00661471s
Dec  9 06:59:38.461: INFO: Pod "test-rs-nt42h" satisfied condition "running"
Dec  9 06:59:38.464: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
STEP: Listing all ReplicaSets 12/09/22 06:59:38.464
STEP: DeleteCollection of the ReplicaSets 12/09/22 06:59:38.469
STEP: After DeleteCollection verify that ReplicaSets have been deleted 12/09/22 06:59:38.479
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Dec  9 06:59:38.492: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-4719" for this suite. 12/09/22 06:59:38.499
------------------------------
â€¢ [SLOW TEST] [7.121 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 06:59:31.4
    Dec  9 06:59:31.400: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename replicaset 12/09/22 06:59:31.401
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 06:59:31.427
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 06:59:31.435
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] should list and delete a collection of ReplicaSets [Conformance]
      test/e2e/apps/replica_set.go:165
    STEP: Create a ReplicaSet 12/09/22 06:59:31.439
    STEP: Verify that the required pods have come up 12/09/22 06:59:31.444
    Dec  9 06:59:31.449: INFO: Pod name sample-pod: Found 0 pods out of 3
    Dec  9 06:59:36.454: INFO: Pod name sample-pod: Found 3 pods out of 3
    STEP: ensuring each pod is running 12/09/22 06:59:36.454
    Dec  9 06:59:36.454: INFO: Waiting up to 5m0s for pod "test-rs-nt42h" in namespace "replicaset-4719" to be "running"
    Dec  9 06:59:36.458: INFO: Pod "test-rs-nt42h": Phase="Pending", Reason="", readiness=false. Elapsed: 3.127348ms
    Dec  9 06:59:38.461: INFO: Pod "test-rs-nt42h": Phase="Running", Reason="", readiness=true. Elapsed: 2.00661471s
    Dec  9 06:59:38.461: INFO: Pod "test-rs-nt42h" satisfied condition "running"
    Dec  9 06:59:38.464: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
    STEP: Listing all ReplicaSets 12/09/22 06:59:38.464
    STEP: DeleteCollection of the ReplicaSets 12/09/22 06:59:38.469
    STEP: After DeleteCollection verify that ReplicaSets have been deleted 12/09/22 06:59:38.479
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Dec  9 06:59:38.492: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-4719" for this suite. 12/09/22 06:59:38.499
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] Garbage collector
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 06:59:38.52
Dec  9 06:59:38.520: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename gc 12/09/22 06:59:38.522
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 06:59:38.641
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 06:59:38.656
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550
STEP: create the deployment 12/09/22 06:59:38.666
STEP: Wait for the Deployment to create new ReplicaSet 12/09/22 06:59:38.672
STEP: delete the deployment 12/09/22 06:59:39.199
STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs 12/09/22 06:59:39.209
STEP: Gathering metrics 12/09/22 06:59:39.729
Dec  9 06:59:39.779: INFO: Waiting up to 5m0s for pod "kube-controller-manager-ip-10-0-7-84" in namespace "kube-system" to be "running and ready"
Dec  9 06:59:39.782: INFO: Pod "kube-controller-manager-ip-10-0-7-84": Phase="Running", Reason="", readiness=true. Elapsed: 2.910417ms
Dec  9 06:59:39.782: INFO: The phase of Pod kube-controller-manager-ip-10-0-7-84 is Running (Ready = true)
Dec  9 06:59:39.782: INFO: Pod "kube-controller-manager-ip-10-0-7-84" satisfied condition "running and ready"
Dec  9 06:59:39.911: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Dec  9 06:59:39.911: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-6781" for this suite. 12/09/22 06:59:39.914
------------------------------
â€¢ [1.399 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 06:59:38.52
    Dec  9 06:59:38.520: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename gc 12/09/22 06:59:38.522
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 06:59:38.641
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 06:59:38.656
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
      test/e2e/apimachinery/garbage_collector.go:550
    STEP: create the deployment 12/09/22 06:59:38.666
    STEP: Wait for the Deployment to create new ReplicaSet 12/09/22 06:59:38.672
    STEP: delete the deployment 12/09/22 06:59:39.199
    STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs 12/09/22 06:59:39.209
    STEP: Gathering metrics 12/09/22 06:59:39.729
    Dec  9 06:59:39.779: INFO: Waiting up to 5m0s for pod "kube-controller-manager-ip-10-0-7-84" in namespace "kube-system" to be "running and ready"
    Dec  9 06:59:39.782: INFO: Pod "kube-controller-manager-ip-10-0-7-84": Phase="Running", Reason="", readiness=true. Elapsed: 2.910417ms
    Dec  9 06:59:39.782: INFO: The phase of Pod kube-controller-manager-ip-10-0-7-84 is Running (Ready = true)
    Dec  9 06:59:39.782: INFO: Pod "kube-controller-manager-ip-10-0-7-84" satisfied condition "running and ready"
    Dec  9 06:59:39.911: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Dec  9 06:59:39.911: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-6781" for this suite. 12/09/22 06:59:39.914
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98
[BeforeEach] [sig-instrumentation] Events API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 06:59:39.923
Dec  9 06:59:39.923: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename events 12/09/22 06:59:39.924
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 06:59:39.94
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 06:59:39.944
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/instrumentation/events.go:84
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98
STEP: creating a test event 12/09/22 06:59:39.949
STEP: listing events in all namespaces 12/09/22 06:59:39.954
STEP: listing events in test namespace 12/09/22 06:59:39.972
STEP: listing events with field selection filtering on source 12/09/22 06:59:39.975
STEP: listing events with field selection filtering on reportingController 12/09/22 06:59:39.978
STEP: getting the test event 12/09/22 06:59:39.981
STEP: patching the test event 12/09/22 06:59:39.983
STEP: getting the test event 12/09/22 06:59:39.99
STEP: updating the test event 12/09/22 06:59:39.993
STEP: getting the test event 12/09/22 06:59:40.003
STEP: deleting the test event 12/09/22 06:59:40.006
STEP: listing events in all namespaces 12/09/22 06:59:40.011
STEP: listing events in test namespace 12/09/22 06:59:40.016
[AfterEach] [sig-instrumentation] Events API
  test/e2e/framework/node/init/init.go:32
Dec  9 06:59:40.022: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-instrumentation] Events API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-instrumentation] Events API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-instrumentation] Events API
  tear down framework | framework.go:193
STEP: Destroying namespace "events-9355" for this suite. 12/09/22 06:59:40.026
------------------------------
â€¢ [0.108 seconds]
[sig-instrumentation] Events API
test/e2e/instrumentation/common/framework.go:23
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 06:59:39.923
    Dec  9 06:59:39.923: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename events 12/09/22 06:59:39.924
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 06:59:39.94
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 06:59:39.944
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/instrumentation/events.go:84
    [It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
      test/e2e/instrumentation/events.go:98
    STEP: creating a test event 12/09/22 06:59:39.949
    STEP: listing events in all namespaces 12/09/22 06:59:39.954
    STEP: listing events in test namespace 12/09/22 06:59:39.972
    STEP: listing events with field selection filtering on source 12/09/22 06:59:39.975
    STEP: listing events with field selection filtering on reportingController 12/09/22 06:59:39.978
    STEP: getting the test event 12/09/22 06:59:39.981
    STEP: patching the test event 12/09/22 06:59:39.983
    STEP: getting the test event 12/09/22 06:59:39.99
    STEP: updating the test event 12/09/22 06:59:39.993
    STEP: getting the test event 12/09/22 06:59:40.003
    STEP: deleting the test event 12/09/22 06:59:40.006
    STEP: listing events in all namespaces 12/09/22 06:59:40.011
    STEP: listing events in test namespace 12/09/22 06:59:40.016
    [AfterEach] [sig-instrumentation] Events API
      test/e2e/framework/node/init/init.go:32
    Dec  9 06:59:40.022: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      tear down framework | framework.go:193
    STEP: Destroying namespace "events-9355" for this suite. 12/09/22 06:59:40.026
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 06:59:40.034
Dec  9 06:59:40.034: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename watch 12/09/22 06:59:40.035
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 06:59:40.05
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 06:59:40.057
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191
STEP: creating a watch on configmaps 12/09/22 06:59:40.068
STEP: creating a new configmap 12/09/22 06:59:40.07
STEP: modifying the configmap once 12/09/22 06:59:40.081
STEP: closing the watch once it receives two notifications 12/09/22 06:59:40.097
Dec  9 06:59:40.097: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-149  e11ca799-052d-4e5f-b06c-e0e48f549e8f 1298 0 2022-12-09 06:59:40 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2022-12-09 06:59:40 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Dec  9 06:59:40.098: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-149  e11ca799-052d-4e5f-b06c-e0e48f549e8f 1299 0 2022-12-09 06:59:40 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2022-12-09 06:59:40 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time, while the watch is closed 12/09/22 06:59:40.098
STEP: creating a new watch on configmaps from the last resource version observed by the first watch 12/09/22 06:59:40.108
STEP: deleting the configmap 12/09/22 06:59:40.11
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed 12/09/22 06:59:40.117
Dec  9 06:59:40.117: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-149  e11ca799-052d-4e5f-b06c-e0e48f549e8f 1300 0 2022-12-09 06:59:40 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2022-12-09 06:59:40 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Dec  9 06:59:40.120: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-149  e11ca799-052d-4e5f-b06c-e0e48f549e8f 1301 0 2022-12-09 06:59:40 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2022-12-09 06:59:40 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Dec  9 06:59:40.120: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-149" for this suite. 12/09/22 06:59:40.128
------------------------------
â€¢ [0.100 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 06:59:40.034
    Dec  9 06:59:40.034: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename watch 12/09/22 06:59:40.035
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 06:59:40.05
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 06:59:40.057
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
      test/e2e/apimachinery/watch.go:191
    STEP: creating a watch on configmaps 12/09/22 06:59:40.068
    STEP: creating a new configmap 12/09/22 06:59:40.07
    STEP: modifying the configmap once 12/09/22 06:59:40.081
    STEP: closing the watch once it receives two notifications 12/09/22 06:59:40.097
    Dec  9 06:59:40.097: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-149  e11ca799-052d-4e5f-b06c-e0e48f549e8f 1298 0 2022-12-09 06:59:40 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2022-12-09 06:59:40 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Dec  9 06:59:40.098: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-149  e11ca799-052d-4e5f-b06c-e0e48f549e8f 1299 0 2022-12-09 06:59:40 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2022-12-09 06:59:40 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying the configmap a second time, while the watch is closed 12/09/22 06:59:40.098
    STEP: creating a new watch on configmaps from the last resource version observed by the first watch 12/09/22 06:59:40.108
    STEP: deleting the configmap 12/09/22 06:59:40.11
    STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed 12/09/22 06:59:40.117
    Dec  9 06:59:40.117: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-149  e11ca799-052d-4e5f-b06c-e0e48f549e8f 1300 0 2022-12-09 06:59:40 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2022-12-09 06:59:40 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Dec  9 06:59:40.120: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-149  e11ca799-052d-4e5f-b06c-e0e48f549e8f 1301 0 2022-12-09 06:59:40 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2022-12-09 06:59:40 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Dec  9 06:59:40.120: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-149" for this suite. 12/09/22 06:59:40.128
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  test/e2e/apps/statefulset.go:587
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 06:59:40.137
Dec  9 06:59:40.141: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename statefulset 12/09/22 06:59:40.143
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 06:59:40.162
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 06:59:40.167
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-6313 12/09/22 06:59:40.172
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  test/e2e/apps/statefulset.go:587
STEP: Initializing watcher for selector baz=blah,foo=bar 12/09/22 06:59:40.181
STEP: Creating stateful set ss in namespace statefulset-6313 12/09/22 06:59:40.19
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-6313 12/09/22 06:59:40.205
Dec  9 06:59:40.215: INFO: Found 0 stateful pods, waiting for 1
Dec  9 06:59:50.220: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod 12/09/22 06:59:50.22
Dec  9 06:59:50.224: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=statefulset-6313 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Dec  9 06:59:50.743: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Dec  9 06:59:50.743: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Dec  9 06:59:50.743: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Dec  9 06:59:50.746: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Dec  9 07:00:00.751: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Dec  9 07:00:00.751: INFO: Waiting for statefulset status.replicas updated to 0
Dec  9 07:00:00.773: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999699s
Dec  9 07:00:01.815: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.991143869s
Dec  9 07:00:02.818: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.949638959s
Dec  9 07:00:03.822: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.946275233s
Dec  9 07:00:04.827: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.941626735s
Dec  9 07:00:05.833: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.936746766s
Dec  9 07:00:06.837: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.931727424s
Dec  9 07:00:07.841: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.926687141s
Dec  9 07:00:08.855: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.922998415s
Dec  9 07:00:09.859: INFO: Verifying statefulset ss doesn't scale past 1 for another 908.575236ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-6313 12/09/22 07:00:10.86
Dec  9 07:00:10.865: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=statefulset-6313 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec  9 07:00:11.015: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Dec  9 07:00:11.015: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Dec  9 07:00:11.015: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Dec  9 07:00:11.019: INFO: Found 1 stateful pods, waiting for 3
Dec  9 07:00:21.023: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Dec  9 07:00:21.023: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Dec  9 07:00:21.023: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order 12/09/22 07:00:21.023
STEP: Scale down will halt with unhealthy stateful pod 12/09/22 07:00:21.024
Dec  9 07:00:21.034: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=statefulset-6313 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Dec  9 07:00:21.236: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Dec  9 07:00:21.236: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Dec  9 07:00:21.236: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Dec  9 07:00:21.236: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=statefulset-6313 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Dec  9 07:00:21.450: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Dec  9 07:00:21.450: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Dec  9 07:00:21.450: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Dec  9 07:00:21.450: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=statefulset-6313 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Dec  9 07:00:21.637: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Dec  9 07:00:21.637: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Dec  9 07:00:21.637: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Dec  9 07:00:21.637: INFO: Waiting for statefulset status.replicas updated to 0
Dec  9 07:00:21.640: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Dec  9 07:00:31.657: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Dec  9 07:00:31.657: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Dec  9 07:00:31.657: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Dec  9 07:00:31.702: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999721s
Dec  9 07:00:32.707: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.97267489s
Dec  9 07:00:33.714: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.968378865s
Dec  9 07:00:34.718: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.960664556s
Dec  9 07:00:35.723: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.956344275s
Dec  9 07:00:36.734: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.947171945s
Dec  9 07:00:37.738: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.941274783s
Dec  9 07:00:38.741: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.93723368s
Dec  9 07:00:39.746: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.932695237s
Dec  9 07:00:40.752: INFO: Verifying statefulset ss doesn't scale past 3 for another 927.982375ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-6313 12/09/22 07:00:41.752
Dec  9 07:00:41.758: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=statefulset-6313 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec  9 07:00:41.929: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Dec  9 07:00:41.929: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Dec  9 07:00:41.929: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Dec  9 07:00:41.929: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=statefulset-6313 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec  9 07:00:42.206: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Dec  9 07:00:42.206: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Dec  9 07:00:42.206: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Dec  9 07:00:42.206: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=statefulset-6313 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec  9 07:00:42.384: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Dec  9 07:00:42.384: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Dec  9 07:00:42.384: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Dec  9 07:00:42.384: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order 12/09/22 07:00:52.398
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Dec  9 07:00:52.399: INFO: Deleting all statefulset in ns statefulset-6313
Dec  9 07:00:52.402: INFO: Scaling statefulset ss to 0
Dec  9 07:00:52.419: INFO: Waiting for statefulset status.replicas updated to 0
Dec  9 07:00:52.422: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Dec  9 07:00:52.488: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-6313" for this suite. 12/09/22 07:00:52.499
------------------------------
â€¢ [SLOW TEST] [72.377 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    test/e2e/apps/statefulset.go:587

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 06:59:40.137
    Dec  9 06:59:40.141: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename statefulset 12/09/22 06:59:40.143
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 06:59:40.162
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 06:59:40.167
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-6313 12/09/22 06:59:40.172
    [It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
      test/e2e/apps/statefulset.go:587
    STEP: Initializing watcher for selector baz=blah,foo=bar 12/09/22 06:59:40.181
    STEP: Creating stateful set ss in namespace statefulset-6313 12/09/22 06:59:40.19
    STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-6313 12/09/22 06:59:40.205
    Dec  9 06:59:40.215: INFO: Found 0 stateful pods, waiting for 1
    Dec  9 06:59:50.220: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod 12/09/22 06:59:50.22
    Dec  9 06:59:50.224: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=statefulset-6313 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Dec  9 06:59:50.743: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Dec  9 06:59:50.743: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Dec  9 06:59:50.743: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Dec  9 06:59:50.746: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
    Dec  9 07:00:00.751: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Dec  9 07:00:00.751: INFO: Waiting for statefulset status.replicas updated to 0
    Dec  9 07:00:00.773: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999699s
    Dec  9 07:00:01.815: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.991143869s
    Dec  9 07:00:02.818: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.949638959s
    Dec  9 07:00:03.822: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.946275233s
    Dec  9 07:00:04.827: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.941626735s
    Dec  9 07:00:05.833: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.936746766s
    Dec  9 07:00:06.837: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.931727424s
    Dec  9 07:00:07.841: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.926687141s
    Dec  9 07:00:08.855: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.922998415s
    Dec  9 07:00:09.859: INFO: Verifying statefulset ss doesn't scale past 1 for another 908.575236ms
    STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-6313 12/09/22 07:00:10.86
    Dec  9 07:00:10.865: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=statefulset-6313 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Dec  9 07:00:11.015: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Dec  9 07:00:11.015: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Dec  9 07:00:11.015: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Dec  9 07:00:11.019: INFO: Found 1 stateful pods, waiting for 3
    Dec  9 07:00:21.023: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Dec  9 07:00:21.023: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
    Dec  9 07:00:21.023: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Verifying that stateful set ss was scaled up in order 12/09/22 07:00:21.023
    STEP: Scale down will halt with unhealthy stateful pod 12/09/22 07:00:21.024
    Dec  9 07:00:21.034: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=statefulset-6313 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Dec  9 07:00:21.236: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Dec  9 07:00:21.236: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Dec  9 07:00:21.236: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Dec  9 07:00:21.236: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=statefulset-6313 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Dec  9 07:00:21.450: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Dec  9 07:00:21.450: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Dec  9 07:00:21.450: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Dec  9 07:00:21.450: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=statefulset-6313 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Dec  9 07:00:21.637: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Dec  9 07:00:21.637: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Dec  9 07:00:21.637: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Dec  9 07:00:21.637: INFO: Waiting for statefulset status.replicas updated to 0
    Dec  9 07:00:21.640: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
    Dec  9 07:00:31.657: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Dec  9 07:00:31.657: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
    Dec  9 07:00:31.657: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
    Dec  9 07:00:31.702: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999721s
    Dec  9 07:00:32.707: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.97267489s
    Dec  9 07:00:33.714: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.968378865s
    Dec  9 07:00:34.718: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.960664556s
    Dec  9 07:00:35.723: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.956344275s
    Dec  9 07:00:36.734: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.947171945s
    Dec  9 07:00:37.738: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.941274783s
    Dec  9 07:00:38.741: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.93723368s
    Dec  9 07:00:39.746: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.932695237s
    Dec  9 07:00:40.752: INFO: Verifying statefulset ss doesn't scale past 3 for another 927.982375ms
    STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-6313 12/09/22 07:00:41.752
    Dec  9 07:00:41.758: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=statefulset-6313 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Dec  9 07:00:41.929: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Dec  9 07:00:41.929: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Dec  9 07:00:41.929: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Dec  9 07:00:41.929: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=statefulset-6313 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Dec  9 07:00:42.206: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Dec  9 07:00:42.206: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Dec  9 07:00:42.206: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Dec  9 07:00:42.206: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=statefulset-6313 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Dec  9 07:00:42.384: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Dec  9 07:00:42.384: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Dec  9 07:00:42.384: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Dec  9 07:00:42.384: INFO: Scaling statefulset ss to 0
    STEP: Verifying that stateful set ss was scaled down in reverse order 12/09/22 07:00:52.398
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Dec  9 07:00:52.399: INFO: Deleting all statefulset in ns statefulset-6313
    Dec  9 07:00:52.402: INFO: Scaling statefulset ss to 0
    Dec  9 07:00:52.419: INFO: Waiting for statefulset status.replicas updated to 0
    Dec  9 07:00:52.422: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:00:52.488: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-6313" for this suite. 12/09/22 07:00:52.499
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute poststart http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:167
[BeforeEach] [sig-node] Container Lifecycle Hook
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:00:52.522
Dec  9 07:00:52.522: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename container-lifecycle-hook 12/09/22 07:00:52.524
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:00:52.545
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:00:52.55
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:77
STEP: create the container to handle the HTTPGet hook request. 12/09/22 07:00:52.557
Dec  9 07:00:52.566: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-4570" to be "running and ready"
Dec  9 07:00:52.573: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 6.969972ms
Dec  9 07:00:52.573: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Dec  9 07:00:54.578: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.011422228s
Dec  9 07:00:54.578: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Dec  9 07:00:54.578: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:167
STEP: create the pod with lifecycle hook 12/09/22 07:00:54.581
Dec  9 07:00:54.587: INFO: Waiting up to 5m0s for pod "pod-with-poststart-http-hook" in namespace "container-lifecycle-hook-4570" to be "running and ready"
Dec  9 07:00:54.589: INFO: Pod "pod-with-poststart-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 2.657243ms
Dec  9 07:00:54.590: INFO: The phase of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
Dec  9 07:00:56.601: INFO: Pod "pod-with-poststart-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014534434s
Dec  9 07:00:56.602: INFO: The phase of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
Dec  9 07:00:58.597: INFO: Pod "pod-with-poststart-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 4.009699702s
Dec  9 07:00:58.597: INFO: The phase of Pod pod-with-poststart-http-hook is Running (Ready = true)
Dec  9 07:00:58.597: INFO: Pod "pod-with-poststart-http-hook" satisfied condition "running and ready"
STEP: check poststart hook 12/09/22 07:00:58.6
STEP: delete the pod with lifecycle hook 12/09/22 07:00:58.613
Dec  9 07:00:58.619: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Dec  9 07:00:58.625: INFO: Pod pod-with-poststart-http-hook still exists
Dec  9 07:01:00.625: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Dec  9 07:01:00.629: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/node/init/init.go:32
Dec  9 07:01:00.629: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  tear down framework | framework.go:193
STEP: Destroying namespace "container-lifecycle-hook-4570" for this suite. 12/09/22 07:01:00.633
------------------------------
â€¢ [SLOW TEST] [8.118 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute poststart http hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:167

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:00:52.522
    Dec  9 07:00:52.522: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename container-lifecycle-hook 12/09/22 07:00:52.524
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:00:52.545
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:00:52.55
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:77
    STEP: create the container to handle the HTTPGet hook request. 12/09/22 07:00:52.557
    Dec  9 07:00:52.566: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-4570" to be "running and ready"
    Dec  9 07:00:52.573: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 6.969972ms
    Dec  9 07:00:52.573: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Dec  9 07:00:54.578: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.011422228s
    Dec  9 07:00:54.578: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Dec  9 07:00:54.578: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute poststart http hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:167
    STEP: create the pod with lifecycle hook 12/09/22 07:00:54.581
    Dec  9 07:00:54.587: INFO: Waiting up to 5m0s for pod "pod-with-poststart-http-hook" in namespace "container-lifecycle-hook-4570" to be "running and ready"
    Dec  9 07:00:54.589: INFO: Pod "pod-with-poststart-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 2.657243ms
    Dec  9 07:00:54.590: INFO: The phase of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
    Dec  9 07:00:56.601: INFO: Pod "pod-with-poststart-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014534434s
    Dec  9 07:00:56.602: INFO: The phase of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
    Dec  9 07:00:58.597: INFO: Pod "pod-with-poststart-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 4.009699702s
    Dec  9 07:00:58.597: INFO: The phase of Pod pod-with-poststart-http-hook is Running (Ready = true)
    Dec  9 07:00:58.597: INFO: Pod "pod-with-poststart-http-hook" satisfied condition "running and ready"
    STEP: check poststart hook 12/09/22 07:00:58.6
    STEP: delete the pod with lifecycle hook 12/09/22 07:00:58.613
    Dec  9 07:00:58.619: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Dec  9 07:00:58.625: INFO: Pod pod-with-poststart-http-hook still exists
    Dec  9 07:01:00.625: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Dec  9 07:01:00.629: INFO: Pod pod-with-poststart-http-hook no longer exists
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:01:00.629: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-lifecycle-hook-4570" for this suite. 12/09/22 07:01:00.633
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:177
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:01:00.641
Dec  9 07:01:00.641: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename init-container 12/09/22 07:01:00.643
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:01:00.659
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:01:00.665
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:165
[It] should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:177
STEP: creating the pod 12/09/22 07:01:00.67
Dec  9 07:01:00.671: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Dec  9 07:01:08.892: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "init-container-324" for this suite. 12/09/22 07:01:08.898
------------------------------
â€¢ [SLOW TEST] [8.267 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:177

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:01:00.641
    Dec  9 07:01:00.641: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename init-container 12/09/22 07:01:00.643
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:01:00.659
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:01:00.665
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:165
    [It] should invoke init containers on a RestartNever pod [Conformance]
      test/e2e/common/node/init_container.go:177
    STEP: creating the pod 12/09/22 07:01:00.67
    Dec  9 07:01:00.671: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:01:08.892: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "init-container-324" for this suite. 12/09/22 07:01:08.898
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:88
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:01:08.912
Dec  9 07:01:08.913: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename projected 12/09/22 07:01:08.916
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:01:08.939
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:01:08.944
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:88
STEP: Creating projection with secret that has name projected-secret-test-map-e397ce97-a154-41da-b38b-6fa598c51401 12/09/22 07:01:08.956
STEP: Creating a pod to test consume secrets 12/09/22 07:01:08.965
Dec  9 07:01:08.980: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-ed3b33cb-e828-48d8-850d-6f14062f5f7c" in namespace "projected-2825" to be "Succeeded or Failed"
Dec  9 07:01:08.985: INFO: Pod "pod-projected-secrets-ed3b33cb-e828-48d8-850d-6f14062f5f7c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.937897ms
Dec  9 07:01:10.989: INFO: Pod "pod-projected-secrets-ed3b33cb-e828-48d8-850d-6f14062f5f7c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009071059s
Dec  9 07:01:12.990: INFO: Pod "pod-projected-secrets-ed3b33cb-e828-48d8-850d-6f14062f5f7c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010125708s
STEP: Saw pod success 12/09/22 07:01:12.99
Dec  9 07:01:12.990: INFO: Pod "pod-projected-secrets-ed3b33cb-e828-48d8-850d-6f14062f5f7c" satisfied condition "Succeeded or Failed"
Dec  9 07:01:12.993: INFO: Trying to get logs from node ip-10-0-10-179 pod pod-projected-secrets-ed3b33cb-e828-48d8-850d-6f14062f5f7c container projected-secret-volume-test: <nil>
STEP: delete the pod 12/09/22 07:01:12.998
Dec  9 07:01:13.011: INFO: Waiting for pod pod-projected-secrets-ed3b33cb-e828-48d8-850d-6f14062f5f7c to disappear
Dec  9 07:01:13.013: INFO: Pod pod-projected-secrets-ed3b33cb-e828-48d8-850d-6f14062f5f7c no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Dec  9 07:01:13.013: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-2825" for this suite. 12/09/22 07:01:13.016
------------------------------
â€¢ [4.109 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:88

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:01:08.912
    Dec  9 07:01:08.913: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename projected 12/09/22 07:01:08.916
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:01:08.939
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:01:08.944
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:88
    STEP: Creating projection with secret that has name projected-secret-test-map-e397ce97-a154-41da-b38b-6fa598c51401 12/09/22 07:01:08.956
    STEP: Creating a pod to test consume secrets 12/09/22 07:01:08.965
    Dec  9 07:01:08.980: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-ed3b33cb-e828-48d8-850d-6f14062f5f7c" in namespace "projected-2825" to be "Succeeded or Failed"
    Dec  9 07:01:08.985: INFO: Pod "pod-projected-secrets-ed3b33cb-e828-48d8-850d-6f14062f5f7c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.937897ms
    Dec  9 07:01:10.989: INFO: Pod "pod-projected-secrets-ed3b33cb-e828-48d8-850d-6f14062f5f7c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009071059s
    Dec  9 07:01:12.990: INFO: Pod "pod-projected-secrets-ed3b33cb-e828-48d8-850d-6f14062f5f7c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010125708s
    STEP: Saw pod success 12/09/22 07:01:12.99
    Dec  9 07:01:12.990: INFO: Pod "pod-projected-secrets-ed3b33cb-e828-48d8-850d-6f14062f5f7c" satisfied condition "Succeeded or Failed"
    Dec  9 07:01:12.993: INFO: Trying to get logs from node ip-10-0-10-179 pod pod-projected-secrets-ed3b33cb-e828-48d8-850d-6f14062f5f7c container projected-secret-volume-test: <nil>
    STEP: delete the pod 12/09/22 07:01:12.998
    Dec  9 07:01:13.011: INFO: Waiting for pod pod-projected-secrets-ed3b33cb-e828-48d8-850d-6f14062f5f7c to disappear
    Dec  9 07:01:13.013: INFO: Pod pod-projected-secrets-ed3b33cb-e828-48d8-850d-6f14062f5f7c no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:01:13.013: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-2825" for this suite. 12/09/22 07:01:13.016
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command in a pod
  should print the output to logs [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:52
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:01:13.022
Dec  9 07:01:13.022: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename kubelet-test 12/09/22 07:01:13.023
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:01:13.04
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:01:13.045
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should print the output to logs [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:52
Dec  9 07:01:13.056: INFO: Waiting up to 5m0s for pod "busybox-scheduling-e4d6c07e-6886-48e1-8233-21ac0846f788" in namespace "kubelet-test-7137" to be "running and ready"
Dec  9 07:01:13.070: INFO: Pod "busybox-scheduling-e4d6c07e-6886-48e1-8233-21ac0846f788": Phase="Pending", Reason="", readiness=false. Elapsed: 13.958046ms
Dec  9 07:01:13.070: INFO: The phase of Pod busybox-scheduling-e4d6c07e-6886-48e1-8233-21ac0846f788 is Pending, waiting for it to be Running (with Ready = true)
Dec  9 07:01:15.074: INFO: Pod "busybox-scheduling-e4d6c07e-6886-48e1-8233-21ac0846f788": Phase="Running", Reason="", readiness=true. Elapsed: 2.01768839s
Dec  9 07:01:15.074: INFO: The phase of Pod busybox-scheduling-e4d6c07e-6886-48e1-8233-21ac0846f788 is Running (Ready = true)
Dec  9 07:01:15.074: INFO: Pod "busybox-scheduling-e4d6c07e-6886-48e1-8233-21ac0846f788" satisfied condition "running and ready"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Dec  9 07:01:15.083: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-7137" for this suite. 12/09/22 07:01:15.087
------------------------------
â€¢ [2.082 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command in a pod
  test/e2e/common/node/kubelet.go:44
    should print the output to logs [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:52

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:01:13.022
    Dec  9 07:01:13.022: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename kubelet-test 12/09/22 07:01:13.023
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:01:13.04
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:01:13.045
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should print the output to logs [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:52
    Dec  9 07:01:13.056: INFO: Waiting up to 5m0s for pod "busybox-scheduling-e4d6c07e-6886-48e1-8233-21ac0846f788" in namespace "kubelet-test-7137" to be "running and ready"
    Dec  9 07:01:13.070: INFO: Pod "busybox-scheduling-e4d6c07e-6886-48e1-8233-21ac0846f788": Phase="Pending", Reason="", readiness=false. Elapsed: 13.958046ms
    Dec  9 07:01:13.070: INFO: The phase of Pod busybox-scheduling-e4d6c07e-6886-48e1-8233-21ac0846f788 is Pending, waiting for it to be Running (with Ready = true)
    Dec  9 07:01:15.074: INFO: Pod "busybox-scheduling-e4d6c07e-6886-48e1-8233-21ac0846f788": Phase="Running", Reason="", readiness=true. Elapsed: 2.01768839s
    Dec  9 07:01:15.074: INFO: The phase of Pod busybox-scheduling-e4d6c07e-6886-48e1-8233-21ac0846f788 is Running (Ready = true)
    Dec  9 07:01:15.074: INFO: Pod "busybox-scheduling-e4d6c07e-6886-48e1-8233-21ac0846f788" satisfied condition "running and ready"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:01:15.083: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-7137" for this suite. 12/09/22 07:01:15.087
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:44
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:01:15.107
Dec  9 07:01:15.108: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename downward-api 12/09/22 07:01:15.11
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:01:15.141
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:01:15.15
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:44
STEP: Creating a pod to test downward api env vars 12/09/22 07:01:15.156
Dec  9 07:01:15.171: INFO: Waiting up to 5m0s for pod "downward-api-be1bf1ba-07d7-4a7f-a548-9879005713f1" in namespace "downward-api-2489" to be "Succeeded or Failed"
Dec  9 07:01:15.175: INFO: Pod "downward-api-be1bf1ba-07d7-4a7f-a548-9879005713f1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.805948ms
Dec  9 07:01:17.179: INFO: Pod "downward-api-be1bf1ba-07d7-4a7f-a548-9879005713f1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008627892s
Dec  9 07:01:19.180: INFO: Pod "downward-api-be1bf1ba-07d7-4a7f-a548-9879005713f1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009081826s
STEP: Saw pod success 12/09/22 07:01:19.18
Dec  9 07:01:19.180: INFO: Pod "downward-api-be1bf1ba-07d7-4a7f-a548-9879005713f1" satisfied condition "Succeeded or Failed"
Dec  9 07:01:19.183: INFO: Trying to get logs from node ip-10-0-10-179 pod downward-api-be1bf1ba-07d7-4a7f-a548-9879005713f1 container dapi-container: <nil>
STEP: delete the pod 12/09/22 07:01:19.189
Dec  9 07:01:19.198: INFO: Waiting for pod downward-api-be1bf1ba-07d7-4a7f-a548-9879005713f1 to disappear
Dec  9 07:01:19.201: INFO: Pod downward-api-be1bf1ba-07d7-4a7f-a548-9879005713f1 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Dec  9 07:01:19.201: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-2489" for this suite. 12/09/22 07:01:19.204
------------------------------
â€¢ [4.106 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:44

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:01:15.107
    Dec  9 07:01:15.108: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename downward-api 12/09/22 07:01:15.11
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:01:15.141
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:01:15.15
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:44
    STEP: Creating a pod to test downward api env vars 12/09/22 07:01:15.156
    Dec  9 07:01:15.171: INFO: Waiting up to 5m0s for pod "downward-api-be1bf1ba-07d7-4a7f-a548-9879005713f1" in namespace "downward-api-2489" to be "Succeeded or Failed"
    Dec  9 07:01:15.175: INFO: Pod "downward-api-be1bf1ba-07d7-4a7f-a548-9879005713f1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.805948ms
    Dec  9 07:01:17.179: INFO: Pod "downward-api-be1bf1ba-07d7-4a7f-a548-9879005713f1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008627892s
    Dec  9 07:01:19.180: INFO: Pod "downward-api-be1bf1ba-07d7-4a7f-a548-9879005713f1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009081826s
    STEP: Saw pod success 12/09/22 07:01:19.18
    Dec  9 07:01:19.180: INFO: Pod "downward-api-be1bf1ba-07d7-4a7f-a548-9879005713f1" satisfied condition "Succeeded or Failed"
    Dec  9 07:01:19.183: INFO: Trying to get logs from node ip-10-0-10-179 pod downward-api-be1bf1ba-07d7-4a7f-a548-9879005713f1 container dapi-container: <nil>
    STEP: delete the pod 12/09/22 07:01:19.189
    Dec  9 07:01:19.198: INFO: Waiting for pod downward-api-be1bf1ba-07d7-4a7f-a548-9879005713f1 to disappear
    Dec  9 07:01:19.201: INFO: Pod downward-api-be1bf1ba-07d7-4a7f-a548-9879005713f1 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:01:19.201: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-2489" for this suite. 12/09/22 07:01:19.204
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Probing container
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:72
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:01:19.213
Dec  9 07:01:19.213: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename container-probe 12/09/22 07:01:19.214
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:01:19.229
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:01:19.233
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:72
Dec  9 07:01:19.245: INFO: Waiting up to 5m0s for pod "test-webserver-30d2f624-9fa4-4264-bca9-14dbafbd7af1" in namespace "container-probe-7960" to be "running and ready"
Dec  9 07:01:19.253: INFO: Pod "test-webserver-30d2f624-9fa4-4264-bca9-14dbafbd7af1": Phase="Pending", Reason="", readiness=false. Elapsed: 8.075723ms
Dec  9 07:01:19.253: INFO: The phase of Pod test-webserver-30d2f624-9fa4-4264-bca9-14dbafbd7af1 is Pending, waiting for it to be Running (with Ready = true)
Dec  9 07:01:21.256: INFO: Pod "test-webserver-30d2f624-9fa4-4264-bca9-14dbafbd7af1": Phase="Running", Reason="", readiness=false. Elapsed: 2.011350115s
Dec  9 07:01:21.256: INFO: The phase of Pod test-webserver-30d2f624-9fa4-4264-bca9-14dbafbd7af1 is Running (Ready = false)
Dec  9 07:01:23.258: INFO: Pod "test-webserver-30d2f624-9fa4-4264-bca9-14dbafbd7af1": Phase="Running", Reason="", readiness=false. Elapsed: 4.012814862s
Dec  9 07:01:23.258: INFO: The phase of Pod test-webserver-30d2f624-9fa4-4264-bca9-14dbafbd7af1 is Running (Ready = false)
Dec  9 07:01:25.257: INFO: Pod "test-webserver-30d2f624-9fa4-4264-bca9-14dbafbd7af1": Phase="Running", Reason="", readiness=false. Elapsed: 6.012665019s
Dec  9 07:01:25.258: INFO: The phase of Pod test-webserver-30d2f624-9fa4-4264-bca9-14dbafbd7af1 is Running (Ready = false)
Dec  9 07:01:27.257: INFO: Pod "test-webserver-30d2f624-9fa4-4264-bca9-14dbafbd7af1": Phase="Running", Reason="", readiness=false. Elapsed: 8.011794864s
Dec  9 07:01:27.257: INFO: The phase of Pod test-webserver-30d2f624-9fa4-4264-bca9-14dbafbd7af1 is Running (Ready = false)
Dec  9 07:01:29.258: INFO: Pod "test-webserver-30d2f624-9fa4-4264-bca9-14dbafbd7af1": Phase="Running", Reason="", readiness=false. Elapsed: 10.012889818s
Dec  9 07:01:29.258: INFO: The phase of Pod test-webserver-30d2f624-9fa4-4264-bca9-14dbafbd7af1 is Running (Ready = false)
Dec  9 07:01:31.258: INFO: Pod "test-webserver-30d2f624-9fa4-4264-bca9-14dbafbd7af1": Phase="Running", Reason="", readiness=false. Elapsed: 12.013312093s
Dec  9 07:01:31.258: INFO: The phase of Pod test-webserver-30d2f624-9fa4-4264-bca9-14dbafbd7af1 is Running (Ready = false)
Dec  9 07:01:33.257: INFO: Pod "test-webserver-30d2f624-9fa4-4264-bca9-14dbafbd7af1": Phase="Running", Reason="", readiness=false. Elapsed: 14.011928447s
Dec  9 07:01:33.257: INFO: The phase of Pod test-webserver-30d2f624-9fa4-4264-bca9-14dbafbd7af1 is Running (Ready = false)
Dec  9 07:01:35.257: INFO: Pod "test-webserver-30d2f624-9fa4-4264-bca9-14dbafbd7af1": Phase="Running", Reason="", readiness=false. Elapsed: 16.012143623s
Dec  9 07:01:35.257: INFO: The phase of Pod test-webserver-30d2f624-9fa4-4264-bca9-14dbafbd7af1 is Running (Ready = false)
Dec  9 07:01:37.256: INFO: Pod "test-webserver-30d2f624-9fa4-4264-bca9-14dbafbd7af1": Phase="Running", Reason="", readiness=false. Elapsed: 18.011591828s
Dec  9 07:01:37.256: INFO: The phase of Pod test-webserver-30d2f624-9fa4-4264-bca9-14dbafbd7af1 is Running (Ready = false)
Dec  9 07:01:39.259: INFO: Pod "test-webserver-30d2f624-9fa4-4264-bca9-14dbafbd7af1": Phase="Running", Reason="", readiness=false. Elapsed: 20.014506171s
Dec  9 07:01:39.259: INFO: The phase of Pod test-webserver-30d2f624-9fa4-4264-bca9-14dbafbd7af1 is Running (Ready = false)
Dec  9 07:01:41.259: INFO: Pod "test-webserver-30d2f624-9fa4-4264-bca9-14dbafbd7af1": Phase="Running", Reason="", readiness=true. Elapsed: 22.01419298s
Dec  9 07:01:41.259: INFO: The phase of Pod test-webserver-30d2f624-9fa4-4264-bca9-14dbafbd7af1 is Running (Ready = true)
Dec  9 07:01:41.259: INFO: Pod "test-webserver-30d2f624-9fa4-4264-bca9-14dbafbd7af1" satisfied condition "running and ready"
Dec  9 07:01:41.266: INFO: Container started at 2022-12-09 07:01:20 +0000 UTC, pod became ready at 2022-12-09 07:01:39 +0000 UTC
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Dec  9 07:01:41.267: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-7960" for this suite. 12/09/22 07:01:41.272
------------------------------
â€¢ [SLOW TEST] [22.069 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:72

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:01:19.213
    Dec  9 07:01:19.213: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename container-probe 12/09/22 07:01:19.214
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:01:19.229
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:01:19.233
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:72
    Dec  9 07:01:19.245: INFO: Waiting up to 5m0s for pod "test-webserver-30d2f624-9fa4-4264-bca9-14dbafbd7af1" in namespace "container-probe-7960" to be "running and ready"
    Dec  9 07:01:19.253: INFO: Pod "test-webserver-30d2f624-9fa4-4264-bca9-14dbafbd7af1": Phase="Pending", Reason="", readiness=false. Elapsed: 8.075723ms
    Dec  9 07:01:19.253: INFO: The phase of Pod test-webserver-30d2f624-9fa4-4264-bca9-14dbafbd7af1 is Pending, waiting for it to be Running (with Ready = true)
    Dec  9 07:01:21.256: INFO: Pod "test-webserver-30d2f624-9fa4-4264-bca9-14dbafbd7af1": Phase="Running", Reason="", readiness=false. Elapsed: 2.011350115s
    Dec  9 07:01:21.256: INFO: The phase of Pod test-webserver-30d2f624-9fa4-4264-bca9-14dbafbd7af1 is Running (Ready = false)
    Dec  9 07:01:23.258: INFO: Pod "test-webserver-30d2f624-9fa4-4264-bca9-14dbafbd7af1": Phase="Running", Reason="", readiness=false. Elapsed: 4.012814862s
    Dec  9 07:01:23.258: INFO: The phase of Pod test-webserver-30d2f624-9fa4-4264-bca9-14dbafbd7af1 is Running (Ready = false)
    Dec  9 07:01:25.257: INFO: Pod "test-webserver-30d2f624-9fa4-4264-bca9-14dbafbd7af1": Phase="Running", Reason="", readiness=false. Elapsed: 6.012665019s
    Dec  9 07:01:25.258: INFO: The phase of Pod test-webserver-30d2f624-9fa4-4264-bca9-14dbafbd7af1 is Running (Ready = false)
    Dec  9 07:01:27.257: INFO: Pod "test-webserver-30d2f624-9fa4-4264-bca9-14dbafbd7af1": Phase="Running", Reason="", readiness=false. Elapsed: 8.011794864s
    Dec  9 07:01:27.257: INFO: The phase of Pod test-webserver-30d2f624-9fa4-4264-bca9-14dbafbd7af1 is Running (Ready = false)
    Dec  9 07:01:29.258: INFO: Pod "test-webserver-30d2f624-9fa4-4264-bca9-14dbafbd7af1": Phase="Running", Reason="", readiness=false. Elapsed: 10.012889818s
    Dec  9 07:01:29.258: INFO: The phase of Pod test-webserver-30d2f624-9fa4-4264-bca9-14dbafbd7af1 is Running (Ready = false)
    Dec  9 07:01:31.258: INFO: Pod "test-webserver-30d2f624-9fa4-4264-bca9-14dbafbd7af1": Phase="Running", Reason="", readiness=false. Elapsed: 12.013312093s
    Dec  9 07:01:31.258: INFO: The phase of Pod test-webserver-30d2f624-9fa4-4264-bca9-14dbafbd7af1 is Running (Ready = false)
    Dec  9 07:01:33.257: INFO: Pod "test-webserver-30d2f624-9fa4-4264-bca9-14dbafbd7af1": Phase="Running", Reason="", readiness=false. Elapsed: 14.011928447s
    Dec  9 07:01:33.257: INFO: The phase of Pod test-webserver-30d2f624-9fa4-4264-bca9-14dbafbd7af1 is Running (Ready = false)
    Dec  9 07:01:35.257: INFO: Pod "test-webserver-30d2f624-9fa4-4264-bca9-14dbafbd7af1": Phase="Running", Reason="", readiness=false. Elapsed: 16.012143623s
    Dec  9 07:01:35.257: INFO: The phase of Pod test-webserver-30d2f624-9fa4-4264-bca9-14dbafbd7af1 is Running (Ready = false)
    Dec  9 07:01:37.256: INFO: Pod "test-webserver-30d2f624-9fa4-4264-bca9-14dbafbd7af1": Phase="Running", Reason="", readiness=false. Elapsed: 18.011591828s
    Dec  9 07:01:37.256: INFO: The phase of Pod test-webserver-30d2f624-9fa4-4264-bca9-14dbafbd7af1 is Running (Ready = false)
    Dec  9 07:01:39.259: INFO: Pod "test-webserver-30d2f624-9fa4-4264-bca9-14dbafbd7af1": Phase="Running", Reason="", readiness=false. Elapsed: 20.014506171s
    Dec  9 07:01:39.259: INFO: The phase of Pod test-webserver-30d2f624-9fa4-4264-bca9-14dbafbd7af1 is Running (Ready = false)
    Dec  9 07:01:41.259: INFO: Pod "test-webserver-30d2f624-9fa4-4264-bca9-14dbafbd7af1": Phase="Running", Reason="", readiness=true. Elapsed: 22.01419298s
    Dec  9 07:01:41.259: INFO: The phase of Pod test-webserver-30d2f624-9fa4-4264-bca9-14dbafbd7af1 is Running (Ready = true)
    Dec  9 07:01:41.259: INFO: Pod "test-webserver-30d2f624-9fa4-4264-bca9-14dbafbd7af1" satisfied condition "running and ready"
    Dec  9 07:01:41.266: INFO: Container started at 2022-12-09 07:01:20 +0000 UTC, pod became ready at 2022-12-09 07:01:39 +0000 UTC
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:01:41.267: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-7960" for this suite. 12/09/22 07:01:41.272
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1477
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:01:41.29
Dec  9 07:01:41.290: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename services 12/09/22 07:01:41.291
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:01:41.326
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:01:41.339
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1477
STEP: creating a service externalname-service with the type=ExternalName in namespace services-1352 12/09/22 07:01:41.353
STEP: changing the ExternalName service to type=NodePort 12/09/22 07:01:41.382
STEP: creating replication controller externalname-service in namespace services-1352 12/09/22 07:01:41.505
I1209 07:01:41.565684      20 runners.go:193] Created replication controller with name: externalname-service, namespace: services-1352, replica count: 2
I1209 07:01:44.616883      20 runners.go:193] externalname-service Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1209 07:01:47.617318      20 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Dec  9 07:01:47.617: INFO: Creating new exec pod
Dec  9 07:01:47.624: INFO: Waiting up to 5m0s for pod "execpod2vmtc" in namespace "services-1352" to be "running"
Dec  9 07:01:47.628: INFO: Pod "execpod2vmtc": Phase="Pending", Reason="", readiness=false. Elapsed: 4.03144ms
Dec  9 07:01:49.632: INFO: Pod "execpod2vmtc": Phase="Running", Reason="", readiness=true. Elapsed: 2.008327468s
Dec  9 07:01:49.632: INFO: Pod "execpod2vmtc" satisfied condition "running"
Dec  9 07:01:50.637: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=services-1352 exec execpod2vmtc -- /bin/sh -x -c nc -v -z -w 2 externalname-service 80'
Dec  9 07:01:50.829: INFO: stderr: "+ nc -v -z -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Dec  9 07:01:50.829: INFO: stdout: ""
Dec  9 07:01:50.829: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=services-1352 exec execpod2vmtc -- /bin/sh -x -c nc -v -z -w 2 10.3.81.37 80'
Dec  9 07:01:50.969: INFO: stderr: "+ nc -v -z -w 2 10.3.81.37 80\nConnection to 10.3.81.37 80 port [tcp/http] succeeded!\n"
Dec  9 07:01:50.969: INFO: stdout: ""
Dec  9 07:01:50.969: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=services-1352 exec execpod2vmtc -- /bin/sh -x -c nc -v -z -w 2 10.0.10.179 31345'
Dec  9 07:01:51.161: INFO: stderr: "+ nc -v -z -w 2 10.0.10.179 31345\nConnection to 10.0.10.179 31345 port [tcp/*] succeeded!\n"
Dec  9 07:01:51.161: INFO: stdout: ""
Dec  9 07:01:51.161: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=services-1352 exec execpod2vmtc -- /bin/sh -x -c nc -v -z -w 2 10.0.17.108 31345'
Dec  9 07:01:51.299: INFO: stderr: "+ nc -v -z -w 2 10.0.17.108 31345\nConnection to 10.0.17.108 31345 port [tcp/*] succeeded!\n"
Dec  9 07:01:51.299: INFO: stdout: ""
Dec  9 07:01:51.299: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Dec  9 07:01:51.357: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-1352" for this suite. 12/09/22 07:01:51.36
------------------------------
â€¢ [SLOW TEST] [10.089 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1477

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:01:41.29
    Dec  9 07:01:41.290: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename services 12/09/22 07:01:41.291
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:01:41.326
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:01:41.339
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to change the type from ExternalName to NodePort [Conformance]
      test/e2e/network/service.go:1477
    STEP: creating a service externalname-service with the type=ExternalName in namespace services-1352 12/09/22 07:01:41.353
    STEP: changing the ExternalName service to type=NodePort 12/09/22 07:01:41.382
    STEP: creating replication controller externalname-service in namespace services-1352 12/09/22 07:01:41.505
    I1209 07:01:41.565684      20 runners.go:193] Created replication controller with name: externalname-service, namespace: services-1352, replica count: 2
    I1209 07:01:44.616883      20 runners.go:193] externalname-service Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I1209 07:01:47.617318      20 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Dec  9 07:01:47.617: INFO: Creating new exec pod
    Dec  9 07:01:47.624: INFO: Waiting up to 5m0s for pod "execpod2vmtc" in namespace "services-1352" to be "running"
    Dec  9 07:01:47.628: INFO: Pod "execpod2vmtc": Phase="Pending", Reason="", readiness=false. Elapsed: 4.03144ms
    Dec  9 07:01:49.632: INFO: Pod "execpod2vmtc": Phase="Running", Reason="", readiness=true. Elapsed: 2.008327468s
    Dec  9 07:01:49.632: INFO: Pod "execpod2vmtc" satisfied condition "running"
    Dec  9 07:01:50.637: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=services-1352 exec execpod2vmtc -- /bin/sh -x -c nc -v -z -w 2 externalname-service 80'
    Dec  9 07:01:50.829: INFO: stderr: "+ nc -v -z -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Dec  9 07:01:50.829: INFO: stdout: ""
    Dec  9 07:01:50.829: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=services-1352 exec execpod2vmtc -- /bin/sh -x -c nc -v -z -w 2 10.3.81.37 80'
    Dec  9 07:01:50.969: INFO: stderr: "+ nc -v -z -w 2 10.3.81.37 80\nConnection to 10.3.81.37 80 port [tcp/http] succeeded!\n"
    Dec  9 07:01:50.969: INFO: stdout: ""
    Dec  9 07:01:50.969: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=services-1352 exec execpod2vmtc -- /bin/sh -x -c nc -v -z -w 2 10.0.10.179 31345'
    Dec  9 07:01:51.161: INFO: stderr: "+ nc -v -z -w 2 10.0.10.179 31345\nConnection to 10.0.10.179 31345 port [tcp/*] succeeded!\n"
    Dec  9 07:01:51.161: INFO: stdout: ""
    Dec  9 07:01:51.161: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=services-1352 exec execpod2vmtc -- /bin/sh -x -c nc -v -z -w 2 10.0.17.108 31345'
    Dec  9 07:01:51.299: INFO: stderr: "+ nc -v -z -w 2 10.0.17.108 31345\nConnection to 10.0.17.108 31345 port [tcp/*] succeeded!\n"
    Dec  9 07:01:51.299: INFO: stdout: ""
    Dec  9 07:01:51.299: INFO: Cleaning up the ExternalName to NodePort test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:01:51.357: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-1352" for this suite. 12/09/22 07:01:51.36
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:99
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:01:51.38
Dec  9 07:01:51.380: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename projected 12/09/22 07:01:51.381
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:01:51.406
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:01:51.411
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:99
STEP: Creating configMap with name projected-configmap-test-volume-map-7f36df53-21a8-449c-b81f-9655f14aee4c 12/09/22 07:01:51.416
STEP: Creating a pod to test consume configMaps 12/09/22 07:01:51.423
Dec  9 07:01:51.434: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-d639c7c6-fbd1-47e9-8c44-9ca2b473c03c" in namespace "projected-9581" to be "Succeeded or Failed"
Dec  9 07:01:51.452: INFO: Pod "pod-projected-configmaps-d639c7c6-fbd1-47e9-8c44-9ca2b473c03c": Phase="Pending", Reason="", readiness=false. Elapsed: 18.385614ms
Dec  9 07:01:53.460: INFO: Pod "pod-projected-configmaps-d639c7c6-fbd1-47e9-8c44-9ca2b473c03c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026406814s
Dec  9 07:01:55.457: INFO: Pod "pod-projected-configmaps-d639c7c6-fbd1-47e9-8c44-9ca2b473c03c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023586268s
STEP: Saw pod success 12/09/22 07:01:55.457
Dec  9 07:01:55.458: INFO: Pod "pod-projected-configmaps-d639c7c6-fbd1-47e9-8c44-9ca2b473c03c" satisfied condition "Succeeded or Failed"
Dec  9 07:01:55.461: INFO: Trying to get logs from node ip-10-0-10-179 pod pod-projected-configmaps-d639c7c6-fbd1-47e9-8c44-9ca2b473c03c container agnhost-container: <nil>
STEP: delete the pod 12/09/22 07:01:55.468
Dec  9 07:01:55.549: INFO: Waiting for pod pod-projected-configmaps-d639c7c6-fbd1-47e9-8c44-9ca2b473c03c to disappear
Dec  9 07:01:55.569: INFO: Pod pod-projected-configmaps-d639c7c6-fbd1-47e9-8c44-9ca2b473c03c no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Dec  9 07:01:55.569: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-9581" for this suite. 12/09/22 07:01:55.575
------------------------------
â€¢ [4.204 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:99

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:01:51.38
    Dec  9 07:01:51.380: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename projected 12/09/22 07:01:51.381
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:01:51.406
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:01:51.411
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:99
    STEP: Creating configMap with name projected-configmap-test-volume-map-7f36df53-21a8-449c-b81f-9655f14aee4c 12/09/22 07:01:51.416
    STEP: Creating a pod to test consume configMaps 12/09/22 07:01:51.423
    Dec  9 07:01:51.434: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-d639c7c6-fbd1-47e9-8c44-9ca2b473c03c" in namespace "projected-9581" to be "Succeeded or Failed"
    Dec  9 07:01:51.452: INFO: Pod "pod-projected-configmaps-d639c7c6-fbd1-47e9-8c44-9ca2b473c03c": Phase="Pending", Reason="", readiness=false. Elapsed: 18.385614ms
    Dec  9 07:01:53.460: INFO: Pod "pod-projected-configmaps-d639c7c6-fbd1-47e9-8c44-9ca2b473c03c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026406814s
    Dec  9 07:01:55.457: INFO: Pod "pod-projected-configmaps-d639c7c6-fbd1-47e9-8c44-9ca2b473c03c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023586268s
    STEP: Saw pod success 12/09/22 07:01:55.457
    Dec  9 07:01:55.458: INFO: Pod "pod-projected-configmaps-d639c7c6-fbd1-47e9-8c44-9ca2b473c03c" satisfied condition "Succeeded or Failed"
    Dec  9 07:01:55.461: INFO: Trying to get logs from node ip-10-0-10-179 pod pod-projected-configmaps-d639c7c6-fbd1-47e9-8c44-9ca2b473c03c container agnhost-container: <nil>
    STEP: delete the pod 12/09/22 07:01:55.468
    Dec  9 07:01:55.549: INFO: Waiting for pod pod-projected-configmaps-d639c7c6-fbd1-47e9-8c44-9ca2b473c03c to disappear
    Dec  9 07:01:55.569: INFO: Pod pod-projected-configmaps-d639c7c6-fbd1-47e9-8c44-9ca2b473c03c no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:01:55.569: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-9581" for this suite. 12/09/22 07:01:55.575
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server
  should support --unix-socket=/path  [Conformance]
  test/e2e/kubectl/kubectl.go:1812
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:01:55.586
Dec  9 07:01:55.586: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename kubectl 12/09/22 07:01:55.587
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:01:55.626
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:01:55.639
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should support --unix-socket=/path  [Conformance]
  test/e2e/kubectl/kubectl.go:1812
STEP: Starting the proxy 12/09/22 07:01:55.644
Dec  9 07:01:55.645: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=kubectl-929 proxy --unix-socket=/tmp/kubectl-proxy-unix2029952463/test'
STEP: retrieving proxy /api/ output 12/09/22 07:01:55.71
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Dec  9 07:01:55.712: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-929" for this suite. 12/09/22 07:01:55.716
------------------------------
â€¢ [0.143 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Proxy server
  test/e2e/kubectl/kubectl.go:1780
    should support --unix-socket=/path  [Conformance]
    test/e2e/kubectl/kubectl.go:1812

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:01:55.586
    Dec  9 07:01:55.586: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename kubectl 12/09/22 07:01:55.587
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:01:55.626
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:01:55.639
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should support --unix-socket=/path  [Conformance]
      test/e2e/kubectl/kubectl.go:1812
    STEP: Starting the proxy 12/09/22 07:01:55.644
    Dec  9 07:01:55.645: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=kubectl-929 proxy --unix-socket=/tmp/kubectl-proxy-unix2029952463/test'
    STEP: retrieving proxy /api/ output 12/09/22 07:01:55.71
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:01:55.712: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-929" for this suite. 12/09/22 07:01:55.716
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace
  should update a single-container pod's image  [Conformance]
  test/e2e/kubectl/kubectl.go:1747
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:01:55.731
Dec  9 07:01:55.731: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename kubectl 12/09/22 07:01:55.732
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:01:55.766
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:01:55.773
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Kubectl replace
  test/e2e/kubectl/kubectl.go:1734
[It] should update a single-container pod's image  [Conformance]
  test/e2e/kubectl/kubectl.go:1747
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 12/09/22 07:01:55.778
Dec  9 07:01:55.779: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=kubectl-9046 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Dec  9 07:01:55.896: INFO: stderr: ""
Dec  9 07:01:55.896: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running 12/09/22 07:01:55.896
STEP: verifying the pod e2e-test-httpd-pod was created 12/09/22 07:02:00.949
Dec  9 07:02:00.949: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=kubectl-9046 get pod e2e-test-httpd-pod -o json'
Dec  9 07:02:01.084: INFO: stderr: ""
Dec  9 07:02:01.084: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/containerID\": \"99d8c2f35e8572e871ca8884640c0c295edf44acf27d255608481f16c4f8b49c\",\n            \"cni.projectcalico.org/podIP\": \"10.2.136.21/32\",\n            \"cni.projectcalico.org/podIPs\": \"10.2.136.21/32\"\n        },\n        \"creationTimestamp\": \"2022-12-09T07:01:55Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-9046\",\n        \"resourceVersion\": \"2175\",\n        \"uid\": \"4afad254-b899-468f-877d-e641b0e90da0\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-gbn4h\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"ip-10-0-10-179\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-gbn4h\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2022-12-09T07:01:55Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2022-12-09T07:01:57Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2022-12-09T07:01:57Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2022-12-09T07:01:55Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://991e0725847d9c464bff5cb043ae051b79995c747eabd6020dd8a0446f942c7f\",\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imageID\": \"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2022-12-09T07:01:56Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.0.10.179\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.2.136.21\",\n        \"podIPs\": [\n            {\n                \"ip\": \"10.2.136.21\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2022-12-09T07:01:55Z\"\n    }\n}\n"
STEP: replace the image in the pod 12/09/22 07:02:01.084
Dec  9 07:02:01.084: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=kubectl-9046 replace -f -'
Dec  9 07:02:01.507: INFO: stderr: ""
Dec  9 07:02:01.507: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/busybox:1.29-4 12/09/22 07:02:01.507
[AfterEach] Kubectl replace
  test/e2e/kubectl/kubectl.go:1738
Dec  9 07:02:01.516: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=kubectl-9046 delete pods e2e-test-httpd-pod'
Dec  9 07:02:03.214: INFO: stderr: ""
Dec  9 07:02:03.214: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Dec  9 07:02:03.214: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-9046" for this suite. 12/09/22 07:02:03.22
------------------------------
â€¢ [SLOW TEST] [7.497 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl replace
  test/e2e/kubectl/kubectl.go:1731
    should update a single-container pod's image  [Conformance]
    test/e2e/kubectl/kubectl.go:1747

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:01:55.731
    Dec  9 07:01:55.731: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename kubectl 12/09/22 07:01:55.732
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:01:55.766
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:01:55.773
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Kubectl replace
      test/e2e/kubectl/kubectl.go:1734
    [It] should update a single-container pod's image  [Conformance]
      test/e2e/kubectl/kubectl.go:1747
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 12/09/22 07:01:55.778
    Dec  9 07:01:55.779: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=kubectl-9046 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
    Dec  9 07:01:55.896: INFO: stderr: ""
    Dec  9 07:01:55.896: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: verifying the pod e2e-test-httpd-pod is running 12/09/22 07:01:55.896
    STEP: verifying the pod e2e-test-httpd-pod was created 12/09/22 07:02:00.949
    Dec  9 07:02:00.949: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=kubectl-9046 get pod e2e-test-httpd-pod -o json'
    Dec  9 07:02:01.084: INFO: stderr: ""
    Dec  9 07:02:01.084: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/containerID\": \"99d8c2f35e8572e871ca8884640c0c295edf44acf27d255608481f16c4f8b49c\",\n            \"cni.projectcalico.org/podIP\": \"10.2.136.21/32\",\n            \"cni.projectcalico.org/podIPs\": \"10.2.136.21/32\"\n        },\n        \"creationTimestamp\": \"2022-12-09T07:01:55Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-9046\",\n        \"resourceVersion\": \"2175\",\n        \"uid\": \"4afad254-b899-468f-877d-e641b0e90da0\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-gbn4h\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"ip-10-0-10-179\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-gbn4h\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2022-12-09T07:01:55Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2022-12-09T07:01:57Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2022-12-09T07:01:57Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2022-12-09T07:01:55Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://991e0725847d9c464bff5cb043ae051b79995c747eabd6020dd8a0446f942c7f\",\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imageID\": \"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2022-12-09T07:01:56Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.0.10.179\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.2.136.21\",\n        \"podIPs\": [\n            {\n                \"ip\": \"10.2.136.21\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2022-12-09T07:01:55Z\"\n    }\n}\n"
    STEP: replace the image in the pod 12/09/22 07:02:01.084
    Dec  9 07:02:01.084: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=kubectl-9046 replace -f -'
    Dec  9 07:02:01.507: INFO: stderr: ""
    Dec  9 07:02:01.507: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
    STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/busybox:1.29-4 12/09/22 07:02:01.507
    [AfterEach] Kubectl replace
      test/e2e/kubectl/kubectl.go:1738
    Dec  9 07:02:01.516: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=kubectl-9046 delete pods e2e-test-httpd-pod'
    Dec  9 07:02:03.214: INFO: stderr: ""
    Dec  9 07:02:03.214: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:02:03.214: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-9046" for this suite. 12/09/22 07:02:03.22
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:252
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:02:03.228
Dec  9 07:02:03.229: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename webhook 12/09/22 07:02:03.23
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:02:03.263
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:02:03.279
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 12/09/22 07:02:03.317
STEP: Create role binding to let webhook read extension-apiserver-authentication 12/09/22 07:02:04.121
STEP: Deploying the webhook pod 12/09/22 07:02:04.13
STEP: Wait for the deployment to be ready 12/09/22 07:02:04.146
Dec  9 07:02:04.161: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 12/09/22 07:02:06.181
STEP: Verifying the service has paired with the endpoint 12/09/22 07:02:06.206
Dec  9 07:02:07.207: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:252
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API 12/09/22 07:02:07.211
STEP: create a configmap that should be updated by the webhook 12/09/22 07:02:07.238
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Dec  9 07:02:07.262: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-749" for this suite. 12/09/22 07:02:07.322
STEP: Destroying namespace "webhook-749-markers" for this suite. 12/09/22 07:02:07.329
------------------------------
â€¢ [4.119 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:252

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:02:03.228
    Dec  9 07:02:03.229: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename webhook 12/09/22 07:02:03.23
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:02:03.263
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:02:03.279
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 12/09/22 07:02:03.317
    STEP: Create role binding to let webhook read extension-apiserver-authentication 12/09/22 07:02:04.121
    STEP: Deploying the webhook pod 12/09/22 07:02:04.13
    STEP: Wait for the deployment to be ready 12/09/22 07:02:04.146
    Dec  9 07:02:04.161: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 12/09/22 07:02:06.181
    STEP: Verifying the service has paired with the endpoint 12/09/22 07:02:06.206
    Dec  9 07:02:07.207: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate configmap [Conformance]
      test/e2e/apimachinery/webhook.go:252
    STEP: Registering the mutating configmap webhook via the AdmissionRegistration API 12/09/22 07:02:07.211
    STEP: create a configmap that should be updated by the webhook 12/09/22 07:02:07.238
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:02:07.262: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-749" for this suite. 12/09/22 07:02:07.322
    STEP: Destroying namespace "webhook-749-markers" for this suite. 12/09/22 07:02:07.329
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:275
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:02:07.354
Dec  9 07:02:07.354: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename svcaccounts 12/09/22 07:02:07.356
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:02:07.374
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:02:07.379
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:275
STEP: Creating a pod to test service account token:  12/09/22 07:02:07.387
Dec  9 07:02:07.396: INFO: Waiting up to 5m0s for pod "test-pod-2c040aba-96be-43fd-90fb-c984f3c3db1e" in namespace "svcaccounts-5302" to be "Succeeded or Failed"
Dec  9 07:02:07.402: INFO: Pod "test-pod-2c040aba-96be-43fd-90fb-c984f3c3db1e": Phase="Pending", Reason="", readiness=false. Elapsed: 5.356737ms
Dec  9 07:02:09.406: INFO: Pod "test-pod-2c040aba-96be-43fd-90fb-c984f3c3db1e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009748823s
Dec  9 07:02:11.406: INFO: Pod "test-pod-2c040aba-96be-43fd-90fb-c984f3c3db1e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009206857s
STEP: Saw pod success 12/09/22 07:02:11.406
Dec  9 07:02:11.406: INFO: Pod "test-pod-2c040aba-96be-43fd-90fb-c984f3c3db1e" satisfied condition "Succeeded or Failed"
Dec  9 07:02:11.409: INFO: Trying to get logs from node ip-10-0-10-179 pod test-pod-2c040aba-96be-43fd-90fb-c984f3c3db1e container agnhost-container: <nil>
STEP: delete the pod 12/09/22 07:02:11.416
Dec  9 07:02:11.428: INFO: Waiting for pod test-pod-2c040aba-96be-43fd-90fb-c984f3c3db1e to disappear
Dec  9 07:02:11.431: INFO: Pod test-pod-2c040aba-96be-43fd-90fb-c984f3c3db1e no longer exists
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Dec  9 07:02:11.431: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-5302" for this suite. 12/09/22 07:02:11.436
------------------------------
â€¢ [4.089 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:275

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:02:07.354
    Dec  9 07:02:07.354: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename svcaccounts 12/09/22 07:02:07.356
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:02:07.374
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:02:07.379
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should mount projected service account token [Conformance]
      test/e2e/auth/service_accounts.go:275
    STEP: Creating a pod to test service account token:  12/09/22 07:02:07.387
    Dec  9 07:02:07.396: INFO: Waiting up to 5m0s for pod "test-pod-2c040aba-96be-43fd-90fb-c984f3c3db1e" in namespace "svcaccounts-5302" to be "Succeeded or Failed"
    Dec  9 07:02:07.402: INFO: Pod "test-pod-2c040aba-96be-43fd-90fb-c984f3c3db1e": Phase="Pending", Reason="", readiness=false. Elapsed: 5.356737ms
    Dec  9 07:02:09.406: INFO: Pod "test-pod-2c040aba-96be-43fd-90fb-c984f3c3db1e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009748823s
    Dec  9 07:02:11.406: INFO: Pod "test-pod-2c040aba-96be-43fd-90fb-c984f3c3db1e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009206857s
    STEP: Saw pod success 12/09/22 07:02:11.406
    Dec  9 07:02:11.406: INFO: Pod "test-pod-2c040aba-96be-43fd-90fb-c984f3c3db1e" satisfied condition "Succeeded or Failed"
    Dec  9 07:02:11.409: INFO: Trying to get logs from node ip-10-0-10-179 pod test-pod-2c040aba-96be-43fd-90fb-c984f3c3db1e container agnhost-container: <nil>
    STEP: delete the pod 12/09/22 07:02:11.416
    Dec  9 07:02:11.428: INFO: Waiting for pod test-pod-2c040aba-96be-43fd-90fb-c984f3c3db1e to disappear
    Dec  9 07:02:11.431: INFO: Pod test-pod-2c040aba-96be-43fd-90fb-c984f3c3db1e no longer exists
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:02:11.431: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-5302" for this suite. 12/09/22 07:02:11.436
  << End Captured GinkgoWriter Output
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:466
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:02:11.446
Dec  9 07:02:11.446: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename sched-pred 12/09/22 07:02:11.448
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:02:11.468
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:02:11.48
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:97
Dec  9 07:02:11.486: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Dec  9 07:02:11.492: INFO: Waiting for terminating namespaces to be deleted...
Dec  9 07:02:11.495: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-10-179 before test
Dec  9 07:02:11.504: INFO: calico-node-9zhz5 from kube-system started at 2022-12-09 06:56:15 +0000 UTC (1 container statuses recorded)
Dec  9 07:02:11.505: INFO: 	Container calico-node ready: true, restart count 0
Dec  9 07:02:11.505: INFO: coredns-844594ffdd-q2bzk from kube-system started at 2022-12-09 06:56:25 +0000 UTC (1 container statuses recorded)
Dec  9 07:02:11.505: INFO: 	Container coredns ready: true, restart count 0
Dec  9 07:02:11.505: INFO: coredns-844594ffdd-rls2f from kube-system started at 2022-12-09 06:56:25 +0000 UTC (1 container statuses recorded)
Dec  9 07:02:11.505: INFO: 	Container coredns ready: true, restart count 0
Dec  9 07:02:11.505: INFO: kube-proxy-dv66w from kube-system started at 2022-12-09 06:56:15 +0000 UTC (1 container statuses recorded)
Dec  9 07:02:11.505: INFO: 	Container kube-proxy ready: true, restart count 0
Dec  9 07:02:11.505: INFO: sonobuoy-systemd-logs-daemon-set-8dafb6c50b3e4797-scljx from sonobuoy started at 2022-12-09 06:58:30 +0000 UTC (2 container statuses recorded)
Dec  9 07:02:11.505: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec  9 07:02:11.505: INFO: 	Container systemd-logs ready: true, restart count 0
Dec  9 07:02:11.505: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-17-108 before test
Dec  9 07:02:11.518: INFO: calico-node-flv5c from kube-system started at 2022-12-09 06:56:14 +0000 UTC (1 container statuses recorded)
Dec  9 07:02:11.518: INFO: 	Container calico-node ready: true, restart count 0
Dec  9 07:02:11.518: INFO: kube-proxy-r75k5 from kube-system started at 2022-12-09 06:56:14 +0000 UTC (1 container statuses recorded)
Dec  9 07:02:11.518: INFO: 	Container kube-proxy ready: true, restart count 0
Dec  9 07:02:11.518: INFO: sonobuoy from sonobuoy started at 2022-12-09 06:58:27 +0000 UTC (1 container statuses recorded)
Dec  9 07:02:11.518: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Dec  9 07:02:11.518: INFO: sonobuoy-e2e-job-023adc8283f34c8c from sonobuoy started at 2022-12-09 06:58:30 +0000 UTC (2 container statuses recorded)
Dec  9 07:02:11.518: INFO: 	Container e2e ready: true, restart count 0
Dec  9 07:02:11.518: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec  9 07:02:11.518: INFO: sonobuoy-systemd-logs-daemon-set-8dafb6c50b3e4797-jg2s9 from sonobuoy started at 2022-12-09 06:58:30 +0000 UTC (2 container statuses recorded)
Dec  9 07:02:11.518: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec  9 07:02:11.518: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:466
STEP: Trying to launch a pod without a label to get a node which can launch it. 12/09/22 07:02:11.518
Dec  9 07:02:11.530: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-4694" to be "running"
Dec  9 07:02:11.535: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 5.267495ms
Dec  9 07:02:13.539: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.008891731s
Dec  9 07:02:13.539: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 12/09/22 07:02:13.542
STEP: Trying to apply a random label on the found node. 12/09/22 07:02:13.552
STEP: verifying the node has the label kubernetes.io/e2e-674f27ef-9a93-492e-af4d-c9d2591796ef 42 12/09/22 07:02:13.567
STEP: Trying to relaunch the pod, now with labels. 12/09/22 07:02:13.572
Dec  9 07:02:13.586: INFO: Waiting up to 5m0s for pod "with-labels" in namespace "sched-pred-4694" to be "not pending"
Dec  9 07:02:13.595: INFO: Pod "with-labels": Phase="Pending", Reason="", readiness=false. Elapsed: 9.011419ms
Dec  9 07:02:15.600: INFO: Pod "with-labels": Phase="Running", Reason="", readiness=true. Elapsed: 2.0140297s
Dec  9 07:02:15.600: INFO: Pod "with-labels" satisfied condition "not pending"
STEP: removing the label kubernetes.io/e2e-674f27ef-9a93-492e-af4d-c9d2591796ef off the node ip-10-0-10-179 12/09/22 07:02:15.603
STEP: verifying the node doesn't have the label kubernetes.io/e2e-674f27ef-9a93-492e-af4d-c9d2591796ef 12/09/22 07:02:15.623
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/node/init/init.go:32
Dec  9 07:02:15.630: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:88
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-pred-4694" for this suite. 12/09/22 07:02:15.637
------------------------------
â€¢ [4.196 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:466

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:02:11.446
    Dec  9 07:02:11.446: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename sched-pred 12/09/22 07:02:11.448
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:02:11.468
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:02:11.48
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:97
    Dec  9 07:02:11.486: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Dec  9 07:02:11.492: INFO: Waiting for terminating namespaces to be deleted...
    Dec  9 07:02:11.495: INFO: 
    Logging pods the apiserver thinks is on node ip-10-0-10-179 before test
    Dec  9 07:02:11.504: INFO: calico-node-9zhz5 from kube-system started at 2022-12-09 06:56:15 +0000 UTC (1 container statuses recorded)
    Dec  9 07:02:11.505: INFO: 	Container calico-node ready: true, restart count 0
    Dec  9 07:02:11.505: INFO: coredns-844594ffdd-q2bzk from kube-system started at 2022-12-09 06:56:25 +0000 UTC (1 container statuses recorded)
    Dec  9 07:02:11.505: INFO: 	Container coredns ready: true, restart count 0
    Dec  9 07:02:11.505: INFO: coredns-844594ffdd-rls2f from kube-system started at 2022-12-09 06:56:25 +0000 UTC (1 container statuses recorded)
    Dec  9 07:02:11.505: INFO: 	Container coredns ready: true, restart count 0
    Dec  9 07:02:11.505: INFO: kube-proxy-dv66w from kube-system started at 2022-12-09 06:56:15 +0000 UTC (1 container statuses recorded)
    Dec  9 07:02:11.505: INFO: 	Container kube-proxy ready: true, restart count 0
    Dec  9 07:02:11.505: INFO: sonobuoy-systemd-logs-daemon-set-8dafb6c50b3e4797-scljx from sonobuoy started at 2022-12-09 06:58:30 +0000 UTC (2 container statuses recorded)
    Dec  9 07:02:11.505: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Dec  9 07:02:11.505: INFO: 	Container systemd-logs ready: true, restart count 0
    Dec  9 07:02:11.505: INFO: 
    Logging pods the apiserver thinks is on node ip-10-0-17-108 before test
    Dec  9 07:02:11.518: INFO: calico-node-flv5c from kube-system started at 2022-12-09 06:56:14 +0000 UTC (1 container statuses recorded)
    Dec  9 07:02:11.518: INFO: 	Container calico-node ready: true, restart count 0
    Dec  9 07:02:11.518: INFO: kube-proxy-r75k5 from kube-system started at 2022-12-09 06:56:14 +0000 UTC (1 container statuses recorded)
    Dec  9 07:02:11.518: INFO: 	Container kube-proxy ready: true, restart count 0
    Dec  9 07:02:11.518: INFO: sonobuoy from sonobuoy started at 2022-12-09 06:58:27 +0000 UTC (1 container statuses recorded)
    Dec  9 07:02:11.518: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Dec  9 07:02:11.518: INFO: sonobuoy-e2e-job-023adc8283f34c8c from sonobuoy started at 2022-12-09 06:58:30 +0000 UTC (2 container statuses recorded)
    Dec  9 07:02:11.518: INFO: 	Container e2e ready: true, restart count 0
    Dec  9 07:02:11.518: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Dec  9 07:02:11.518: INFO: sonobuoy-systemd-logs-daemon-set-8dafb6c50b3e4797-jg2s9 from sonobuoy started at 2022-12-09 06:58:30 +0000 UTC (2 container statuses recorded)
    Dec  9 07:02:11.518: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Dec  9 07:02:11.518: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates that NodeSelector is respected if matching  [Conformance]
      test/e2e/scheduling/predicates.go:466
    STEP: Trying to launch a pod without a label to get a node which can launch it. 12/09/22 07:02:11.518
    Dec  9 07:02:11.530: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-4694" to be "running"
    Dec  9 07:02:11.535: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 5.267495ms
    Dec  9 07:02:13.539: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.008891731s
    Dec  9 07:02:13.539: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 12/09/22 07:02:13.542
    STEP: Trying to apply a random label on the found node. 12/09/22 07:02:13.552
    STEP: verifying the node has the label kubernetes.io/e2e-674f27ef-9a93-492e-af4d-c9d2591796ef 42 12/09/22 07:02:13.567
    STEP: Trying to relaunch the pod, now with labels. 12/09/22 07:02:13.572
    Dec  9 07:02:13.586: INFO: Waiting up to 5m0s for pod "with-labels" in namespace "sched-pred-4694" to be "not pending"
    Dec  9 07:02:13.595: INFO: Pod "with-labels": Phase="Pending", Reason="", readiness=false. Elapsed: 9.011419ms
    Dec  9 07:02:15.600: INFO: Pod "with-labels": Phase="Running", Reason="", readiness=true. Elapsed: 2.0140297s
    Dec  9 07:02:15.600: INFO: Pod "with-labels" satisfied condition "not pending"
    STEP: removing the label kubernetes.io/e2e-674f27ef-9a93-492e-af4d-c9d2591796ef off the node ip-10-0-10-179 12/09/22 07:02:15.603
    STEP: verifying the node doesn't have the label kubernetes.io/e2e-674f27ef-9a93-492e-af4d-c9d2591796ef 12/09/22 07:02:15.623
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:02:15.630: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:88
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-pred-4694" for this suite. 12/09/22 07:02:15.637
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-network] EndpointSliceMirroring
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53
[BeforeEach] [sig-network] EndpointSliceMirroring
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:02:15.644
Dec  9 07:02:15.644: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename endpointslicemirroring 12/09/22 07:02:15.645
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:02:15.671
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:02:15.675
[BeforeEach] [sig-network] EndpointSliceMirroring
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSliceMirroring
  test/e2e/network/endpointslicemirroring.go:41
[It] should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53
STEP: mirroring a new custom Endpoint 12/09/22 07:02:15.69
Dec  9 07:02:15.706: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
STEP: mirroring an update to a custom Endpoint 12/09/22 07:02:17.71
Dec  9 07:02:17.719: INFO: Expected EndpointSlice to have 10.2.3.4 as address, got 10.1.2.3
STEP: mirroring deletion of a custom Endpoint 12/09/22 07:02:19.725
[AfterEach] [sig-network] EndpointSliceMirroring
  test/e2e/framework/node/init/init.go:32
Dec  9 07:02:19.739: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslicemirroring-3695" for this suite. 12/09/22 07:02:19.744
------------------------------
â€¢ [4.106 seconds]
[sig-network] EndpointSliceMirroring
test/e2e/network/common/framework.go:23
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSliceMirroring
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:02:15.644
    Dec  9 07:02:15.644: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename endpointslicemirroring 12/09/22 07:02:15.645
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:02:15.671
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:02:15.675
    [BeforeEach] [sig-network] EndpointSliceMirroring
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSliceMirroring
      test/e2e/network/endpointslicemirroring.go:41
    [It] should mirror a custom Endpoints resource through create update and delete [Conformance]
      test/e2e/network/endpointslicemirroring.go:53
    STEP: mirroring a new custom Endpoint 12/09/22 07:02:15.69
    Dec  9 07:02:15.706: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
    STEP: mirroring an update to a custom Endpoint 12/09/22 07:02:17.71
    Dec  9 07:02:17.719: INFO: Expected EndpointSlice to have 10.2.3.4 as address, got 10.1.2.3
    STEP: mirroring deletion of a custom Endpoint 12/09/22 07:02:19.725
    [AfterEach] [sig-network] EndpointSliceMirroring
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:02:19.739: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslicemirroring-3695" for this suite. 12/09/22 07:02:19.744
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:84
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:02:19.757
Dec  9 07:02:19.757: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename projected 12/09/22 07:02:19.758
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:02:19.78
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:02:19.786
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:84
STEP: Creating a pod to test downward API volume plugin 12/09/22 07:02:19.792
Dec  9 07:02:19.806: INFO: Waiting up to 5m0s for pod "downwardapi-volume-85e57d26-a5fb-41a0-8c40-8608b404d228" in namespace "projected-2870" to be "Succeeded or Failed"
Dec  9 07:02:19.812: INFO: Pod "downwardapi-volume-85e57d26-a5fb-41a0-8c40-8608b404d228": Phase="Pending", Reason="", readiness=false. Elapsed: 5.909538ms
Dec  9 07:02:21.816: INFO: Pod "downwardapi-volume-85e57d26-a5fb-41a0-8c40-8608b404d228": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009893619s
Dec  9 07:02:23.818: INFO: Pod "downwardapi-volume-85e57d26-a5fb-41a0-8c40-8608b404d228": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011883783s
STEP: Saw pod success 12/09/22 07:02:23.819
Dec  9 07:02:23.819: INFO: Pod "downwardapi-volume-85e57d26-a5fb-41a0-8c40-8608b404d228" satisfied condition "Succeeded or Failed"
Dec  9 07:02:23.822: INFO: Trying to get logs from node ip-10-0-10-179 pod downwardapi-volume-85e57d26-a5fb-41a0-8c40-8608b404d228 container client-container: <nil>
STEP: delete the pod 12/09/22 07:02:23.826
Dec  9 07:02:23.837: INFO: Waiting for pod downwardapi-volume-85e57d26-a5fb-41a0-8c40-8608b404d228 to disappear
Dec  9 07:02:23.839: INFO: Pod downwardapi-volume-85e57d26-a5fb-41a0-8c40-8608b404d228 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Dec  9 07:02:23.840: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-2870" for this suite. 12/09/22 07:02:23.843
------------------------------
â€¢ [4.094 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:84

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:02:19.757
    Dec  9 07:02:19.757: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename projected 12/09/22 07:02:19.758
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:02:19.78
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:02:19.786
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:84
    STEP: Creating a pod to test downward API volume plugin 12/09/22 07:02:19.792
    Dec  9 07:02:19.806: INFO: Waiting up to 5m0s for pod "downwardapi-volume-85e57d26-a5fb-41a0-8c40-8608b404d228" in namespace "projected-2870" to be "Succeeded or Failed"
    Dec  9 07:02:19.812: INFO: Pod "downwardapi-volume-85e57d26-a5fb-41a0-8c40-8608b404d228": Phase="Pending", Reason="", readiness=false. Elapsed: 5.909538ms
    Dec  9 07:02:21.816: INFO: Pod "downwardapi-volume-85e57d26-a5fb-41a0-8c40-8608b404d228": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009893619s
    Dec  9 07:02:23.818: INFO: Pod "downwardapi-volume-85e57d26-a5fb-41a0-8c40-8608b404d228": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011883783s
    STEP: Saw pod success 12/09/22 07:02:23.819
    Dec  9 07:02:23.819: INFO: Pod "downwardapi-volume-85e57d26-a5fb-41a0-8c40-8608b404d228" satisfied condition "Succeeded or Failed"
    Dec  9 07:02:23.822: INFO: Trying to get logs from node ip-10-0-10-179 pod downwardapi-volume-85e57d26-a5fb-41a0-8c40-8608b404d228 container client-container: <nil>
    STEP: delete the pod 12/09/22 07:02:23.826
    Dec  9 07:02:23.837: INFO: Waiting for pod downwardapi-volume-85e57d26-a5fb-41a0-8c40-8608b404d228 to disappear
    Dec  9 07:02:23.839: INFO: Pod downwardapi-volume-85e57d26-a5fb-41a0-8c40-8608b404d228 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:02:23.840: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-2870" for this suite. 12/09/22 07:02:23.843
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Projected secret
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:215
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:02:23.851
Dec  9 07:02:23.851: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename projected 12/09/22 07:02:23.852
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:02:23.869
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:02:23.873
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:215
STEP: Creating secret with name s-test-opt-del-11eff84c-8fe2-4498-a93d-20884568efef 12/09/22 07:02:23.883
STEP: Creating secret with name s-test-opt-upd-89966189-aef5-41c0-ac31-3febce132245 12/09/22 07:02:23.889
STEP: Creating the pod 12/09/22 07:02:23.893
Dec  9 07:02:23.902: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-efba2ae6-7a81-4699-944f-1458bd3f04e0" in namespace "projected-656" to be "running and ready"
Dec  9 07:02:23.907: INFO: Pod "pod-projected-secrets-efba2ae6-7a81-4699-944f-1458bd3f04e0": Phase="Pending", Reason="", readiness=false. Elapsed: 5.88129ms
Dec  9 07:02:23.908: INFO: The phase of Pod pod-projected-secrets-efba2ae6-7a81-4699-944f-1458bd3f04e0 is Pending, waiting for it to be Running (with Ready = true)
Dec  9 07:02:25.912: INFO: Pod "pod-projected-secrets-efba2ae6-7a81-4699-944f-1458bd3f04e0": Phase="Running", Reason="", readiness=true. Elapsed: 2.010444922s
Dec  9 07:02:25.912: INFO: The phase of Pod pod-projected-secrets-efba2ae6-7a81-4699-944f-1458bd3f04e0 is Running (Ready = true)
Dec  9 07:02:25.912: INFO: Pod "pod-projected-secrets-efba2ae6-7a81-4699-944f-1458bd3f04e0" satisfied condition "running and ready"
STEP: Deleting secret s-test-opt-del-11eff84c-8fe2-4498-a93d-20884568efef 12/09/22 07:02:25.928
STEP: Updating secret s-test-opt-upd-89966189-aef5-41c0-ac31-3febce132245 12/09/22 07:02:25.935
STEP: Creating secret with name s-test-opt-create-83ed7c41-ec60-43b5-8567-771ed39d0e51 12/09/22 07:02:25.941
STEP: waiting to observe update in volume 12/09/22 07:02:25.945
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Dec  9 07:02:27.975: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-656" for this suite. 12/09/22 07:02:27.979
------------------------------
â€¢ [4.134 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:215

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:02:23.851
    Dec  9 07:02:23.851: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename projected 12/09/22 07:02:23.852
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:02:23.869
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:02:23.873
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:215
    STEP: Creating secret with name s-test-opt-del-11eff84c-8fe2-4498-a93d-20884568efef 12/09/22 07:02:23.883
    STEP: Creating secret with name s-test-opt-upd-89966189-aef5-41c0-ac31-3febce132245 12/09/22 07:02:23.889
    STEP: Creating the pod 12/09/22 07:02:23.893
    Dec  9 07:02:23.902: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-efba2ae6-7a81-4699-944f-1458bd3f04e0" in namespace "projected-656" to be "running and ready"
    Dec  9 07:02:23.907: INFO: Pod "pod-projected-secrets-efba2ae6-7a81-4699-944f-1458bd3f04e0": Phase="Pending", Reason="", readiness=false. Elapsed: 5.88129ms
    Dec  9 07:02:23.908: INFO: The phase of Pod pod-projected-secrets-efba2ae6-7a81-4699-944f-1458bd3f04e0 is Pending, waiting for it to be Running (with Ready = true)
    Dec  9 07:02:25.912: INFO: Pod "pod-projected-secrets-efba2ae6-7a81-4699-944f-1458bd3f04e0": Phase="Running", Reason="", readiness=true. Elapsed: 2.010444922s
    Dec  9 07:02:25.912: INFO: The phase of Pod pod-projected-secrets-efba2ae6-7a81-4699-944f-1458bd3f04e0 is Running (Ready = true)
    Dec  9 07:02:25.912: INFO: Pod "pod-projected-secrets-efba2ae6-7a81-4699-944f-1458bd3f04e0" satisfied condition "running and ready"
    STEP: Deleting secret s-test-opt-del-11eff84c-8fe2-4498-a93d-20884568efef 12/09/22 07:02:25.928
    STEP: Updating secret s-test-opt-upd-89966189-aef5-41c0-ac31-3febce132245 12/09/22 07:02:25.935
    STEP: Creating secret with name s-test-opt-create-83ed7c41-ec60-43b5-8567-771ed39d0e51 12/09/22 07:02:25.941
    STEP: waiting to observe update in volume 12/09/22 07:02:25.945
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:02:27.975: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-656" for this suite. 12/09/22 07:02:27.979
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-network] Proxy version v1
  A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  test/e2e/network/proxy.go:286
[BeforeEach] version v1
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:02:27.987
Dec  9 07:02:27.987: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename proxy 12/09/22 07:02:27.992
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:02:28.009
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:02:28.014
[BeforeEach] version v1
  test/e2e/framework/metrics/init/init.go:31
[It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  test/e2e/network/proxy.go:286
Dec  9 07:02:28.018: INFO: Creating pod...
Dec  9 07:02:28.026: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-7536" to be "running"
Dec  9 07:02:28.030: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 3.492732ms
Dec  9 07:02:30.034: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.007207945s
Dec  9 07:02:30.034: INFO: Pod "agnhost" satisfied condition "running"
Dec  9 07:02:30.034: INFO: Creating service...
Dec  9 07:02:30.050: INFO: Starting http.Client for https://10.3.0.1:443/api/v1/namespaces/proxy-7536/pods/agnhost/proxy/some/path/with/DELETE
Dec  9 07:02:30.070: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Dec  9 07:02:30.070: INFO: Starting http.Client for https://10.3.0.1:443/api/v1/namespaces/proxy-7536/pods/agnhost/proxy/some/path/with/GET
Dec  9 07:02:30.076: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Dec  9 07:02:30.077: INFO: Starting http.Client for https://10.3.0.1:443/api/v1/namespaces/proxy-7536/pods/agnhost/proxy/some/path/with/HEAD
Dec  9 07:02:30.085: INFO: http.Client request:HEAD | StatusCode:200
Dec  9 07:02:30.086: INFO: Starting http.Client for https://10.3.0.1:443/api/v1/namespaces/proxy-7536/pods/agnhost/proxy/some/path/with/OPTIONS
Dec  9 07:02:30.094: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Dec  9 07:02:30.095: INFO: Starting http.Client for https://10.3.0.1:443/api/v1/namespaces/proxy-7536/pods/agnhost/proxy/some/path/with/PATCH
Dec  9 07:02:30.109: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Dec  9 07:02:30.110: INFO: Starting http.Client for https://10.3.0.1:443/api/v1/namespaces/proxy-7536/pods/agnhost/proxy/some/path/with/POST
Dec  9 07:02:30.116: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Dec  9 07:02:30.117: INFO: Starting http.Client for https://10.3.0.1:443/api/v1/namespaces/proxy-7536/pods/agnhost/proxy/some/path/with/PUT
Dec  9 07:02:30.124: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Dec  9 07:02:30.124: INFO: Starting http.Client for https://10.3.0.1:443/api/v1/namespaces/proxy-7536/services/test-service/proxy/some/path/with/DELETE
Dec  9 07:02:30.137: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Dec  9 07:02:30.138: INFO: Starting http.Client for https://10.3.0.1:443/api/v1/namespaces/proxy-7536/services/test-service/proxy/some/path/with/GET
Dec  9 07:02:30.175: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Dec  9 07:02:30.176: INFO: Starting http.Client for https://10.3.0.1:443/api/v1/namespaces/proxy-7536/services/test-service/proxy/some/path/with/HEAD
Dec  9 07:02:30.184: INFO: http.Client request:HEAD | StatusCode:200
Dec  9 07:02:30.184: INFO: Starting http.Client for https://10.3.0.1:443/api/v1/namespaces/proxy-7536/services/test-service/proxy/some/path/with/OPTIONS
Dec  9 07:02:30.190: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Dec  9 07:02:30.191: INFO: Starting http.Client for https://10.3.0.1:443/api/v1/namespaces/proxy-7536/services/test-service/proxy/some/path/with/PATCH
Dec  9 07:02:30.201: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Dec  9 07:02:30.202: INFO: Starting http.Client for https://10.3.0.1:443/api/v1/namespaces/proxy-7536/services/test-service/proxy/some/path/with/POST
Dec  9 07:02:30.207: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Dec  9 07:02:30.208: INFO: Starting http.Client for https://10.3.0.1:443/api/v1/namespaces/proxy-7536/services/test-service/proxy/some/path/with/PUT
Dec  9 07:02:30.214: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
[AfterEach] version v1
  test/e2e/framework/node/init/init.go:32
Dec  9 07:02:30.215: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] version v1
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] version v1
  dump namespaces | framework.go:196
[DeferCleanup (Each)] version v1
  tear down framework | framework.go:193
STEP: Destroying namespace "proxy-7536" for this suite. 12/09/22 07:02:30.221
------------------------------
â€¢ [2.242 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
    test/e2e/network/proxy.go:286

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:02:27.987
    Dec  9 07:02:27.987: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename proxy 12/09/22 07:02:27.992
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:02:28.009
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:02:28.014
    [BeforeEach] version v1
      test/e2e/framework/metrics/init/init.go:31
    [It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
      test/e2e/network/proxy.go:286
    Dec  9 07:02:28.018: INFO: Creating pod...
    Dec  9 07:02:28.026: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-7536" to be "running"
    Dec  9 07:02:28.030: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 3.492732ms
    Dec  9 07:02:30.034: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.007207945s
    Dec  9 07:02:30.034: INFO: Pod "agnhost" satisfied condition "running"
    Dec  9 07:02:30.034: INFO: Creating service...
    Dec  9 07:02:30.050: INFO: Starting http.Client for https://10.3.0.1:443/api/v1/namespaces/proxy-7536/pods/agnhost/proxy/some/path/with/DELETE
    Dec  9 07:02:30.070: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Dec  9 07:02:30.070: INFO: Starting http.Client for https://10.3.0.1:443/api/v1/namespaces/proxy-7536/pods/agnhost/proxy/some/path/with/GET
    Dec  9 07:02:30.076: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
    Dec  9 07:02:30.077: INFO: Starting http.Client for https://10.3.0.1:443/api/v1/namespaces/proxy-7536/pods/agnhost/proxy/some/path/with/HEAD
    Dec  9 07:02:30.085: INFO: http.Client request:HEAD | StatusCode:200
    Dec  9 07:02:30.086: INFO: Starting http.Client for https://10.3.0.1:443/api/v1/namespaces/proxy-7536/pods/agnhost/proxy/some/path/with/OPTIONS
    Dec  9 07:02:30.094: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Dec  9 07:02:30.095: INFO: Starting http.Client for https://10.3.0.1:443/api/v1/namespaces/proxy-7536/pods/agnhost/proxy/some/path/with/PATCH
    Dec  9 07:02:30.109: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Dec  9 07:02:30.110: INFO: Starting http.Client for https://10.3.0.1:443/api/v1/namespaces/proxy-7536/pods/agnhost/proxy/some/path/with/POST
    Dec  9 07:02:30.116: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Dec  9 07:02:30.117: INFO: Starting http.Client for https://10.3.0.1:443/api/v1/namespaces/proxy-7536/pods/agnhost/proxy/some/path/with/PUT
    Dec  9 07:02:30.124: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Dec  9 07:02:30.124: INFO: Starting http.Client for https://10.3.0.1:443/api/v1/namespaces/proxy-7536/services/test-service/proxy/some/path/with/DELETE
    Dec  9 07:02:30.137: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Dec  9 07:02:30.138: INFO: Starting http.Client for https://10.3.0.1:443/api/v1/namespaces/proxy-7536/services/test-service/proxy/some/path/with/GET
    Dec  9 07:02:30.175: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
    Dec  9 07:02:30.176: INFO: Starting http.Client for https://10.3.0.1:443/api/v1/namespaces/proxy-7536/services/test-service/proxy/some/path/with/HEAD
    Dec  9 07:02:30.184: INFO: http.Client request:HEAD | StatusCode:200
    Dec  9 07:02:30.184: INFO: Starting http.Client for https://10.3.0.1:443/api/v1/namespaces/proxy-7536/services/test-service/proxy/some/path/with/OPTIONS
    Dec  9 07:02:30.190: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Dec  9 07:02:30.191: INFO: Starting http.Client for https://10.3.0.1:443/api/v1/namespaces/proxy-7536/services/test-service/proxy/some/path/with/PATCH
    Dec  9 07:02:30.201: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Dec  9 07:02:30.202: INFO: Starting http.Client for https://10.3.0.1:443/api/v1/namespaces/proxy-7536/services/test-service/proxy/some/path/with/POST
    Dec  9 07:02:30.207: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Dec  9 07:02:30.208: INFO: Starting http.Client for https://10.3.0.1:443/api/v1/namespaces/proxy-7536/services/test-service/proxy/some/path/with/PUT
    Dec  9 07:02:30.214: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    [AfterEach] version v1
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:02:30.215: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] version v1
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] version v1
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] version v1
      tear down framework | framework.go:193
    STEP: Destroying namespace "proxy-7536" for this suite. 12/09/22 07:02:30.221
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] Projected secret
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:119
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:02:30.229
Dec  9 07:02:30.233: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename projected 12/09/22 07:02:30.234
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:02:30.258
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:02:30.265
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:119
STEP: Creating secret with name projected-secret-test-68c13fd1-428b-449b-91b0-1e400ead0f5d 12/09/22 07:02:30.273
STEP: Creating a pod to test consume secrets 12/09/22 07:02:30.279
Dec  9 07:02:30.290: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-331bb76a-7d63-4435-9631-c70992494802" in namespace "projected-9289" to be "Succeeded or Failed"
Dec  9 07:02:30.310: INFO: Pod "pod-projected-secrets-331bb76a-7d63-4435-9631-c70992494802": Phase="Pending", Reason="", readiness=false. Elapsed: 18.87159ms
Dec  9 07:02:32.316: INFO: Pod "pod-projected-secrets-331bb76a-7d63-4435-9631-c70992494802": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024761555s
Dec  9 07:02:34.315: INFO: Pod "pod-projected-secrets-331bb76a-7d63-4435-9631-c70992494802": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024289331s
STEP: Saw pod success 12/09/22 07:02:34.315
Dec  9 07:02:34.315: INFO: Pod "pod-projected-secrets-331bb76a-7d63-4435-9631-c70992494802" satisfied condition "Succeeded or Failed"
Dec  9 07:02:34.318: INFO: Trying to get logs from node ip-10-0-17-108 pod pod-projected-secrets-331bb76a-7d63-4435-9631-c70992494802 container secret-volume-test: <nil>
STEP: delete the pod 12/09/22 07:02:34.332
Dec  9 07:02:34.342: INFO: Waiting for pod pod-projected-secrets-331bb76a-7d63-4435-9631-c70992494802 to disappear
Dec  9 07:02:34.345: INFO: Pod pod-projected-secrets-331bb76a-7d63-4435-9631-c70992494802 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Dec  9 07:02:34.345: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-9289" for this suite. 12/09/22 07:02:34.348
------------------------------
â€¢ [4.124 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:119

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:02:30.229
    Dec  9 07:02:30.233: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename projected 12/09/22 07:02:30.234
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:02:30.258
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:02:30.265
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:119
    STEP: Creating secret with name projected-secret-test-68c13fd1-428b-449b-91b0-1e400ead0f5d 12/09/22 07:02:30.273
    STEP: Creating a pod to test consume secrets 12/09/22 07:02:30.279
    Dec  9 07:02:30.290: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-331bb76a-7d63-4435-9631-c70992494802" in namespace "projected-9289" to be "Succeeded or Failed"
    Dec  9 07:02:30.310: INFO: Pod "pod-projected-secrets-331bb76a-7d63-4435-9631-c70992494802": Phase="Pending", Reason="", readiness=false. Elapsed: 18.87159ms
    Dec  9 07:02:32.316: INFO: Pod "pod-projected-secrets-331bb76a-7d63-4435-9631-c70992494802": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024761555s
    Dec  9 07:02:34.315: INFO: Pod "pod-projected-secrets-331bb76a-7d63-4435-9631-c70992494802": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024289331s
    STEP: Saw pod success 12/09/22 07:02:34.315
    Dec  9 07:02:34.315: INFO: Pod "pod-projected-secrets-331bb76a-7d63-4435-9631-c70992494802" satisfied condition "Succeeded or Failed"
    Dec  9 07:02:34.318: INFO: Trying to get logs from node ip-10-0-17-108 pod pod-projected-secrets-331bb76a-7d63-4435-9631-c70992494802 container secret-volume-test: <nil>
    STEP: delete the pod 12/09/22 07:02:34.332
    Dec  9 07:02:34.342: INFO: Waiting for pod pod-projected-secrets-331bb76a-7d63-4435-9631-c70992494802 to disappear
    Dec  9 07:02:34.345: INFO: Pod pod-projected-secrets-331bb76a-7d63-4435-9631-c70992494802 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:02:34.345: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-9289" for this suite. 12/09/22 07:02:34.348
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] EmptyDir volumes
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:87
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:02:34.354
Dec  9 07:02:34.354: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename emptydir 12/09/22 07:02:34.355
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:02:34.369
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:02:34.374
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:87
STEP: Creating a pod to test emptydir volume type on tmpfs 12/09/22 07:02:34.377
Dec  9 07:02:34.388: INFO: Waiting up to 5m0s for pod "pod-3ad2fb53-635a-4553-baf9-167d9994712f" in namespace "emptydir-1381" to be "Succeeded or Failed"
Dec  9 07:02:34.394: INFO: Pod "pod-3ad2fb53-635a-4553-baf9-167d9994712f": Phase="Pending", Reason="", readiness=false. Elapsed: 5.375924ms
Dec  9 07:02:36.397: INFO: Pod "pod-3ad2fb53-635a-4553-baf9-167d9994712f": Phase="Running", Reason="", readiness=false. Elapsed: 2.009286556s
Dec  9 07:02:38.399: INFO: Pod "pod-3ad2fb53-635a-4553-baf9-167d9994712f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011264991s
STEP: Saw pod success 12/09/22 07:02:38.4
Dec  9 07:02:38.400: INFO: Pod "pod-3ad2fb53-635a-4553-baf9-167d9994712f" satisfied condition "Succeeded or Failed"
Dec  9 07:02:38.403: INFO: Trying to get logs from node ip-10-0-10-179 pod pod-3ad2fb53-635a-4553-baf9-167d9994712f container test-container: <nil>
STEP: delete the pod 12/09/22 07:02:38.41
Dec  9 07:02:38.421: INFO: Waiting for pod pod-3ad2fb53-635a-4553-baf9-167d9994712f to disappear
Dec  9 07:02:38.436: INFO: Pod pod-3ad2fb53-635a-4553-baf9-167d9994712f no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Dec  9 07:02:38.436: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-1381" for this suite. 12/09/22 07:02:38.445
------------------------------
â€¢ [4.101 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:87

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:02:34.354
    Dec  9 07:02:34.354: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename emptydir 12/09/22 07:02:34.355
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:02:34.369
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:02:34.374
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:87
    STEP: Creating a pod to test emptydir volume type on tmpfs 12/09/22 07:02:34.377
    Dec  9 07:02:34.388: INFO: Waiting up to 5m0s for pod "pod-3ad2fb53-635a-4553-baf9-167d9994712f" in namespace "emptydir-1381" to be "Succeeded or Failed"
    Dec  9 07:02:34.394: INFO: Pod "pod-3ad2fb53-635a-4553-baf9-167d9994712f": Phase="Pending", Reason="", readiness=false. Elapsed: 5.375924ms
    Dec  9 07:02:36.397: INFO: Pod "pod-3ad2fb53-635a-4553-baf9-167d9994712f": Phase="Running", Reason="", readiness=false. Elapsed: 2.009286556s
    Dec  9 07:02:38.399: INFO: Pod "pod-3ad2fb53-635a-4553-baf9-167d9994712f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011264991s
    STEP: Saw pod success 12/09/22 07:02:38.4
    Dec  9 07:02:38.400: INFO: Pod "pod-3ad2fb53-635a-4553-baf9-167d9994712f" satisfied condition "Succeeded or Failed"
    Dec  9 07:02:38.403: INFO: Trying to get logs from node ip-10-0-10-179 pod pod-3ad2fb53-635a-4553-baf9-167d9994712f container test-container: <nil>
    STEP: delete the pod 12/09/22 07:02:38.41
    Dec  9 07:02:38.421: INFO: Waiting for pod pod-3ad2fb53-635a-4553-baf9-167d9994712f to disappear
    Dec  9 07:02:38.436: INFO: Pod pod-3ad2fb53-635a-4553-baf9-167d9994712f no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:02:38.436: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-1381" for this suite. 12/09/22 07:02:38.445
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:323
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:02:38.464
Dec  9 07:02:38.465: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename webhook 12/09/22 07:02:38.466
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:02:38.49
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:02:38.498
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 12/09/22 07:02:38.533
STEP: Create role binding to let webhook read extension-apiserver-authentication 12/09/22 07:02:38.978
STEP: Deploying the webhook pod 12/09/22 07:02:38.986
STEP: Wait for the deployment to be ready 12/09/22 07:02:39.002
Dec  9 07:02:39.013: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 12/09/22 07:02:41.022
STEP: Verifying the service has paired with the endpoint 12/09/22 07:02:41.039
Dec  9 07:02:42.039: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:323
Dec  9 07:02:42.043: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-5532-crds.webhook.example.com via the AdmissionRegistration API 12/09/22 07:02:42.626
STEP: Creating a custom resource while v1 is storage version 12/09/22 07:02:42.703
STEP: Patching Custom Resource Definition to set v2 as storage 12/09/22 07:02:44.871
STEP: Patching the custom resource while v2 is storage version 12/09/22 07:02:44.878
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Dec  9 07:02:45.462: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-5943" for this suite. 12/09/22 07:02:45.524
STEP: Destroying namespace "webhook-5943-markers" for this suite. 12/09/22 07:02:45.531
------------------------------
â€¢ [SLOW TEST] [7.080 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:323

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:02:38.464
    Dec  9 07:02:38.465: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename webhook 12/09/22 07:02:38.466
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:02:38.49
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:02:38.498
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 12/09/22 07:02:38.533
    STEP: Create role binding to let webhook read extension-apiserver-authentication 12/09/22 07:02:38.978
    STEP: Deploying the webhook pod 12/09/22 07:02:38.986
    STEP: Wait for the deployment to be ready 12/09/22 07:02:39.002
    Dec  9 07:02:39.013: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 12/09/22 07:02:41.022
    STEP: Verifying the service has paired with the endpoint 12/09/22 07:02:41.039
    Dec  9 07:02:42.039: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource with different stored version [Conformance]
      test/e2e/apimachinery/webhook.go:323
    Dec  9 07:02:42.043: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-5532-crds.webhook.example.com via the AdmissionRegistration API 12/09/22 07:02:42.626
    STEP: Creating a custom resource while v1 is storage version 12/09/22 07:02:42.703
    STEP: Patching Custom Resource Definition to set v2 as storage 12/09/22 07:02:44.871
    STEP: Patching the custom resource while v2 is storage version 12/09/22 07:02:44.878
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:02:45.462: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-5943" for this suite. 12/09/22 07:02:45.524
    STEP: Destroying namespace "webhook-5943-markers" for this suite. 12/09/22 07:02:45.531
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:443
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:02:45.548
Dec  9 07:02:45.548: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename sched-pred 12/09/22 07:02:45.549
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:02:45.566
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:02:45.57
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:97
Dec  9 07:02:45.576: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Dec  9 07:02:45.586: INFO: Waiting for terminating namespaces to be deleted...
Dec  9 07:02:45.590: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-10-179 before test
Dec  9 07:02:45.601: INFO: calico-node-9zhz5 from kube-system started at 2022-12-09 06:56:15 +0000 UTC (1 container statuses recorded)
Dec  9 07:02:45.601: INFO: 	Container calico-node ready: true, restart count 0
Dec  9 07:02:45.601: INFO: coredns-844594ffdd-q2bzk from kube-system started at 2022-12-09 06:56:25 +0000 UTC (1 container statuses recorded)
Dec  9 07:02:45.601: INFO: 	Container coredns ready: true, restart count 0
Dec  9 07:02:45.601: INFO: coredns-844594ffdd-rls2f from kube-system started at 2022-12-09 06:56:25 +0000 UTC (1 container statuses recorded)
Dec  9 07:02:45.601: INFO: 	Container coredns ready: true, restart count 0
Dec  9 07:02:45.601: INFO: kube-proxy-dv66w from kube-system started at 2022-12-09 06:56:15 +0000 UTC (1 container statuses recorded)
Dec  9 07:02:45.601: INFO: 	Container kube-proxy ready: true, restart count 0
Dec  9 07:02:45.601: INFO: sonobuoy-systemd-logs-daemon-set-8dafb6c50b3e4797-scljx from sonobuoy started at 2022-12-09 06:58:30 +0000 UTC (2 container statuses recorded)
Dec  9 07:02:45.601: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec  9 07:02:45.601: INFO: 	Container systemd-logs ready: true, restart count 0
Dec  9 07:02:45.601: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-17-108 before test
Dec  9 07:02:45.609: INFO: calico-node-flv5c from kube-system started at 2022-12-09 06:56:14 +0000 UTC (1 container statuses recorded)
Dec  9 07:02:45.609: INFO: 	Container calico-node ready: true, restart count 0
Dec  9 07:02:45.609: INFO: kube-proxy-r75k5 from kube-system started at 2022-12-09 06:56:14 +0000 UTC (1 container statuses recorded)
Dec  9 07:02:45.610: INFO: 	Container kube-proxy ready: true, restart count 0
Dec  9 07:02:45.610: INFO: sonobuoy from sonobuoy started at 2022-12-09 06:58:27 +0000 UTC (1 container statuses recorded)
Dec  9 07:02:45.610: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Dec  9 07:02:45.610: INFO: sonobuoy-e2e-job-023adc8283f34c8c from sonobuoy started at 2022-12-09 06:58:30 +0000 UTC (2 container statuses recorded)
Dec  9 07:02:45.610: INFO: 	Container e2e ready: true, restart count 0
Dec  9 07:02:45.610: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec  9 07:02:45.610: INFO: sonobuoy-systemd-logs-daemon-set-8dafb6c50b3e4797-jg2s9 from sonobuoy started at 2022-12-09 06:58:30 +0000 UTC (2 container statuses recorded)
Dec  9 07:02:45.611: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec  9 07:02:45.611: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:443
STEP: Trying to schedule Pod with nonempty NodeSelector. 12/09/22 07:02:45.611
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.172f0dfcabb8b171], Reason = [FailedScheduling], Message = [0/3 nodes are available: 1 node(s) had untolerated taint {node-role.kubernetes.io/controller: }, 3 node(s) didn't match Pod's node affinity/selector. preemption: 0/3 nodes are available: 3 Preemption is not helpful for scheduling..] 12/09/22 07:02:45.788
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/node/init/init.go:32
Dec  9 07:02:46.638: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:88
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-pred-7470" for this suite. 12/09/22 07:02:46.642
------------------------------
â€¢ [1.102 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:443

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:02:45.548
    Dec  9 07:02:45.548: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename sched-pred 12/09/22 07:02:45.549
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:02:45.566
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:02:45.57
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:97
    Dec  9 07:02:45.576: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Dec  9 07:02:45.586: INFO: Waiting for terminating namespaces to be deleted...
    Dec  9 07:02:45.590: INFO: 
    Logging pods the apiserver thinks is on node ip-10-0-10-179 before test
    Dec  9 07:02:45.601: INFO: calico-node-9zhz5 from kube-system started at 2022-12-09 06:56:15 +0000 UTC (1 container statuses recorded)
    Dec  9 07:02:45.601: INFO: 	Container calico-node ready: true, restart count 0
    Dec  9 07:02:45.601: INFO: coredns-844594ffdd-q2bzk from kube-system started at 2022-12-09 06:56:25 +0000 UTC (1 container statuses recorded)
    Dec  9 07:02:45.601: INFO: 	Container coredns ready: true, restart count 0
    Dec  9 07:02:45.601: INFO: coredns-844594ffdd-rls2f from kube-system started at 2022-12-09 06:56:25 +0000 UTC (1 container statuses recorded)
    Dec  9 07:02:45.601: INFO: 	Container coredns ready: true, restart count 0
    Dec  9 07:02:45.601: INFO: kube-proxy-dv66w from kube-system started at 2022-12-09 06:56:15 +0000 UTC (1 container statuses recorded)
    Dec  9 07:02:45.601: INFO: 	Container kube-proxy ready: true, restart count 0
    Dec  9 07:02:45.601: INFO: sonobuoy-systemd-logs-daemon-set-8dafb6c50b3e4797-scljx from sonobuoy started at 2022-12-09 06:58:30 +0000 UTC (2 container statuses recorded)
    Dec  9 07:02:45.601: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Dec  9 07:02:45.601: INFO: 	Container systemd-logs ready: true, restart count 0
    Dec  9 07:02:45.601: INFO: 
    Logging pods the apiserver thinks is on node ip-10-0-17-108 before test
    Dec  9 07:02:45.609: INFO: calico-node-flv5c from kube-system started at 2022-12-09 06:56:14 +0000 UTC (1 container statuses recorded)
    Dec  9 07:02:45.609: INFO: 	Container calico-node ready: true, restart count 0
    Dec  9 07:02:45.609: INFO: kube-proxy-r75k5 from kube-system started at 2022-12-09 06:56:14 +0000 UTC (1 container statuses recorded)
    Dec  9 07:02:45.610: INFO: 	Container kube-proxy ready: true, restart count 0
    Dec  9 07:02:45.610: INFO: sonobuoy from sonobuoy started at 2022-12-09 06:58:27 +0000 UTC (1 container statuses recorded)
    Dec  9 07:02:45.610: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Dec  9 07:02:45.610: INFO: sonobuoy-e2e-job-023adc8283f34c8c from sonobuoy started at 2022-12-09 06:58:30 +0000 UTC (2 container statuses recorded)
    Dec  9 07:02:45.610: INFO: 	Container e2e ready: true, restart count 0
    Dec  9 07:02:45.610: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Dec  9 07:02:45.610: INFO: sonobuoy-systemd-logs-daemon-set-8dafb6c50b3e4797-jg2s9 from sonobuoy started at 2022-12-09 06:58:30 +0000 UTC (2 container statuses recorded)
    Dec  9 07:02:45.611: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Dec  9 07:02:45.611: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates that NodeSelector is respected if not matching  [Conformance]
      test/e2e/scheduling/predicates.go:443
    STEP: Trying to schedule Pod with nonempty NodeSelector. 12/09/22 07:02:45.611
    STEP: Considering event: 
    Type = [Warning], Name = [restricted-pod.172f0dfcabb8b171], Reason = [FailedScheduling], Message = [0/3 nodes are available: 1 node(s) had untolerated taint {node-role.kubernetes.io/controller: }, 3 node(s) didn't match Pod's node affinity/selector. preemption: 0/3 nodes are available: 3 Preemption is not helpful for scheduling..] 12/09/22 07:02:45.788
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:02:46.638: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:88
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-pred-7470" for this suite. 12/09/22 07:02:46.642
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:124
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:02:46.654
Dec  9 07:02:46.654: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename configmap 12/09/22 07:02:46.655
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:02:46.669
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:02:46.674
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:124
STEP: Creating configMap with name configmap-test-upd-60c33b4b-27d4-4178-8b31-a965954d4c8c 12/09/22 07:02:46.681
STEP: Creating the pod 12/09/22 07:02:46.685
Dec  9 07:02:46.695: INFO: Waiting up to 5m0s for pod "pod-configmaps-175c3e5e-9cbb-4573-bef0-d5e48b0088ba" in namespace "configmap-2035" to be "running and ready"
Dec  9 07:02:46.701: INFO: Pod "pod-configmaps-175c3e5e-9cbb-4573-bef0-d5e48b0088ba": Phase="Pending", Reason="", readiness=false. Elapsed: 5.191686ms
Dec  9 07:02:46.701: INFO: The phase of Pod pod-configmaps-175c3e5e-9cbb-4573-bef0-d5e48b0088ba is Pending, waiting for it to be Running (with Ready = true)
Dec  9 07:02:48.706: INFO: Pod "pod-configmaps-175c3e5e-9cbb-4573-bef0-d5e48b0088ba": Phase="Running", Reason="", readiness=true. Elapsed: 2.010191488s
Dec  9 07:02:48.706: INFO: The phase of Pod pod-configmaps-175c3e5e-9cbb-4573-bef0-d5e48b0088ba is Running (Ready = true)
Dec  9 07:02:48.706: INFO: Pod "pod-configmaps-175c3e5e-9cbb-4573-bef0-d5e48b0088ba" satisfied condition "running and ready"
STEP: Updating configmap configmap-test-upd-60c33b4b-27d4-4178-8b31-a965954d4c8c 12/09/22 07:02:48.714
STEP: waiting to observe update in volume 12/09/22 07:02:48.721
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Dec  9 07:02:50.798: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-2035" for this suite. 12/09/22 07:02:50.812
------------------------------
â€¢ [4.168 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:02:46.654
    Dec  9 07:02:46.654: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename configmap 12/09/22 07:02:46.655
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:02:46.669
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:02:46.674
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:124
    STEP: Creating configMap with name configmap-test-upd-60c33b4b-27d4-4178-8b31-a965954d4c8c 12/09/22 07:02:46.681
    STEP: Creating the pod 12/09/22 07:02:46.685
    Dec  9 07:02:46.695: INFO: Waiting up to 5m0s for pod "pod-configmaps-175c3e5e-9cbb-4573-bef0-d5e48b0088ba" in namespace "configmap-2035" to be "running and ready"
    Dec  9 07:02:46.701: INFO: Pod "pod-configmaps-175c3e5e-9cbb-4573-bef0-d5e48b0088ba": Phase="Pending", Reason="", readiness=false. Elapsed: 5.191686ms
    Dec  9 07:02:46.701: INFO: The phase of Pod pod-configmaps-175c3e5e-9cbb-4573-bef0-d5e48b0088ba is Pending, waiting for it to be Running (with Ready = true)
    Dec  9 07:02:48.706: INFO: Pod "pod-configmaps-175c3e5e-9cbb-4573-bef0-d5e48b0088ba": Phase="Running", Reason="", readiness=true. Elapsed: 2.010191488s
    Dec  9 07:02:48.706: INFO: The phase of Pod pod-configmaps-175c3e5e-9cbb-4573-bef0-d5e48b0088ba is Running (Ready = true)
    Dec  9 07:02:48.706: INFO: Pod "pod-configmaps-175c3e5e-9cbb-4573-bef0-d5e48b0088ba" satisfied condition "running and ready"
    STEP: Updating configmap configmap-test-upd-60c33b4b-27d4-4178-8b31-a965954d4c8c 12/09/22 07:02:48.714
    STEP: waiting to observe update in volume 12/09/22 07:02:48.721
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:02:50.798: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-2035" for this suite. 12/09/22 07:02:50.812
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice
  should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:353
[BeforeEach] [sig-network] EndpointSlice
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:02:50.838
Dec  9 07:02:50.838: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename endpointslice 12/09/22 07:02:50.84
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:02:50.883
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:02:50.895
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:52
[It] should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:353
STEP: getting /apis 12/09/22 07:02:50.904
STEP: getting /apis/discovery.k8s.io 12/09/22 07:02:50.928
STEP: getting /apis/discovery.k8s.iov1 12/09/22 07:02:50.931
STEP: creating 12/09/22 07:02:50.934
STEP: getting 12/09/22 07:02:50.972
STEP: listing 12/09/22 07:02:50.977
STEP: watching 12/09/22 07:02:50.98
Dec  9 07:02:50.981: INFO: starting watch
STEP: cluster-wide listing 12/09/22 07:02:50.996
STEP: cluster-wide watching 12/09/22 07:02:51.006
Dec  9 07:02:51.006: INFO: starting watch
STEP: patching 12/09/22 07:02:51.018
STEP: updating 12/09/22 07:02:51.03
Dec  9 07:02:51.064: INFO: waiting for watch events with expected annotations
Dec  9 07:02:51.064: INFO: saw patched and updated annotations
STEP: deleting 12/09/22 07:02:51.065
STEP: deleting a collection 12/09/22 07:02:51.115
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/node/init/init.go:32
Dec  9 07:02:51.146: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSlice
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSlice
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslice-7680" for this suite. 12/09/22 07:02:51.152
------------------------------
â€¢ [0.330 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:353

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:02:50.838
    Dec  9 07:02:50.838: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename endpointslice 12/09/22 07:02:50.84
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:02:50.883
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:02:50.895
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:52
    [It] should support creating EndpointSlice API operations [Conformance]
      test/e2e/network/endpointslice.go:353
    STEP: getting /apis 12/09/22 07:02:50.904
    STEP: getting /apis/discovery.k8s.io 12/09/22 07:02:50.928
    STEP: getting /apis/discovery.k8s.iov1 12/09/22 07:02:50.931
    STEP: creating 12/09/22 07:02:50.934
    STEP: getting 12/09/22 07:02:50.972
    STEP: listing 12/09/22 07:02:50.977
    STEP: watching 12/09/22 07:02:50.98
    Dec  9 07:02:50.981: INFO: starting watch
    STEP: cluster-wide listing 12/09/22 07:02:50.996
    STEP: cluster-wide watching 12/09/22 07:02:51.006
    Dec  9 07:02:51.006: INFO: starting watch
    STEP: patching 12/09/22 07:02:51.018
    STEP: updating 12/09/22 07:02:51.03
    Dec  9 07:02:51.064: INFO: waiting for watch events with expected annotations
    Dec  9 07:02:51.064: INFO: saw patched and updated annotations
    STEP: deleting 12/09/22 07:02:51.065
    STEP: deleting a collection 12/09/22 07:02:51.115
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:02:51.146: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslice-7680" for this suite. 12/09/22 07:02:51.152
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:649
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:02:51.168
Dec  9 07:02:51.168: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename svcaccounts 12/09/22 07:02:51.17
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:02:51.218
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:02:51.225
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:649
STEP: creating a ServiceAccount 12/09/22 07:02:51.231
STEP: watching for the ServiceAccount to be added 12/09/22 07:02:51.248
STEP: patching the ServiceAccount 12/09/22 07:02:51.252
STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector) 12/09/22 07:02:51.262
STEP: deleting the ServiceAccount 12/09/22 07:02:51.267
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Dec  9 07:02:51.283: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-5792" for this suite. 12/09/22 07:02:51.294
------------------------------
â€¢ [0.135 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:649

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:02:51.168
    Dec  9 07:02:51.168: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename svcaccounts 12/09/22 07:02:51.17
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:02:51.218
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:02:51.225
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should run through the lifecycle of a ServiceAccount [Conformance]
      test/e2e/auth/service_accounts.go:649
    STEP: creating a ServiceAccount 12/09/22 07:02:51.231
    STEP: watching for the ServiceAccount to be added 12/09/22 07:02:51.248
    STEP: patching the ServiceAccount 12/09/22 07:02:51.252
    STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector) 12/09/22 07:02:51.262
    STEP: deleting the ServiceAccount 12/09/22 07:02:51.267
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:02:51.283: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-5792" for this suite. 12/09/22 07:02:51.294
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] HostPort
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63
[BeforeEach] [sig-network] HostPort
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:02:51.309
Dec  9 07:02:51.309: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename hostport 12/09/22 07:02:51.31
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:02:51.346
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:02:51.354
[BeforeEach] [sig-network] HostPort
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] HostPort
  test/e2e/network/hostport.go:49
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63
STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled 12/09/22 07:02:51.369
Dec  9 07:02:51.401: INFO: Waiting up to 5m0s for pod "pod1" in namespace "hostport-9091" to be "running and ready"
Dec  9 07:02:51.407: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 5.89284ms
Dec  9 07:02:51.407: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Dec  9 07:02:53.412: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.010803788s
Dec  9 07:02:53.412: INFO: The phase of Pod pod1 is Running (Ready = true)
Dec  9 07:02:53.412: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 10.0.10.179 on the node which pod1 resides and expect scheduled 12/09/22 07:02:53.412
Dec  9 07:02:53.419: INFO: Waiting up to 5m0s for pod "pod2" in namespace "hostport-9091" to be "running and ready"
Dec  9 07:02:53.425: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 5.81788ms
Dec  9 07:02:53.425: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Dec  9 07:02:55.428: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.009211021s
Dec  9 07:02:55.428: INFO: The phase of Pod pod2 is Running (Ready = true)
Dec  9 07:02:55.429: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 10.0.10.179 but use UDP protocol on the node which pod2 resides 12/09/22 07:02:55.429
Dec  9 07:02:55.442: INFO: Waiting up to 5m0s for pod "pod3" in namespace "hostport-9091" to be "running and ready"
Dec  9 07:02:55.451: INFO: Pod "pod3": Phase="Pending", Reason="", readiness=false. Elapsed: 9.204019ms
Dec  9 07:02:55.452: INFO: The phase of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
Dec  9 07:02:57.456: INFO: Pod "pod3": Phase="Running", Reason="", readiness=true. Elapsed: 2.01366035s
Dec  9 07:02:57.456: INFO: The phase of Pod pod3 is Running (Ready = true)
Dec  9 07:02:57.456: INFO: Pod "pod3" satisfied condition "running and ready"
Dec  9 07:02:57.462: INFO: Waiting up to 5m0s for pod "e2e-host-exec" in namespace "hostport-9091" to be "running and ready"
Dec  9 07:02:57.469: INFO: Pod "e2e-host-exec": Phase="Pending", Reason="", readiness=false. Elapsed: 6.209167ms
Dec  9 07:02:57.469: INFO: The phase of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
Dec  9 07:02:59.473: INFO: Pod "e2e-host-exec": Phase="Running", Reason="", readiness=true. Elapsed: 2.010522999s
Dec  9 07:02:59.473: INFO: The phase of Pod e2e-host-exec is Running (Ready = true)
Dec  9 07:02:59.473: INFO: Pod "e2e-host-exec" satisfied condition "running and ready"
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323 12/09/22 07:02:59.476
Dec  9 07:02:59.477: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.0.10.179 http://127.0.0.1:54323/hostname] Namespace:hostport-9091 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec  9 07:02:59.477: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
Dec  9 07:02:59.478: INFO: ExecWithOptions: Clientset creation
Dec  9 07:02:59.478: INFO: ExecWithOptions: execute(POST https://10.3.0.1:443/api/v1/namespaces/hostport-9091/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+10.0.10.179+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.0.10.179, port: 54323 12/09/22 07:02:59.559
Dec  9 07:02:59.559: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.0.10.179:54323/hostname] Namespace:hostport-9091 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec  9 07:02:59.559: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
Dec  9 07:02:59.560: INFO: ExecWithOptions: Clientset creation
Dec  9 07:02:59.560: INFO: ExecWithOptions: execute(POST https://10.3.0.1:443/api/v1/namespaces/hostport-9091/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F10.0.10.179%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.0.10.179, port: 54323 UDP 12/09/22 07:02:59.63
Dec  9 07:02:59.630: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostname | nc -u -w 5 10.0.10.179 54323] Namespace:hostport-9091 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec  9 07:02:59.630: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
Dec  9 07:02:59.630: INFO: ExecWithOptions: Clientset creation
Dec  9 07:02:59.631: INFO: ExecWithOptions: execute(POST https://10.3.0.1:443/api/v1/namespaces/hostport-9091/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostname+%7C+nc+-u+-w+5+10.0.10.179+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
[AfterEach] [sig-network] HostPort
  test/e2e/framework/node/init/init.go:32
Dec  9 07:03:04.709: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] HostPort
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] HostPort
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] HostPort
  tear down framework | framework.go:193
STEP: Destroying namespace "hostport-9091" for this suite. 12/09/22 07:03:04.715
------------------------------
â€¢ [SLOW TEST] [13.420 seconds]
[sig-network] HostPort
test/e2e/network/common/framework.go:23
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] HostPort
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:02:51.309
    Dec  9 07:02:51.309: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename hostport 12/09/22 07:02:51.31
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:02:51.346
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:02:51.354
    [BeforeEach] [sig-network] HostPort
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] HostPort
      test/e2e/network/hostport.go:49
    [It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
      test/e2e/network/hostport.go:63
    STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled 12/09/22 07:02:51.369
    Dec  9 07:02:51.401: INFO: Waiting up to 5m0s for pod "pod1" in namespace "hostport-9091" to be "running and ready"
    Dec  9 07:02:51.407: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 5.89284ms
    Dec  9 07:02:51.407: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Dec  9 07:02:53.412: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.010803788s
    Dec  9 07:02:53.412: INFO: The phase of Pod pod1 is Running (Ready = true)
    Dec  9 07:02:53.412: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 10.0.10.179 on the node which pod1 resides and expect scheduled 12/09/22 07:02:53.412
    Dec  9 07:02:53.419: INFO: Waiting up to 5m0s for pod "pod2" in namespace "hostport-9091" to be "running and ready"
    Dec  9 07:02:53.425: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 5.81788ms
    Dec  9 07:02:53.425: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Dec  9 07:02:55.428: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.009211021s
    Dec  9 07:02:55.428: INFO: The phase of Pod pod2 is Running (Ready = true)
    Dec  9 07:02:55.429: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 10.0.10.179 but use UDP protocol on the node which pod2 resides 12/09/22 07:02:55.429
    Dec  9 07:02:55.442: INFO: Waiting up to 5m0s for pod "pod3" in namespace "hostport-9091" to be "running and ready"
    Dec  9 07:02:55.451: INFO: Pod "pod3": Phase="Pending", Reason="", readiness=false. Elapsed: 9.204019ms
    Dec  9 07:02:55.452: INFO: The phase of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
    Dec  9 07:02:57.456: INFO: Pod "pod3": Phase="Running", Reason="", readiness=true. Elapsed: 2.01366035s
    Dec  9 07:02:57.456: INFO: The phase of Pod pod3 is Running (Ready = true)
    Dec  9 07:02:57.456: INFO: Pod "pod3" satisfied condition "running and ready"
    Dec  9 07:02:57.462: INFO: Waiting up to 5m0s for pod "e2e-host-exec" in namespace "hostport-9091" to be "running and ready"
    Dec  9 07:02:57.469: INFO: Pod "e2e-host-exec": Phase="Pending", Reason="", readiness=false. Elapsed: 6.209167ms
    Dec  9 07:02:57.469: INFO: The phase of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
    Dec  9 07:02:59.473: INFO: Pod "e2e-host-exec": Phase="Running", Reason="", readiness=true. Elapsed: 2.010522999s
    Dec  9 07:02:59.473: INFO: The phase of Pod e2e-host-exec is Running (Ready = true)
    Dec  9 07:02:59.473: INFO: Pod "e2e-host-exec" satisfied condition "running and ready"
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323 12/09/22 07:02:59.476
    Dec  9 07:02:59.477: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.0.10.179 http://127.0.0.1:54323/hostname] Namespace:hostport-9091 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Dec  9 07:02:59.477: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    Dec  9 07:02:59.478: INFO: ExecWithOptions: Clientset creation
    Dec  9 07:02:59.478: INFO: ExecWithOptions: execute(POST https://10.3.0.1:443/api/v1/namespaces/hostport-9091/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+10.0.10.179+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.0.10.179, port: 54323 12/09/22 07:02:59.559
    Dec  9 07:02:59.559: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.0.10.179:54323/hostname] Namespace:hostport-9091 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Dec  9 07:02:59.559: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    Dec  9 07:02:59.560: INFO: ExecWithOptions: Clientset creation
    Dec  9 07:02:59.560: INFO: ExecWithOptions: execute(POST https://10.3.0.1:443/api/v1/namespaces/hostport-9091/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F10.0.10.179%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.0.10.179, port: 54323 UDP 12/09/22 07:02:59.63
    Dec  9 07:02:59.630: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostname | nc -u -w 5 10.0.10.179 54323] Namespace:hostport-9091 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Dec  9 07:02:59.630: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    Dec  9 07:02:59.630: INFO: ExecWithOptions: Clientset creation
    Dec  9 07:02:59.631: INFO: ExecWithOptions: execute(POST https://10.3.0.1:443/api/v1/namespaces/hostport-9091/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostname+%7C+nc+-u+-w+5+10.0.10.179+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    [AfterEach] [sig-network] HostPort
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:03:04.709: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] HostPort
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] HostPort
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] HostPort
      tear down framework | framework.go:193
    STEP: Destroying namespace "hostport-9091" for this suite. 12/09/22 07:03:04.715
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:169
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:03:04.735
Dec  9 07:03:04.735: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename container-probe 12/09/22 07:03:04.737
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:03:04.759
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:03:04.765
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:169
STEP: Creating pod liveness-98e2e82b-68ae-4033-9c81-18cc88a0baef in namespace container-probe-4880 12/09/22 07:03:04.771
Dec  9 07:03:04.780: INFO: Waiting up to 5m0s for pod "liveness-98e2e82b-68ae-4033-9c81-18cc88a0baef" in namespace "container-probe-4880" to be "not pending"
Dec  9 07:03:04.784: INFO: Pod "liveness-98e2e82b-68ae-4033-9c81-18cc88a0baef": Phase="Pending", Reason="", readiness=false. Elapsed: 4.191984ms
Dec  9 07:03:06.790: INFO: Pod "liveness-98e2e82b-68ae-4033-9c81-18cc88a0baef": Phase="Running", Reason="", readiness=true. Elapsed: 2.009469452s
Dec  9 07:03:06.790: INFO: Pod "liveness-98e2e82b-68ae-4033-9c81-18cc88a0baef" satisfied condition "not pending"
Dec  9 07:03:06.790: INFO: Started pod liveness-98e2e82b-68ae-4033-9c81-18cc88a0baef in namespace container-probe-4880
STEP: checking the pod's current state and verifying that restartCount is present 12/09/22 07:03:06.79
Dec  9 07:03:06.793: INFO: Initial restart count of pod liveness-98e2e82b-68ae-4033-9c81-18cc88a0baef is 0
Dec  9 07:03:26.878: INFO: Restart count of pod container-probe-4880/liveness-98e2e82b-68ae-4033-9c81-18cc88a0baef is now 1 (20.08524164s elapsed)
STEP: deleting the pod 12/09/22 07:03:26.878
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Dec  9 07:03:26.902: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-4880" for this suite. 12/09/22 07:03:26.906
------------------------------
â€¢ [SLOW TEST] [22.177 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:169

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:03:04.735
    Dec  9 07:03:04.735: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename container-probe 12/09/22 07:03:04.737
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:03:04.759
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:03:04.765
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:169
    STEP: Creating pod liveness-98e2e82b-68ae-4033-9c81-18cc88a0baef in namespace container-probe-4880 12/09/22 07:03:04.771
    Dec  9 07:03:04.780: INFO: Waiting up to 5m0s for pod "liveness-98e2e82b-68ae-4033-9c81-18cc88a0baef" in namespace "container-probe-4880" to be "not pending"
    Dec  9 07:03:04.784: INFO: Pod "liveness-98e2e82b-68ae-4033-9c81-18cc88a0baef": Phase="Pending", Reason="", readiness=false. Elapsed: 4.191984ms
    Dec  9 07:03:06.790: INFO: Pod "liveness-98e2e82b-68ae-4033-9c81-18cc88a0baef": Phase="Running", Reason="", readiness=true. Elapsed: 2.009469452s
    Dec  9 07:03:06.790: INFO: Pod "liveness-98e2e82b-68ae-4033-9c81-18cc88a0baef" satisfied condition "not pending"
    Dec  9 07:03:06.790: INFO: Started pod liveness-98e2e82b-68ae-4033-9c81-18cc88a0baef in namespace container-probe-4880
    STEP: checking the pod's current state and verifying that restartCount is present 12/09/22 07:03:06.79
    Dec  9 07:03:06.793: INFO: Initial restart count of pod liveness-98e2e82b-68ae-4033-9c81-18cc88a0baef is 0
    Dec  9 07:03:26.878: INFO: Restart count of pod container-probe-4880/liveness-98e2e82b-68ae-4033-9c81-18cc88a0baef is now 1 (20.08524164s elapsed)
    STEP: deleting the pod 12/09/22 07:03:26.878
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:03:26.902: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-4880" for this suite. 12/09/22 07:03:26.906
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo
  should create and stop a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:339
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:03:26.913
Dec  9 07:03:26.913: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename kubectl 12/09/22 07:03:26.916
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:03:26.934
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:03:26.939
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Update Demo
  test/e2e/kubectl/kubectl.go:326
[It] should create and stop a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:339
STEP: creating a replication controller 12/09/22 07:03:26.943
Dec  9 07:03:26.943: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=kubectl-7182 create -f -'
Dec  9 07:03:28.018: INFO: stderr: ""
Dec  9 07:03:28.018: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up. 12/09/22 07:03:28.018
Dec  9 07:03:28.018: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=kubectl-7182 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Dec  9 07:03:28.118: INFO: stderr: ""
Dec  9 07:03:28.118: INFO: stdout: "update-demo-nautilus-28tt7 update-demo-nautilus-x2qs6 "
Dec  9 07:03:28.118: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=kubectl-7182 get pods update-demo-nautilus-28tt7 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Dec  9 07:03:28.201: INFO: stderr: ""
Dec  9 07:03:28.201: INFO: stdout: ""
Dec  9 07:03:28.201: INFO: update-demo-nautilus-28tt7 is created but not running
Dec  9 07:03:33.203: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=kubectl-7182 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Dec  9 07:03:33.277: INFO: stderr: ""
Dec  9 07:03:33.277: INFO: stdout: "update-demo-nautilus-28tt7 update-demo-nautilus-x2qs6 "
Dec  9 07:03:33.277: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=kubectl-7182 get pods update-demo-nautilus-28tt7 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Dec  9 07:03:33.345: INFO: stderr: ""
Dec  9 07:03:33.345: INFO: stdout: "true"
Dec  9 07:03:33.345: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=kubectl-7182 get pods update-demo-nautilus-28tt7 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Dec  9 07:03:33.410: INFO: stderr: ""
Dec  9 07:03:33.410: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Dec  9 07:03:33.410: INFO: validating pod update-demo-nautilus-28tt7
Dec  9 07:03:33.414: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec  9 07:03:33.415: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec  9 07:03:33.415: INFO: update-demo-nautilus-28tt7 is verified up and running
Dec  9 07:03:33.415: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=kubectl-7182 get pods update-demo-nautilus-x2qs6 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Dec  9 07:03:33.485: INFO: stderr: ""
Dec  9 07:03:33.485: INFO: stdout: "true"
Dec  9 07:03:33.485: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=kubectl-7182 get pods update-demo-nautilus-x2qs6 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Dec  9 07:03:33.552: INFO: stderr: ""
Dec  9 07:03:33.552: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Dec  9 07:03:33.552: INFO: validating pod update-demo-nautilus-x2qs6
Dec  9 07:03:33.558: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec  9 07:03:33.558: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec  9 07:03:33.558: INFO: update-demo-nautilus-x2qs6 is verified up and running
STEP: using delete to clean up resources 12/09/22 07:03:33.558
Dec  9 07:03:33.558: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=kubectl-7182 delete --grace-period=0 --force -f -'
Dec  9 07:03:33.635: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec  9 07:03:33.635: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Dec  9 07:03:33.635: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=kubectl-7182 get rc,svc -l name=update-demo --no-headers'
Dec  9 07:03:33.742: INFO: stderr: "No resources found in kubectl-7182 namespace.\n"
Dec  9 07:03:33.742: INFO: stdout: ""
Dec  9 07:03:33.742: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=kubectl-7182 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Dec  9 07:03:33.817: INFO: stderr: ""
Dec  9 07:03:33.817: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Dec  9 07:03:33.817: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-7182" for this suite. 12/09/22 07:03:33.82
------------------------------
â€¢ [SLOW TEST] [6.913 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Update Demo
  test/e2e/kubectl/kubectl.go:324
    should create and stop a replication controller  [Conformance]
    test/e2e/kubectl/kubectl.go:339

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:03:26.913
    Dec  9 07:03:26.913: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename kubectl 12/09/22 07:03:26.916
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:03:26.934
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:03:26.939
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Update Demo
      test/e2e/kubectl/kubectl.go:326
    [It] should create and stop a replication controller  [Conformance]
      test/e2e/kubectl/kubectl.go:339
    STEP: creating a replication controller 12/09/22 07:03:26.943
    Dec  9 07:03:26.943: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=kubectl-7182 create -f -'
    Dec  9 07:03:28.018: INFO: stderr: ""
    Dec  9 07:03:28.018: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 12/09/22 07:03:28.018
    Dec  9 07:03:28.018: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=kubectl-7182 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Dec  9 07:03:28.118: INFO: stderr: ""
    Dec  9 07:03:28.118: INFO: stdout: "update-demo-nautilus-28tt7 update-demo-nautilus-x2qs6 "
    Dec  9 07:03:28.118: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=kubectl-7182 get pods update-demo-nautilus-28tt7 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Dec  9 07:03:28.201: INFO: stderr: ""
    Dec  9 07:03:28.201: INFO: stdout: ""
    Dec  9 07:03:28.201: INFO: update-demo-nautilus-28tt7 is created but not running
    Dec  9 07:03:33.203: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=kubectl-7182 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Dec  9 07:03:33.277: INFO: stderr: ""
    Dec  9 07:03:33.277: INFO: stdout: "update-demo-nautilus-28tt7 update-demo-nautilus-x2qs6 "
    Dec  9 07:03:33.277: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=kubectl-7182 get pods update-demo-nautilus-28tt7 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Dec  9 07:03:33.345: INFO: stderr: ""
    Dec  9 07:03:33.345: INFO: stdout: "true"
    Dec  9 07:03:33.345: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=kubectl-7182 get pods update-demo-nautilus-28tt7 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Dec  9 07:03:33.410: INFO: stderr: ""
    Dec  9 07:03:33.410: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Dec  9 07:03:33.410: INFO: validating pod update-demo-nautilus-28tt7
    Dec  9 07:03:33.414: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Dec  9 07:03:33.415: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Dec  9 07:03:33.415: INFO: update-demo-nautilus-28tt7 is verified up and running
    Dec  9 07:03:33.415: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=kubectl-7182 get pods update-demo-nautilus-x2qs6 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Dec  9 07:03:33.485: INFO: stderr: ""
    Dec  9 07:03:33.485: INFO: stdout: "true"
    Dec  9 07:03:33.485: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=kubectl-7182 get pods update-demo-nautilus-x2qs6 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Dec  9 07:03:33.552: INFO: stderr: ""
    Dec  9 07:03:33.552: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Dec  9 07:03:33.552: INFO: validating pod update-demo-nautilus-x2qs6
    Dec  9 07:03:33.558: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Dec  9 07:03:33.558: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Dec  9 07:03:33.558: INFO: update-demo-nautilus-x2qs6 is verified up and running
    STEP: using delete to clean up resources 12/09/22 07:03:33.558
    Dec  9 07:03:33.558: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=kubectl-7182 delete --grace-period=0 --force -f -'
    Dec  9 07:03:33.635: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Dec  9 07:03:33.635: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
    Dec  9 07:03:33.635: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=kubectl-7182 get rc,svc -l name=update-demo --no-headers'
    Dec  9 07:03:33.742: INFO: stderr: "No resources found in kubectl-7182 namespace.\n"
    Dec  9 07:03:33.742: INFO: stdout: ""
    Dec  9 07:03:33.742: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=kubectl-7182 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Dec  9 07:03:33.817: INFO: stderr: ""
    Dec  9 07:03:33.817: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:03:33.817: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-7182" for this suite. 12/09/22 07:03:33.82
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-apps] ReplicationController
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:67
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:03:33.827
Dec  9 07:03:33.827: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename replication-controller 12/09/22 07:03:33.827
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:03:33.842
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:03:33.846
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:67
STEP: Creating replication controller my-hostname-basic-cb9a3723-f3d4-430d-8b51-0797067843e9 12/09/22 07:03:33.849
Dec  9 07:03:33.857: INFO: Pod name my-hostname-basic-cb9a3723-f3d4-430d-8b51-0797067843e9: Found 0 pods out of 1
Dec  9 07:03:38.866: INFO: Pod name my-hostname-basic-cb9a3723-f3d4-430d-8b51-0797067843e9: Found 1 pods out of 1
Dec  9 07:03:38.866: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-cb9a3723-f3d4-430d-8b51-0797067843e9" are running
Dec  9 07:03:38.866: INFO: Waiting up to 5m0s for pod "my-hostname-basic-cb9a3723-f3d4-430d-8b51-0797067843e9-9lv9v" in namespace "replication-controller-6347" to be "running"
Dec  9 07:03:38.869: INFO: Pod "my-hostname-basic-cb9a3723-f3d4-430d-8b51-0797067843e9-9lv9v": Phase="Running", Reason="", readiness=true. Elapsed: 3.510189ms
Dec  9 07:03:38.869: INFO: Pod "my-hostname-basic-cb9a3723-f3d4-430d-8b51-0797067843e9-9lv9v" satisfied condition "running"
Dec  9 07:03:38.869: INFO: Pod "my-hostname-basic-cb9a3723-f3d4-430d-8b51-0797067843e9-9lv9v" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-12-09 07:03:33 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-12-09 07:03:35 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-12-09 07:03:35 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-12-09 07:03:33 +0000 UTC Reason: Message:}])
Dec  9 07:03:38.870: INFO: Trying to dial the pod
Dec  9 07:03:43.884: INFO: Controller my-hostname-basic-cb9a3723-f3d4-430d-8b51-0797067843e9: Got expected result from replica 1 [my-hostname-basic-cb9a3723-f3d4-430d-8b51-0797067843e9-9lv9v]: "my-hostname-basic-cb9a3723-f3d4-430d-8b51-0797067843e9-9lv9v", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Dec  9 07:03:43.885: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-6347" for this suite. 12/09/22 07:03:43.888
------------------------------
â€¢ [SLOW TEST] [10.073 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:03:33.827
    Dec  9 07:03:33.827: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename replication-controller 12/09/22 07:03:33.827
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:03:33.842
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:03:33.846
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should serve a basic image on each replica with a public image  [Conformance]
      test/e2e/apps/rc.go:67
    STEP: Creating replication controller my-hostname-basic-cb9a3723-f3d4-430d-8b51-0797067843e9 12/09/22 07:03:33.849
    Dec  9 07:03:33.857: INFO: Pod name my-hostname-basic-cb9a3723-f3d4-430d-8b51-0797067843e9: Found 0 pods out of 1
    Dec  9 07:03:38.866: INFO: Pod name my-hostname-basic-cb9a3723-f3d4-430d-8b51-0797067843e9: Found 1 pods out of 1
    Dec  9 07:03:38.866: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-cb9a3723-f3d4-430d-8b51-0797067843e9" are running
    Dec  9 07:03:38.866: INFO: Waiting up to 5m0s for pod "my-hostname-basic-cb9a3723-f3d4-430d-8b51-0797067843e9-9lv9v" in namespace "replication-controller-6347" to be "running"
    Dec  9 07:03:38.869: INFO: Pod "my-hostname-basic-cb9a3723-f3d4-430d-8b51-0797067843e9-9lv9v": Phase="Running", Reason="", readiness=true. Elapsed: 3.510189ms
    Dec  9 07:03:38.869: INFO: Pod "my-hostname-basic-cb9a3723-f3d4-430d-8b51-0797067843e9-9lv9v" satisfied condition "running"
    Dec  9 07:03:38.869: INFO: Pod "my-hostname-basic-cb9a3723-f3d4-430d-8b51-0797067843e9-9lv9v" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-12-09 07:03:33 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-12-09 07:03:35 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-12-09 07:03:35 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-12-09 07:03:33 +0000 UTC Reason: Message:}])
    Dec  9 07:03:38.870: INFO: Trying to dial the pod
    Dec  9 07:03:43.884: INFO: Controller my-hostname-basic-cb9a3723-f3d4-430d-8b51-0797067843e9: Got expected result from replica 1 [my-hostname-basic-cb9a3723-f3d4-430d-8b51-0797067843e9-9lv9v]: "my-hostname-basic-cb9a3723-f3d4-430d-8b51-0797067843e9-9lv9v", 1 of 1 required successes so far
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:03:43.885: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-6347" for this suite. 12/09/22 07:03:43.888
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:03:43.905
Dec  9 07:03:43.906: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename dns 12/09/22 07:03:43.907
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:03:43.936
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:03:43.94
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248
STEP: Creating a test headless service 12/09/22 07:03:43.944
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-649.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-649.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done
 12/09/22 07:03:43.954
STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-649.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-649.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done
 12/09/22 07:03:43.954
STEP: creating a pod to probe DNS 12/09/22 07:03:43.954
STEP: submitting the pod to kubernetes 12/09/22 07:03:43.954
Dec  9 07:03:43.968: INFO: Waiting up to 15m0s for pod "dns-test-0f4f1ee6-4619-4ff8-8deb-e1a50ecf370d" in namespace "dns-649" to be "running"
Dec  9 07:03:43.973: INFO: Pod "dns-test-0f4f1ee6-4619-4ff8-8deb-e1a50ecf370d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.380562ms
Dec  9 07:03:45.977: INFO: Pod "dns-test-0f4f1ee6-4619-4ff8-8deb-e1a50ecf370d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008438234s
Dec  9 07:03:47.978: INFO: Pod "dns-test-0f4f1ee6-4619-4ff8-8deb-e1a50ecf370d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009255486s
Dec  9 07:03:49.978: INFO: Pod "dns-test-0f4f1ee6-4619-4ff8-8deb-e1a50ecf370d": Phase="Pending", Reason="", readiness=false. Elapsed: 6.00984959s
Dec  9 07:03:51.978: INFO: Pod "dns-test-0f4f1ee6-4619-4ff8-8deb-e1a50ecf370d": Phase="Running", Reason="", readiness=true. Elapsed: 8.009588975s
Dec  9 07:03:51.978: INFO: Pod "dns-test-0f4f1ee6-4619-4ff8-8deb-e1a50ecf370d" satisfied condition "running"
STEP: retrieving the pod 12/09/22 07:03:51.978
STEP: looking for the results for each expected name from probers 12/09/22 07:03:51.983
Dec  9 07:03:52.008: INFO: DNS probes using dns-649/dns-test-0f4f1ee6-4619-4ff8-8deb-e1a50ecf370d succeeded

STEP: deleting the pod 12/09/22 07:03:52.008
STEP: deleting the test headless service 12/09/22 07:03:52.027
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Dec  9 07:03:52.091: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-649" for this suite. 12/09/22 07:03:52.118
------------------------------
â€¢ [SLOW TEST] [8.230 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:03:43.905
    Dec  9 07:03:43.906: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename dns 12/09/22 07:03:43.907
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:03:43.936
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:03:43.94
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for pods for Hostname [Conformance]
      test/e2e/network/dns.go:248
    STEP: Creating a test headless service 12/09/22 07:03:43.944
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-649.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-649.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done
     12/09/22 07:03:43.954
    STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-649.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-649.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done
     12/09/22 07:03:43.954
    STEP: creating a pod to probe DNS 12/09/22 07:03:43.954
    STEP: submitting the pod to kubernetes 12/09/22 07:03:43.954
    Dec  9 07:03:43.968: INFO: Waiting up to 15m0s for pod "dns-test-0f4f1ee6-4619-4ff8-8deb-e1a50ecf370d" in namespace "dns-649" to be "running"
    Dec  9 07:03:43.973: INFO: Pod "dns-test-0f4f1ee6-4619-4ff8-8deb-e1a50ecf370d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.380562ms
    Dec  9 07:03:45.977: INFO: Pod "dns-test-0f4f1ee6-4619-4ff8-8deb-e1a50ecf370d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008438234s
    Dec  9 07:03:47.978: INFO: Pod "dns-test-0f4f1ee6-4619-4ff8-8deb-e1a50ecf370d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009255486s
    Dec  9 07:03:49.978: INFO: Pod "dns-test-0f4f1ee6-4619-4ff8-8deb-e1a50ecf370d": Phase="Pending", Reason="", readiness=false. Elapsed: 6.00984959s
    Dec  9 07:03:51.978: INFO: Pod "dns-test-0f4f1ee6-4619-4ff8-8deb-e1a50ecf370d": Phase="Running", Reason="", readiness=true. Elapsed: 8.009588975s
    Dec  9 07:03:51.978: INFO: Pod "dns-test-0f4f1ee6-4619-4ff8-8deb-e1a50ecf370d" satisfied condition "running"
    STEP: retrieving the pod 12/09/22 07:03:51.978
    STEP: looking for the results for each expected name from probers 12/09/22 07:03:51.983
    Dec  9 07:03:52.008: INFO: DNS probes using dns-649/dns-test-0f4f1ee6-4619-4ff8-8deb-e1a50ecf370d succeeded

    STEP: deleting the pod 12/09/22 07:03:52.008
    STEP: deleting the test headless service 12/09/22 07:03:52.027
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:03:52.091: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-649" for this suite. 12/09/22 07:03:52.118
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] ConfigMap
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:423
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:03:52.136
Dec  9 07:03:52.137: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename configmap 12/09/22 07:03:52.138
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:03:52.201
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:03:52.207
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:423
STEP: Creating configMap with name configmap-test-volume-aee3f6b7-186e-4a84-bcc5-da2b74f30ad2 12/09/22 07:03:52.213
STEP: Creating a pod to test consume configMaps 12/09/22 07:03:52.224
Dec  9 07:03:52.266: INFO: Waiting up to 5m0s for pod "pod-configmaps-adfe91ad-4140-4fe5-b4d0-84168b9e1269" in namespace "configmap-7236" to be "Succeeded or Failed"
Dec  9 07:03:52.286: INFO: Pod "pod-configmaps-adfe91ad-4140-4fe5-b4d0-84168b9e1269": Phase="Pending", Reason="", readiness=false. Elapsed: 19.840732ms
Dec  9 07:03:54.290: INFO: Pod "pod-configmaps-adfe91ad-4140-4fe5-b4d0-84168b9e1269": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023831216s
Dec  9 07:03:56.294: INFO: Pod "pod-configmaps-adfe91ad-4140-4fe5-b4d0-84168b9e1269": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.028436163s
STEP: Saw pod success 12/09/22 07:03:56.294
Dec  9 07:03:56.295: INFO: Pod "pod-configmaps-adfe91ad-4140-4fe5-b4d0-84168b9e1269" satisfied condition "Succeeded or Failed"
Dec  9 07:03:56.299: INFO: Trying to get logs from node ip-10-0-10-179 pod pod-configmaps-adfe91ad-4140-4fe5-b4d0-84168b9e1269 container configmap-volume-test: <nil>
STEP: delete the pod 12/09/22 07:03:56.325
Dec  9 07:03:56.397: INFO: Waiting for pod pod-configmaps-adfe91ad-4140-4fe5-b4d0-84168b9e1269 to disappear
Dec  9 07:03:56.402: INFO: Pod pod-configmaps-adfe91ad-4140-4fe5-b4d0-84168b9e1269 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Dec  9 07:03:56.402: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-7236" for this suite. 12/09/22 07:03:56.407
------------------------------
â€¢ [4.280 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:423

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:03:52.136
    Dec  9 07:03:52.137: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename configmap 12/09/22 07:03:52.138
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:03:52.201
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:03:52.207
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:423
    STEP: Creating configMap with name configmap-test-volume-aee3f6b7-186e-4a84-bcc5-da2b74f30ad2 12/09/22 07:03:52.213
    STEP: Creating a pod to test consume configMaps 12/09/22 07:03:52.224
    Dec  9 07:03:52.266: INFO: Waiting up to 5m0s for pod "pod-configmaps-adfe91ad-4140-4fe5-b4d0-84168b9e1269" in namespace "configmap-7236" to be "Succeeded or Failed"
    Dec  9 07:03:52.286: INFO: Pod "pod-configmaps-adfe91ad-4140-4fe5-b4d0-84168b9e1269": Phase="Pending", Reason="", readiness=false. Elapsed: 19.840732ms
    Dec  9 07:03:54.290: INFO: Pod "pod-configmaps-adfe91ad-4140-4fe5-b4d0-84168b9e1269": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023831216s
    Dec  9 07:03:56.294: INFO: Pod "pod-configmaps-adfe91ad-4140-4fe5-b4d0-84168b9e1269": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.028436163s
    STEP: Saw pod success 12/09/22 07:03:56.294
    Dec  9 07:03:56.295: INFO: Pod "pod-configmaps-adfe91ad-4140-4fe5-b4d0-84168b9e1269" satisfied condition "Succeeded or Failed"
    Dec  9 07:03:56.299: INFO: Trying to get logs from node ip-10-0-10-179 pod pod-configmaps-adfe91ad-4140-4fe5-b4d0-84168b9e1269 container configmap-volume-test: <nil>
    STEP: delete the pod 12/09/22 07:03:56.325
    Dec  9 07:03:56.397: INFO: Waiting for pod pod-configmaps-adfe91ad-4140-4fe5-b4d0-84168b9e1269 to disappear
    Dec  9 07:03:56.402: INFO: Pod pod-configmaps-adfe91ad-4140-4fe5-b4d0-84168b9e1269 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:03:56.402: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-7236" for this suite. 12/09/22 07:03:56.407
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-network] Services
  should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:787
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:03:56.424
Dec  9 07:03:56.426: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename services 12/09/22 07:03:56.433
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:03:56.452
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:03:56.46
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:787
STEP: creating service endpoint-test2 in namespace services-9888 12/09/22 07:03:56.478
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-9888 to expose endpoints map[] 12/09/22 07:03:56.502
Dec  9 07:03:56.533: INFO: successfully validated that service endpoint-test2 in namespace services-9888 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-9888 12/09/22 07:03:56.538
Dec  9 07:03:56.549: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-9888" to be "running and ready"
Dec  9 07:03:56.554: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 5.319522ms
Dec  9 07:03:56.555: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Dec  9 07:03:58.559: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.009717253s
Dec  9 07:03:58.559: INFO: The phase of Pod pod1 is Running (Ready = true)
Dec  9 07:03:58.559: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-9888 to expose endpoints map[pod1:[80]] 12/09/22 07:03:58.563
Dec  9 07:03:58.573: INFO: successfully validated that service endpoint-test2 in namespace services-9888 exposes endpoints map[pod1:[80]]
STEP: Checking if the Service forwards traffic to pod1 12/09/22 07:03:58.573
Dec  9 07:03:58.573: INFO: Creating new exec pod
Dec  9 07:03:58.579: INFO: Waiting up to 5m0s for pod "execpod64x5b" in namespace "services-9888" to be "running"
Dec  9 07:03:58.583: INFO: Pod "execpod64x5b": Phase="Pending", Reason="", readiness=false. Elapsed: 3.821144ms
Dec  9 07:04:00.587: INFO: Pod "execpod64x5b": Phase="Running", Reason="", readiness=true. Elapsed: 2.007415461s
Dec  9 07:04:00.587: INFO: Pod "execpod64x5b" satisfied condition "running"
Dec  9 07:04:01.588: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=services-9888 exec execpod64x5b -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
Dec  9 07:04:01.982: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Dec  9 07:04:01.982: INFO: stdout: ""
Dec  9 07:04:01.982: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=services-9888 exec execpod64x5b -- /bin/sh -x -c nc -v -z -w 2 10.3.92.48 80'
Dec  9 07:04:02.356: INFO: stderr: "+ nc -v -z -w 2 10.3.92.48 80\nConnection to 10.3.92.48 80 port [tcp/http] succeeded!\n"
Dec  9 07:04:02.356: INFO: stdout: ""
STEP: Creating pod pod2 in namespace services-9888 12/09/22 07:04:02.356
Dec  9 07:04:02.392: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-9888" to be "running and ready"
Dec  9 07:04:02.453: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 60.738173ms
Dec  9 07:04:02.453: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Dec  9 07:04:04.461: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.069337063s
Dec  9 07:04:04.461: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Dec  9 07:04:06.458: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 4.066168261s
Dec  9 07:04:06.458: INFO: The phase of Pod pod2 is Running (Ready = true)
Dec  9 07:04:06.458: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-9888 to expose endpoints map[pod1:[80] pod2:[80]] 12/09/22 07:04:06.465
Dec  9 07:04:06.488: INFO: successfully validated that service endpoint-test2 in namespace services-9888 exposes endpoints map[pod1:[80] pod2:[80]]
STEP: Checking if the Service forwards traffic to pod1 and pod2 12/09/22 07:04:06.488
Dec  9 07:04:07.489: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=services-9888 exec execpod64x5b -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
Dec  9 07:04:07.645: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Dec  9 07:04:07.645: INFO: stdout: ""
Dec  9 07:04:07.645: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=services-9888 exec execpod64x5b -- /bin/sh -x -c nc -v -z -w 2 10.3.92.48 80'
Dec  9 07:04:07.820: INFO: stderr: "+ nc -v -z -w 2 10.3.92.48 80\nConnection to 10.3.92.48 80 port [tcp/http] succeeded!\n"
Dec  9 07:04:07.820: INFO: stdout: ""
STEP: Deleting pod pod1 in namespace services-9888 12/09/22 07:04:07.82
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-9888 to expose endpoints map[pod2:[80]] 12/09/22 07:04:07.835
Dec  9 07:04:07.871: INFO: successfully validated that service endpoint-test2 in namespace services-9888 exposes endpoints map[pod2:[80]]
STEP: Checking if the Service forwards traffic to pod2 12/09/22 07:04:07.871
Dec  9 07:04:08.872: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=services-9888 exec execpod64x5b -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
Dec  9 07:04:09.018: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Dec  9 07:04:09.018: INFO: stdout: ""
Dec  9 07:04:09.018: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=services-9888 exec execpod64x5b -- /bin/sh -x -c nc -v -z -w 2 10.3.92.48 80'
Dec  9 07:04:09.156: INFO: stderr: "+ nc -v -z -w 2 10.3.92.48 80\nConnection to 10.3.92.48 80 port [tcp/http] succeeded!\n"
Dec  9 07:04:09.156: INFO: stdout: ""
STEP: Deleting pod pod2 in namespace services-9888 12/09/22 07:04:09.156
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-9888 to expose endpoints map[] 12/09/22 07:04:09.186
Dec  9 07:04:09.239: INFO: successfully validated that service endpoint-test2 in namespace services-9888 exposes endpoints map[]
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Dec  9 07:04:09.267: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-9888" for this suite. 12/09/22 07:04:09.285
------------------------------
â€¢ [SLOW TEST] [12.869 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:787

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:03:56.424
    Dec  9 07:03:56.426: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename services 12/09/22 07:03:56.433
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:03:56.452
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:03:56.46
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should serve a basic endpoint from pods  [Conformance]
      test/e2e/network/service.go:787
    STEP: creating service endpoint-test2 in namespace services-9888 12/09/22 07:03:56.478
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-9888 to expose endpoints map[] 12/09/22 07:03:56.502
    Dec  9 07:03:56.533: INFO: successfully validated that service endpoint-test2 in namespace services-9888 exposes endpoints map[]
    STEP: Creating pod pod1 in namespace services-9888 12/09/22 07:03:56.538
    Dec  9 07:03:56.549: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-9888" to be "running and ready"
    Dec  9 07:03:56.554: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 5.319522ms
    Dec  9 07:03:56.555: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Dec  9 07:03:58.559: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.009717253s
    Dec  9 07:03:58.559: INFO: The phase of Pod pod1 is Running (Ready = true)
    Dec  9 07:03:58.559: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-9888 to expose endpoints map[pod1:[80]] 12/09/22 07:03:58.563
    Dec  9 07:03:58.573: INFO: successfully validated that service endpoint-test2 in namespace services-9888 exposes endpoints map[pod1:[80]]
    STEP: Checking if the Service forwards traffic to pod1 12/09/22 07:03:58.573
    Dec  9 07:03:58.573: INFO: Creating new exec pod
    Dec  9 07:03:58.579: INFO: Waiting up to 5m0s for pod "execpod64x5b" in namespace "services-9888" to be "running"
    Dec  9 07:03:58.583: INFO: Pod "execpod64x5b": Phase="Pending", Reason="", readiness=false. Elapsed: 3.821144ms
    Dec  9 07:04:00.587: INFO: Pod "execpod64x5b": Phase="Running", Reason="", readiness=true. Elapsed: 2.007415461s
    Dec  9 07:04:00.587: INFO: Pod "execpod64x5b" satisfied condition "running"
    Dec  9 07:04:01.588: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=services-9888 exec execpod64x5b -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
    Dec  9 07:04:01.982: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Dec  9 07:04:01.982: INFO: stdout: ""
    Dec  9 07:04:01.982: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=services-9888 exec execpod64x5b -- /bin/sh -x -c nc -v -z -w 2 10.3.92.48 80'
    Dec  9 07:04:02.356: INFO: stderr: "+ nc -v -z -w 2 10.3.92.48 80\nConnection to 10.3.92.48 80 port [tcp/http] succeeded!\n"
    Dec  9 07:04:02.356: INFO: stdout: ""
    STEP: Creating pod pod2 in namespace services-9888 12/09/22 07:04:02.356
    Dec  9 07:04:02.392: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-9888" to be "running and ready"
    Dec  9 07:04:02.453: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 60.738173ms
    Dec  9 07:04:02.453: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Dec  9 07:04:04.461: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.069337063s
    Dec  9 07:04:04.461: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Dec  9 07:04:06.458: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 4.066168261s
    Dec  9 07:04:06.458: INFO: The phase of Pod pod2 is Running (Ready = true)
    Dec  9 07:04:06.458: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-9888 to expose endpoints map[pod1:[80] pod2:[80]] 12/09/22 07:04:06.465
    Dec  9 07:04:06.488: INFO: successfully validated that service endpoint-test2 in namespace services-9888 exposes endpoints map[pod1:[80] pod2:[80]]
    STEP: Checking if the Service forwards traffic to pod1 and pod2 12/09/22 07:04:06.488
    Dec  9 07:04:07.489: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=services-9888 exec execpod64x5b -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
    Dec  9 07:04:07.645: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Dec  9 07:04:07.645: INFO: stdout: ""
    Dec  9 07:04:07.645: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=services-9888 exec execpod64x5b -- /bin/sh -x -c nc -v -z -w 2 10.3.92.48 80'
    Dec  9 07:04:07.820: INFO: stderr: "+ nc -v -z -w 2 10.3.92.48 80\nConnection to 10.3.92.48 80 port [tcp/http] succeeded!\n"
    Dec  9 07:04:07.820: INFO: stdout: ""
    STEP: Deleting pod pod1 in namespace services-9888 12/09/22 07:04:07.82
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-9888 to expose endpoints map[pod2:[80]] 12/09/22 07:04:07.835
    Dec  9 07:04:07.871: INFO: successfully validated that service endpoint-test2 in namespace services-9888 exposes endpoints map[pod2:[80]]
    STEP: Checking if the Service forwards traffic to pod2 12/09/22 07:04:07.871
    Dec  9 07:04:08.872: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=services-9888 exec execpod64x5b -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
    Dec  9 07:04:09.018: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Dec  9 07:04:09.018: INFO: stdout: ""
    Dec  9 07:04:09.018: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=services-9888 exec execpod64x5b -- /bin/sh -x -c nc -v -z -w 2 10.3.92.48 80'
    Dec  9 07:04:09.156: INFO: stderr: "+ nc -v -z -w 2 10.3.92.48 80\nConnection to 10.3.92.48 80 port [tcp/http] succeeded!\n"
    Dec  9 07:04:09.156: INFO: stdout: ""
    STEP: Deleting pod pod2 in namespace services-9888 12/09/22 07:04:09.156
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-9888 to expose endpoints map[] 12/09/22 07:04:09.186
    Dec  9 07:04:09.239: INFO: successfully validated that service endpoint-test2 in namespace services-9888 exposes endpoints map[]
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:04:09.267: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-9888" for this suite. 12/09/22 07:04:09.285
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:251
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:04:09.304
Dec  9 07:04:09.304: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename namespaces 12/09/22 07:04:09.309
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:04:09.332
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:04:09.345
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:251
STEP: Creating a test namespace 12/09/22 07:04:09.356
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:04:09.376
STEP: Creating a service in the namespace 12/09/22 07:04:09.382
STEP: Deleting the namespace 12/09/22 07:04:09.394
STEP: Waiting for the namespace to be removed. 12/09/22 07:04:09.414
STEP: Recreating the namespace 12/09/22 07:04:15.419
STEP: Verifying there is no service in the namespace 12/09/22 07:04:15.434
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Dec  9 07:04:15.438: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-7081" for this suite. 12/09/22 07:04:15.442
STEP: Destroying namespace "nsdeletetest-621" for this suite. 12/09/22 07:04:15.446
Dec  9 07:04:15.450: INFO: Namespace nsdeletetest-621 was already deleted
STEP: Destroying namespace "nsdeletetest-7718" for this suite. 12/09/22 07:04:15.451
------------------------------
â€¢ [SLOW TEST] [6.163 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:251

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:04:09.304
    Dec  9 07:04:09.304: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename namespaces 12/09/22 07:04:09.309
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:04:09.332
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:04:09.345
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should ensure that all services are removed when a namespace is deleted [Conformance]
      test/e2e/apimachinery/namespace.go:251
    STEP: Creating a test namespace 12/09/22 07:04:09.356
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:04:09.376
    STEP: Creating a service in the namespace 12/09/22 07:04:09.382
    STEP: Deleting the namespace 12/09/22 07:04:09.394
    STEP: Waiting for the namespace to be removed. 12/09/22 07:04:09.414
    STEP: Recreating the namespace 12/09/22 07:04:15.419
    STEP: Verifying there is no service in the namespace 12/09/22 07:04:15.434
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:04:15.438: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-7081" for this suite. 12/09/22 07:04:15.442
    STEP: Destroying namespace "nsdeletetest-621" for this suite. 12/09/22 07:04:15.446
    Dec  9 07:04:15.450: INFO: Namespace nsdeletetest-621 was already deleted
    STEP: Destroying namespace "nsdeletetest-7718" for this suite. 12/09/22 07:04:15.451
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:204
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:04:15.461
Dec  9 07:04:15.461: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename pods 12/09/22 07:04:15.465
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:04:15.482
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:04:15.487
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:204
STEP: creating pod 12/09/22 07:04:15.495
Dec  9 07:04:15.515: INFO: Waiting up to 5m0s for pod "pod-hostip-c9828ade-90e9-4650-be78-a3afb90da87f" in namespace "pods-3223" to be "running and ready"
Dec  9 07:04:15.522: INFO: Pod "pod-hostip-c9828ade-90e9-4650-be78-a3afb90da87f": Phase="Pending", Reason="", readiness=false. Elapsed: 6.724545ms
Dec  9 07:04:15.522: INFO: The phase of Pod pod-hostip-c9828ade-90e9-4650-be78-a3afb90da87f is Pending, waiting for it to be Running (with Ready = true)
Dec  9 07:04:17.526: INFO: Pod "pod-hostip-c9828ade-90e9-4650-be78-a3afb90da87f": Phase="Running", Reason="", readiness=true. Elapsed: 2.011189393s
Dec  9 07:04:17.526: INFO: The phase of Pod pod-hostip-c9828ade-90e9-4650-be78-a3afb90da87f is Running (Ready = true)
Dec  9 07:04:17.526: INFO: Pod "pod-hostip-c9828ade-90e9-4650-be78-a3afb90da87f" satisfied condition "running and ready"
Dec  9 07:04:17.532: INFO: Pod pod-hostip-c9828ade-90e9-4650-be78-a3afb90da87f has hostIP: 10.0.10.179
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Dec  9 07:04:17.532: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-3223" for this suite. 12/09/22 07:04:17.536
------------------------------
â€¢ [2.080 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:204

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:04:15.461
    Dec  9 07:04:15.461: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename pods 12/09/22 07:04:15.465
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:04:15.482
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:04:15.487
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should get a host IP [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:204
    STEP: creating pod 12/09/22 07:04:15.495
    Dec  9 07:04:15.515: INFO: Waiting up to 5m0s for pod "pod-hostip-c9828ade-90e9-4650-be78-a3afb90da87f" in namespace "pods-3223" to be "running and ready"
    Dec  9 07:04:15.522: INFO: Pod "pod-hostip-c9828ade-90e9-4650-be78-a3afb90da87f": Phase="Pending", Reason="", readiness=false. Elapsed: 6.724545ms
    Dec  9 07:04:15.522: INFO: The phase of Pod pod-hostip-c9828ade-90e9-4650-be78-a3afb90da87f is Pending, waiting for it to be Running (with Ready = true)
    Dec  9 07:04:17.526: INFO: Pod "pod-hostip-c9828ade-90e9-4650-be78-a3afb90da87f": Phase="Running", Reason="", readiness=true. Elapsed: 2.011189393s
    Dec  9 07:04:17.526: INFO: The phase of Pod pod-hostip-c9828ade-90e9-4650-be78-a3afb90da87f is Running (Ready = true)
    Dec  9 07:04:17.526: INFO: Pod "pod-hostip-c9828ade-90e9-4650-be78-a3afb90da87f" satisfied condition "running and ready"
    Dec  9 07:04:17.532: INFO: Pod pod-hostip-c9828ade-90e9-4650-be78-a3afb90da87f has hostIP: 10.0.10.179
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:04:17.532: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-3223" for this suite. 12/09/22 07:04:17.536
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:134
[BeforeEach] [sig-node] Container Lifecycle Hook
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:04:17.542
Dec  9 07:04:17.542: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename container-lifecycle-hook 12/09/22 07:04:17.543
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:04:17.562
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:04:17.568
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:77
STEP: create the container to handle the HTTPGet hook request. 12/09/22 07:04:17.575
Dec  9 07:04:17.583: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-2619" to be "running and ready"
Dec  9 07:04:17.589: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 5.564662ms
Dec  9 07:04:17.589: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Dec  9 07:04:19.592: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.009289212s
Dec  9 07:04:19.592: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Dec  9 07:04:19.592: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:134
STEP: create the pod with lifecycle hook 12/09/22 07:04:19.596
Dec  9 07:04:19.603: INFO: Waiting up to 5m0s for pod "pod-with-poststart-exec-hook" in namespace "container-lifecycle-hook-2619" to be "running and ready"
Dec  9 07:04:19.608: INFO: Pod "pod-with-poststart-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 5.526055ms
Dec  9 07:04:19.608: INFO: The phase of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Dec  9 07:04:21.614: INFO: Pod "pod-with-poststart-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.010841539s
Dec  9 07:04:21.614: INFO: The phase of Pod pod-with-poststart-exec-hook is Running (Ready = true)
Dec  9 07:04:21.614: INFO: Pod "pod-with-poststart-exec-hook" satisfied condition "running and ready"
STEP: check poststart hook 12/09/22 07:04:21.616
STEP: delete the pod with lifecycle hook 12/09/22 07:04:21.621
Dec  9 07:04:21.629: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec  9 07:04:21.635: INFO: Pod pod-with-poststart-exec-hook still exists
Dec  9 07:04:23.635: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec  9 07:04:23.642: INFO: Pod pod-with-poststart-exec-hook still exists
Dec  9 07:04:25.635: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec  9 07:04:25.644: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/node/init/init.go:32
Dec  9 07:04:25.644: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  tear down framework | framework.go:193
STEP: Destroying namespace "container-lifecycle-hook-2619" for this suite. 12/09/22 07:04:25.649
------------------------------
â€¢ [SLOW TEST] [8.123 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:134

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:04:17.542
    Dec  9 07:04:17.542: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename container-lifecycle-hook 12/09/22 07:04:17.543
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:04:17.562
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:04:17.568
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:77
    STEP: create the container to handle the HTTPGet hook request. 12/09/22 07:04:17.575
    Dec  9 07:04:17.583: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-2619" to be "running and ready"
    Dec  9 07:04:17.589: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 5.564662ms
    Dec  9 07:04:17.589: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Dec  9 07:04:19.592: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.009289212s
    Dec  9 07:04:19.592: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Dec  9 07:04:19.592: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute poststart exec hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:134
    STEP: create the pod with lifecycle hook 12/09/22 07:04:19.596
    Dec  9 07:04:19.603: INFO: Waiting up to 5m0s for pod "pod-with-poststart-exec-hook" in namespace "container-lifecycle-hook-2619" to be "running and ready"
    Dec  9 07:04:19.608: INFO: Pod "pod-with-poststart-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 5.526055ms
    Dec  9 07:04:19.608: INFO: The phase of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
    Dec  9 07:04:21.614: INFO: Pod "pod-with-poststart-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.010841539s
    Dec  9 07:04:21.614: INFO: The phase of Pod pod-with-poststart-exec-hook is Running (Ready = true)
    Dec  9 07:04:21.614: INFO: Pod "pod-with-poststart-exec-hook" satisfied condition "running and ready"
    STEP: check poststart hook 12/09/22 07:04:21.616
    STEP: delete the pod with lifecycle hook 12/09/22 07:04:21.621
    Dec  9 07:04:21.629: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    Dec  9 07:04:21.635: INFO: Pod pod-with-poststart-exec-hook still exists
    Dec  9 07:04:23.635: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    Dec  9 07:04:23.642: INFO: Pod pod-with-poststart-exec-hook still exists
    Dec  9 07:04:25.635: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    Dec  9 07:04:25.644: INFO: Pod pod-with-poststart-exec-hook no longer exists
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:04:25.644: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-lifecycle-hook-2619" for this suite. 12/09/22 07:04:25.649
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:458
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:04:25.686
Dec  9 07:04:25.686: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename init-container 12/09/22 07:04:25.688
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:04:25.727
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:04:25.748
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:165
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:458
STEP: creating the pod 12/09/22 07:04:25.754
Dec  9 07:04:25.755: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Dec  9 07:04:30.416: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "init-container-3730" for this suite. 12/09/22 07:04:30.429
------------------------------
â€¢ [4.753 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:458

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:04:25.686
    Dec  9 07:04:25.686: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename init-container 12/09/22 07:04:25.688
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:04:25.727
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:04:25.748
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:165
    [It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
      test/e2e/common/node/init_container.go:458
    STEP: creating the pod 12/09/22 07:04:25.754
    Dec  9 07:04:25.755: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:04:30.416: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "init-container-3730" for this suite. 12/09/22 07:04:30.429
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:125
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:04:30.44
Dec  9 07:04:30.440: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename secrets 12/09/22 07:04:30.441
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:04:30.462
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:04:30.467
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:125
STEP: Creating secret with name secret-test-9b922287-b857-4b3b-9fcf-8cbd7558a168 12/09/22 07:04:30.471
STEP: Creating a pod to test consume secrets 12/09/22 07:04:30.478
Dec  9 07:04:30.489: INFO: Waiting up to 5m0s for pod "pod-secrets-eb336598-883f-4d20-82bf-b4cbeeb0ef81" in namespace "secrets-4049" to be "Succeeded or Failed"
Dec  9 07:04:30.496: INFO: Pod "pod-secrets-eb336598-883f-4d20-82bf-b4cbeeb0ef81": Phase="Pending", Reason="", readiness=false. Elapsed: 6.840154ms
Dec  9 07:04:32.499: INFO: Pod "pod-secrets-eb336598-883f-4d20-82bf-b4cbeeb0ef81": Phase="Running", Reason="", readiness=false. Elapsed: 2.010356367s
Dec  9 07:04:34.500: INFO: Pod "pod-secrets-eb336598-883f-4d20-82bf-b4cbeeb0ef81": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011752861s
STEP: Saw pod success 12/09/22 07:04:34.5
Dec  9 07:04:34.501: INFO: Pod "pod-secrets-eb336598-883f-4d20-82bf-b4cbeeb0ef81" satisfied condition "Succeeded or Failed"
Dec  9 07:04:34.504: INFO: Trying to get logs from node ip-10-0-17-108 pod pod-secrets-eb336598-883f-4d20-82bf-b4cbeeb0ef81 container secret-volume-test: <nil>
STEP: delete the pod 12/09/22 07:04:34.519
Dec  9 07:04:34.532: INFO: Waiting for pod pod-secrets-eb336598-883f-4d20-82bf-b4cbeeb0ef81 to disappear
Dec  9 07:04:34.534: INFO: Pod pod-secrets-eb336598-883f-4d20-82bf-b4cbeeb0ef81 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Dec  9 07:04:34.535: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-4049" for this suite. 12/09/22 07:04:34.538
------------------------------
â€¢ [4.104 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:125

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:04:30.44
    Dec  9 07:04:30.440: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename secrets 12/09/22 07:04:30.441
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:04:30.462
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:04:30.467
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:125
    STEP: Creating secret with name secret-test-9b922287-b857-4b3b-9fcf-8cbd7558a168 12/09/22 07:04:30.471
    STEP: Creating a pod to test consume secrets 12/09/22 07:04:30.478
    Dec  9 07:04:30.489: INFO: Waiting up to 5m0s for pod "pod-secrets-eb336598-883f-4d20-82bf-b4cbeeb0ef81" in namespace "secrets-4049" to be "Succeeded or Failed"
    Dec  9 07:04:30.496: INFO: Pod "pod-secrets-eb336598-883f-4d20-82bf-b4cbeeb0ef81": Phase="Pending", Reason="", readiness=false. Elapsed: 6.840154ms
    Dec  9 07:04:32.499: INFO: Pod "pod-secrets-eb336598-883f-4d20-82bf-b4cbeeb0ef81": Phase="Running", Reason="", readiness=false. Elapsed: 2.010356367s
    Dec  9 07:04:34.500: INFO: Pod "pod-secrets-eb336598-883f-4d20-82bf-b4cbeeb0ef81": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011752861s
    STEP: Saw pod success 12/09/22 07:04:34.5
    Dec  9 07:04:34.501: INFO: Pod "pod-secrets-eb336598-883f-4d20-82bf-b4cbeeb0ef81" satisfied condition "Succeeded or Failed"
    Dec  9 07:04:34.504: INFO: Trying to get logs from node ip-10-0-17-108 pod pod-secrets-eb336598-883f-4d20-82bf-b4cbeeb0ef81 container secret-volume-test: <nil>
    STEP: delete the pod 12/09/22 07:04:34.519
    Dec  9 07:04:34.532: INFO: Waiting for pod pod-secrets-eb336598-883f-4d20-82bf-b4cbeeb0ef81 to disappear
    Dec  9 07:04:34.534: INFO: Pod pod-secrets-eb336598-883f-4d20-82bf-b4cbeeb0ef81 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:04:34.535: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-4049" for this suite. 12/09/22 07:04:34.538
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-architecture] Conformance Tests
  should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38
[BeforeEach] [sig-architecture] Conformance Tests
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:04:34.545
Dec  9 07:04:34.545: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename conformance-tests 12/09/22 07:04:34.546
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:04:34.561
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:04:34.565
[BeforeEach] [sig-architecture] Conformance Tests
  test/e2e/framework/metrics/init/init.go:31
[It] should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38
STEP: Getting node addresses 12/09/22 07:04:34.569
Dec  9 07:04:34.569: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
[AfterEach] [sig-architecture] Conformance Tests
  test/e2e/framework/node/init/init.go:32
Dec  9 07:04:34.573: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-architecture] Conformance Tests
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-architecture] Conformance Tests
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-architecture] Conformance Tests
  tear down framework | framework.go:193
STEP: Destroying namespace "conformance-tests-8079" for this suite. 12/09/22 07:04:34.576
------------------------------
â€¢ [0.036 seconds]
[sig-architecture] Conformance Tests
test/e2e/architecture/framework.go:23
  should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-architecture] Conformance Tests
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:04:34.545
    Dec  9 07:04:34.545: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename conformance-tests 12/09/22 07:04:34.546
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:04:34.561
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:04:34.565
    [BeforeEach] [sig-architecture] Conformance Tests
      test/e2e/framework/metrics/init/init.go:31
    [It] should have at least two untainted nodes [Conformance]
      test/e2e/architecture/conformance.go:38
    STEP: Getting node addresses 12/09/22 07:04:34.569
    Dec  9 07:04:34.569: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    [AfterEach] [sig-architecture] Conformance Tests
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:04:34.573: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-architecture] Conformance Tests
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-architecture] Conformance Tests
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-architecture] Conformance Tests
      tear down framework | framework.go:193
    STEP: Destroying namespace "conformance-tests-8079" for this suite. 12/09/22 07:04:34.576
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] Garbage collector
  should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:04:34.581
Dec  9 07:04:34.581: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename gc 12/09/22 07:04:34.582
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:04:34.6
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:04:34.607
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370
STEP: create the rc 12/09/22 07:04:34.613
STEP: delete the rc 12/09/22 07:04:39.628
STEP: wait for the rc to be deleted 12/09/22 07:04:39.642
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods 12/09/22 07:04:44.679
STEP: Gathering metrics 12/09/22 07:05:14.698
Dec  9 07:05:14.722: INFO: Waiting up to 5m0s for pod "kube-controller-manager-ip-10-0-7-84" in namespace "kube-system" to be "running and ready"
Dec  9 07:05:14.728: INFO: Pod "kube-controller-manager-ip-10-0-7-84": Phase="Running", Reason="", readiness=true. Elapsed: 6.404615ms
Dec  9 07:05:14.728: INFO: The phase of Pod kube-controller-manager-ip-10-0-7-84 is Running (Ready = true)
Dec  9 07:05:14.728: INFO: Pod "kube-controller-manager-ip-10-0-7-84" satisfied condition "running and ready"
Dec  9 07:05:14.838: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Dec  9 07:05:14.838: INFO: Deleting pod "simpletest.rc-26mb8" in namespace "gc-8736"
Dec  9 07:05:14.884: INFO: Deleting pod "simpletest.rc-2nl8d" in namespace "gc-8736"
Dec  9 07:05:14.906: INFO: Deleting pod "simpletest.rc-2pn48" in namespace "gc-8736"
Dec  9 07:05:14.924: INFO: Deleting pod "simpletest.rc-2rggv" in namespace "gc-8736"
Dec  9 07:05:14.945: INFO: Deleting pod "simpletest.rc-44m6n" in namespace "gc-8736"
Dec  9 07:05:14.976: INFO: Deleting pod "simpletest.rc-44z4k" in namespace "gc-8736"
Dec  9 07:05:14.995: INFO: Deleting pod "simpletest.rc-48ndm" in namespace "gc-8736"
Dec  9 07:05:15.011: INFO: Deleting pod "simpletest.rc-4dncd" in namespace "gc-8736"
Dec  9 07:05:15.029: INFO: Deleting pod "simpletest.rc-4dtzc" in namespace "gc-8736"
Dec  9 07:05:15.053: INFO: Deleting pod "simpletest.rc-4wswz" in namespace "gc-8736"
Dec  9 07:05:15.092: INFO: Deleting pod "simpletest.rc-59b7r" in namespace "gc-8736"
Dec  9 07:05:15.121: INFO: Deleting pod "simpletest.rc-5fm86" in namespace "gc-8736"
Dec  9 07:05:15.166: INFO: Deleting pod "simpletest.rc-5ksm5" in namespace "gc-8736"
Dec  9 07:05:15.185: INFO: Deleting pod "simpletest.rc-66tv7" in namespace "gc-8736"
Dec  9 07:05:15.204: INFO: Deleting pod "simpletest.rc-6kx4t" in namespace "gc-8736"
Dec  9 07:05:15.229: INFO: Deleting pod "simpletest.rc-6qtln" in namespace "gc-8736"
Dec  9 07:05:15.266: INFO: Deleting pod "simpletest.rc-6v55n" in namespace "gc-8736"
Dec  9 07:05:15.285: INFO: Deleting pod "simpletest.rc-75hl4" in namespace "gc-8736"
Dec  9 07:05:15.319: INFO: Deleting pod "simpletest.rc-764rs" in namespace "gc-8736"
Dec  9 07:05:15.349: INFO: Deleting pod "simpletest.rc-7c67r" in namespace "gc-8736"
Dec  9 07:05:15.421: INFO: Deleting pod "simpletest.rc-7fsh7" in namespace "gc-8736"
Dec  9 07:05:15.443: INFO: Deleting pod "simpletest.rc-7k9w6" in namespace "gc-8736"
Dec  9 07:05:15.478: INFO: Deleting pod "simpletest.rc-7ps6n" in namespace "gc-8736"
Dec  9 07:05:15.532: INFO: Deleting pod "simpletest.rc-7pw2f" in namespace "gc-8736"
Dec  9 07:05:15.582: INFO: Deleting pod "simpletest.rc-84vcm" in namespace "gc-8736"
Dec  9 07:05:15.604: INFO: Deleting pod "simpletest.rc-89gnv" in namespace "gc-8736"
Dec  9 07:05:15.630: INFO: Deleting pod "simpletest.rc-8bfnn" in namespace "gc-8736"
Dec  9 07:05:15.641: INFO: Deleting pod "simpletest.rc-8bw6c" in namespace "gc-8736"
Dec  9 07:05:15.654: INFO: Deleting pod "simpletest.rc-8d4sc" in namespace "gc-8736"
Dec  9 07:05:15.674: INFO: Deleting pod "simpletest.rc-8vfll" in namespace "gc-8736"
Dec  9 07:05:15.696: INFO: Deleting pod "simpletest.rc-976nx" in namespace "gc-8736"
Dec  9 07:05:15.710: INFO: Deleting pod "simpletest.rc-9b6gk" in namespace "gc-8736"
Dec  9 07:05:15.721: INFO: Deleting pod "simpletest.rc-9z4qz" in namespace "gc-8736"
Dec  9 07:05:15.731: INFO: Deleting pod "simpletest.rc-b6fvp" in namespace "gc-8736"
Dec  9 07:05:15.750: INFO: Deleting pod "simpletest.rc-bzk69" in namespace "gc-8736"
Dec  9 07:05:15.760: INFO: Deleting pod "simpletest.rc-cmsb6" in namespace "gc-8736"
Dec  9 07:05:15.772: INFO: Deleting pod "simpletest.rc-dzdm7" in namespace "gc-8736"
Dec  9 07:05:15.794: INFO: Deleting pod "simpletest.rc-f4jxv" in namespace "gc-8736"
Dec  9 07:05:15.821: INFO: Deleting pod "simpletest.rc-f9h9m" in namespace "gc-8736"
Dec  9 07:05:15.836: INFO: Deleting pod "simpletest.rc-fbnsv" in namespace "gc-8736"
Dec  9 07:05:15.851: INFO: Deleting pod "simpletest.rc-fgxwj" in namespace "gc-8736"
Dec  9 07:05:15.868: INFO: Deleting pod "simpletest.rc-fwssj" in namespace "gc-8736"
Dec  9 07:05:15.878: INFO: Deleting pod "simpletest.rc-fzbvc" in namespace "gc-8736"
Dec  9 07:05:15.893: INFO: Deleting pod "simpletest.rc-g4ghz" in namespace "gc-8736"
Dec  9 07:05:15.929: INFO: Deleting pod "simpletest.rc-gm4lg" in namespace "gc-8736"
Dec  9 07:05:15.948: INFO: Deleting pod "simpletest.rc-gsfht" in namespace "gc-8736"
Dec  9 07:05:15.965: INFO: Deleting pod "simpletest.rc-gt6dz" in namespace "gc-8736"
Dec  9 07:05:15.995: INFO: Deleting pod "simpletest.rc-h29tg" in namespace "gc-8736"
Dec  9 07:05:16.022: INFO: Deleting pod "simpletest.rc-hn94w" in namespace "gc-8736"
Dec  9 07:05:16.038: INFO: Deleting pod "simpletest.rc-j226h" in namespace "gc-8736"
Dec  9 07:05:16.050: INFO: Deleting pod "simpletest.rc-jbj4k" in namespace "gc-8736"
Dec  9 07:05:16.060: INFO: Deleting pod "simpletest.rc-jcwdq" in namespace "gc-8736"
Dec  9 07:05:16.070: INFO: Deleting pod "simpletest.rc-jjx9h" in namespace "gc-8736"
Dec  9 07:05:16.081: INFO: Deleting pod "simpletest.rc-jlnsf" in namespace "gc-8736"
Dec  9 07:05:16.109: INFO: Deleting pod "simpletest.rc-jqmzj" in namespace "gc-8736"
Dec  9 07:05:16.131: INFO: Deleting pod "simpletest.rc-jxg25" in namespace "gc-8736"
Dec  9 07:05:16.170: INFO: Deleting pod "simpletest.rc-jzgq2" in namespace "gc-8736"
Dec  9 07:05:16.202: INFO: Deleting pod "simpletest.rc-kcc6w" in namespace "gc-8736"
Dec  9 07:05:16.224: INFO: Deleting pod "simpletest.rc-khgts" in namespace "gc-8736"
Dec  9 07:05:16.271: INFO: Deleting pod "simpletest.rc-krj8k" in namespace "gc-8736"
Dec  9 07:05:16.285: INFO: Deleting pod "simpletest.rc-lc9hz" in namespace "gc-8736"
Dec  9 07:05:16.305: INFO: Deleting pod "simpletest.rc-ld8ch" in namespace "gc-8736"
Dec  9 07:05:16.319: INFO: Deleting pod "simpletest.rc-ldlj8" in namespace "gc-8736"
Dec  9 07:05:16.348: INFO: Deleting pod "simpletest.rc-lgnzf" in namespace "gc-8736"
Dec  9 07:05:16.359: INFO: Deleting pod "simpletest.rc-lhdr4" in namespace "gc-8736"
Dec  9 07:05:16.376: INFO: Deleting pod "simpletest.rc-lqbqq" in namespace "gc-8736"
Dec  9 07:05:16.390: INFO: Deleting pod "simpletest.rc-lw522" in namespace "gc-8736"
Dec  9 07:05:16.403: INFO: Deleting pod "simpletest.rc-mkhf4" in namespace "gc-8736"
Dec  9 07:05:16.427: INFO: Deleting pod "simpletest.rc-ngcm9" in namespace "gc-8736"
Dec  9 07:05:16.448: INFO: Deleting pod "simpletest.rc-nnggr" in namespace "gc-8736"
Dec  9 07:05:16.475: INFO: Deleting pod "simpletest.rc-nv5wm" in namespace "gc-8736"
Dec  9 07:05:16.498: INFO: Deleting pod "simpletest.rc-p78bz" in namespace "gc-8736"
Dec  9 07:05:16.521: INFO: Deleting pod "simpletest.rc-pp6sr" in namespace "gc-8736"
Dec  9 07:05:16.547: INFO: Deleting pod "simpletest.rc-prk8z" in namespace "gc-8736"
Dec  9 07:05:16.567: INFO: Deleting pod "simpletest.rc-ps6xk" in namespace "gc-8736"
Dec  9 07:05:16.593: INFO: Deleting pod "simpletest.rc-qds6b" in namespace "gc-8736"
Dec  9 07:05:16.620: INFO: Deleting pod "simpletest.rc-qjskj" in namespace "gc-8736"
Dec  9 07:05:16.646: INFO: Deleting pod "simpletest.rc-rhl6g" in namespace "gc-8736"
Dec  9 07:05:16.669: INFO: Deleting pod "simpletest.rc-rn89n" in namespace "gc-8736"
Dec  9 07:05:16.711: INFO: Deleting pod "simpletest.rc-s22nm" in namespace "gc-8736"
Dec  9 07:05:16.732: INFO: Deleting pod "simpletest.rc-tb5pk" in namespace "gc-8736"
Dec  9 07:05:16.744: INFO: Deleting pod "simpletest.rc-tf87z" in namespace "gc-8736"
Dec  9 07:05:16.763: INFO: Deleting pod "simpletest.rc-tsfw8" in namespace "gc-8736"
Dec  9 07:05:16.775: INFO: Deleting pod "simpletest.rc-txkn4" in namespace "gc-8736"
Dec  9 07:05:16.794: INFO: Deleting pod "simpletest.rc-v8vfr" in namespace "gc-8736"
Dec  9 07:05:16.804: INFO: Deleting pod "simpletest.rc-v9rvd" in namespace "gc-8736"
Dec  9 07:05:16.819: INFO: Deleting pod "simpletest.rc-vtjrg" in namespace "gc-8736"
Dec  9 07:05:16.829: INFO: Deleting pod "simpletest.rc-wf7qr" in namespace "gc-8736"
Dec  9 07:05:16.846: INFO: Deleting pod "simpletest.rc-wgxdz" in namespace "gc-8736"
Dec  9 07:05:16.859: INFO: Deleting pod "simpletest.rc-wzw2s" in namespace "gc-8736"
Dec  9 07:05:16.901: INFO: Deleting pod "simpletest.rc-x8ddn" in namespace "gc-8736"
Dec  9 07:05:16.946: INFO: Deleting pod "simpletest.rc-xd2k4" in namespace "gc-8736"
Dec  9 07:05:17.067: INFO: Deleting pod "simpletest.rc-xvjgm" in namespace "gc-8736"
Dec  9 07:05:17.088: INFO: Deleting pod "simpletest.rc-xwb47" in namespace "gc-8736"
Dec  9 07:05:17.117: INFO: Deleting pod "simpletest.rc-z2dv9" in namespace "gc-8736"
Dec  9 07:05:17.159: INFO: Deleting pod "simpletest.rc-z59l2" in namespace "gc-8736"
Dec  9 07:05:17.210: INFO: Deleting pod "simpletest.rc-z5h6g" in namespace "gc-8736"
Dec  9 07:05:17.260: INFO: Deleting pod "simpletest.rc-zgtdn" in namespace "gc-8736"
Dec  9 07:05:17.295: INFO: Deleting pod "simpletest.rc-zsw68" in namespace "gc-8736"
Dec  9 07:05:17.351: INFO: Deleting pod "simpletest.rc-zwsg2" in namespace "gc-8736"
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Dec  9 07:05:17.415: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-8736" for this suite. 12/09/22 07:05:17.454
------------------------------
â€¢ [SLOW TEST] [42.917 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:04:34.581
    Dec  9 07:04:34.581: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename gc 12/09/22 07:04:34.582
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:04:34.6
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:04:34.607
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should orphan pods created by rc if delete options say so [Conformance]
      test/e2e/apimachinery/garbage_collector.go:370
    STEP: create the rc 12/09/22 07:04:34.613
    STEP: delete the rc 12/09/22 07:04:39.628
    STEP: wait for the rc to be deleted 12/09/22 07:04:39.642
    STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods 12/09/22 07:04:44.679
    STEP: Gathering metrics 12/09/22 07:05:14.698
    Dec  9 07:05:14.722: INFO: Waiting up to 5m0s for pod "kube-controller-manager-ip-10-0-7-84" in namespace "kube-system" to be "running and ready"
    Dec  9 07:05:14.728: INFO: Pod "kube-controller-manager-ip-10-0-7-84": Phase="Running", Reason="", readiness=true. Elapsed: 6.404615ms
    Dec  9 07:05:14.728: INFO: The phase of Pod kube-controller-manager-ip-10-0-7-84 is Running (Ready = true)
    Dec  9 07:05:14.728: INFO: Pod "kube-controller-manager-ip-10-0-7-84" satisfied condition "running and ready"
    Dec  9 07:05:14.838: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    Dec  9 07:05:14.838: INFO: Deleting pod "simpletest.rc-26mb8" in namespace "gc-8736"
    Dec  9 07:05:14.884: INFO: Deleting pod "simpletest.rc-2nl8d" in namespace "gc-8736"
    Dec  9 07:05:14.906: INFO: Deleting pod "simpletest.rc-2pn48" in namespace "gc-8736"
    Dec  9 07:05:14.924: INFO: Deleting pod "simpletest.rc-2rggv" in namespace "gc-8736"
    Dec  9 07:05:14.945: INFO: Deleting pod "simpletest.rc-44m6n" in namespace "gc-8736"
    Dec  9 07:05:14.976: INFO: Deleting pod "simpletest.rc-44z4k" in namespace "gc-8736"
    Dec  9 07:05:14.995: INFO: Deleting pod "simpletest.rc-48ndm" in namespace "gc-8736"
    Dec  9 07:05:15.011: INFO: Deleting pod "simpletest.rc-4dncd" in namespace "gc-8736"
    Dec  9 07:05:15.029: INFO: Deleting pod "simpletest.rc-4dtzc" in namespace "gc-8736"
    Dec  9 07:05:15.053: INFO: Deleting pod "simpletest.rc-4wswz" in namespace "gc-8736"
    Dec  9 07:05:15.092: INFO: Deleting pod "simpletest.rc-59b7r" in namespace "gc-8736"
    Dec  9 07:05:15.121: INFO: Deleting pod "simpletest.rc-5fm86" in namespace "gc-8736"
    Dec  9 07:05:15.166: INFO: Deleting pod "simpletest.rc-5ksm5" in namespace "gc-8736"
    Dec  9 07:05:15.185: INFO: Deleting pod "simpletest.rc-66tv7" in namespace "gc-8736"
    Dec  9 07:05:15.204: INFO: Deleting pod "simpletest.rc-6kx4t" in namespace "gc-8736"
    Dec  9 07:05:15.229: INFO: Deleting pod "simpletest.rc-6qtln" in namespace "gc-8736"
    Dec  9 07:05:15.266: INFO: Deleting pod "simpletest.rc-6v55n" in namespace "gc-8736"
    Dec  9 07:05:15.285: INFO: Deleting pod "simpletest.rc-75hl4" in namespace "gc-8736"
    Dec  9 07:05:15.319: INFO: Deleting pod "simpletest.rc-764rs" in namespace "gc-8736"
    Dec  9 07:05:15.349: INFO: Deleting pod "simpletest.rc-7c67r" in namespace "gc-8736"
    Dec  9 07:05:15.421: INFO: Deleting pod "simpletest.rc-7fsh7" in namespace "gc-8736"
    Dec  9 07:05:15.443: INFO: Deleting pod "simpletest.rc-7k9w6" in namespace "gc-8736"
    Dec  9 07:05:15.478: INFO: Deleting pod "simpletest.rc-7ps6n" in namespace "gc-8736"
    Dec  9 07:05:15.532: INFO: Deleting pod "simpletest.rc-7pw2f" in namespace "gc-8736"
    Dec  9 07:05:15.582: INFO: Deleting pod "simpletest.rc-84vcm" in namespace "gc-8736"
    Dec  9 07:05:15.604: INFO: Deleting pod "simpletest.rc-89gnv" in namespace "gc-8736"
    Dec  9 07:05:15.630: INFO: Deleting pod "simpletest.rc-8bfnn" in namespace "gc-8736"
    Dec  9 07:05:15.641: INFO: Deleting pod "simpletest.rc-8bw6c" in namespace "gc-8736"
    Dec  9 07:05:15.654: INFO: Deleting pod "simpletest.rc-8d4sc" in namespace "gc-8736"
    Dec  9 07:05:15.674: INFO: Deleting pod "simpletest.rc-8vfll" in namespace "gc-8736"
    Dec  9 07:05:15.696: INFO: Deleting pod "simpletest.rc-976nx" in namespace "gc-8736"
    Dec  9 07:05:15.710: INFO: Deleting pod "simpletest.rc-9b6gk" in namespace "gc-8736"
    Dec  9 07:05:15.721: INFO: Deleting pod "simpletest.rc-9z4qz" in namespace "gc-8736"
    Dec  9 07:05:15.731: INFO: Deleting pod "simpletest.rc-b6fvp" in namespace "gc-8736"
    Dec  9 07:05:15.750: INFO: Deleting pod "simpletest.rc-bzk69" in namespace "gc-8736"
    Dec  9 07:05:15.760: INFO: Deleting pod "simpletest.rc-cmsb6" in namespace "gc-8736"
    Dec  9 07:05:15.772: INFO: Deleting pod "simpletest.rc-dzdm7" in namespace "gc-8736"
    Dec  9 07:05:15.794: INFO: Deleting pod "simpletest.rc-f4jxv" in namespace "gc-8736"
    Dec  9 07:05:15.821: INFO: Deleting pod "simpletest.rc-f9h9m" in namespace "gc-8736"
    Dec  9 07:05:15.836: INFO: Deleting pod "simpletest.rc-fbnsv" in namespace "gc-8736"
    Dec  9 07:05:15.851: INFO: Deleting pod "simpletest.rc-fgxwj" in namespace "gc-8736"
    Dec  9 07:05:15.868: INFO: Deleting pod "simpletest.rc-fwssj" in namespace "gc-8736"
    Dec  9 07:05:15.878: INFO: Deleting pod "simpletest.rc-fzbvc" in namespace "gc-8736"
    Dec  9 07:05:15.893: INFO: Deleting pod "simpletest.rc-g4ghz" in namespace "gc-8736"
    Dec  9 07:05:15.929: INFO: Deleting pod "simpletest.rc-gm4lg" in namespace "gc-8736"
    Dec  9 07:05:15.948: INFO: Deleting pod "simpletest.rc-gsfht" in namespace "gc-8736"
    Dec  9 07:05:15.965: INFO: Deleting pod "simpletest.rc-gt6dz" in namespace "gc-8736"
    Dec  9 07:05:15.995: INFO: Deleting pod "simpletest.rc-h29tg" in namespace "gc-8736"
    Dec  9 07:05:16.022: INFO: Deleting pod "simpletest.rc-hn94w" in namespace "gc-8736"
    Dec  9 07:05:16.038: INFO: Deleting pod "simpletest.rc-j226h" in namespace "gc-8736"
    Dec  9 07:05:16.050: INFO: Deleting pod "simpletest.rc-jbj4k" in namespace "gc-8736"
    Dec  9 07:05:16.060: INFO: Deleting pod "simpletest.rc-jcwdq" in namespace "gc-8736"
    Dec  9 07:05:16.070: INFO: Deleting pod "simpletest.rc-jjx9h" in namespace "gc-8736"
    Dec  9 07:05:16.081: INFO: Deleting pod "simpletest.rc-jlnsf" in namespace "gc-8736"
    Dec  9 07:05:16.109: INFO: Deleting pod "simpletest.rc-jqmzj" in namespace "gc-8736"
    Dec  9 07:05:16.131: INFO: Deleting pod "simpletest.rc-jxg25" in namespace "gc-8736"
    Dec  9 07:05:16.170: INFO: Deleting pod "simpletest.rc-jzgq2" in namespace "gc-8736"
    Dec  9 07:05:16.202: INFO: Deleting pod "simpletest.rc-kcc6w" in namespace "gc-8736"
    Dec  9 07:05:16.224: INFO: Deleting pod "simpletest.rc-khgts" in namespace "gc-8736"
    Dec  9 07:05:16.271: INFO: Deleting pod "simpletest.rc-krj8k" in namespace "gc-8736"
    Dec  9 07:05:16.285: INFO: Deleting pod "simpletest.rc-lc9hz" in namespace "gc-8736"
    Dec  9 07:05:16.305: INFO: Deleting pod "simpletest.rc-ld8ch" in namespace "gc-8736"
    Dec  9 07:05:16.319: INFO: Deleting pod "simpletest.rc-ldlj8" in namespace "gc-8736"
    Dec  9 07:05:16.348: INFO: Deleting pod "simpletest.rc-lgnzf" in namespace "gc-8736"
    Dec  9 07:05:16.359: INFO: Deleting pod "simpletest.rc-lhdr4" in namespace "gc-8736"
    Dec  9 07:05:16.376: INFO: Deleting pod "simpletest.rc-lqbqq" in namespace "gc-8736"
    Dec  9 07:05:16.390: INFO: Deleting pod "simpletest.rc-lw522" in namespace "gc-8736"
    Dec  9 07:05:16.403: INFO: Deleting pod "simpletest.rc-mkhf4" in namespace "gc-8736"
    Dec  9 07:05:16.427: INFO: Deleting pod "simpletest.rc-ngcm9" in namespace "gc-8736"
    Dec  9 07:05:16.448: INFO: Deleting pod "simpletest.rc-nnggr" in namespace "gc-8736"
    Dec  9 07:05:16.475: INFO: Deleting pod "simpletest.rc-nv5wm" in namespace "gc-8736"
    Dec  9 07:05:16.498: INFO: Deleting pod "simpletest.rc-p78bz" in namespace "gc-8736"
    Dec  9 07:05:16.521: INFO: Deleting pod "simpletest.rc-pp6sr" in namespace "gc-8736"
    Dec  9 07:05:16.547: INFO: Deleting pod "simpletest.rc-prk8z" in namespace "gc-8736"
    Dec  9 07:05:16.567: INFO: Deleting pod "simpletest.rc-ps6xk" in namespace "gc-8736"
    Dec  9 07:05:16.593: INFO: Deleting pod "simpletest.rc-qds6b" in namespace "gc-8736"
    Dec  9 07:05:16.620: INFO: Deleting pod "simpletest.rc-qjskj" in namespace "gc-8736"
    Dec  9 07:05:16.646: INFO: Deleting pod "simpletest.rc-rhl6g" in namespace "gc-8736"
    Dec  9 07:05:16.669: INFO: Deleting pod "simpletest.rc-rn89n" in namespace "gc-8736"
    Dec  9 07:05:16.711: INFO: Deleting pod "simpletest.rc-s22nm" in namespace "gc-8736"
    Dec  9 07:05:16.732: INFO: Deleting pod "simpletest.rc-tb5pk" in namespace "gc-8736"
    Dec  9 07:05:16.744: INFO: Deleting pod "simpletest.rc-tf87z" in namespace "gc-8736"
    Dec  9 07:05:16.763: INFO: Deleting pod "simpletest.rc-tsfw8" in namespace "gc-8736"
    Dec  9 07:05:16.775: INFO: Deleting pod "simpletest.rc-txkn4" in namespace "gc-8736"
    Dec  9 07:05:16.794: INFO: Deleting pod "simpletest.rc-v8vfr" in namespace "gc-8736"
    Dec  9 07:05:16.804: INFO: Deleting pod "simpletest.rc-v9rvd" in namespace "gc-8736"
    Dec  9 07:05:16.819: INFO: Deleting pod "simpletest.rc-vtjrg" in namespace "gc-8736"
    Dec  9 07:05:16.829: INFO: Deleting pod "simpletest.rc-wf7qr" in namespace "gc-8736"
    Dec  9 07:05:16.846: INFO: Deleting pod "simpletest.rc-wgxdz" in namespace "gc-8736"
    Dec  9 07:05:16.859: INFO: Deleting pod "simpletest.rc-wzw2s" in namespace "gc-8736"
    Dec  9 07:05:16.901: INFO: Deleting pod "simpletest.rc-x8ddn" in namespace "gc-8736"
    Dec  9 07:05:16.946: INFO: Deleting pod "simpletest.rc-xd2k4" in namespace "gc-8736"
    Dec  9 07:05:17.067: INFO: Deleting pod "simpletest.rc-xvjgm" in namespace "gc-8736"
    Dec  9 07:05:17.088: INFO: Deleting pod "simpletest.rc-xwb47" in namespace "gc-8736"
    Dec  9 07:05:17.117: INFO: Deleting pod "simpletest.rc-z2dv9" in namespace "gc-8736"
    Dec  9 07:05:17.159: INFO: Deleting pod "simpletest.rc-z59l2" in namespace "gc-8736"
    Dec  9 07:05:17.210: INFO: Deleting pod "simpletest.rc-z5h6g" in namespace "gc-8736"
    Dec  9 07:05:17.260: INFO: Deleting pod "simpletest.rc-zgtdn" in namespace "gc-8736"
    Dec  9 07:05:17.295: INFO: Deleting pod "simpletest.rc-zsw68" in namespace "gc-8736"
    Dec  9 07:05:17.351: INFO: Deleting pod "simpletest.rc-zwsg2" in namespace "gc-8736"
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:05:17.415: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-8736" for this suite. 12/09/22 07:05:17.454
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates
  should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122
[BeforeEach] [sig-node] PodTemplates
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:05:17.503
Dec  9 07:05:17.503: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename podtemplate 12/09/22 07:05:17.505
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:05:17.521
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:05:17.525
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:31
[It] should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122
STEP: Create set of pod templates 12/09/22 07:05:17.529
Dec  9 07:05:17.536: INFO: created test-podtemplate-1
Dec  9 07:05:17.542: INFO: created test-podtemplate-2
Dec  9 07:05:17.553: INFO: created test-podtemplate-3
STEP: get a list of pod templates with a label in the current namespace 12/09/22 07:05:17.553
STEP: delete collection of pod templates 12/09/22 07:05:17.558
Dec  9 07:05:17.559: INFO: requesting DeleteCollection of pod templates
STEP: check that the list of pod templates matches the requested quantity 12/09/22 07:05:17.575
Dec  9 07:05:17.575: INFO: requesting list of pod templates to confirm quantity
[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/node/init/init.go:32
Dec  9 07:05:17.580: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] PodTemplates
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] PodTemplates
  tear down framework | framework.go:193
STEP: Destroying namespace "podtemplate-683" for this suite. 12/09/22 07:05:17.585
------------------------------
â€¢ [0.091 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:05:17.503
    Dec  9 07:05:17.503: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename podtemplate 12/09/22 07:05:17.505
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:05:17.521
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:05:17.525
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete a collection of pod templates [Conformance]
      test/e2e/common/node/podtemplates.go:122
    STEP: Create set of pod templates 12/09/22 07:05:17.529
    Dec  9 07:05:17.536: INFO: created test-podtemplate-1
    Dec  9 07:05:17.542: INFO: created test-podtemplate-2
    Dec  9 07:05:17.553: INFO: created test-podtemplate-3
    STEP: get a list of pod templates with a label in the current namespace 12/09/22 07:05:17.553
    STEP: delete collection of pod templates 12/09/22 07:05:17.558
    Dec  9 07:05:17.559: INFO: requesting DeleteCollection of pod templates
    STEP: check that the list of pod templates matches the requested quantity 12/09/22 07:05:17.575
    Dec  9 07:05:17.575: INFO: requesting list of pod templates to confirm quantity
    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:05:17.580: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] PodTemplates
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] PodTemplates
      tear down framework | framework.go:193
    STEP: Destroying namespace "podtemplate-683" for this suite. 12/09/22 07:05:17.585
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:93
[BeforeEach] [sig-network] Networking
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:05:17.595
Dec  9 07:05:17.595: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename pod-network-test 12/09/22 07:05:17.596
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:05:17.621
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:05:17.626
[BeforeEach] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:31
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:93
STEP: Performing setup for networking test in namespace pod-network-test-4219 12/09/22 07:05:17.635
STEP: creating a selector 12/09/22 07:05:17.637
STEP: Creating the service pods in kubernetes 12/09/22 07:05:17.637
Dec  9 07:05:17.638: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Dec  9 07:05:17.678: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-4219" to be "running and ready"
Dec  9 07:05:17.687: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 9.33409ms
Dec  9 07:05:17.687: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Dec  9 07:05:19.692: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014332133s
Dec  9 07:05:19.692: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Dec  9 07:05:21.695: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.017034115s
Dec  9 07:05:21.695: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Dec  9 07:05:23.694: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 6.015767441s
Dec  9 07:05:23.694: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Dec  9 07:05:25.695: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 8.016724529s
Dec  9 07:05:25.695: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Dec  9 07:05:27.705: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.027620064s
Dec  9 07:05:27.706: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Dec  9 07:05:29.692: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.014447784s
Dec  9 07:05:29.692: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Dec  9 07:05:31.691: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.013303581s
Dec  9 07:05:31.691: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Dec  9 07:05:33.692: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.014284175s
Dec  9 07:05:33.692: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Dec  9 07:05:35.690: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.012545516s
Dec  9 07:05:35.690: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Dec  9 07:05:37.692: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.014070826s
Dec  9 07:05:37.692: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Dec  9 07:05:39.692: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.014304107s
Dec  9 07:05:39.692: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Dec  9 07:05:39.692: INFO: Pod "netserver-0" satisfied condition "running and ready"
Dec  9 07:05:39.695: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-4219" to be "running and ready"
Dec  9 07:05:39.698: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 2.747065ms
Dec  9 07:05:39.698: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Dec  9 07:05:39.698: INFO: Pod "netserver-1" satisfied condition "running and ready"
STEP: Creating test pods 12/09/22 07:05:39.701
Dec  9 07:05:39.706: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-4219" to be "running"
Dec  9 07:05:39.712: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 5.264053ms
Dec  9 07:05:41.716: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.009159405s
Dec  9 07:05:41.716: INFO: Pod "test-container-pod" satisfied condition "running"
Dec  9 07:05:41.719: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
Dec  9 07:05:41.719: INFO: Breadth first check of 10.2.136.92 on host 10.0.10.179...
Dec  9 07:05:41.722: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.2.136.93:9080/dial?request=hostname&protocol=udp&host=10.2.136.92&port=8081&tries=1'] Namespace:pod-network-test-4219 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec  9 07:05:41.722: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
Dec  9 07:05:41.722: INFO: ExecWithOptions: Clientset creation
Dec  9 07:05:41.723: INFO: ExecWithOptions: execute(POST https://10.3.0.1:443/api/v1/namespaces/pod-network-test-4219/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.2.136.93%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.2.136.92%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Dec  9 07:05:41.799: INFO: Waiting for responses: map[]
Dec  9 07:05:41.799: INFO: reached 10.2.136.92 after 0/1 tries
Dec  9 07:05:41.799: INFO: Breadth first check of 10.2.166.66 on host 10.0.17.108...
Dec  9 07:05:41.802: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.2.136.93:9080/dial?request=hostname&protocol=udp&host=10.2.166.66&port=8081&tries=1'] Namespace:pod-network-test-4219 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec  9 07:05:41.802: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
Dec  9 07:05:41.805: INFO: ExecWithOptions: Clientset creation
Dec  9 07:05:41.805: INFO: ExecWithOptions: execute(POST https://10.3.0.1:443/api/v1/namespaces/pod-network-test-4219/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.2.136.93%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.2.166.66%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Dec  9 07:05:41.871: INFO: Waiting for responses: map[]
Dec  9 07:05:41.871: INFO: reached 10.2.166.66 after 0/1 tries
Dec  9 07:05:41.871: INFO: Going to retry 0 out of 2 pods....
[AfterEach] [sig-network] Networking
  test/e2e/framework/node/init/init.go:32
Dec  9 07:05:41.871: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Networking
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Networking
  tear down framework | framework.go:193
STEP: Destroying namespace "pod-network-test-4219" for this suite. 12/09/22 07:05:41.874
------------------------------
â€¢ [SLOW TEST] [24.284 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:93

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:05:17.595
    Dec  9 07:05:17.595: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename pod-network-test 12/09/22 07:05:17.596
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:05:17.621
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:05:17.626
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:31
    [It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:93
    STEP: Performing setup for networking test in namespace pod-network-test-4219 12/09/22 07:05:17.635
    STEP: creating a selector 12/09/22 07:05:17.637
    STEP: Creating the service pods in kubernetes 12/09/22 07:05:17.637
    Dec  9 07:05:17.638: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Dec  9 07:05:17.678: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-4219" to be "running and ready"
    Dec  9 07:05:17.687: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 9.33409ms
    Dec  9 07:05:17.687: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Dec  9 07:05:19.692: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014332133s
    Dec  9 07:05:19.692: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Dec  9 07:05:21.695: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.017034115s
    Dec  9 07:05:21.695: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Dec  9 07:05:23.694: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 6.015767441s
    Dec  9 07:05:23.694: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Dec  9 07:05:25.695: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 8.016724529s
    Dec  9 07:05:25.695: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Dec  9 07:05:27.705: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.027620064s
    Dec  9 07:05:27.706: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Dec  9 07:05:29.692: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.014447784s
    Dec  9 07:05:29.692: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Dec  9 07:05:31.691: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.013303581s
    Dec  9 07:05:31.691: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Dec  9 07:05:33.692: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.014284175s
    Dec  9 07:05:33.692: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Dec  9 07:05:35.690: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.012545516s
    Dec  9 07:05:35.690: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Dec  9 07:05:37.692: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.014070826s
    Dec  9 07:05:37.692: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Dec  9 07:05:39.692: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.014304107s
    Dec  9 07:05:39.692: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Dec  9 07:05:39.692: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Dec  9 07:05:39.695: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-4219" to be "running and ready"
    Dec  9 07:05:39.698: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 2.747065ms
    Dec  9 07:05:39.698: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Dec  9 07:05:39.698: INFO: Pod "netserver-1" satisfied condition "running and ready"
    STEP: Creating test pods 12/09/22 07:05:39.701
    Dec  9 07:05:39.706: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-4219" to be "running"
    Dec  9 07:05:39.712: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 5.264053ms
    Dec  9 07:05:41.716: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.009159405s
    Dec  9 07:05:41.716: INFO: Pod "test-container-pod" satisfied condition "running"
    Dec  9 07:05:41.719: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
    Dec  9 07:05:41.719: INFO: Breadth first check of 10.2.136.92 on host 10.0.10.179...
    Dec  9 07:05:41.722: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.2.136.93:9080/dial?request=hostname&protocol=udp&host=10.2.136.92&port=8081&tries=1'] Namespace:pod-network-test-4219 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Dec  9 07:05:41.722: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    Dec  9 07:05:41.722: INFO: ExecWithOptions: Clientset creation
    Dec  9 07:05:41.723: INFO: ExecWithOptions: execute(POST https://10.3.0.1:443/api/v1/namespaces/pod-network-test-4219/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.2.136.93%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.2.136.92%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Dec  9 07:05:41.799: INFO: Waiting for responses: map[]
    Dec  9 07:05:41.799: INFO: reached 10.2.136.92 after 0/1 tries
    Dec  9 07:05:41.799: INFO: Breadth first check of 10.2.166.66 on host 10.0.17.108...
    Dec  9 07:05:41.802: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.2.136.93:9080/dial?request=hostname&protocol=udp&host=10.2.166.66&port=8081&tries=1'] Namespace:pod-network-test-4219 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Dec  9 07:05:41.802: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    Dec  9 07:05:41.805: INFO: ExecWithOptions: Clientset creation
    Dec  9 07:05:41.805: INFO: ExecWithOptions: execute(POST https://10.3.0.1:443/api/v1/namespaces/pod-network-test-4219/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.2.136.93%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.2.166.66%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Dec  9 07:05:41.871: INFO: Waiting for responses: map[]
    Dec  9 07:05:41.871: INFO: reached 10.2.166.66 after 0/1 tries
    Dec  9 07:05:41.871: INFO: Going to retry 0 out of 2 pods....
    [AfterEach] [sig-network] Networking
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:05:41.871: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Networking
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Networking
      tear down framework | framework.go:193
    STEP: Destroying namespace "pod-network-test-4219" for this suite. 12/09/22 07:05:41.874
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-network] Proxy version v1
  A set of valid responses are returned for both pod and service Proxy [Conformance]
  test/e2e/network/proxy.go:380
[BeforeEach] version v1
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:05:41.879
Dec  9 07:05:41.880: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename proxy 12/09/22 07:05:41.88
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:05:41.908
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:05:41.912
[BeforeEach] version v1
  test/e2e/framework/metrics/init/init.go:31
[It] A set of valid responses are returned for both pod and service Proxy [Conformance]
  test/e2e/network/proxy.go:380
Dec  9 07:05:41.915: INFO: Creating pod...
Dec  9 07:05:41.923: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-479" to be "running"
Dec  9 07:05:41.925: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 2.666486ms
Dec  9 07:05:43.930: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.006979975s
Dec  9 07:05:43.930: INFO: Pod "agnhost" satisfied condition "running"
Dec  9 07:05:43.930: INFO: Creating service...
Dec  9 07:05:43.945: INFO: Starting http.Client for https://10.3.0.1:443/api/v1/namespaces/proxy-479/pods/agnhost/proxy?method=DELETE
Dec  9 07:05:43.956: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Dec  9 07:05:43.959: INFO: Starting http.Client for https://10.3.0.1:443/api/v1/namespaces/proxy-479/pods/agnhost/proxy?method=OPTIONS
Dec  9 07:05:43.964: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Dec  9 07:05:43.965: INFO: Starting http.Client for https://10.3.0.1:443/api/v1/namespaces/proxy-479/pods/agnhost/proxy?method=PATCH
Dec  9 07:05:43.968: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Dec  9 07:05:43.968: INFO: Starting http.Client for https://10.3.0.1:443/api/v1/namespaces/proxy-479/pods/agnhost/proxy?method=POST
Dec  9 07:05:43.977: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Dec  9 07:05:43.977: INFO: Starting http.Client for https://10.3.0.1:443/api/v1/namespaces/proxy-479/pods/agnhost/proxy?method=PUT
Dec  9 07:05:43.984: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Dec  9 07:05:43.984: INFO: Starting http.Client for https://10.3.0.1:443/api/v1/namespaces/proxy-479/services/e2e-proxy-test-service/proxy?method=DELETE
Dec  9 07:05:43.992: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Dec  9 07:05:43.992: INFO: Starting http.Client for https://10.3.0.1:443/api/v1/namespaces/proxy-479/services/e2e-proxy-test-service/proxy?method=OPTIONS
Dec  9 07:05:43.998: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Dec  9 07:05:43.998: INFO: Starting http.Client for https://10.3.0.1:443/api/v1/namespaces/proxy-479/services/e2e-proxy-test-service/proxy?method=PATCH
Dec  9 07:05:44.003: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Dec  9 07:05:44.004: INFO: Starting http.Client for https://10.3.0.1:443/api/v1/namespaces/proxy-479/services/e2e-proxy-test-service/proxy?method=POST
Dec  9 07:05:44.012: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Dec  9 07:05:44.013: INFO: Starting http.Client for https://10.3.0.1:443/api/v1/namespaces/proxy-479/services/e2e-proxy-test-service/proxy?method=PUT
Dec  9 07:05:44.022: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Dec  9 07:05:44.022: INFO: Starting http.Client for https://10.3.0.1:443/api/v1/namespaces/proxy-479/pods/agnhost/proxy?method=GET
Dec  9 07:05:44.026: INFO: http.Client request:GET StatusCode:301
Dec  9 07:05:44.026: INFO: Starting http.Client for https://10.3.0.1:443/api/v1/namespaces/proxy-479/services/e2e-proxy-test-service/proxy?method=GET
Dec  9 07:05:44.030: INFO: http.Client request:GET StatusCode:301
Dec  9 07:05:44.030: INFO: Starting http.Client for https://10.3.0.1:443/api/v1/namespaces/proxy-479/pods/agnhost/proxy?method=HEAD
Dec  9 07:05:44.033: INFO: http.Client request:HEAD StatusCode:301
Dec  9 07:05:44.033: INFO: Starting http.Client for https://10.3.0.1:443/api/v1/namespaces/proxy-479/services/e2e-proxy-test-service/proxy?method=HEAD
Dec  9 07:05:44.040: INFO: http.Client request:HEAD StatusCode:301
[AfterEach] version v1
  test/e2e/framework/node/init/init.go:32
Dec  9 07:05:44.040: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] version v1
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] version v1
  dump namespaces | framework.go:196
[DeferCleanup (Each)] version v1
  tear down framework | framework.go:193
STEP: Destroying namespace "proxy-479" for this suite. 12/09/22 07:05:44.045
------------------------------
â€¢ [2.181 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    A set of valid responses are returned for both pod and service Proxy [Conformance]
    test/e2e/network/proxy.go:380

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:05:41.879
    Dec  9 07:05:41.880: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename proxy 12/09/22 07:05:41.88
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:05:41.908
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:05:41.912
    [BeforeEach] version v1
      test/e2e/framework/metrics/init/init.go:31
    [It] A set of valid responses are returned for both pod and service Proxy [Conformance]
      test/e2e/network/proxy.go:380
    Dec  9 07:05:41.915: INFO: Creating pod...
    Dec  9 07:05:41.923: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-479" to be "running"
    Dec  9 07:05:41.925: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 2.666486ms
    Dec  9 07:05:43.930: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.006979975s
    Dec  9 07:05:43.930: INFO: Pod "agnhost" satisfied condition "running"
    Dec  9 07:05:43.930: INFO: Creating service...
    Dec  9 07:05:43.945: INFO: Starting http.Client for https://10.3.0.1:443/api/v1/namespaces/proxy-479/pods/agnhost/proxy?method=DELETE
    Dec  9 07:05:43.956: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Dec  9 07:05:43.959: INFO: Starting http.Client for https://10.3.0.1:443/api/v1/namespaces/proxy-479/pods/agnhost/proxy?method=OPTIONS
    Dec  9 07:05:43.964: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Dec  9 07:05:43.965: INFO: Starting http.Client for https://10.3.0.1:443/api/v1/namespaces/proxy-479/pods/agnhost/proxy?method=PATCH
    Dec  9 07:05:43.968: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Dec  9 07:05:43.968: INFO: Starting http.Client for https://10.3.0.1:443/api/v1/namespaces/proxy-479/pods/agnhost/proxy?method=POST
    Dec  9 07:05:43.977: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Dec  9 07:05:43.977: INFO: Starting http.Client for https://10.3.0.1:443/api/v1/namespaces/proxy-479/pods/agnhost/proxy?method=PUT
    Dec  9 07:05:43.984: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Dec  9 07:05:43.984: INFO: Starting http.Client for https://10.3.0.1:443/api/v1/namespaces/proxy-479/services/e2e-proxy-test-service/proxy?method=DELETE
    Dec  9 07:05:43.992: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Dec  9 07:05:43.992: INFO: Starting http.Client for https://10.3.0.1:443/api/v1/namespaces/proxy-479/services/e2e-proxy-test-service/proxy?method=OPTIONS
    Dec  9 07:05:43.998: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Dec  9 07:05:43.998: INFO: Starting http.Client for https://10.3.0.1:443/api/v1/namespaces/proxy-479/services/e2e-proxy-test-service/proxy?method=PATCH
    Dec  9 07:05:44.003: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Dec  9 07:05:44.004: INFO: Starting http.Client for https://10.3.0.1:443/api/v1/namespaces/proxy-479/services/e2e-proxy-test-service/proxy?method=POST
    Dec  9 07:05:44.012: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Dec  9 07:05:44.013: INFO: Starting http.Client for https://10.3.0.1:443/api/v1/namespaces/proxy-479/services/e2e-proxy-test-service/proxy?method=PUT
    Dec  9 07:05:44.022: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Dec  9 07:05:44.022: INFO: Starting http.Client for https://10.3.0.1:443/api/v1/namespaces/proxy-479/pods/agnhost/proxy?method=GET
    Dec  9 07:05:44.026: INFO: http.Client request:GET StatusCode:301
    Dec  9 07:05:44.026: INFO: Starting http.Client for https://10.3.0.1:443/api/v1/namespaces/proxy-479/services/e2e-proxy-test-service/proxy?method=GET
    Dec  9 07:05:44.030: INFO: http.Client request:GET StatusCode:301
    Dec  9 07:05:44.030: INFO: Starting http.Client for https://10.3.0.1:443/api/v1/namespaces/proxy-479/pods/agnhost/proxy?method=HEAD
    Dec  9 07:05:44.033: INFO: http.Client request:HEAD StatusCode:301
    Dec  9 07:05:44.033: INFO: Starting http.Client for https://10.3.0.1:443/api/v1/namespaces/proxy-479/services/e2e-proxy-test-service/proxy?method=HEAD
    Dec  9 07:05:44.040: INFO: http.Client request:HEAD StatusCode:301
    [AfterEach] version v1
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:05:44.040: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] version v1
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] version v1
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] version v1
      tear down framework | framework.go:193
    STEP: Destroying namespace "proxy-479" for this suite. 12/09/22 07:05:44.045
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should list, patch and delete a collection of StatefulSets [Conformance]
  test/e2e/apps/statefulset.go:908
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:05:44.064
Dec  9 07:05:44.064: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename statefulset 12/09/22 07:05:44.074
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:05:44.1
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:05:44.109
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-4490 12/09/22 07:05:44.115
[It] should list, patch and delete a collection of StatefulSets [Conformance]
  test/e2e/apps/statefulset.go:908
Dec  9 07:05:44.150: INFO: Found 0 stateful pods, waiting for 1
Dec  9 07:05:54.155: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: patching the StatefulSet 12/09/22 07:05:54.167
W1209 07:05:54.187416      20 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Dec  9 07:05:54.223: INFO: Found 1 stateful pods, waiting for 2
Dec  9 07:06:04.230: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
Dec  9 07:06:04.231: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
STEP: Listing all StatefulSets 12/09/22 07:06:04.239
STEP: Delete all of the StatefulSets 12/09/22 07:06:04.244
STEP: Verify that StatefulSets have been deleted 12/09/22 07:06:04.251
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Dec  9 07:06:04.260: INFO: Deleting all statefulset in ns statefulset-4490
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Dec  9 07:06:04.381: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-4490" for this suite. 12/09/22 07:06:04.388
------------------------------
â€¢ [SLOW TEST] [20.365 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should list, patch and delete a collection of StatefulSets [Conformance]
    test/e2e/apps/statefulset.go:908

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:05:44.064
    Dec  9 07:05:44.064: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename statefulset 12/09/22 07:05:44.074
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:05:44.1
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:05:44.109
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-4490 12/09/22 07:05:44.115
    [It] should list, patch and delete a collection of StatefulSets [Conformance]
      test/e2e/apps/statefulset.go:908
    Dec  9 07:05:44.150: INFO: Found 0 stateful pods, waiting for 1
    Dec  9 07:05:54.155: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: patching the StatefulSet 12/09/22 07:05:54.167
    W1209 07:05:54.187416      20 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Dec  9 07:05:54.223: INFO: Found 1 stateful pods, waiting for 2
    Dec  9 07:06:04.230: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Dec  9 07:06:04.231: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Listing all StatefulSets 12/09/22 07:06:04.239
    STEP: Delete all of the StatefulSets 12/09/22 07:06:04.244
    STEP: Verify that StatefulSets have been deleted 12/09/22 07:06:04.251
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Dec  9 07:06:04.260: INFO: Deleting all statefulset in ns statefulset-4490
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:06:04.381: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-4490" for this suite. 12/09/22 07:06:04.388
  << End Captured GinkgoWriter Output
------------------------------
[sig-network] DNS
  should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:06:04.429
Dec  9 07:06:04.429: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename dns 12/09/22 07:06:04.43
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:06:04.513
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:06:04.519
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290
STEP: Creating a test headless service 12/09/22 07:06:04.533
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-9072.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-9072.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-9072.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9072.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-9072.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-9072.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-9072.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-9072.svc.cluster.local;sleep 1; done
 12/09/22 07:06:04.546
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-9072.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-9072.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-9072.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-9072.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-9072.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-9072.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-9072.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-9072.svc.cluster.local;sleep 1; done
 12/09/22 07:06:04.546
STEP: creating a pod to probe DNS 12/09/22 07:06:04.546
STEP: submitting the pod to kubernetes 12/09/22 07:06:04.546
Dec  9 07:06:04.579: INFO: Waiting up to 15m0s for pod "dns-test-3204fa39-fbd4-46ff-990c-0553ec1e1a09" in namespace "dns-9072" to be "running"
Dec  9 07:06:04.593: INFO: Pod "dns-test-3204fa39-fbd4-46ff-990c-0553ec1e1a09": Phase="Pending", Reason="", readiness=false. Elapsed: 14.702011ms
Dec  9 07:06:06.597: INFO: Pod "dns-test-3204fa39-fbd4-46ff-990c-0553ec1e1a09": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018397315s
Dec  9 07:06:08.597: INFO: Pod "dns-test-3204fa39-fbd4-46ff-990c-0553ec1e1a09": Phase="Running", Reason="", readiness=true. Elapsed: 4.018447285s
Dec  9 07:06:08.597: INFO: Pod "dns-test-3204fa39-fbd4-46ff-990c-0553ec1e1a09" satisfied condition "running"
STEP: retrieving the pod 12/09/22 07:06:08.597
STEP: looking for the results for each expected name from probers 12/09/22 07:06:08.6
Dec  9 07:06:08.606: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-9072.svc.cluster.local from pod dns-9072/dns-test-3204fa39-fbd4-46ff-990c-0553ec1e1a09: the server could not find the requested resource (get pods dns-test-3204fa39-fbd4-46ff-990c-0553ec1e1a09)
Dec  9 07:06:08.611: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9072.svc.cluster.local from pod dns-9072/dns-test-3204fa39-fbd4-46ff-990c-0553ec1e1a09: the server could not find the requested resource (get pods dns-test-3204fa39-fbd4-46ff-990c-0553ec1e1a09)
Dec  9 07:06:08.614: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-9072.svc.cluster.local from pod dns-9072/dns-test-3204fa39-fbd4-46ff-990c-0553ec1e1a09: the server could not find the requested resource (get pods dns-test-3204fa39-fbd4-46ff-990c-0553ec1e1a09)
Dec  9 07:06:08.617: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-9072.svc.cluster.local from pod dns-9072/dns-test-3204fa39-fbd4-46ff-990c-0553ec1e1a09: the server could not find the requested resource (get pods dns-test-3204fa39-fbd4-46ff-990c-0553ec1e1a09)
Dec  9 07:06:08.620: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-9072.svc.cluster.local from pod dns-9072/dns-test-3204fa39-fbd4-46ff-990c-0553ec1e1a09: the server could not find the requested resource (get pods dns-test-3204fa39-fbd4-46ff-990c-0553ec1e1a09)
Dec  9 07:06:08.626: INFO: Unable to read jessie_udp@dns-test-service-2.dns-9072.svc.cluster.local from pod dns-9072/dns-test-3204fa39-fbd4-46ff-990c-0553ec1e1a09: the server could not find the requested resource (get pods dns-test-3204fa39-fbd4-46ff-990c-0553ec1e1a09)
Dec  9 07:06:08.629: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-9072.svc.cluster.local from pod dns-9072/dns-test-3204fa39-fbd4-46ff-990c-0553ec1e1a09: the server could not find the requested resource (get pods dns-test-3204fa39-fbd4-46ff-990c-0553ec1e1a09)
Dec  9 07:06:08.629: INFO: Lookups using dns-9072/dns-test-3204fa39-fbd4-46ff-990c-0553ec1e1a09 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-9072.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9072.svc.cluster.local wheezy_udp@dns-test-service-2.dns-9072.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-9072.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-9072.svc.cluster.local jessie_udp@dns-test-service-2.dns-9072.svc.cluster.local jessie_tcp@dns-test-service-2.dns-9072.svc.cluster.local]

Dec  9 07:06:13.656: INFO: DNS probes using dns-9072/dns-test-3204fa39-fbd4-46ff-990c-0553ec1e1a09 succeeded

STEP: deleting the pod 12/09/22 07:06:13.656
STEP: deleting the test headless service 12/09/22 07:06:13.67
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Dec  9 07:06:13.741: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-9072" for this suite. 12/09/22 07:06:13.755
------------------------------
â€¢ [SLOW TEST] [9.335 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:06:04.429
    Dec  9 07:06:04.429: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename dns 12/09/22 07:06:04.43
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:06:04.513
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:06:04.519
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for pods for Subdomain [Conformance]
      test/e2e/network/dns.go:290
    STEP: Creating a test headless service 12/09/22 07:06:04.533
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-9072.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-9072.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-9072.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9072.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-9072.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-9072.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-9072.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-9072.svc.cluster.local;sleep 1; done
     12/09/22 07:06:04.546
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-9072.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-9072.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-9072.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-9072.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-9072.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-9072.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-9072.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-9072.svc.cluster.local;sleep 1; done
     12/09/22 07:06:04.546
    STEP: creating a pod to probe DNS 12/09/22 07:06:04.546
    STEP: submitting the pod to kubernetes 12/09/22 07:06:04.546
    Dec  9 07:06:04.579: INFO: Waiting up to 15m0s for pod "dns-test-3204fa39-fbd4-46ff-990c-0553ec1e1a09" in namespace "dns-9072" to be "running"
    Dec  9 07:06:04.593: INFO: Pod "dns-test-3204fa39-fbd4-46ff-990c-0553ec1e1a09": Phase="Pending", Reason="", readiness=false. Elapsed: 14.702011ms
    Dec  9 07:06:06.597: INFO: Pod "dns-test-3204fa39-fbd4-46ff-990c-0553ec1e1a09": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018397315s
    Dec  9 07:06:08.597: INFO: Pod "dns-test-3204fa39-fbd4-46ff-990c-0553ec1e1a09": Phase="Running", Reason="", readiness=true. Elapsed: 4.018447285s
    Dec  9 07:06:08.597: INFO: Pod "dns-test-3204fa39-fbd4-46ff-990c-0553ec1e1a09" satisfied condition "running"
    STEP: retrieving the pod 12/09/22 07:06:08.597
    STEP: looking for the results for each expected name from probers 12/09/22 07:06:08.6
    Dec  9 07:06:08.606: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-9072.svc.cluster.local from pod dns-9072/dns-test-3204fa39-fbd4-46ff-990c-0553ec1e1a09: the server could not find the requested resource (get pods dns-test-3204fa39-fbd4-46ff-990c-0553ec1e1a09)
    Dec  9 07:06:08.611: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9072.svc.cluster.local from pod dns-9072/dns-test-3204fa39-fbd4-46ff-990c-0553ec1e1a09: the server could not find the requested resource (get pods dns-test-3204fa39-fbd4-46ff-990c-0553ec1e1a09)
    Dec  9 07:06:08.614: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-9072.svc.cluster.local from pod dns-9072/dns-test-3204fa39-fbd4-46ff-990c-0553ec1e1a09: the server could not find the requested resource (get pods dns-test-3204fa39-fbd4-46ff-990c-0553ec1e1a09)
    Dec  9 07:06:08.617: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-9072.svc.cluster.local from pod dns-9072/dns-test-3204fa39-fbd4-46ff-990c-0553ec1e1a09: the server could not find the requested resource (get pods dns-test-3204fa39-fbd4-46ff-990c-0553ec1e1a09)
    Dec  9 07:06:08.620: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-9072.svc.cluster.local from pod dns-9072/dns-test-3204fa39-fbd4-46ff-990c-0553ec1e1a09: the server could not find the requested resource (get pods dns-test-3204fa39-fbd4-46ff-990c-0553ec1e1a09)
    Dec  9 07:06:08.626: INFO: Unable to read jessie_udp@dns-test-service-2.dns-9072.svc.cluster.local from pod dns-9072/dns-test-3204fa39-fbd4-46ff-990c-0553ec1e1a09: the server could not find the requested resource (get pods dns-test-3204fa39-fbd4-46ff-990c-0553ec1e1a09)
    Dec  9 07:06:08.629: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-9072.svc.cluster.local from pod dns-9072/dns-test-3204fa39-fbd4-46ff-990c-0553ec1e1a09: the server could not find the requested resource (get pods dns-test-3204fa39-fbd4-46ff-990c-0553ec1e1a09)
    Dec  9 07:06:08.629: INFO: Lookups using dns-9072/dns-test-3204fa39-fbd4-46ff-990c-0553ec1e1a09 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-9072.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9072.svc.cluster.local wheezy_udp@dns-test-service-2.dns-9072.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-9072.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-9072.svc.cluster.local jessie_udp@dns-test-service-2.dns-9072.svc.cluster.local jessie_tcp@dns-test-service-2.dns-9072.svc.cluster.local]

    Dec  9 07:06:13.656: INFO: DNS probes using dns-9072/dns-test-3204fa39-fbd4-46ff-990c-0553ec1e1a09 succeeded

    STEP: deleting the pod 12/09/22 07:06:13.656
    STEP: deleting the test headless service 12/09/22 07:06:13.67
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:06:13.741: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-9072" for this suite. 12/09/22 07:06:13.755
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:95
[BeforeEach] [sig-node] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:06:13.776
Dec  9 07:06:13.777: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename secrets 12/09/22 07:06:13.778
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:06:13.795
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:06:13.806
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:95
STEP: creating secret secrets-8326/secret-test-2509bcb1-ddf7-4ad9-9216-ddc8a88f9ae3 12/09/22 07:06:13.812
STEP: Creating a pod to test consume secrets 12/09/22 07:06:13.828
Dec  9 07:06:13.840: INFO: Waiting up to 5m0s for pod "pod-configmaps-89c0ddad-4a7e-4f07-bffb-8f9cd89b6372" in namespace "secrets-8326" to be "Succeeded or Failed"
Dec  9 07:06:13.845: INFO: Pod "pod-configmaps-89c0ddad-4a7e-4f07-bffb-8f9cd89b6372": Phase="Pending", Reason="", readiness=false. Elapsed: 4.903968ms
Dec  9 07:06:15.849: INFO: Pod "pod-configmaps-89c0ddad-4a7e-4f07-bffb-8f9cd89b6372": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009012173s
Dec  9 07:06:17.849: INFO: Pod "pod-configmaps-89c0ddad-4a7e-4f07-bffb-8f9cd89b6372": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009411959s
Dec  9 07:06:19.849: INFO: Pod "pod-configmaps-89c0ddad-4a7e-4f07-bffb-8f9cd89b6372": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.008957613s
STEP: Saw pod success 12/09/22 07:06:19.849
Dec  9 07:06:19.849: INFO: Pod "pod-configmaps-89c0ddad-4a7e-4f07-bffb-8f9cd89b6372" satisfied condition "Succeeded or Failed"
Dec  9 07:06:19.852: INFO: Trying to get logs from node ip-10-0-10-179 pod pod-configmaps-89c0ddad-4a7e-4f07-bffb-8f9cd89b6372 container env-test: <nil>
STEP: delete the pod 12/09/22 07:06:19.864
Dec  9 07:06:19.874: INFO: Waiting for pod pod-configmaps-89c0ddad-4a7e-4f07-bffb-8f9cd89b6372 to disappear
Dec  9 07:06:19.876: INFO: Pod pod-configmaps-89c0ddad-4a7e-4f07-bffb-8f9cd89b6372 no longer exists
[AfterEach] [sig-node] Secrets
  test/e2e/framework/node/init/init.go:32
Dec  9 07:06:19.876: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-8326" for this suite. 12/09/22 07:06:19.879
------------------------------
â€¢ [SLOW TEST] [6.109 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:95

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:06:13.776
    Dec  9 07:06:13.777: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename secrets 12/09/22 07:06:13.778
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:06:13.795
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:06:13.806
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable via the environment [NodeConformance] [Conformance]
      test/e2e/common/node/secrets.go:95
    STEP: creating secret secrets-8326/secret-test-2509bcb1-ddf7-4ad9-9216-ddc8a88f9ae3 12/09/22 07:06:13.812
    STEP: Creating a pod to test consume secrets 12/09/22 07:06:13.828
    Dec  9 07:06:13.840: INFO: Waiting up to 5m0s for pod "pod-configmaps-89c0ddad-4a7e-4f07-bffb-8f9cd89b6372" in namespace "secrets-8326" to be "Succeeded or Failed"
    Dec  9 07:06:13.845: INFO: Pod "pod-configmaps-89c0ddad-4a7e-4f07-bffb-8f9cd89b6372": Phase="Pending", Reason="", readiness=false. Elapsed: 4.903968ms
    Dec  9 07:06:15.849: INFO: Pod "pod-configmaps-89c0ddad-4a7e-4f07-bffb-8f9cd89b6372": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009012173s
    Dec  9 07:06:17.849: INFO: Pod "pod-configmaps-89c0ddad-4a7e-4f07-bffb-8f9cd89b6372": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009411959s
    Dec  9 07:06:19.849: INFO: Pod "pod-configmaps-89c0ddad-4a7e-4f07-bffb-8f9cd89b6372": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.008957613s
    STEP: Saw pod success 12/09/22 07:06:19.849
    Dec  9 07:06:19.849: INFO: Pod "pod-configmaps-89c0ddad-4a7e-4f07-bffb-8f9cd89b6372" satisfied condition "Succeeded or Failed"
    Dec  9 07:06:19.852: INFO: Trying to get logs from node ip-10-0-10-179 pod pod-configmaps-89c0ddad-4a7e-4f07-bffb-8f9cd89b6372 container env-test: <nil>
    STEP: delete the pod 12/09/22 07:06:19.864
    Dec  9 07:06:19.874: INFO: Waiting for pod pod-configmaps-89c0ddad-4a7e-4f07-bffb-8f9cd89b6372 to disappear
    Dec  9 07:06:19.876: INFO: Pod pod-configmaps-89c0ddad-4a7e-4f07-bffb-8f9cd89b6372 no longer exists
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:06:19.876: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-8326" for this suite. 12/09/22 07:06:19.879
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] CSIStorageCapacity
   should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49
[BeforeEach] [sig-storage] CSIStorageCapacity
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:06:19.891
Dec  9 07:06:19.892: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename csistoragecapacity 12/09/22 07:06:19.894
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:06:19.911
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:06:19.916
[BeforeEach] [sig-storage] CSIStorageCapacity
  test/e2e/framework/metrics/init/init.go:31
[It]  should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49
STEP: getting /apis 12/09/22 07:06:19.92
STEP: getting /apis/storage.k8s.io 12/09/22 07:06:19.928
STEP: getting /apis/storage.k8s.io/v1 12/09/22 07:06:19.93
STEP: creating 12/09/22 07:06:19.932
STEP: watching 12/09/22 07:06:19.952
Dec  9 07:06:19.952: INFO: starting watch
STEP: getting 12/09/22 07:06:19.961
STEP: listing in namespace 12/09/22 07:06:19.966
STEP: listing across namespaces 12/09/22 07:06:19.971
STEP: patching 12/09/22 07:06:19.974
STEP: updating 12/09/22 07:06:19.979
Dec  9 07:06:19.984: INFO: waiting for watch events with expected annotations in namespace
Dec  9 07:06:19.984: INFO: waiting for watch events with expected annotations across namespace
STEP: deleting 12/09/22 07:06:19.984
STEP: deleting a collection 12/09/22 07:06:19.993
[AfterEach] [sig-storage] CSIStorageCapacity
  test/e2e/framework/node/init/init.go:32
Dec  9 07:06:20.006: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
  tear down framework | framework.go:193
STEP: Destroying namespace "csistoragecapacity-5538" for this suite. 12/09/22 07:06:20.01
------------------------------
â€¢ [0.124 seconds]
[sig-storage] CSIStorageCapacity
test/e2e/storage/utils/framework.go:23
   should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] CSIStorageCapacity
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:06:19.891
    Dec  9 07:06:19.892: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename csistoragecapacity 12/09/22 07:06:19.894
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:06:19.911
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:06:19.916
    [BeforeEach] [sig-storage] CSIStorageCapacity
      test/e2e/framework/metrics/init/init.go:31
    [It]  should support CSIStorageCapacities API operations [Conformance]
      test/e2e/storage/csistoragecapacity.go:49
    STEP: getting /apis 12/09/22 07:06:19.92
    STEP: getting /apis/storage.k8s.io 12/09/22 07:06:19.928
    STEP: getting /apis/storage.k8s.io/v1 12/09/22 07:06:19.93
    STEP: creating 12/09/22 07:06:19.932
    STEP: watching 12/09/22 07:06:19.952
    Dec  9 07:06:19.952: INFO: starting watch
    STEP: getting 12/09/22 07:06:19.961
    STEP: listing in namespace 12/09/22 07:06:19.966
    STEP: listing across namespaces 12/09/22 07:06:19.971
    STEP: patching 12/09/22 07:06:19.974
    STEP: updating 12/09/22 07:06:19.979
    Dec  9 07:06:19.984: INFO: waiting for watch events with expected annotations in namespace
    Dec  9 07:06:19.984: INFO: waiting for watch events with expected annotations across namespace
    STEP: deleting 12/09/22 07:06:19.984
    STEP: deleting a collection 12/09/22 07:06:19.993
    [AfterEach] [sig-storage] CSIStorageCapacity
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:06:20.006: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
      tear down framework | framework.go:193
    STEP: Destroying namespace "csistoragecapacity-5538" for this suite. 12/09/22 07:06:20.01
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:161
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:06:20.02
Dec  9 07:06:20.020: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename svcaccounts 12/09/22 07:06:20.023
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:06:20.039
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:06:20.044
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:161
Dec  9 07:06:20.069: INFO: created pod pod-service-account-defaultsa
Dec  9 07:06:20.069: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Dec  9 07:06:20.076: INFO: created pod pod-service-account-mountsa
Dec  9 07:06:20.076: INFO: pod pod-service-account-mountsa service account token volume mount: true
Dec  9 07:06:20.087: INFO: created pod pod-service-account-nomountsa
Dec  9 07:06:20.087: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Dec  9 07:06:20.101: INFO: created pod pod-service-account-defaultsa-mountspec
Dec  9 07:06:20.101: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Dec  9 07:06:20.117: INFO: created pod pod-service-account-mountsa-mountspec
Dec  9 07:06:20.118: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Dec  9 07:06:20.138: INFO: created pod pod-service-account-nomountsa-mountspec
Dec  9 07:06:20.138: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Dec  9 07:06:20.161: INFO: created pod pod-service-account-defaultsa-nomountspec
Dec  9 07:06:20.161: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Dec  9 07:06:20.178: INFO: created pod pod-service-account-mountsa-nomountspec
Dec  9 07:06:20.178: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Dec  9 07:06:20.187: INFO: created pod pod-service-account-nomountsa-nomountspec
Dec  9 07:06:20.187: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Dec  9 07:06:20.187: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-832" for this suite. 12/09/22 07:06:20.203
------------------------------
â€¢ [0.209 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:161

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:06:20.02
    Dec  9 07:06:20.020: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename svcaccounts 12/09/22 07:06:20.023
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:06:20.039
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:06:20.044
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow opting out of API token automount  [Conformance]
      test/e2e/auth/service_accounts.go:161
    Dec  9 07:06:20.069: INFO: created pod pod-service-account-defaultsa
    Dec  9 07:06:20.069: INFO: pod pod-service-account-defaultsa service account token volume mount: true
    Dec  9 07:06:20.076: INFO: created pod pod-service-account-mountsa
    Dec  9 07:06:20.076: INFO: pod pod-service-account-mountsa service account token volume mount: true
    Dec  9 07:06:20.087: INFO: created pod pod-service-account-nomountsa
    Dec  9 07:06:20.087: INFO: pod pod-service-account-nomountsa service account token volume mount: false
    Dec  9 07:06:20.101: INFO: created pod pod-service-account-defaultsa-mountspec
    Dec  9 07:06:20.101: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
    Dec  9 07:06:20.117: INFO: created pod pod-service-account-mountsa-mountspec
    Dec  9 07:06:20.118: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
    Dec  9 07:06:20.138: INFO: created pod pod-service-account-nomountsa-mountspec
    Dec  9 07:06:20.138: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
    Dec  9 07:06:20.161: INFO: created pod pod-service-account-defaultsa-nomountspec
    Dec  9 07:06:20.161: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
    Dec  9 07:06:20.178: INFO: created pod pod-service-account-mountsa-nomountspec
    Dec  9 07:06:20.178: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
    Dec  9 07:06:20.187: INFO: created pod pod-service-account-nomountsa-nomountspec
    Dec  9 07:06:20.187: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:06:20.187: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-832" for this suite. 12/09/22 07:06:20.203
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with privileged
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:528
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:06:20.235
Dec  9 07:06:20.235: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename security-context-test 12/09/22 07:06:20.238
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:06:20.258
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:06:20.262
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:50
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:528
Dec  9 07:06:20.273: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-03116f51-71b3-4a47-a02b-db4f3a6cd488" in namespace "security-context-test-554" to be "Succeeded or Failed"
Dec  9 07:06:20.277: INFO: Pod "busybox-privileged-false-03116f51-71b3-4a47-a02b-db4f3a6cd488": Phase="Pending", Reason="", readiness=false. Elapsed: 3.841961ms
Dec  9 07:06:22.282: INFO: Pod "busybox-privileged-false-03116f51-71b3-4a47-a02b-db4f3a6cd488": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008823219s
Dec  9 07:06:24.282: INFO: Pod "busybox-privileged-false-03116f51-71b3-4a47-a02b-db4f3a6cd488": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008558481s
Dec  9 07:06:26.284: INFO: Pod "busybox-privileged-false-03116f51-71b3-4a47-a02b-db4f3a6cd488": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.011299818s
Dec  9 07:06:26.284: INFO: Pod "busybox-privileged-false-03116f51-71b3-4a47-a02b-db4f3a6cd488" satisfied condition "Succeeded or Failed"
Dec  9 07:06:26.311: INFO: Got logs for pod "busybox-privileged-false-03116f51-71b3-4a47-a02b-db4f3a6cd488": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Dec  9 07:06:26.311: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-test-554" for this suite. 12/09/22 07:06:26.321
------------------------------
â€¢ [SLOW TEST] [6.102 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a pod with privileged
  test/e2e/common/node/security_context.go:491
    should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:528

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:06:20.235
    Dec  9 07:06:20.235: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename security-context-test 12/09/22 07:06:20.238
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:06:20.258
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:06:20.262
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:50
    [It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:528
    Dec  9 07:06:20.273: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-03116f51-71b3-4a47-a02b-db4f3a6cd488" in namespace "security-context-test-554" to be "Succeeded or Failed"
    Dec  9 07:06:20.277: INFO: Pod "busybox-privileged-false-03116f51-71b3-4a47-a02b-db4f3a6cd488": Phase="Pending", Reason="", readiness=false. Elapsed: 3.841961ms
    Dec  9 07:06:22.282: INFO: Pod "busybox-privileged-false-03116f51-71b3-4a47-a02b-db4f3a6cd488": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008823219s
    Dec  9 07:06:24.282: INFO: Pod "busybox-privileged-false-03116f51-71b3-4a47-a02b-db4f3a6cd488": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008558481s
    Dec  9 07:06:26.284: INFO: Pod "busybox-privileged-false-03116f51-71b3-4a47-a02b-db4f3a6cd488": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.011299818s
    Dec  9 07:06:26.284: INFO: Pod "busybox-privileged-false-03116f51-71b3-4a47-a02b-db4f3a6cd488" satisfied condition "Succeeded or Failed"
    Dec  9 07:06:26.311: INFO: Got logs for pod "busybox-privileged-false-03116f51-71b3-4a47-a02b-db4f3a6cd488": "ip: RTNETLINK answers: Operation not permitted\n"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:06:26.311: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-test-554" for this suite. 12/09/22 07:06:26.321
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
  should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:06:26.345
Dec  9 07:06:26.346: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename runtimeclass 12/09/22 07:06:26.347
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:06:26.389
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:06:26.408
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It] should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Dec  9 07:06:26.430: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-4839" for this suite. 12/09/22 07:06:26.434
------------------------------
â€¢ [0.098 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:06:26.345
    Dec  9 07:06:26.346: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename runtimeclass 12/09/22 07:06:26.347
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:06:26.389
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:06:26.408
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It] should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:55
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:06:26.430: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-4839" for this suite. 12/09/22 07:06:26.434
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:413
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:06:26.445
Dec  9 07:06:26.445: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename webhook 12/09/22 07:06:26.448
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:06:26.468
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:06:26.473
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 12/09/22 07:06:26.491
STEP: Create role binding to let webhook read extension-apiserver-authentication 12/09/22 07:06:26.767
STEP: Deploying the webhook pod 12/09/22 07:06:26.773
STEP: Wait for the deployment to be ready 12/09/22 07:06:26.783
Dec  9 07:06:26.794: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 12/09/22 07:06:28.806
STEP: Verifying the service has paired with the endpoint 12/09/22 07:06:28.817
Dec  9 07:06:29.818: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:413
STEP: Creating a validating webhook configuration 12/09/22 07:06:29.821
STEP: Creating a configMap that does not comply to the validation webhook rules 12/09/22 07:06:29.846
STEP: Updating a validating webhook configuration's rules to not include the create operation 12/09/22 07:06:29.855
STEP: Creating a configMap that does not comply to the validation webhook rules 12/09/22 07:06:29.866
STEP: Patching a validating webhook configuration's rules to include the create operation 12/09/22 07:06:29.875
STEP: Creating a configMap that does not comply to the validation webhook rules 12/09/22 07:06:29.882
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Dec  9 07:06:29.891: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-8198" for this suite. 12/09/22 07:06:29.934
STEP: Destroying namespace "webhook-8198-markers" for this suite. 12/09/22 07:06:29.94
------------------------------
â€¢ [3.507 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:413

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:06:26.445
    Dec  9 07:06:26.445: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename webhook 12/09/22 07:06:26.448
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:06:26.468
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:06:26.473
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 12/09/22 07:06:26.491
    STEP: Create role binding to let webhook read extension-apiserver-authentication 12/09/22 07:06:26.767
    STEP: Deploying the webhook pod 12/09/22 07:06:26.773
    STEP: Wait for the deployment to be ready 12/09/22 07:06:26.783
    Dec  9 07:06:26.794: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 12/09/22 07:06:28.806
    STEP: Verifying the service has paired with the endpoint 12/09/22 07:06:28.817
    Dec  9 07:06:29.818: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] patching/updating a validating webhook should work [Conformance]
      test/e2e/apimachinery/webhook.go:413
    STEP: Creating a validating webhook configuration 12/09/22 07:06:29.821
    STEP: Creating a configMap that does not comply to the validation webhook rules 12/09/22 07:06:29.846
    STEP: Updating a validating webhook configuration's rules to not include the create operation 12/09/22 07:06:29.855
    STEP: Creating a configMap that does not comply to the validation webhook rules 12/09/22 07:06:29.866
    STEP: Patching a validating webhook configuration's rules to include the create operation 12/09/22 07:06:29.875
    STEP: Creating a configMap that does not comply to the validation webhook rules 12/09/22 07:06:29.882
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:06:29.891: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-8198" for this suite. 12/09/22 07:06:29.934
    STEP: Destroying namespace "webhook-8198-markers" for this suite. 12/09/22 07:06:29.94
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should apply changes to a resourcequota status [Conformance]
  test/e2e/apimachinery/resource_quota.go:1010
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:06:29.965
Dec  9 07:06:29.965: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename resourcequota 12/09/22 07:06:29.967
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:06:29.987
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:06:29.993
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should apply changes to a resourcequota status [Conformance]
  test/e2e/apimachinery/resource_quota.go:1010
STEP: Creating resourceQuota "e2e-rq-status-gb4b5" 12/09/22 07:06:30.028
Dec  9 07:06:30.050: INFO: Resource quota "e2e-rq-status-gb4b5" reports spec: hard cpu limit of 500m
Dec  9 07:06:30.050: INFO: Resource quota "e2e-rq-status-gb4b5" reports spec: hard memory limit of 500Mi
STEP: Updating resourceQuota "e2e-rq-status-gb4b5" /status 12/09/22 07:06:30.05
STEP: Confirm /status for "e2e-rq-status-gb4b5" resourceQuota via watch 12/09/22 07:06:30.061
Dec  9 07:06:30.065: INFO: observed resourceQuota "e2e-rq-status-gb4b5" in namespace "resourcequota-3510" with hard status: v1.ResourceList(nil)
Dec  9 07:06:30.067: INFO: Found resourceQuota "e2e-rq-status-gb4b5" in namespace "resourcequota-3510" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
Dec  9 07:06:30.067: INFO: ResourceQuota "e2e-rq-status-gb4b5" /status was updated
STEP: Patching hard spec values for cpu & memory 12/09/22 07:06:30.07
Dec  9 07:06:30.079: INFO: Resource quota "e2e-rq-status-gb4b5" reports spec: hard cpu limit of 1
Dec  9 07:06:30.079: INFO: Resource quota "e2e-rq-status-gb4b5" reports spec: hard memory limit of 1Gi
STEP: Patching "e2e-rq-status-gb4b5" /status 12/09/22 07:06:30.079
STEP: Confirm /status for "e2e-rq-status-gb4b5" resourceQuota via watch 12/09/22 07:06:30.087
Dec  9 07:06:30.090: INFO: observed resourceQuota "e2e-rq-status-gb4b5" in namespace "resourcequota-3510" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
Dec  9 07:06:30.090: INFO: Found resourceQuota "e2e-rq-status-gb4b5" in namespace "resourcequota-3510" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:1, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:1073741824, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1Gi", Format:"BinarySI"}}
Dec  9 07:06:30.090: INFO: ResourceQuota "e2e-rq-status-gb4b5" /status was patched
STEP: Get "e2e-rq-status-gb4b5" /status 12/09/22 07:06:30.09
Dec  9 07:06:30.099: INFO: Resourcequota "e2e-rq-status-gb4b5" reports status: hard cpu of 1
Dec  9 07:06:30.099: INFO: Resourcequota "e2e-rq-status-gb4b5" reports status: hard memory of 1Gi
STEP: Repatching "e2e-rq-status-gb4b5" /status before checking Spec is unchanged 12/09/22 07:06:30.103
Dec  9 07:06:30.108: INFO: Resourcequota "e2e-rq-status-gb4b5" reports status: hard cpu of 2
Dec  9 07:06:30.108: INFO: Resourcequota "e2e-rq-status-gb4b5" reports status: hard memory of 2Gi
Dec  9 07:06:30.110: INFO: Found resourceQuota "e2e-rq-status-gb4b5" in namespace "resourcequota-3510" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:2, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:2147483648, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2Gi", Format:"BinarySI"}}
Dec  9 07:10:50.120: INFO: ResourceQuota "e2e-rq-status-gb4b5" Spec was unchanged and /status reset
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Dec  9 07:10:50.120: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-3510" for this suite. 12/09/22 07:10:50.125
------------------------------
â€¢ [SLOW TEST] [260.174 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should apply changes to a resourcequota status [Conformance]
  test/e2e/apimachinery/resource_quota.go:1010

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:06:29.965
    Dec  9 07:06:29.965: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename resourcequota 12/09/22 07:06:29.967
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:06:29.987
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:06:29.993
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply changes to a resourcequota status [Conformance]
      test/e2e/apimachinery/resource_quota.go:1010
    STEP: Creating resourceQuota "e2e-rq-status-gb4b5" 12/09/22 07:06:30.028
    Dec  9 07:06:30.050: INFO: Resource quota "e2e-rq-status-gb4b5" reports spec: hard cpu limit of 500m
    Dec  9 07:06:30.050: INFO: Resource quota "e2e-rq-status-gb4b5" reports spec: hard memory limit of 500Mi
    STEP: Updating resourceQuota "e2e-rq-status-gb4b5" /status 12/09/22 07:06:30.05
    STEP: Confirm /status for "e2e-rq-status-gb4b5" resourceQuota via watch 12/09/22 07:06:30.061
    Dec  9 07:06:30.065: INFO: observed resourceQuota "e2e-rq-status-gb4b5" in namespace "resourcequota-3510" with hard status: v1.ResourceList(nil)
    Dec  9 07:06:30.067: INFO: Found resourceQuota "e2e-rq-status-gb4b5" in namespace "resourcequota-3510" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
    Dec  9 07:06:30.067: INFO: ResourceQuota "e2e-rq-status-gb4b5" /status was updated
    STEP: Patching hard spec values for cpu & memory 12/09/22 07:06:30.07
    Dec  9 07:06:30.079: INFO: Resource quota "e2e-rq-status-gb4b5" reports spec: hard cpu limit of 1
    Dec  9 07:06:30.079: INFO: Resource quota "e2e-rq-status-gb4b5" reports spec: hard memory limit of 1Gi
    STEP: Patching "e2e-rq-status-gb4b5" /status 12/09/22 07:06:30.079
    STEP: Confirm /status for "e2e-rq-status-gb4b5" resourceQuota via watch 12/09/22 07:06:30.087
    Dec  9 07:06:30.090: INFO: observed resourceQuota "e2e-rq-status-gb4b5" in namespace "resourcequota-3510" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
    Dec  9 07:06:30.090: INFO: Found resourceQuota "e2e-rq-status-gb4b5" in namespace "resourcequota-3510" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:1, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:1073741824, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1Gi", Format:"BinarySI"}}
    Dec  9 07:06:30.090: INFO: ResourceQuota "e2e-rq-status-gb4b5" /status was patched
    STEP: Get "e2e-rq-status-gb4b5" /status 12/09/22 07:06:30.09
    Dec  9 07:06:30.099: INFO: Resourcequota "e2e-rq-status-gb4b5" reports status: hard cpu of 1
    Dec  9 07:06:30.099: INFO: Resourcequota "e2e-rq-status-gb4b5" reports status: hard memory of 1Gi
    STEP: Repatching "e2e-rq-status-gb4b5" /status before checking Spec is unchanged 12/09/22 07:06:30.103
    Dec  9 07:06:30.108: INFO: Resourcequota "e2e-rq-status-gb4b5" reports status: hard cpu of 2
    Dec  9 07:06:30.108: INFO: Resourcequota "e2e-rq-status-gb4b5" reports status: hard memory of 2Gi
    Dec  9 07:06:30.110: INFO: Found resourceQuota "e2e-rq-status-gb4b5" in namespace "resourcequota-3510" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:2, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:2147483648, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2Gi", Format:"BinarySI"}}
    Dec  9 07:10:50.120: INFO: ResourceQuota "e2e-rq-status-gb4b5" Spec was unchanged and /status reset
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:10:50.120: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-3510" for this suite. 12/09/22 07:10:50.125
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:299
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:10:50.139
Dec  9 07:10:50.139: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename namespaces 12/09/22 07:10:50.141
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:10:50.161
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:10:50.168
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:299
STEP: Read namespace status 12/09/22 07:10:50.173
Dec  9 07:10:50.178: INFO: Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
STEP: Patch namespace status 12/09/22 07:10:50.178
Dec  9 07:10:50.184: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
STEP: Update namespace status 12/09/22 07:10:50.184
Dec  9 07:10:50.193: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Dec  9 07:10:50.193: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-2142" for this suite. 12/09/22 07:10:50.198
------------------------------
â€¢ [0.066 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:299

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:10:50.139
    Dec  9 07:10:50.139: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename namespaces 12/09/22 07:10:50.141
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:10:50.161
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:10:50.168
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply changes to a namespace status [Conformance]
      test/e2e/apimachinery/namespace.go:299
    STEP: Read namespace status 12/09/22 07:10:50.173
    Dec  9 07:10:50.178: INFO: Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
    STEP: Patch namespace status 12/09/22 07:10:50.178
    Dec  9 07:10:50.184: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
    STEP: Update namespace status 12/09/22 07:10:50.184
    Dec  9 07:10:50.193: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:10:50.193: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-2142" for this suite. 12/09/22 07:10:50.198
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] PodTemplates
  should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176
[BeforeEach] [sig-node] PodTemplates
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:10:50.207
Dec  9 07:10:50.207: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename podtemplate 12/09/22 07:10:50.208
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:10:50.231
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:10:50.237
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:31
[It] should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176
STEP: Create a pod template 12/09/22 07:10:50.241
STEP: Replace a pod template 12/09/22 07:10:50.249
Dec  9 07:10:50.260: INFO: Found updated podtemplate annotation: "true"

[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/node/init/init.go:32
Dec  9 07:10:50.260: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] PodTemplates
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] PodTemplates
  tear down framework | framework.go:193
STEP: Destroying namespace "podtemplate-5149" for this suite. 12/09/22 07:10:50.264
------------------------------
â€¢ [0.063 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:10:50.207
    Dec  9 07:10:50.207: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename podtemplate 12/09/22 07:10:50.208
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:10:50.231
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:10:50.237
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:31
    [It] should replace a pod template [Conformance]
      test/e2e/common/node/podtemplates.go:176
    STEP: Create a pod template 12/09/22 07:10:50.241
    STEP: Replace a pod template 12/09/22 07:10:50.249
    Dec  9 07:10:50.260: INFO: Found updated podtemplate annotation: "true"

    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:10:50.260: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] PodTemplates
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] PodTemplates
      tear down framework | framework.go:193
    STEP: Destroying namespace "podtemplate-5149" for this suite. 12/09/22 07:10:50.264
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:374
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:10:50.276
Dec  9 07:10:50.276: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename daemonsets 12/09/22 07:10:50.277
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:10:50.304
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:10:50.311
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:374
Dec  9 07:10:50.360: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster. 12/09/22 07:10:50.385
Dec  9 07:10:50.396: INFO: DaemonSet pods can't tolerate node ip-10-0-7-84 with taints [{Key:node-role.kubernetes.io/controller Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 07:10:50.400: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Dec  9 07:10:50.400: INFO: Node ip-10-0-10-179 is running 0 daemon pod, expected 1
Dec  9 07:10:51.405: INFO: DaemonSet pods can't tolerate node ip-10-0-7-84 with taints [{Key:node-role.kubernetes.io/controller Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 07:10:51.408: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Dec  9 07:10:51.408: INFO: Node ip-10-0-10-179 is running 0 daemon pod, expected 1
Dec  9 07:10:52.404: INFO: DaemonSet pods can't tolerate node ip-10-0-7-84 with taints [{Key:node-role.kubernetes.io/controller Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 07:10:52.409: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Dec  9 07:10:52.409: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
STEP: Update daemon pods image. 12/09/22 07:10:52.433
STEP: Check that daemon pods images are updated. 12/09/22 07:10:52.446
Dec  9 07:10:52.450: INFO: Wrong image for pod: daemon-set-7b7h2. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Dec  9 07:10:52.451: INFO: Wrong image for pod: daemon-set-zfmmm. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Dec  9 07:10:52.455: INFO: DaemonSet pods can't tolerate node ip-10-0-7-84 with taints [{Key:node-role.kubernetes.io/controller Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 07:10:53.460: INFO: Wrong image for pod: daemon-set-7b7h2. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Dec  9 07:10:53.464: INFO: DaemonSet pods can't tolerate node ip-10-0-7-84 with taints [{Key:node-role.kubernetes.io/controller Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 07:10:54.459: INFO: Wrong image for pod: daemon-set-7b7h2. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Dec  9 07:10:54.462: INFO: DaemonSet pods can't tolerate node ip-10-0-7-84 with taints [{Key:node-role.kubernetes.io/controller Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 07:10:55.463: INFO: Pod daemon-set-4flk4 is not available
Dec  9 07:10:55.464: INFO: Wrong image for pod: daemon-set-7b7h2. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Dec  9 07:10:55.469: INFO: DaemonSet pods can't tolerate node ip-10-0-7-84 with taints [{Key:node-role.kubernetes.io/controller Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 07:10:56.462: INFO: DaemonSet pods can't tolerate node ip-10-0-7-84 with taints [{Key:node-role.kubernetes.io/controller Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 07:10:57.468: INFO: Pod daemon-set-6bj47 is not available
Dec  9 07:10:57.473: INFO: DaemonSet pods can't tolerate node ip-10-0-7-84 with taints [{Key:node-role.kubernetes.io/controller Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
STEP: Check that daemon pods are still running on every node of the cluster. 12/09/22 07:10:57.473
Dec  9 07:10:57.476: INFO: DaemonSet pods can't tolerate node ip-10-0-7-84 with taints [{Key:node-role.kubernetes.io/controller Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 07:10:57.480: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Dec  9 07:10:57.480: INFO: Node ip-10-0-10-179 is running 0 daemon pod, expected 1
Dec  9 07:10:58.483: INFO: DaemonSet pods can't tolerate node ip-10-0-7-84 with taints [{Key:node-role.kubernetes.io/controller Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 07:10:58.494: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Dec  9 07:10:58.494: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 12/09/22 07:10:58.547
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5300, will wait for the garbage collector to delete the pods 12/09/22 07:10:58.548
Dec  9 07:10:58.618: INFO: Deleting DaemonSet.extensions daemon-set took: 7.750667ms
Dec  9 07:10:58.719: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.83665ms
Dec  9 07:11:00.623: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Dec  9 07:11:00.624: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Dec  9 07:11:00.640: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"7204"},"items":null}

Dec  9 07:11:00.643: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"7204"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Dec  9 07:11:00.654: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-5300" for this suite. 12/09/22 07:11:00.657
------------------------------
â€¢ [SLOW TEST] [10.389 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:374

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:10:50.276
    Dec  9 07:10:50.276: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename daemonsets 12/09/22 07:10:50.277
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:10:50.304
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:10:50.311
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
      test/e2e/apps/daemon_set.go:374
    Dec  9 07:10:50.360: INFO: Creating simple daemon set daemon-set
    STEP: Check that daemon pods launch on every node of the cluster. 12/09/22 07:10:50.385
    Dec  9 07:10:50.396: INFO: DaemonSet pods can't tolerate node ip-10-0-7-84 with taints [{Key:node-role.kubernetes.io/controller Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Dec  9 07:10:50.400: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Dec  9 07:10:50.400: INFO: Node ip-10-0-10-179 is running 0 daemon pod, expected 1
    Dec  9 07:10:51.405: INFO: DaemonSet pods can't tolerate node ip-10-0-7-84 with taints [{Key:node-role.kubernetes.io/controller Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Dec  9 07:10:51.408: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Dec  9 07:10:51.408: INFO: Node ip-10-0-10-179 is running 0 daemon pod, expected 1
    Dec  9 07:10:52.404: INFO: DaemonSet pods can't tolerate node ip-10-0-7-84 with taints [{Key:node-role.kubernetes.io/controller Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Dec  9 07:10:52.409: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Dec  9 07:10:52.409: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    STEP: Update daemon pods image. 12/09/22 07:10:52.433
    STEP: Check that daemon pods images are updated. 12/09/22 07:10:52.446
    Dec  9 07:10:52.450: INFO: Wrong image for pod: daemon-set-7b7h2. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Dec  9 07:10:52.451: INFO: Wrong image for pod: daemon-set-zfmmm. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Dec  9 07:10:52.455: INFO: DaemonSet pods can't tolerate node ip-10-0-7-84 with taints [{Key:node-role.kubernetes.io/controller Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Dec  9 07:10:53.460: INFO: Wrong image for pod: daemon-set-7b7h2. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Dec  9 07:10:53.464: INFO: DaemonSet pods can't tolerate node ip-10-0-7-84 with taints [{Key:node-role.kubernetes.io/controller Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Dec  9 07:10:54.459: INFO: Wrong image for pod: daemon-set-7b7h2. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Dec  9 07:10:54.462: INFO: DaemonSet pods can't tolerate node ip-10-0-7-84 with taints [{Key:node-role.kubernetes.io/controller Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Dec  9 07:10:55.463: INFO: Pod daemon-set-4flk4 is not available
    Dec  9 07:10:55.464: INFO: Wrong image for pod: daemon-set-7b7h2. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Dec  9 07:10:55.469: INFO: DaemonSet pods can't tolerate node ip-10-0-7-84 with taints [{Key:node-role.kubernetes.io/controller Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Dec  9 07:10:56.462: INFO: DaemonSet pods can't tolerate node ip-10-0-7-84 with taints [{Key:node-role.kubernetes.io/controller Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Dec  9 07:10:57.468: INFO: Pod daemon-set-6bj47 is not available
    Dec  9 07:10:57.473: INFO: DaemonSet pods can't tolerate node ip-10-0-7-84 with taints [{Key:node-role.kubernetes.io/controller Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    STEP: Check that daemon pods are still running on every node of the cluster. 12/09/22 07:10:57.473
    Dec  9 07:10:57.476: INFO: DaemonSet pods can't tolerate node ip-10-0-7-84 with taints [{Key:node-role.kubernetes.io/controller Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Dec  9 07:10:57.480: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Dec  9 07:10:57.480: INFO: Node ip-10-0-10-179 is running 0 daemon pod, expected 1
    Dec  9 07:10:58.483: INFO: DaemonSet pods can't tolerate node ip-10-0-7-84 with taints [{Key:node-role.kubernetes.io/controller Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Dec  9 07:10:58.494: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Dec  9 07:10:58.494: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 12/09/22 07:10:58.547
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5300, will wait for the garbage collector to delete the pods 12/09/22 07:10:58.548
    Dec  9 07:10:58.618: INFO: Deleting DaemonSet.extensions daemon-set took: 7.750667ms
    Dec  9 07:10:58.719: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.83665ms
    Dec  9 07:11:00.623: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Dec  9 07:11:00.624: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Dec  9 07:11:00.640: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"7204"},"items":null}

    Dec  9 07:11:00.643: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"7204"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:11:00.654: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-5300" for this suite. 12/09/22 07:11:00.657
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:277
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:11:00.668
Dec  9 07:11:00.669: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename webhook 12/09/22 07:11:00.671
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:11:00.691
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:11:00.695
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 12/09/22 07:11:00.721
STEP: Create role binding to let webhook read extension-apiserver-authentication 12/09/22 07:11:01.333
STEP: Deploying the webhook pod 12/09/22 07:11:01.34
STEP: Wait for the deployment to be ready 12/09/22 07:11:01.351
Dec  9 07:11:01.362: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 12/09/22 07:11:03.373
STEP: Verifying the service has paired with the endpoint 12/09/22 07:11:03.388
Dec  9 07:11:04.388: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:277
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 12/09/22 07:11:04.392
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 12/09/22 07:11:04.411
STEP: Creating a dummy validating-webhook-configuration object 12/09/22 07:11:04.428
STEP: Deleting the validating-webhook-configuration, which should be possible to remove 12/09/22 07:11:04.439
STEP: Creating a dummy mutating-webhook-configuration object 12/09/22 07:11:04.445
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove 12/09/22 07:11:04.454
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Dec  9 07:11:04.470: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-3507" for this suite. 12/09/22 07:11:04.535
STEP: Destroying namespace "webhook-3507-markers" for this suite. 12/09/22 07:11:04.545
------------------------------
â€¢ [3.916 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:277

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:11:00.668
    Dec  9 07:11:00.669: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename webhook 12/09/22 07:11:00.671
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:11:00.691
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:11:00.695
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 12/09/22 07:11:00.721
    STEP: Create role binding to let webhook read extension-apiserver-authentication 12/09/22 07:11:01.333
    STEP: Deploying the webhook pod 12/09/22 07:11:01.34
    STEP: Wait for the deployment to be ready 12/09/22 07:11:01.351
    Dec  9 07:11:01.362: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 12/09/22 07:11:03.373
    STEP: Verifying the service has paired with the endpoint 12/09/22 07:11:03.388
    Dec  9 07:11:04.388: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
      test/e2e/apimachinery/webhook.go:277
    STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 12/09/22 07:11:04.392
    STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 12/09/22 07:11:04.411
    STEP: Creating a dummy validating-webhook-configuration object 12/09/22 07:11:04.428
    STEP: Deleting the validating-webhook-configuration, which should be possible to remove 12/09/22 07:11:04.439
    STEP: Creating a dummy mutating-webhook-configuration object 12/09/22 07:11:04.445
    STEP: Deleting the mutating-webhook-configuration, which should be possible to remove 12/09/22 07:11:04.454
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:11:04.470: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-3507" for this suite. 12/09/22 07:11:04.535
    STEP: Destroying namespace "webhook-3507-markers" for this suite. 12/09/22 07:11:04.545
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:186
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:11:04.586
Dec  9 07:11:04.586: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename var-expansion 12/09/22 07:11:04.587
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:11:04.617
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:11:04.622
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:186
Dec  9 07:11:04.645: INFO: Waiting up to 2m0s for pod "var-expansion-94119f62-0ba2-4305-b594-5e7a219a95f5" in namespace "var-expansion-5835" to be "container 0 failed with reason CreateContainerConfigError"
Dec  9 07:11:04.653: INFO: Pod "var-expansion-94119f62-0ba2-4305-b594-5e7a219a95f5": Phase="Pending", Reason="", readiness=false. Elapsed: 7.654116ms
Dec  9 07:11:06.657: INFO: Pod "var-expansion-94119f62-0ba2-4305-b594-5e7a219a95f5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011547149s
Dec  9 07:11:06.657: INFO: Pod "var-expansion-94119f62-0ba2-4305-b594-5e7a219a95f5" satisfied condition "container 0 failed with reason CreateContainerConfigError"
Dec  9 07:11:06.657: INFO: Deleting pod "var-expansion-94119f62-0ba2-4305-b594-5e7a219a95f5" in namespace "var-expansion-5835"
Dec  9 07:11:06.663: INFO: Wait up to 5m0s for pod "var-expansion-94119f62-0ba2-4305-b594-5e7a219a95f5" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Dec  9 07:11:08.670: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-5835" for this suite. 12/09/22 07:11:08.674
------------------------------
â€¢ [4.097 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:186

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:11:04.586
    Dec  9 07:11:04.586: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename var-expansion 12/09/22 07:11:04.587
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:11:04.617
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:11:04.622
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
      test/e2e/common/node/expansion.go:186
    Dec  9 07:11:04.645: INFO: Waiting up to 2m0s for pod "var-expansion-94119f62-0ba2-4305-b594-5e7a219a95f5" in namespace "var-expansion-5835" to be "container 0 failed with reason CreateContainerConfigError"
    Dec  9 07:11:04.653: INFO: Pod "var-expansion-94119f62-0ba2-4305-b594-5e7a219a95f5": Phase="Pending", Reason="", readiness=false. Elapsed: 7.654116ms
    Dec  9 07:11:06.657: INFO: Pod "var-expansion-94119f62-0ba2-4305-b594-5e7a219a95f5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011547149s
    Dec  9 07:11:06.657: INFO: Pod "var-expansion-94119f62-0ba2-4305-b594-5e7a219a95f5" satisfied condition "container 0 failed with reason CreateContainerConfigError"
    Dec  9 07:11:06.657: INFO: Deleting pod "var-expansion-94119f62-0ba2-4305-b594-5e7a219a95f5" in namespace "var-expansion-5835"
    Dec  9 07:11:06.663: INFO: Wait up to 5m0s for pod "var-expansion-94119f62-0ba2-4305-b594-5e7a219a95f5" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:11:08.670: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-5835" for this suite. 12/09/22 07:11:08.674
  << End Captured GinkgoWriter Output
------------------------------
[sig-auth] ServiceAccounts
  should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:78
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:11:08.685
Dec  9 07:11:08.685: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename svcaccounts 12/09/22 07:11:08.686
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:11:08.703
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:11:08.709
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:78
Dec  9 07:11:08.732: INFO: Waiting up to 5m0s for pod "pod-service-account-b74944ce-0fa2-4886-b8af-72b174348772" in namespace "svcaccounts-4200" to be "running"
Dec  9 07:11:08.737: INFO: Pod "pod-service-account-b74944ce-0fa2-4886-b8af-72b174348772": Phase="Pending", Reason="", readiness=false. Elapsed: 4.851396ms
Dec  9 07:11:10.740: INFO: Pod "pod-service-account-b74944ce-0fa2-4886-b8af-72b174348772": Phase="Running", Reason="", readiness=true. Elapsed: 2.008270686s
Dec  9 07:11:10.740: INFO: Pod "pod-service-account-b74944ce-0fa2-4886-b8af-72b174348772" satisfied condition "running"
STEP: reading a file in the container 12/09/22 07:11:10.74
Dec  9 07:11:10.748: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-4200 pod-service-account-b74944ce-0fa2-4886-b8af-72b174348772 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container 12/09/22 07:11:11.169
Dec  9 07:11:11.169: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-4200 pod-service-account-b74944ce-0fa2-4886-b8af-72b174348772 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container 12/09/22 07:11:11.398
Dec  9 07:11:11.398: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-4200 pod-service-account-b74944ce-0fa2-4886-b8af-72b174348772 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
Dec  9 07:11:11.545: INFO: Got root ca configmap in namespace "svcaccounts-4200"
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Dec  9 07:11:11.554: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-4200" for this suite. 12/09/22 07:11:11.577
------------------------------
â€¢ [2.914 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:78

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:11:08.685
    Dec  9 07:11:08.685: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename svcaccounts 12/09/22 07:11:08.686
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:11:08.703
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:11:08.709
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should mount an API token into pods  [Conformance]
      test/e2e/auth/service_accounts.go:78
    Dec  9 07:11:08.732: INFO: Waiting up to 5m0s for pod "pod-service-account-b74944ce-0fa2-4886-b8af-72b174348772" in namespace "svcaccounts-4200" to be "running"
    Dec  9 07:11:08.737: INFO: Pod "pod-service-account-b74944ce-0fa2-4886-b8af-72b174348772": Phase="Pending", Reason="", readiness=false. Elapsed: 4.851396ms
    Dec  9 07:11:10.740: INFO: Pod "pod-service-account-b74944ce-0fa2-4886-b8af-72b174348772": Phase="Running", Reason="", readiness=true. Elapsed: 2.008270686s
    Dec  9 07:11:10.740: INFO: Pod "pod-service-account-b74944ce-0fa2-4886-b8af-72b174348772" satisfied condition "running"
    STEP: reading a file in the container 12/09/22 07:11:10.74
    Dec  9 07:11:10.748: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-4200 pod-service-account-b74944ce-0fa2-4886-b8af-72b174348772 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
    STEP: reading a file in the container 12/09/22 07:11:11.169
    Dec  9 07:11:11.169: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-4200 pod-service-account-b74944ce-0fa2-4886-b8af-72b174348772 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
    STEP: reading a file in the container 12/09/22 07:11:11.398
    Dec  9 07:11:11.398: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-4200 pod-service-account-b74944ce-0fa2-4886-b8af-72b174348772 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
    Dec  9 07:11:11.545: INFO: Got root ca configmap in namespace "svcaccounts-4200"
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:11:11.554: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-4200" for this suite. 12/09/22 07:11:11.577
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Pods
  should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:444
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:11:11.598
Dec  9 07:11:11.599: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename pods 12/09/22 07:11:11.6
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:11:11.626
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:11:11.63
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:444
Dec  9 07:11:11.654: INFO: Waiting up to 5m0s for pod "server-envvars-3de91539-f7f8-4955-91b4-3e01bca00be2" in namespace "pods-2018" to be "running and ready"
Dec  9 07:11:11.660: INFO: Pod "server-envvars-3de91539-f7f8-4955-91b4-3e01bca00be2": Phase="Pending", Reason="", readiness=false. Elapsed: 5.60771ms
Dec  9 07:11:11.660: INFO: The phase of Pod server-envvars-3de91539-f7f8-4955-91b4-3e01bca00be2 is Pending, waiting for it to be Running (with Ready = true)
Dec  9 07:11:13.664: INFO: Pod "server-envvars-3de91539-f7f8-4955-91b4-3e01bca00be2": Phase="Running", Reason="", readiness=true. Elapsed: 2.009282257s
Dec  9 07:11:13.664: INFO: The phase of Pod server-envvars-3de91539-f7f8-4955-91b4-3e01bca00be2 is Running (Ready = true)
Dec  9 07:11:13.664: INFO: Pod "server-envvars-3de91539-f7f8-4955-91b4-3e01bca00be2" satisfied condition "running and ready"
Dec  9 07:11:13.689: INFO: Waiting up to 5m0s for pod "client-envvars-0d632b40-500d-422f-bcbb-b4db5e3a2984" in namespace "pods-2018" to be "Succeeded or Failed"
Dec  9 07:11:13.704: INFO: Pod "client-envvars-0d632b40-500d-422f-bcbb-b4db5e3a2984": Phase="Pending", Reason="", readiness=false. Elapsed: 14.91975ms
Dec  9 07:11:15.707: INFO: Pod "client-envvars-0d632b40-500d-422f-bcbb-b4db5e3a2984": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018501037s
Dec  9 07:11:17.708: INFO: Pod "client-envvars-0d632b40-500d-422f-bcbb-b4db5e3a2984": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01968256s
STEP: Saw pod success 12/09/22 07:11:17.708
Dec  9 07:11:17.709: INFO: Pod "client-envvars-0d632b40-500d-422f-bcbb-b4db5e3a2984" satisfied condition "Succeeded or Failed"
Dec  9 07:11:17.711: INFO: Trying to get logs from node ip-10-0-17-108 pod client-envvars-0d632b40-500d-422f-bcbb-b4db5e3a2984 container env3cont: <nil>
STEP: delete the pod 12/09/22 07:11:17.726
Dec  9 07:11:17.740: INFO: Waiting for pod client-envvars-0d632b40-500d-422f-bcbb-b4db5e3a2984 to disappear
Dec  9 07:11:17.743: INFO: Pod client-envvars-0d632b40-500d-422f-bcbb-b4db5e3a2984 no longer exists
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Dec  9 07:11:17.743: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-2018" for this suite. 12/09/22 07:11:17.747
------------------------------
â€¢ [SLOW TEST] [6.155 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:444

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:11:11.598
    Dec  9 07:11:11.599: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename pods 12/09/22 07:11:11.6
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:11:11.626
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:11:11.63
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should contain environment variables for services [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:444
    Dec  9 07:11:11.654: INFO: Waiting up to 5m0s for pod "server-envvars-3de91539-f7f8-4955-91b4-3e01bca00be2" in namespace "pods-2018" to be "running and ready"
    Dec  9 07:11:11.660: INFO: Pod "server-envvars-3de91539-f7f8-4955-91b4-3e01bca00be2": Phase="Pending", Reason="", readiness=false. Elapsed: 5.60771ms
    Dec  9 07:11:11.660: INFO: The phase of Pod server-envvars-3de91539-f7f8-4955-91b4-3e01bca00be2 is Pending, waiting for it to be Running (with Ready = true)
    Dec  9 07:11:13.664: INFO: Pod "server-envvars-3de91539-f7f8-4955-91b4-3e01bca00be2": Phase="Running", Reason="", readiness=true. Elapsed: 2.009282257s
    Dec  9 07:11:13.664: INFO: The phase of Pod server-envvars-3de91539-f7f8-4955-91b4-3e01bca00be2 is Running (Ready = true)
    Dec  9 07:11:13.664: INFO: Pod "server-envvars-3de91539-f7f8-4955-91b4-3e01bca00be2" satisfied condition "running and ready"
    Dec  9 07:11:13.689: INFO: Waiting up to 5m0s for pod "client-envvars-0d632b40-500d-422f-bcbb-b4db5e3a2984" in namespace "pods-2018" to be "Succeeded or Failed"
    Dec  9 07:11:13.704: INFO: Pod "client-envvars-0d632b40-500d-422f-bcbb-b4db5e3a2984": Phase="Pending", Reason="", readiness=false. Elapsed: 14.91975ms
    Dec  9 07:11:15.707: INFO: Pod "client-envvars-0d632b40-500d-422f-bcbb-b4db5e3a2984": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018501037s
    Dec  9 07:11:17.708: INFO: Pod "client-envvars-0d632b40-500d-422f-bcbb-b4db5e3a2984": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01968256s
    STEP: Saw pod success 12/09/22 07:11:17.708
    Dec  9 07:11:17.709: INFO: Pod "client-envvars-0d632b40-500d-422f-bcbb-b4db5e3a2984" satisfied condition "Succeeded or Failed"
    Dec  9 07:11:17.711: INFO: Trying to get logs from node ip-10-0-17-108 pod client-envvars-0d632b40-500d-422f-bcbb-b4db5e3a2984 container env3cont: <nil>
    STEP: delete the pod 12/09/22 07:11:17.726
    Dec  9 07:11:17.740: INFO: Waiting for pod client-envvars-0d632b40-500d-422f-bcbb-b4db5e3a2984 to disappear
    Dec  9 07:11:17.743: INFO: Pod client-envvars-0d632b40-500d-422f-bcbb-b4db5e3a2984 no longer exists
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:11:17.743: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-2018" for this suite. 12/09/22 07:11:17.747
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:232
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:11:17.754
Dec  9 07:11:17.754: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename container-runtime 12/09/22 07:11:17.755
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:11:17.775
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:11:17.778
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:232
STEP: create the container 12/09/22 07:11:17.783
STEP: wait for the container to reach Succeeded 12/09/22 07:11:17.791
STEP: get the container status 12/09/22 07:11:21.825
STEP: the container should be terminated 12/09/22 07:11:21.828
STEP: the termination message should be set 12/09/22 07:11:21.829
Dec  9 07:11:21.829: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container 12/09/22 07:11:21.83
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Dec  9 07:11:21.860: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-2438" for this suite. 12/09/22 07:11:21.864
------------------------------
â€¢ [4.117 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    on terminated container
    test/e2e/common/node/runtime.go:137
      should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:232

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:11:17.754
    Dec  9 07:11:17.754: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename container-runtime 12/09/22 07:11:17.755
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:11:17.775
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:11:17.778
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:232
    STEP: create the container 12/09/22 07:11:17.783
    STEP: wait for the container to reach Succeeded 12/09/22 07:11:17.791
    STEP: get the container status 12/09/22 07:11:21.825
    STEP: the container should be terminated 12/09/22 07:11:21.828
    STEP: the termination message should be set 12/09/22 07:11:21.829
    Dec  9 07:11:21.829: INFO: Expected: &{} to match Container's Termination Message:  --
    STEP: delete the container 12/09/22 07:11:21.83
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:11:21.860: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-2438" for this suite. 12/09/22 07:11:21.864
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:164
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:11:21.875
Dec  9 07:11:21.875: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename disruption 12/09/22 07:11:21.876
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:11:21.924
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:11:21.933
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[It] should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:164
STEP: Waiting for the pdb to be processed 12/09/22 07:11:21.953
STEP: Updating PodDisruptionBudget status 12/09/22 07:11:23.96
STEP: Waiting for all pods to be running 12/09/22 07:11:23.966
Dec  9 07:11:23.973: INFO: running pods: 0 < 1
STEP: locating a running pod 12/09/22 07:11:25.978
STEP: Waiting for the pdb to be processed 12/09/22 07:11:25.996
STEP: Patching PodDisruptionBudget status 12/09/22 07:11:26.02
STEP: Waiting for the pdb to be processed 12/09/22 07:11:26.034
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Dec  9 07:11:26.046: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-6163" for this suite. 12/09/22 07:11:26.053
------------------------------
â€¢ [4.185 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:164

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:11:21.875
    Dec  9 07:11:21.875: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename disruption 12/09/22 07:11:21.876
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:11:21.924
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:11:21.933
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [It] should update/patch PodDisruptionBudget status [Conformance]
      test/e2e/apps/disruption.go:164
    STEP: Waiting for the pdb to be processed 12/09/22 07:11:21.953
    STEP: Updating PodDisruptionBudget status 12/09/22 07:11:23.96
    STEP: Waiting for all pods to be running 12/09/22 07:11:23.966
    Dec  9 07:11:23.973: INFO: running pods: 0 < 1
    STEP: locating a running pod 12/09/22 07:11:25.978
    STEP: Waiting for the pdb to be processed 12/09/22 07:11:25.996
    STEP: Patching PodDisruptionBudget status 12/09/22 07:11:26.02
    STEP: Waiting for the pdb to be processed 12/09/22 07:11:26.034
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:11:26.046: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-6163" for this suite. 12/09/22 07:11:26.053
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:75
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:11:26.063
Dec  9 07:11:26.063: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename resourcequota 12/09/22 07:11:26.064
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:11:26.132
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:11:26.139
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:75
STEP: Counting existing ResourceQuota 12/09/22 07:11:26.143
STEP: Creating a ResourceQuota 12/09/22 07:11:31.148
STEP: Ensuring resource quota status is calculated 12/09/22 07:11:31.154
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Dec  9 07:11:33.157: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-6014" for this suite. 12/09/22 07:11:33.161
------------------------------
â€¢ [SLOW TEST] [7.104 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:75

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:11:26.063
    Dec  9 07:11:26.063: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename resourcequota 12/09/22 07:11:26.064
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:11:26.132
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:11:26.139
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
      test/e2e/apimachinery/resource_quota.go:75
    STEP: Counting existing ResourceQuota 12/09/22 07:11:26.143
    STEP: Creating a ResourceQuota 12/09/22 07:11:31.148
    STEP: Ensuring resource quota status is calculated 12/09/22 07:11:31.154
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:11:33.157: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-6014" for this suite. 12/09/22 07:11:33.161
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets
  should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:154
[BeforeEach] [sig-node] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:11:33.169
Dec  9 07:11:33.171: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename secrets 12/09/22 07:11:33.174
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:11:33.19
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:11:33.196
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:154
STEP: creating a secret 12/09/22 07:11:33.2
STEP: listing secrets in all namespaces to ensure that there are more than zero 12/09/22 07:11:33.205
STEP: patching the secret 12/09/22 07:11:33.208
STEP: deleting the secret using a LabelSelector 12/09/22 07:11:33.217
STEP: listing secrets in all namespaces, searching for label name and value in patch 12/09/22 07:11:33.223
[AfterEach] [sig-node] Secrets
  test/e2e/framework/node/init/init.go:32
Dec  9 07:11:33.225: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-5859" for this suite. 12/09/22 07:11:33.229
------------------------------
â€¢ [0.066 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:11:33.169
    Dec  9 07:11:33.171: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename secrets 12/09/22 07:11:33.174
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:11:33.19
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:11:33.196
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should patch a secret [Conformance]
      test/e2e/common/node/secrets.go:154
    STEP: creating a secret 12/09/22 07:11:33.2
    STEP: listing secrets in all namespaces to ensure that there are more than zero 12/09/22 07:11:33.205
    STEP: patching the secret 12/09/22 07:11:33.208
    STEP: deleting the secret using a LabelSelector 12/09/22 07:11:33.217
    STEP: listing secrets in all namespaces, searching for label name and value in patch 12/09/22 07:11:33.223
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:11:33.225: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-5859" for this suite. 12/09/22 07:11:33.229
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin]
  should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:11:33.236
Dec  9 07:11:33.236: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename certificates 12/09/22 07:11:33.237
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:11:33.253
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:11:33.259
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200
STEP: getting /apis 12/09/22 07:11:34.305
STEP: getting /apis/certificates.k8s.io 12/09/22 07:11:34.309
STEP: getting /apis/certificates.k8s.io/v1 12/09/22 07:11:34.31
STEP: creating 12/09/22 07:11:34.312
STEP: getting 12/09/22 07:11:34.328
STEP: listing 12/09/22 07:11:34.331
STEP: watching 12/09/22 07:11:34.334
Dec  9 07:11:34.334: INFO: starting watch
STEP: patching 12/09/22 07:11:34.336
STEP: updating 12/09/22 07:11:34.342
Dec  9 07:11:34.347: INFO: waiting for watch events with expected annotations
Dec  9 07:11:34.347: INFO: saw patched and updated annotations
STEP: getting /approval 12/09/22 07:11:34.347
STEP: patching /approval 12/09/22 07:11:34.35
STEP: updating /approval 12/09/22 07:11:34.357
STEP: getting /status 12/09/22 07:11:34.362
STEP: patching /status 12/09/22 07:11:34.365
STEP: updating /status 12/09/22 07:11:34.371
STEP: deleting 12/09/22 07:11:34.377
STEP: deleting a collection 12/09/22 07:11:34.387
[AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Dec  9 07:11:34.398: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "certificates-3247" for this suite. 12/09/22 07:11:34.401
------------------------------
â€¢ [1.170 seconds]
[sig-auth] Certificates API [Privileged:ClusterAdmin]
test/e2e/auth/framework.go:23
  should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:11:33.236
    Dec  9 07:11:33.236: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename certificates 12/09/22 07:11:33.237
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:11:33.253
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:11:33.259
    [BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] should support CSR API operations [Conformance]
      test/e2e/auth/certificates.go:200
    STEP: getting /apis 12/09/22 07:11:34.305
    STEP: getting /apis/certificates.k8s.io 12/09/22 07:11:34.309
    STEP: getting /apis/certificates.k8s.io/v1 12/09/22 07:11:34.31
    STEP: creating 12/09/22 07:11:34.312
    STEP: getting 12/09/22 07:11:34.328
    STEP: listing 12/09/22 07:11:34.331
    STEP: watching 12/09/22 07:11:34.334
    Dec  9 07:11:34.334: INFO: starting watch
    STEP: patching 12/09/22 07:11:34.336
    STEP: updating 12/09/22 07:11:34.342
    Dec  9 07:11:34.347: INFO: waiting for watch events with expected annotations
    Dec  9 07:11:34.347: INFO: saw patched and updated annotations
    STEP: getting /approval 12/09/22 07:11:34.347
    STEP: patching /approval 12/09/22 07:11:34.35
    STEP: updating /approval 12/09/22 07:11:34.357
    STEP: getting /status 12/09/22 07:11:34.362
    STEP: patching /status 12/09/22 07:11:34.365
    STEP: updating /status 12/09/22 07:11:34.371
    STEP: deleting 12/09/22 07:11:34.377
    STEP: deleting a collection 12/09/22 07:11:34.387
    [AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:11:34.398: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "certificates-3247" for this suite. 12/09/22 07:11:34.401
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:45
[BeforeEach] [sig-node] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:11:34.408
Dec  9 07:11:34.408: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename configmap 12/09/22 07:11:34.409
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:11:34.421
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:11:34.425
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:45
STEP: Creating configMap configmap-1307/configmap-test-6366be3e-749d-4bab-91d2-fa2185860166 12/09/22 07:11:34.428
STEP: Creating a pod to test consume configMaps 12/09/22 07:11:34.432
Dec  9 07:11:34.438: INFO: Waiting up to 5m0s for pod "pod-configmaps-d4185ad5-8f45-4a0e-8d3f-12f9e546bc37" in namespace "configmap-1307" to be "Succeeded or Failed"
Dec  9 07:11:34.446: INFO: Pod "pod-configmaps-d4185ad5-8f45-4a0e-8d3f-12f9e546bc37": Phase="Pending", Reason="", readiness=false. Elapsed: 7.850074ms
Dec  9 07:11:36.449: INFO: Pod "pod-configmaps-d4185ad5-8f45-4a0e-8d3f-12f9e546bc37": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010904951s
Dec  9 07:11:38.452: INFO: Pod "pod-configmaps-d4185ad5-8f45-4a0e-8d3f-12f9e546bc37": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013985716s
STEP: Saw pod success 12/09/22 07:11:38.452
Dec  9 07:11:38.452: INFO: Pod "pod-configmaps-d4185ad5-8f45-4a0e-8d3f-12f9e546bc37" satisfied condition "Succeeded or Failed"
Dec  9 07:11:38.458: INFO: Trying to get logs from node ip-10-0-10-179 pod pod-configmaps-d4185ad5-8f45-4a0e-8d3f-12f9e546bc37 container env-test: <nil>
STEP: delete the pod 12/09/22 07:11:38.477
Dec  9 07:11:38.488: INFO: Waiting for pod pod-configmaps-d4185ad5-8f45-4a0e-8d3f-12f9e546bc37 to disappear
Dec  9 07:11:38.492: INFO: Pod pod-configmaps-d4185ad5-8f45-4a0e-8d3f-12f9e546bc37 no longer exists
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/node/init/init.go:32
Dec  9 07:11:38.492: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-1307" for this suite. 12/09/22 07:11:38.497
------------------------------
â€¢ [4.099 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:45

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:11:34.408
    Dec  9 07:11:34.408: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename configmap 12/09/22 07:11:34.409
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:11:34.421
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:11:34.425
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable via environment variable [NodeConformance] [Conformance]
      test/e2e/common/node/configmap.go:45
    STEP: Creating configMap configmap-1307/configmap-test-6366be3e-749d-4bab-91d2-fa2185860166 12/09/22 07:11:34.428
    STEP: Creating a pod to test consume configMaps 12/09/22 07:11:34.432
    Dec  9 07:11:34.438: INFO: Waiting up to 5m0s for pod "pod-configmaps-d4185ad5-8f45-4a0e-8d3f-12f9e546bc37" in namespace "configmap-1307" to be "Succeeded or Failed"
    Dec  9 07:11:34.446: INFO: Pod "pod-configmaps-d4185ad5-8f45-4a0e-8d3f-12f9e546bc37": Phase="Pending", Reason="", readiness=false. Elapsed: 7.850074ms
    Dec  9 07:11:36.449: INFO: Pod "pod-configmaps-d4185ad5-8f45-4a0e-8d3f-12f9e546bc37": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010904951s
    Dec  9 07:11:38.452: INFO: Pod "pod-configmaps-d4185ad5-8f45-4a0e-8d3f-12f9e546bc37": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013985716s
    STEP: Saw pod success 12/09/22 07:11:38.452
    Dec  9 07:11:38.452: INFO: Pod "pod-configmaps-d4185ad5-8f45-4a0e-8d3f-12f9e546bc37" satisfied condition "Succeeded or Failed"
    Dec  9 07:11:38.458: INFO: Trying to get logs from node ip-10-0-10-179 pod pod-configmaps-d4185ad5-8f45-4a0e-8d3f-12f9e546bc37 container env-test: <nil>
    STEP: delete the pod 12/09/22 07:11:38.477
    Dec  9 07:11:38.488: INFO: Waiting for pod pod-configmaps-d4185ad5-8f45-4a0e-8d3f-12f9e546bc37 to disappear
    Dec  9 07:11:38.492: INFO: Pod pod-configmaps-d4185ad5-8f45-4a0e-8d3f-12f9e546bc37 no longer exists
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:11:38.492: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-1307" for this suite. 12/09/22 07:11:38.497
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should apply an update to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:366
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:11:38.508
Dec  9 07:11:38.509: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename namespaces 12/09/22 07:11:38.51
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:11:38.546
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:11:38.558
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should apply an update to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:366
STEP: Updating Namespace "namespaces-6655" 12/09/22 07:11:38.562
Dec  9 07:11:38.573: INFO: Namespace "namespaces-6655" now has labels, map[string]string{"e2e-framework":"namespaces", "e2e-run":"2287ad07-0247-4b9b-827b-66dd3791c4ed", "kubernetes.io/metadata.name":"namespaces-6655", "namespaces-6655":"updated", "pod-security.kubernetes.io/enforce":"baseline"}
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Dec  9 07:11:38.573: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-6655" for this suite. 12/09/22 07:11:38.58
------------------------------
â€¢ [0.080 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should apply an update to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:366

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:11:38.508
    Dec  9 07:11:38.509: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename namespaces 12/09/22 07:11:38.51
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:11:38.546
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:11:38.558
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply an update to a Namespace [Conformance]
      test/e2e/apimachinery/namespace.go:366
    STEP: Updating Namespace "namespaces-6655" 12/09/22 07:11:38.562
    Dec  9 07:11:38.573: INFO: Namespace "namespaces-6655" now has labels, map[string]string{"e2e-framework":"namespaces", "e2e-run":"2287ad07-0247-4b9b-827b-66dd3791c4ed", "kubernetes.io/metadata.name":"namespaces-6655", "namespaces-6655":"updated", "pod-security.kubernetes.io/enforce":"baseline"}
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:11:38.573: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-6655" for this suite. 12/09/22 07:11:38.58
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] IngressClass API
   should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223
[BeforeEach] [sig-network] IngressClass API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:11:38.59
Dec  9 07:11:38.590: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename ingressclass 12/09/22 07:11:38.591
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:11:38.614
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:11:38.62
[BeforeEach] [sig-network] IngressClass API
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] IngressClass API
  test/e2e/network/ingressclass.go:211
[It]  should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223
STEP: getting /apis 12/09/22 07:11:38.628
STEP: getting /apis/networking.k8s.io 12/09/22 07:11:38.638
STEP: getting /apis/networking.k8s.iov1 12/09/22 07:11:38.641
STEP: creating 12/09/22 07:11:38.642
STEP: getting 12/09/22 07:11:38.684
STEP: listing 12/09/22 07:11:38.687
STEP: watching 12/09/22 07:11:38.695
Dec  9 07:11:38.695: INFO: starting watch
STEP: patching 12/09/22 07:11:38.698
STEP: updating 12/09/22 07:11:38.705
Dec  9 07:11:38.715: INFO: waiting for watch events with expected annotations
Dec  9 07:11:38.715: INFO: saw patched and updated annotations
STEP: deleting 12/09/22 07:11:38.715
STEP: deleting a collection 12/09/22 07:11:38.729
[AfterEach] [sig-network] IngressClass API
  test/e2e/framework/node/init/init.go:32
Dec  9 07:11:38.741: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] IngressClass API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] IngressClass API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] IngressClass API
  tear down framework | framework.go:193
STEP: Destroying namespace "ingressclass-3107" for this suite. 12/09/22 07:11:38.746
------------------------------
â€¢ [0.164 seconds]
[sig-network] IngressClass API
test/e2e/network/common/framework.go:23
   should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] IngressClass API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:11:38.59
    Dec  9 07:11:38.590: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename ingressclass 12/09/22 07:11:38.591
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:11:38.614
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:11:38.62
    [BeforeEach] [sig-network] IngressClass API
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] IngressClass API
      test/e2e/network/ingressclass.go:211
    [It]  should support creating IngressClass API operations [Conformance]
      test/e2e/network/ingressclass.go:223
    STEP: getting /apis 12/09/22 07:11:38.628
    STEP: getting /apis/networking.k8s.io 12/09/22 07:11:38.638
    STEP: getting /apis/networking.k8s.iov1 12/09/22 07:11:38.641
    STEP: creating 12/09/22 07:11:38.642
    STEP: getting 12/09/22 07:11:38.684
    STEP: listing 12/09/22 07:11:38.687
    STEP: watching 12/09/22 07:11:38.695
    Dec  9 07:11:38.695: INFO: starting watch
    STEP: patching 12/09/22 07:11:38.698
    STEP: updating 12/09/22 07:11:38.705
    Dec  9 07:11:38.715: INFO: waiting for watch events with expected annotations
    Dec  9 07:11:38.715: INFO: saw patched and updated annotations
    STEP: deleting 12/09/22 07:11:38.715
    STEP: deleting a collection 12/09/22 07:11:38.729
    [AfterEach] [sig-network] IngressClass API
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:11:38.741: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] IngressClass API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] IngressClass API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] IngressClass API
      tear down framework | framework.go:193
    STEP: Destroying namespace "ingressclass-3107" for this suite. 12/09/22 07:11:38.746
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Projected configMap
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:375
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:11:38.756
Dec  9 07:11:38.756: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename projected 12/09/22 07:11:38.759
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:11:38.779
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:11:38.786
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:375
STEP: Creating configMap with name projected-configmap-test-volume-c220cb33-3405-40be-a292-e53b05e9cefa 12/09/22 07:11:38.79
STEP: Creating a pod to test consume configMaps 12/09/22 07:11:38.795
Dec  9 07:11:38.803: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-9ac67b82-704e-40d8-a728-8486a7ae71b6" in namespace "projected-7191" to be "Succeeded or Failed"
Dec  9 07:11:38.814: INFO: Pod "pod-projected-configmaps-9ac67b82-704e-40d8-a728-8486a7ae71b6": Phase="Pending", Reason="", readiness=false. Elapsed: 10.085617ms
Dec  9 07:11:40.817: INFO: Pod "pod-projected-configmaps-9ac67b82-704e-40d8-a728-8486a7ae71b6": Phase="Running", Reason="", readiness=false. Elapsed: 2.013548155s
Dec  9 07:11:42.818: INFO: Pod "pod-projected-configmaps-9ac67b82-704e-40d8-a728-8486a7ae71b6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014232286s
STEP: Saw pod success 12/09/22 07:11:42.818
Dec  9 07:11:42.818: INFO: Pod "pod-projected-configmaps-9ac67b82-704e-40d8-a728-8486a7ae71b6" satisfied condition "Succeeded or Failed"
Dec  9 07:11:42.821: INFO: Trying to get logs from node ip-10-0-10-179 pod pod-projected-configmaps-9ac67b82-704e-40d8-a728-8486a7ae71b6 container projected-configmap-volume-test: <nil>
STEP: delete the pod 12/09/22 07:11:42.826
Dec  9 07:11:42.840: INFO: Waiting for pod pod-projected-configmaps-9ac67b82-704e-40d8-a728-8486a7ae71b6 to disappear
Dec  9 07:11:42.843: INFO: Pod pod-projected-configmaps-9ac67b82-704e-40d8-a728-8486a7ae71b6 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Dec  9 07:11:42.843: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-7191" for this suite. 12/09/22 07:11:42.847
------------------------------
â€¢ [4.099 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:375

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:11:38.756
    Dec  9 07:11:38.756: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename projected 12/09/22 07:11:38.759
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:11:38.779
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:11:38.786
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:375
    STEP: Creating configMap with name projected-configmap-test-volume-c220cb33-3405-40be-a292-e53b05e9cefa 12/09/22 07:11:38.79
    STEP: Creating a pod to test consume configMaps 12/09/22 07:11:38.795
    Dec  9 07:11:38.803: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-9ac67b82-704e-40d8-a728-8486a7ae71b6" in namespace "projected-7191" to be "Succeeded or Failed"
    Dec  9 07:11:38.814: INFO: Pod "pod-projected-configmaps-9ac67b82-704e-40d8-a728-8486a7ae71b6": Phase="Pending", Reason="", readiness=false. Elapsed: 10.085617ms
    Dec  9 07:11:40.817: INFO: Pod "pod-projected-configmaps-9ac67b82-704e-40d8-a728-8486a7ae71b6": Phase="Running", Reason="", readiness=false. Elapsed: 2.013548155s
    Dec  9 07:11:42.818: INFO: Pod "pod-projected-configmaps-9ac67b82-704e-40d8-a728-8486a7ae71b6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014232286s
    STEP: Saw pod success 12/09/22 07:11:42.818
    Dec  9 07:11:42.818: INFO: Pod "pod-projected-configmaps-9ac67b82-704e-40d8-a728-8486a7ae71b6" satisfied condition "Succeeded or Failed"
    Dec  9 07:11:42.821: INFO: Trying to get logs from node ip-10-0-10-179 pod pod-projected-configmaps-9ac67b82-704e-40d8-a728-8486a7ae71b6 container projected-configmap-volume-test: <nil>
    STEP: delete the pod 12/09/22 07:11:42.826
    Dec  9 07:11:42.840: INFO: Waiting for pod pod-projected-configmaps-9ac67b82-704e-40d8-a728-8486a7ae71b6 to disappear
    Dec  9 07:11:42.843: INFO: Pod pod-projected-configmaps-9ac67b82-704e-40d8-a728-8486a7ae71b6 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:11:42.843: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-7191" for this suite. 12/09/22 07:11:42.847
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:618
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:11:42.856
Dec  9 07:11:42.856: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename pods 12/09/22 07:11:42.857
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:11:42.873
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:11:42.879
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:618
Dec  9 07:11:42.883: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: creating the pod 12/09/22 07:11:42.883
STEP: submitting the pod to kubernetes 12/09/22 07:11:42.884
Dec  9 07:11:42.896: INFO: Waiting up to 5m0s for pod "pod-logs-websocket-cc2d7ec0-08c6-4541-80ad-af379989d261" in namespace "pods-4448" to be "running and ready"
Dec  9 07:11:42.901: INFO: Pod "pod-logs-websocket-cc2d7ec0-08c6-4541-80ad-af379989d261": Phase="Pending", Reason="", readiness=false. Elapsed: 5.123694ms
Dec  9 07:11:42.901: INFO: The phase of Pod pod-logs-websocket-cc2d7ec0-08c6-4541-80ad-af379989d261 is Pending, waiting for it to be Running (with Ready = true)
Dec  9 07:11:44.905: INFO: Pod "pod-logs-websocket-cc2d7ec0-08c6-4541-80ad-af379989d261": Phase="Running", Reason="", readiness=true. Elapsed: 2.009319886s
Dec  9 07:11:44.906: INFO: The phase of Pod pod-logs-websocket-cc2d7ec0-08c6-4541-80ad-af379989d261 is Running (Ready = true)
Dec  9 07:11:44.906: INFO: Pod "pod-logs-websocket-cc2d7ec0-08c6-4541-80ad-af379989d261" satisfied condition "running and ready"
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Dec  9 07:11:44.925: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-4448" for this suite. 12/09/22 07:11:44.929
------------------------------
â€¢ [2.079 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:618

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:11:42.856
    Dec  9 07:11:42.856: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename pods 12/09/22 07:11:42.857
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:11:42.873
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:11:42.879
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:618
    Dec  9 07:11:42.883: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: creating the pod 12/09/22 07:11:42.883
    STEP: submitting the pod to kubernetes 12/09/22 07:11:42.884
    Dec  9 07:11:42.896: INFO: Waiting up to 5m0s for pod "pod-logs-websocket-cc2d7ec0-08c6-4541-80ad-af379989d261" in namespace "pods-4448" to be "running and ready"
    Dec  9 07:11:42.901: INFO: Pod "pod-logs-websocket-cc2d7ec0-08c6-4541-80ad-af379989d261": Phase="Pending", Reason="", readiness=false. Elapsed: 5.123694ms
    Dec  9 07:11:42.901: INFO: The phase of Pod pod-logs-websocket-cc2d7ec0-08c6-4541-80ad-af379989d261 is Pending, waiting for it to be Running (with Ready = true)
    Dec  9 07:11:44.905: INFO: Pod "pod-logs-websocket-cc2d7ec0-08c6-4541-80ad-af379989d261": Phase="Running", Reason="", readiness=true. Elapsed: 2.009319886s
    Dec  9 07:11:44.906: INFO: The phase of Pod pod-logs-websocket-cc2d7ec0-08c6-4541-80ad-af379989d261 is Running (Ready = true)
    Dec  9 07:11:44.906: INFO: Pod "pod-logs-websocket-cc2d7ec0-08c6-4541-80ad-af379989d261" satisfied condition "running and ready"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:11:44.925: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-4448" for this suite. 12/09/22 07:11:44.929
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:11:44.937
Dec  9 07:11:44.937: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename cronjob 12/09/22 07:11:44.938
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:11:44.953
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:11:44.957
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160
STEP: Creating a ReplaceConcurrent cronjob 12/09/22 07:11:44.963
STEP: Ensuring a job is scheduled 12/09/22 07:11:44.985
STEP: Ensuring exactly one is scheduled 12/09/22 07:12:00.988
STEP: Ensuring exactly one running job exists by listing jobs explicitly 12/09/22 07:12:00.992
STEP: Ensuring the job is replaced with a new one 12/09/22 07:12:00.995
STEP: Removing cronjob 12/09/22 07:13:01.002
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Dec  9 07:13:01.011: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-9585" for this suite. 12/09/22 07:13:01.016
------------------------------
â€¢ [SLOW TEST] [76.087 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:11:44.937
    Dec  9 07:11:44.937: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename cronjob 12/09/22 07:11:44.938
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:11:44.953
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:11:44.957
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should replace jobs when ReplaceConcurrent [Conformance]
      test/e2e/apps/cronjob.go:160
    STEP: Creating a ReplaceConcurrent cronjob 12/09/22 07:11:44.963
    STEP: Ensuring a job is scheduled 12/09/22 07:11:44.985
    STEP: Ensuring exactly one is scheduled 12/09/22 07:12:00.988
    STEP: Ensuring exactly one running job exists by listing jobs explicitly 12/09/22 07:12:00.992
    STEP: Ensuring the job is replaced with a new one 12/09/22 07:12:00.995
    STEP: Removing cronjob 12/09/22 07:13:01.002
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:13:01.011: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-9585" for this suite. 12/09/22 07:13:01.016
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] RuntimeClass
   should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:13:01.025
Dec  9 07:13:01.025: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename runtimeclass 12/09/22 07:13:01.027
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:13:01.058
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:13:01.066
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It]  should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189
STEP: getting /apis 12/09/22 07:13:01.071
STEP: getting /apis/node.k8s.io 12/09/22 07:13:01.074
STEP: getting /apis/node.k8s.io/v1 12/09/22 07:13:01.075
STEP: creating 12/09/22 07:13:01.08
STEP: watching 12/09/22 07:13:01.098
Dec  9 07:13:01.098: INFO: starting watch
STEP: getting 12/09/22 07:13:01.104
STEP: listing 12/09/22 07:13:01.107
STEP: patching 12/09/22 07:13:01.111
STEP: updating 12/09/22 07:13:01.116
Dec  9 07:13:01.121: INFO: waiting for watch events with expected annotations
STEP: deleting 12/09/22 07:13:01.121
STEP: deleting a collection 12/09/22 07:13:01.132
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Dec  9 07:13:01.144: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-630" for this suite. 12/09/22 07:13:01.147
------------------------------
â€¢ [0.129 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
   should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:13:01.025
    Dec  9 07:13:01.025: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename runtimeclass 12/09/22 07:13:01.027
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:13:01.058
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:13:01.066
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It]  should support RuntimeClasses API operations [Conformance]
      test/e2e/common/node/runtimeclass.go:189
    STEP: getting /apis 12/09/22 07:13:01.071
    STEP: getting /apis/node.k8s.io 12/09/22 07:13:01.074
    STEP: getting /apis/node.k8s.io/v1 12/09/22 07:13:01.075
    STEP: creating 12/09/22 07:13:01.08
    STEP: watching 12/09/22 07:13:01.098
    Dec  9 07:13:01.098: INFO: starting watch
    STEP: getting 12/09/22 07:13:01.104
    STEP: listing 12/09/22 07:13:01.107
    STEP: patching 12/09/22 07:13:01.111
    STEP: updating 12/09/22 07:13:01.116
    Dec  9 07:13:01.121: INFO: waiting for watch events with expected annotations
    STEP: deleting 12/09/22 07:13:01.121
    STEP: deleting a collection 12/09/22 07:13:01.132
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:13:01.144: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-630" for this suite. 12/09/22 07:13:01.147
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Secrets
  should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:140
[BeforeEach] [sig-node] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:13:01.154
Dec  9 07:13:01.155: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename secrets 12/09/22 07:13:01.156
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:13:01.171
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:13:01.178
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:140
STEP: Creating projection with secret that has name secret-emptykey-test-70bde53c-8711-4e39-81e6-ac71d80f0aa3 12/09/22 07:13:01.182
[AfterEach] [sig-node] Secrets
  test/e2e/framework/node/init/init.go:32
Dec  9 07:13:01.186: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-5132" for this suite. 12/09/22 07:13:01.205
------------------------------
â€¢ [0.059 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:140

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:13:01.154
    Dec  9 07:13:01.155: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename secrets 12/09/22 07:13:01.156
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:13:01.171
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:13:01.178
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should fail to create secret due to empty secret key [Conformance]
      test/e2e/common/node/secrets.go:140
    STEP: Creating projection with secret that has name secret-emptykey-test-70bde53c-8711-4e39-81e6-ac71d80f0aa3 12/09/22 07:13:01.182
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:13:01.186: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-5132" for this suite. 12/09/22 07:13:01.205
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:13:01.215
Dec  9 07:13:01.215: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename dns 12/09/22 07:13:01.216
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:13:01.242
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:13:01.253
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig... 12/09/22 07:13:01.261
Dec  9 07:13:01.283: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-2648  f5de78b3-e303-4fe4-8483-36571dd576d7 7918 0 2022-12-09 07:13:01 +0000 UTC <nil> <nil> map[] map[] [] [] [{e2e.test Update v1 2022-12-09 07:13:01 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zf6dx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zf6dx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec  9 07:13:01.287: INFO: Waiting up to 5m0s for pod "test-dns-nameservers" in namespace "dns-2648" to be "running and ready"
Dec  9 07:13:01.319: INFO: Pod "test-dns-nameservers": Phase="Pending", Reason="", readiness=false. Elapsed: 31.72658ms
Dec  9 07:13:01.319: INFO: The phase of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
Dec  9 07:13:03.323: INFO: Pod "test-dns-nameservers": Phase="Running", Reason="", readiness=true. Elapsed: 2.035370474s
Dec  9 07:13:03.323: INFO: The phase of Pod test-dns-nameservers is Running (Ready = true)
Dec  9 07:13:03.323: INFO: Pod "test-dns-nameservers" satisfied condition "running and ready"
STEP: Verifying customized DNS suffix list is configured on pod... 12/09/22 07:13:03.323
Dec  9 07:13:03.323: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-2648 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec  9 07:13:03.323: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
Dec  9 07:13:03.325: INFO: ExecWithOptions: Clientset creation
Dec  9 07:13:03.326: INFO: ExecWithOptions: execute(POST https://10.3.0.1:443/api/v1/namespaces/dns-2648/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
STEP: Verifying customized DNS server is configured on pod... 12/09/22 07:13:03.445
Dec  9 07:13:03.445: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-2648 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec  9 07:13:03.445: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
Dec  9 07:13:03.447: INFO: ExecWithOptions: Clientset creation
Dec  9 07:13:03.447: INFO: ExecWithOptions: execute(POST https://10.3.0.1:443/api/v1/namespaces/dns-2648/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Dec  9 07:13:03.539: INFO: Deleting pod test-dns-nameservers...
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Dec  9 07:13:03.552: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-2648" for this suite. 12/09/22 07:13:03.563
------------------------------
â€¢ [2.371 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:13:01.215
    Dec  9 07:13:01.215: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename dns 12/09/22 07:13:01.216
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:13:01.242
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:13:01.253
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should support configurable pod DNS nameservers [Conformance]
      test/e2e/network/dns.go:411
    STEP: Creating a pod with dnsPolicy=None and customized dnsConfig... 12/09/22 07:13:01.261
    Dec  9 07:13:01.283: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-2648  f5de78b3-e303-4fe4-8483-36571dd576d7 7918 0 2022-12-09 07:13:01 +0000 UTC <nil> <nil> map[] map[] [] [] [{e2e.test Update v1 2022-12-09 07:13:01 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zf6dx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zf6dx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Dec  9 07:13:01.287: INFO: Waiting up to 5m0s for pod "test-dns-nameservers" in namespace "dns-2648" to be "running and ready"
    Dec  9 07:13:01.319: INFO: Pod "test-dns-nameservers": Phase="Pending", Reason="", readiness=false. Elapsed: 31.72658ms
    Dec  9 07:13:01.319: INFO: The phase of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
    Dec  9 07:13:03.323: INFO: Pod "test-dns-nameservers": Phase="Running", Reason="", readiness=true. Elapsed: 2.035370474s
    Dec  9 07:13:03.323: INFO: The phase of Pod test-dns-nameservers is Running (Ready = true)
    Dec  9 07:13:03.323: INFO: Pod "test-dns-nameservers" satisfied condition "running and ready"
    STEP: Verifying customized DNS suffix list is configured on pod... 12/09/22 07:13:03.323
    Dec  9 07:13:03.323: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-2648 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Dec  9 07:13:03.323: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    Dec  9 07:13:03.325: INFO: ExecWithOptions: Clientset creation
    Dec  9 07:13:03.326: INFO: ExecWithOptions: execute(POST https://10.3.0.1:443/api/v1/namespaces/dns-2648/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    STEP: Verifying customized DNS server is configured on pod... 12/09/22 07:13:03.445
    Dec  9 07:13:03.445: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-2648 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Dec  9 07:13:03.445: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    Dec  9 07:13:03.447: INFO: ExecWithOptions: Clientset creation
    Dec  9 07:13:03.447: INFO: ExecWithOptions: execute(POST https://10.3.0.1:443/api/v1/namespaces/dns-2648/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Dec  9 07:13:03.539: INFO: Deleting pod test-dns-nameservers...
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:13:03.552: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-2648" for this suite. 12/09/22 07:13:03.563
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:13:03.587
Dec  9 07:13:03.588: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename custom-resource-definition 12/09/22 07:13:03.588
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:13:03.618
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:13:03.625
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269
Dec  9 07:13:03.629: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Dec  9 07:13:07.055: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-1545" for this suite. 12/09/22 07:13:07.061
------------------------------
â€¢ [3.481 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:13:03.587
    Dec  9 07:13:03.588: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename custom-resource-definition 12/09/22 07:13:03.588
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:13:03.618
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:13:03.625
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] custom resource defaulting for requests and from storage works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:269
    Dec  9 07:13:03.629: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:13:07.055: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-1545" for this suite. 12/09/22 07:13:07.061
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:823
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:13:07.073
Dec  9 07:13:07.073: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename daemonsets 12/09/22 07:13:07.074
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:13:07.089
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:13:07.092
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:823
STEP: Creating simple DaemonSet "daemon-set" 12/09/22 07:13:07.111
STEP: Check that daemon pods launch on every node of the cluster. 12/09/22 07:13:07.117
Dec  9 07:13:07.121: INFO: DaemonSet pods can't tolerate node ip-10-0-7-84 with taints [{Key:node-role.kubernetes.io/controller Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 07:13:07.125: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Dec  9 07:13:07.125: INFO: Node ip-10-0-10-179 is running 0 daemon pod, expected 1
Dec  9 07:13:08.129: INFO: DaemonSet pods can't tolerate node ip-10-0-7-84 with taints [{Key:node-role.kubernetes.io/controller Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 07:13:08.132: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Dec  9 07:13:08.132: INFO: Node ip-10-0-10-179 is running 0 daemon pod, expected 1
Dec  9 07:13:09.130: INFO: DaemonSet pods can't tolerate node ip-10-0-7-84 with taints [{Key:node-role.kubernetes.io/controller Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 07:13:09.134: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Dec  9 07:13:09.134: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
STEP: listing all DeamonSets 12/09/22 07:13:09.138
STEP: DeleteCollection of the DaemonSets 12/09/22 07:13:09.159
STEP: Verify that ReplicaSets have been deleted 12/09/22 07:13:09.189
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
Dec  9 07:13:09.264: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"8030"},"items":null}

Dec  9 07:13:09.289: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"8030"},"items":[{"metadata":{"name":"daemon-set-ctlmc","generateName":"daemon-set-","namespace":"daemonsets-5780","uid":"080dafbb-ecc9-4a1a-b98d-af428068f736","resourceVersion":"8015","creationTimestamp":"2022-12-09T07:13:07Z","labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"c5fbd8b4c71e831a64774bc3f07957fe7f28b8e065e6861469e013cc418d3489","cni.projectcalico.org/podIP":"10.2.166.78/32","cni.projectcalico.org/podIPs":"10.2.166.78/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"c50a53fe-3e17-4c36-aa2c-b478c74ce9fc","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2022-12-09T07:13:07Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2022-12-09T07:13:07Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c50a53fe-3e17-4c36-aa2c-b478c74ce9fc\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2022-12-09T07:13:08Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.2.166.78\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-7knfg","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-7knfg","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"ip-10-0-17-108","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["ip-10-0-17-108"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-12-09T07:13:07Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-12-09T07:13:08Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-12-09T07:13:08Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-12-09T07:13:07Z"}],"hostIP":"10.0.17.108","podIP":"10.2.166.78","podIPs":[{"ip":"10.2.166.78"}],"startTime":"2022-12-09T07:13:07Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2022-12-09T07:13:08Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://460eeb42b2aba258bdf43ec67d778533ec9ca3649f3c344244884291ea15e37c","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-swxbc","generateName":"daemon-set-","namespace":"daemonsets-5780","uid":"8777a45e-6480-4c79-946a-abaeaeb4637a","resourceVersion":"8025","creationTimestamp":"2022-12-09T07:13:07Z","labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"7f27586f65779db63eaea6fa52e7664215ab200bd34fa7176d2b7306298e25b1","cni.projectcalico.org/podIP":"10.2.136.112/32","cni.projectcalico.org/podIPs":"10.2.136.112/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"c50a53fe-3e17-4c36-aa2c-b478c74ce9fc","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2022-12-09T07:13:07Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2022-12-09T07:13:07Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c50a53fe-3e17-4c36-aa2c-b478c74ce9fc\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2022-12-09T07:13:08Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.2.136.112\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-qcdx7","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-qcdx7","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"ip-10-0-10-179","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["ip-10-0-10-179"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-12-09T07:13:07Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-12-09T07:13:08Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-12-09T07:13:08Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-12-09T07:13:07Z"}],"hostIP":"10.0.10.179","podIP":"10.2.136.112","podIPs":[{"ip":"10.2.136.112"}],"startTime":"2022-12-09T07:13:07Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2022-12-09T07:13:07Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://d6aa79fd63c9ad7d9d1b7cec5494591854423aefd14b979c2342315b7b310858","started":true}],"qosClass":"BestEffort"}}]}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Dec  9 07:13:09.323: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-5780" for this suite. 12/09/22 07:13:09.333
------------------------------
â€¢ [2.270 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:823

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:13:07.073
    Dec  9 07:13:07.073: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename daemonsets 12/09/22 07:13:07.074
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:13:07.089
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:13:07.092
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should list and delete a collection of DaemonSets [Conformance]
      test/e2e/apps/daemon_set.go:823
    STEP: Creating simple DaemonSet "daemon-set" 12/09/22 07:13:07.111
    STEP: Check that daemon pods launch on every node of the cluster. 12/09/22 07:13:07.117
    Dec  9 07:13:07.121: INFO: DaemonSet pods can't tolerate node ip-10-0-7-84 with taints [{Key:node-role.kubernetes.io/controller Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Dec  9 07:13:07.125: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Dec  9 07:13:07.125: INFO: Node ip-10-0-10-179 is running 0 daemon pod, expected 1
    Dec  9 07:13:08.129: INFO: DaemonSet pods can't tolerate node ip-10-0-7-84 with taints [{Key:node-role.kubernetes.io/controller Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Dec  9 07:13:08.132: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Dec  9 07:13:08.132: INFO: Node ip-10-0-10-179 is running 0 daemon pod, expected 1
    Dec  9 07:13:09.130: INFO: DaemonSet pods can't tolerate node ip-10-0-7-84 with taints [{Key:node-role.kubernetes.io/controller Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Dec  9 07:13:09.134: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Dec  9 07:13:09.134: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    STEP: listing all DeamonSets 12/09/22 07:13:09.138
    STEP: DeleteCollection of the DaemonSets 12/09/22 07:13:09.159
    STEP: Verify that ReplicaSets have been deleted 12/09/22 07:13:09.189
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    Dec  9 07:13:09.264: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"8030"},"items":null}

    Dec  9 07:13:09.289: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"8030"},"items":[{"metadata":{"name":"daemon-set-ctlmc","generateName":"daemon-set-","namespace":"daemonsets-5780","uid":"080dafbb-ecc9-4a1a-b98d-af428068f736","resourceVersion":"8015","creationTimestamp":"2022-12-09T07:13:07Z","labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"c5fbd8b4c71e831a64774bc3f07957fe7f28b8e065e6861469e013cc418d3489","cni.projectcalico.org/podIP":"10.2.166.78/32","cni.projectcalico.org/podIPs":"10.2.166.78/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"c50a53fe-3e17-4c36-aa2c-b478c74ce9fc","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2022-12-09T07:13:07Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2022-12-09T07:13:07Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c50a53fe-3e17-4c36-aa2c-b478c74ce9fc\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2022-12-09T07:13:08Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.2.166.78\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-7knfg","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-7knfg","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"ip-10-0-17-108","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["ip-10-0-17-108"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-12-09T07:13:07Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-12-09T07:13:08Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-12-09T07:13:08Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-12-09T07:13:07Z"}],"hostIP":"10.0.17.108","podIP":"10.2.166.78","podIPs":[{"ip":"10.2.166.78"}],"startTime":"2022-12-09T07:13:07Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2022-12-09T07:13:08Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://460eeb42b2aba258bdf43ec67d778533ec9ca3649f3c344244884291ea15e37c","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-swxbc","generateName":"daemon-set-","namespace":"daemonsets-5780","uid":"8777a45e-6480-4c79-946a-abaeaeb4637a","resourceVersion":"8025","creationTimestamp":"2022-12-09T07:13:07Z","labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"7f27586f65779db63eaea6fa52e7664215ab200bd34fa7176d2b7306298e25b1","cni.projectcalico.org/podIP":"10.2.136.112/32","cni.projectcalico.org/podIPs":"10.2.136.112/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"c50a53fe-3e17-4c36-aa2c-b478c74ce9fc","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2022-12-09T07:13:07Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2022-12-09T07:13:07Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c50a53fe-3e17-4c36-aa2c-b478c74ce9fc\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2022-12-09T07:13:08Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.2.136.112\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-qcdx7","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-qcdx7","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"ip-10-0-10-179","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["ip-10-0-10-179"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-12-09T07:13:07Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-12-09T07:13:08Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-12-09T07:13:08Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-12-09T07:13:07Z"}],"hostIP":"10.0.10.179","podIP":"10.2.136.112","podIPs":[{"ip":"10.2.136.112"}],"startTime":"2022-12-09T07:13:07Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2022-12-09T07:13:07Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://d6aa79fd63c9ad7d9d1b7cec5494591854423aefd14b979c2342315b7b310858","started":true}],"qosClass":"BestEffort"}}]}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:13:09.323: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-5780" for this suite. 12/09/22 07:13:09.333
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] Discovery
  should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122
[BeforeEach] [sig-api-machinery] Discovery
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:13:09.344
Dec  9 07:13:09.345: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename discovery 12/09/22 07:13:09.345
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:13:09.371
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:13:09.376
[BeforeEach] [sig-api-machinery] Discovery
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] Discovery
  test/e2e/apimachinery/discovery.go:43
STEP: Setting up server cert 12/09/22 07:13:09.384
[It] should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122
Dec  9 07:13:09.902: INFO: Checking APIGroup: apiregistration.k8s.io
Dec  9 07:13:09.903: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
Dec  9 07:13:09.903: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
Dec  9 07:13:09.903: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
Dec  9 07:13:09.903: INFO: Checking APIGroup: apps
Dec  9 07:13:09.904: INFO: PreferredVersion.GroupVersion: apps/v1
Dec  9 07:13:09.904: INFO: Versions found [{apps/v1 v1}]
Dec  9 07:13:09.904: INFO: apps/v1 matches apps/v1
Dec  9 07:13:09.904: INFO: Checking APIGroup: events.k8s.io
Dec  9 07:13:09.906: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
Dec  9 07:13:09.906: INFO: Versions found [{events.k8s.io/v1 v1}]
Dec  9 07:13:09.906: INFO: events.k8s.io/v1 matches events.k8s.io/v1
Dec  9 07:13:09.906: INFO: Checking APIGroup: authentication.k8s.io
Dec  9 07:13:09.907: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
Dec  9 07:13:09.907: INFO: Versions found [{authentication.k8s.io/v1 v1}]
Dec  9 07:13:09.907: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
Dec  9 07:13:09.907: INFO: Checking APIGroup: authorization.k8s.io
Dec  9 07:13:09.908: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
Dec  9 07:13:09.908: INFO: Versions found [{authorization.k8s.io/v1 v1}]
Dec  9 07:13:09.908: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
Dec  9 07:13:09.908: INFO: Checking APIGroup: autoscaling
Dec  9 07:13:09.910: INFO: PreferredVersion.GroupVersion: autoscaling/v2
Dec  9 07:13:09.910: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1}]
Dec  9 07:13:09.910: INFO: autoscaling/v2 matches autoscaling/v2
Dec  9 07:13:09.910: INFO: Checking APIGroup: batch
Dec  9 07:13:09.911: INFO: PreferredVersion.GroupVersion: batch/v1
Dec  9 07:13:09.911: INFO: Versions found [{batch/v1 v1}]
Dec  9 07:13:09.911: INFO: batch/v1 matches batch/v1
Dec  9 07:13:09.911: INFO: Checking APIGroup: certificates.k8s.io
Dec  9 07:13:09.912: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
Dec  9 07:13:09.912: INFO: Versions found [{certificates.k8s.io/v1 v1}]
Dec  9 07:13:09.912: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
Dec  9 07:13:09.912: INFO: Checking APIGroup: networking.k8s.io
Dec  9 07:13:09.914: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
Dec  9 07:13:09.914: INFO: Versions found [{networking.k8s.io/v1 v1}]
Dec  9 07:13:09.914: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
Dec  9 07:13:09.914: INFO: Checking APIGroup: policy
Dec  9 07:13:09.915: INFO: PreferredVersion.GroupVersion: policy/v1
Dec  9 07:13:09.915: INFO: Versions found [{policy/v1 v1}]
Dec  9 07:13:09.915: INFO: policy/v1 matches policy/v1
Dec  9 07:13:09.915: INFO: Checking APIGroup: rbac.authorization.k8s.io
Dec  9 07:13:09.917: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
Dec  9 07:13:09.917: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
Dec  9 07:13:09.917: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
Dec  9 07:13:09.917: INFO: Checking APIGroup: storage.k8s.io
Dec  9 07:13:09.918: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
Dec  9 07:13:09.918: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
Dec  9 07:13:09.918: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
Dec  9 07:13:09.918: INFO: Checking APIGroup: admissionregistration.k8s.io
Dec  9 07:13:09.920: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
Dec  9 07:13:09.920: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
Dec  9 07:13:09.920: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
Dec  9 07:13:09.920: INFO: Checking APIGroup: apiextensions.k8s.io
Dec  9 07:13:09.921: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
Dec  9 07:13:09.921: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
Dec  9 07:13:09.921: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
Dec  9 07:13:09.921: INFO: Checking APIGroup: scheduling.k8s.io
Dec  9 07:13:09.922: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
Dec  9 07:13:09.922: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
Dec  9 07:13:09.923: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
Dec  9 07:13:09.923: INFO: Checking APIGroup: coordination.k8s.io
Dec  9 07:13:09.924: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
Dec  9 07:13:09.924: INFO: Versions found [{coordination.k8s.io/v1 v1}]
Dec  9 07:13:09.924: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
Dec  9 07:13:09.924: INFO: Checking APIGroup: node.k8s.io
Dec  9 07:13:09.926: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
Dec  9 07:13:09.926: INFO: Versions found [{node.k8s.io/v1 v1}]
Dec  9 07:13:09.926: INFO: node.k8s.io/v1 matches node.k8s.io/v1
Dec  9 07:13:09.926: INFO: Checking APIGroup: discovery.k8s.io
Dec  9 07:13:09.928: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
Dec  9 07:13:09.928: INFO: Versions found [{discovery.k8s.io/v1 v1}]
Dec  9 07:13:09.928: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
Dec  9 07:13:09.928: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
Dec  9 07:13:09.929: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta3
Dec  9 07:13:09.929: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta3 v1beta3} {flowcontrol.apiserver.k8s.io/v1beta2 v1beta2}]
Dec  9 07:13:09.929: INFO: flowcontrol.apiserver.k8s.io/v1beta3 matches flowcontrol.apiserver.k8s.io/v1beta3
Dec  9 07:13:09.929: INFO: Checking APIGroup: crd.projectcalico.org
Dec  9 07:13:09.930: INFO: PreferredVersion.GroupVersion: crd.projectcalico.org/v1
Dec  9 07:13:09.930: INFO: Versions found [{crd.projectcalico.org/v1 v1}]
Dec  9 07:13:09.930: INFO: crd.projectcalico.org/v1 matches crd.projectcalico.org/v1
[AfterEach] [sig-api-machinery] Discovery
  test/e2e/framework/node/init/init.go:32
Dec  9 07:13:09.931: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Discovery
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Discovery
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Discovery
  tear down framework | framework.go:193
STEP: Destroying namespace "discovery-1375" for this suite. 12/09/22 07:13:09.934
------------------------------
â€¢ [0.595 seconds]
[sig-api-machinery] Discovery
test/e2e/apimachinery/framework.go:23
  should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Discovery
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:13:09.344
    Dec  9 07:13:09.345: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename discovery 12/09/22 07:13:09.345
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:13:09.371
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:13:09.376
    [BeforeEach] [sig-api-machinery] Discovery
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] Discovery
      test/e2e/apimachinery/discovery.go:43
    STEP: Setting up server cert 12/09/22 07:13:09.384
    [It] should validate PreferredVersion for each APIGroup [Conformance]
      test/e2e/apimachinery/discovery.go:122
    Dec  9 07:13:09.902: INFO: Checking APIGroup: apiregistration.k8s.io
    Dec  9 07:13:09.903: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
    Dec  9 07:13:09.903: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
    Dec  9 07:13:09.903: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
    Dec  9 07:13:09.903: INFO: Checking APIGroup: apps
    Dec  9 07:13:09.904: INFO: PreferredVersion.GroupVersion: apps/v1
    Dec  9 07:13:09.904: INFO: Versions found [{apps/v1 v1}]
    Dec  9 07:13:09.904: INFO: apps/v1 matches apps/v1
    Dec  9 07:13:09.904: INFO: Checking APIGroup: events.k8s.io
    Dec  9 07:13:09.906: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
    Dec  9 07:13:09.906: INFO: Versions found [{events.k8s.io/v1 v1}]
    Dec  9 07:13:09.906: INFO: events.k8s.io/v1 matches events.k8s.io/v1
    Dec  9 07:13:09.906: INFO: Checking APIGroup: authentication.k8s.io
    Dec  9 07:13:09.907: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
    Dec  9 07:13:09.907: INFO: Versions found [{authentication.k8s.io/v1 v1}]
    Dec  9 07:13:09.907: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
    Dec  9 07:13:09.907: INFO: Checking APIGroup: authorization.k8s.io
    Dec  9 07:13:09.908: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
    Dec  9 07:13:09.908: INFO: Versions found [{authorization.k8s.io/v1 v1}]
    Dec  9 07:13:09.908: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
    Dec  9 07:13:09.908: INFO: Checking APIGroup: autoscaling
    Dec  9 07:13:09.910: INFO: PreferredVersion.GroupVersion: autoscaling/v2
    Dec  9 07:13:09.910: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1}]
    Dec  9 07:13:09.910: INFO: autoscaling/v2 matches autoscaling/v2
    Dec  9 07:13:09.910: INFO: Checking APIGroup: batch
    Dec  9 07:13:09.911: INFO: PreferredVersion.GroupVersion: batch/v1
    Dec  9 07:13:09.911: INFO: Versions found [{batch/v1 v1}]
    Dec  9 07:13:09.911: INFO: batch/v1 matches batch/v1
    Dec  9 07:13:09.911: INFO: Checking APIGroup: certificates.k8s.io
    Dec  9 07:13:09.912: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
    Dec  9 07:13:09.912: INFO: Versions found [{certificates.k8s.io/v1 v1}]
    Dec  9 07:13:09.912: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
    Dec  9 07:13:09.912: INFO: Checking APIGroup: networking.k8s.io
    Dec  9 07:13:09.914: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
    Dec  9 07:13:09.914: INFO: Versions found [{networking.k8s.io/v1 v1}]
    Dec  9 07:13:09.914: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
    Dec  9 07:13:09.914: INFO: Checking APIGroup: policy
    Dec  9 07:13:09.915: INFO: PreferredVersion.GroupVersion: policy/v1
    Dec  9 07:13:09.915: INFO: Versions found [{policy/v1 v1}]
    Dec  9 07:13:09.915: INFO: policy/v1 matches policy/v1
    Dec  9 07:13:09.915: INFO: Checking APIGroup: rbac.authorization.k8s.io
    Dec  9 07:13:09.917: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
    Dec  9 07:13:09.917: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
    Dec  9 07:13:09.917: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
    Dec  9 07:13:09.917: INFO: Checking APIGroup: storage.k8s.io
    Dec  9 07:13:09.918: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
    Dec  9 07:13:09.918: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
    Dec  9 07:13:09.918: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
    Dec  9 07:13:09.918: INFO: Checking APIGroup: admissionregistration.k8s.io
    Dec  9 07:13:09.920: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
    Dec  9 07:13:09.920: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
    Dec  9 07:13:09.920: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
    Dec  9 07:13:09.920: INFO: Checking APIGroup: apiextensions.k8s.io
    Dec  9 07:13:09.921: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
    Dec  9 07:13:09.921: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
    Dec  9 07:13:09.921: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
    Dec  9 07:13:09.921: INFO: Checking APIGroup: scheduling.k8s.io
    Dec  9 07:13:09.922: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
    Dec  9 07:13:09.922: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
    Dec  9 07:13:09.923: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
    Dec  9 07:13:09.923: INFO: Checking APIGroup: coordination.k8s.io
    Dec  9 07:13:09.924: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
    Dec  9 07:13:09.924: INFO: Versions found [{coordination.k8s.io/v1 v1}]
    Dec  9 07:13:09.924: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
    Dec  9 07:13:09.924: INFO: Checking APIGroup: node.k8s.io
    Dec  9 07:13:09.926: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
    Dec  9 07:13:09.926: INFO: Versions found [{node.k8s.io/v1 v1}]
    Dec  9 07:13:09.926: INFO: node.k8s.io/v1 matches node.k8s.io/v1
    Dec  9 07:13:09.926: INFO: Checking APIGroup: discovery.k8s.io
    Dec  9 07:13:09.928: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
    Dec  9 07:13:09.928: INFO: Versions found [{discovery.k8s.io/v1 v1}]
    Dec  9 07:13:09.928: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
    Dec  9 07:13:09.928: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
    Dec  9 07:13:09.929: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta3
    Dec  9 07:13:09.929: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta3 v1beta3} {flowcontrol.apiserver.k8s.io/v1beta2 v1beta2}]
    Dec  9 07:13:09.929: INFO: flowcontrol.apiserver.k8s.io/v1beta3 matches flowcontrol.apiserver.k8s.io/v1beta3
    Dec  9 07:13:09.929: INFO: Checking APIGroup: crd.projectcalico.org
    Dec  9 07:13:09.930: INFO: PreferredVersion.GroupVersion: crd.projectcalico.org/v1
    Dec  9 07:13:09.930: INFO: Versions found [{crd.projectcalico.org/v1 v1}]
    Dec  9 07:13:09.930: INFO: crd.projectcalico.org/v1 matches crd.projectcalico.org/v1
    [AfterEach] [sig-api-machinery] Discovery
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:13:09.931: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Discovery
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Discovery
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Discovery
      tear down framework | framework.go:193
    STEP: Destroying namespace "discovery-1375" for this suite. 12/09/22 07:13:09.934
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-network] EndpointSlice
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:205
[BeforeEach] [sig-network] EndpointSlice
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:13:09.94
Dec  9 07:13:09.940: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename endpointslice 12/09/22 07:13:09.941
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:13:09.959
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:13:09.963
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:52
[It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:205
STEP: referencing a single matching pod 12/09/22 07:13:15.061
STEP: referencing matching pods with named port 12/09/22 07:13:20.072
STEP: creating empty Endpoints and EndpointSlices for no matching Pods 12/09/22 07:13:25.086
STEP: recreating EndpointSlices after they've been deleted 12/09/22 07:13:30.101
Dec  9 07:13:30.122: INFO: EndpointSlice for Service endpointslice-2392/example-named-port not found
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/node/init/init.go:32
Dec  9 07:13:40.136: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSlice
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSlice
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslice-2392" for this suite. 12/09/22 07:13:40.14
------------------------------
â€¢ [SLOW TEST] [30.208 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:205

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:13:09.94
    Dec  9 07:13:09.940: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename endpointslice 12/09/22 07:13:09.941
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:13:09.959
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:13:09.963
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:52
    [It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
      test/e2e/network/endpointslice.go:205
    STEP: referencing a single matching pod 12/09/22 07:13:15.061
    STEP: referencing matching pods with named port 12/09/22 07:13:20.072
    STEP: creating empty Endpoints and EndpointSlices for no matching Pods 12/09/22 07:13:25.086
    STEP: recreating EndpointSlices after they've been deleted 12/09/22 07:13:30.101
    Dec  9 07:13:30.122: INFO: EndpointSlice for Service endpointslice-2392/example-named-port not found
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:13:40.136: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslice-2392" for this suite. 12/09/22 07:13:40.14
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Should recreate evicted statefulset [Conformance]
  test/e2e/apps/statefulset.go:739
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:13:40.148
Dec  9 07:13:40.149: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename statefulset 12/09/22 07:13:40.149
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:13:40.178
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:13:40.187
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-6725 12/09/22 07:13:40.195
[It] Should recreate evicted statefulset [Conformance]
  test/e2e/apps/statefulset.go:739
STEP: Looking for a node to schedule stateful set and pod 12/09/22 07:13:40.203
STEP: Creating pod with conflicting port in namespace statefulset-6725 12/09/22 07:13:40.209
STEP: Waiting until pod test-pod will start running in namespace statefulset-6725 12/09/22 07:13:40.226
Dec  9 07:13:40.226: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "statefulset-6725" to be "running"
Dec  9 07:13:40.231: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.73964ms
Dec  9 07:13:42.235: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.008581074s
Dec  9 07:13:42.235: INFO: Pod "test-pod" satisfied condition "running"
STEP: Creating statefulset with conflicting port in namespace statefulset-6725 12/09/22 07:13:42.235
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-6725 12/09/22 07:13:42.24
Dec  9 07:13:42.260: INFO: Observed stateful pod in namespace: statefulset-6725, name: ss-0, uid: 85309b97-b2e0-4e48-80ef-516966f98dc8, status phase: Pending. Waiting for statefulset controller to delete.
Dec  9 07:13:42.289: INFO: Observed stateful pod in namespace: statefulset-6725, name: ss-0, uid: 85309b97-b2e0-4e48-80ef-516966f98dc8, status phase: Failed. Waiting for statefulset controller to delete.
Dec  9 07:13:42.297: INFO: Observed stateful pod in namespace: statefulset-6725, name: ss-0, uid: 85309b97-b2e0-4e48-80ef-516966f98dc8, status phase: Failed. Waiting for statefulset controller to delete.
Dec  9 07:13:42.300: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-6725
STEP: Removing pod with conflicting port in namespace statefulset-6725 12/09/22 07:13:42.3
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-6725 and will be in running state 12/09/22 07:13:42.339
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Dec  9 07:13:44.371: INFO: Deleting all statefulset in ns statefulset-6725
Dec  9 07:13:44.374: INFO: Scaling statefulset ss to 0
Dec  9 07:13:54.397: INFO: Waiting for statefulset status.replicas updated to 0
Dec  9 07:13:54.399: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Dec  9 07:13:54.426: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-6725" for this suite. 12/09/22 07:13:54.43
------------------------------
â€¢ [SLOW TEST] [14.289 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    Should recreate evicted statefulset [Conformance]
    test/e2e/apps/statefulset.go:739

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:13:40.148
    Dec  9 07:13:40.149: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename statefulset 12/09/22 07:13:40.149
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:13:40.178
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:13:40.187
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-6725 12/09/22 07:13:40.195
    [It] Should recreate evicted statefulset [Conformance]
      test/e2e/apps/statefulset.go:739
    STEP: Looking for a node to schedule stateful set and pod 12/09/22 07:13:40.203
    STEP: Creating pod with conflicting port in namespace statefulset-6725 12/09/22 07:13:40.209
    STEP: Waiting until pod test-pod will start running in namespace statefulset-6725 12/09/22 07:13:40.226
    Dec  9 07:13:40.226: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "statefulset-6725" to be "running"
    Dec  9 07:13:40.231: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.73964ms
    Dec  9 07:13:42.235: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.008581074s
    Dec  9 07:13:42.235: INFO: Pod "test-pod" satisfied condition "running"
    STEP: Creating statefulset with conflicting port in namespace statefulset-6725 12/09/22 07:13:42.235
    STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-6725 12/09/22 07:13:42.24
    Dec  9 07:13:42.260: INFO: Observed stateful pod in namespace: statefulset-6725, name: ss-0, uid: 85309b97-b2e0-4e48-80ef-516966f98dc8, status phase: Pending. Waiting for statefulset controller to delete.
    Dec  9 07:13:42.289: INFO: Observed stateful pod in namespace: statefulset-6725, name: ss-0, uid: 85309b97-b2e0-4e48-80ef-516966f98dc8, status phase: Failed. Waiting for statefulset controller to delete.
    Dec  9 07:13:42.297: INFO: Observed stateful pod in namespace: statefulset-6725, name: ss-0, uid: 85309b97-b2e0-4e48-80ef-516966f98dc8, status phase: Failed. Waiting for statefulset controller to delete.
    Dec  9 07:13:42.300: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-6725
    STEP: Removing pod with conflicting port in namespace statefulset-6725 12/09/22 07:13:42.3
    STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-6725 and will be in running state 12/09/22 07:13:42.339
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Dec  9 07:13:44.371: INFO: Deleting all statefulset in ns statefulset-6725
    Dec  9 07:13:44.374: INFO: Scaling statefulset ss to 0
    Dec  9 07:13:54.397: INFO: Waiting for statefulset status.replicas updated to 0
    Dec  9 07:13:54.399: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:13:54.426: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-6725" for this suite. 12/09/22 07:13:54.43
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:82
[BeforeEach] [sig-network] Networking
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:13:54.438
Dec  9 07:13:54.438: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename pod-network-test 12/09/22 07:13:54.441
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:13:54.46
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:13:54.465
[BeforeEach] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:31
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:82
STEP: Performing setup for networking test in namespace pod-network-test-1383 12/09/22 07:13:54.469
STEP: creating a selector 12/09/22 07:13:54.469
STEP: Creating the service pods in kubernetes 12/09/22 07:13:54.47
Dec  9 07:13:54.470: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Dec  9 07:13:54.496: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-1383" to be "running and ready"
Dec  9 07:13:54.508: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 11.634734ms
Dec  9 07:13:54.508: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Dec  9 07:13:56.513: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.015894129s
Dec  9 07:13:56.513: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Dec  9 07:13:58.512: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.015535356s
Dec  9 07:13:58.512: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Dec  9 07:14:00.513: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.016702846s
Dec  9 07:14:00.514: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Dec  9 07:14:02.513: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.01660043s
Dec  9 07:14:02.513: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Dec  9 07:14:04.513: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.016238068s
Dec  9 07:14:04.513: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Dec  9 07:14:06.512: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.015152524s
Dec  9 07:14:06.512: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Dec  9 07:14:08.514: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.016860073s
Dec  9 07:14:08.514: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Dec  9 07:14:10.514: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.017664354s
Dec  9 07:14:10.514: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Dec  9 07:14:12.515: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.018350397s
Dec  9 07:14:12.515: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Dec  9 07:14:14.512: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.015221631s
Dec  9 07:14:14.512: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Dec  9 07:14:16.512: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.015034915s
Dec  9 07:14:16.512: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Dec  9 07:14:16.512: INFO: Pod "netserver-0" satisfied condition "running and ready"
Dec  9 07:14:16.515: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-1383" to be "running and ready"
Dec  9 07:14:16.517: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 2.507074ms
Dec  9 07:14:16.517: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Dec  9 07:14:16.517: INFO: Pod "netserver-1" satisfied condition "running and ready"
STEP: Creating test pods 12/09/22 07:14:16.52
Dec  9 07:14:16.525: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-1383" to be "running"
Dec  9 07:14:16.530: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.040838ms
Dec  9 07:14:18.533: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.00790545s
Dec  9 07:14:18.533: INFO: Pod "test-container-pod" satisfied condition "running"
Dec  9 07:14:18.536: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
Dec  9 07:14:18.536: INFO: Breadth first check of 10.2.136.116 on host 10.0.10.179...
Dec  9 07:14:18.539: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.2.136.117:9080/dial?request=hostname&protocol=http&host=10.2.136.116&port=8083&tries=1'] Namespace:pod-network-test-1383 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec  9 07:14:18.539: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
Dec  9 07:14:18.539: INFO: ExecWithOptions: Clientset creation
Dec  9 07:14:18.539: INFO: ExecWithOptions: execute(POST https://10.3.0.1:443/api/v1/namespaces/pod-network-test-1383/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.2.136.117%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.2.136.116%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Dec  9 07:14:18.611: INFO: Waiting for responses: map[]
Dec  9 07:14:18.611: INFO: reached 10.2.136.116 after 0/1 tries
Dec  9 07:14:18.611: INFO: Breadth first check of 10.2.166.80 on host 10.0.17.108...
Dec  9 07:14:18.614: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.2.136.117:9080/dial?request=hostname&protocol=http&host=10.2.166.80&port=8083&tries=1'] Namespace:pod-network-test-1383 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec  9 07:14:18.614: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
Dec  9 07:14:18.615: INFO: ExecWithOptions: Clientset creation
Dec  9 07:14:18.616: INFO: ExecWithOptions: execute(POST https://10.3.0.1:443/api/v1/namespaces/pod-network-test-1383/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.2.136.117%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.2.166.80%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Dec  9 07:14:18.692: INFO: Waiting for responses: map[]
Dec  9 07:14:18.693: INFO: reached 10.2.166.80 after 0/1 tries
Dec  9 07:14:18.693: INFO: Going to retry 0 out of 2 pods....
[AfterEach] [sig-network] Networking
  test/e2e/framework/node/init/init.go:32
Dec  9 07:14:18.693: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Networking
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Networking
  tear down framework | framework.go:193
STEP: Destroying namespace "pod-network-test-1383" for this suite. 12/09/22 07:14:18.697
------------------------------
â€¢ [SLOW TEST] [24.265 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:82

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:13:54.438
    Dec  9 07:13:54.438: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename pod-network-test 12/09/22 07:13:54.441
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:13:54.46
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:13:54.465
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:31
    [It] should function for intra-pod communication: http [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:82
    STEP: Performing setup for networking test in namespace pod-network-test-1383 12/09/22 07:13:54.469
    STEP: creating a selector 12/09/22 07:13:54.469
    STEP: Creating the service pods in kubernetes 12/09/22 07:13:54.47
    Dec  9 07:13:54.470: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Dec  9 07:13:54.496: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-1383" to be "running and ready"
    Dec  9 07:13:54.508: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 11.634734ms
    Dec  9 07:13:54.508: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Dec  9 07:13:56.513: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.015894129s
    Dec  9 07:13:56.513: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Dec  9 07:13:58.512: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.015535356s
    Dec  9 07:13:58.512: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Dec  9 07:14:00.513: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.016702846s
    Dec  9 07:14:00.514: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Dec  9 07:14:02.513: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.01660043s
    Dec  9 07:14:02.513: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Dec  9 07:14:04.513: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.016238068s
    Dec  9 07:14:04.513: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Dec  9 07:14:06.512: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.015152524s
    Dec  9 07:14:06.512: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Dec  9 07:14:08.514: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.016860073s
    Dec  9 07:14:08.514: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Dec  9 07:14:10.514: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.017664354s
    Dec  9 07:14:10.514: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Dec  9 07:14:12.515: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.018350397s
    Dec  9 07:14:12.515: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Dec  9 07:14:14.512: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.015221631s
    Dec  9 07:14:14.512: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Dec  9 07:14:16.512: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.015034915s
    Dec  9 07:14:16.512: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Dec  9 07:14:16.512: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Dec  9 07:14:16.515: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-1383" to be "running and ready"
    Dec  9 07:14:16.517: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 2.507074ms
    Dec  9 07:14:16.517: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Dec  9 07:14:16.517: INFO: Pod "netserver-1" satisfied condition "running and ready"
    STEP: Creating test pods 12/09/22 07:14:16.52
    Dec  9 07:14:16.525: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-1383" to be "running"
    Dec  9 07:14:16.530: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.040838ms
    Dec  9 07:14:18.533: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.00790545s
    Dec  9 07:14:18.533: INFO: Pod "test-container-pod" satisfied condition "running"
    Dec  9 07:14:18.536: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
    Dec  9 07:14:18.536: INFO: Breadth first check of 10.2.136.116 on host 10.0.10.179...
    Dec  9 07:14:18.539: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.2.136.117:9080/dial?request=hostname&protocol=http&host=10.2.136.116&port=8083&tries=1'] Namespace:pod-network-test-1383 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Dec  9 07:14:18.539: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    Dec  9 07:14:18.539: INFO: ExecWithOptions: Clientset creation
    Dec  9 07:14:18.539: INFO: ExecWithOptions: execute(POST https://10.3.0.1:443/api/v1/namespaces/pod-network-test-1383/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.2.136.117%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.2.136.116%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Dec  9 07:14:18.611: INFO: Waiting for responses: map[]
    Dec  9 07:14:18.611: INFO: reached 10.2.136.116 after 0/1 tries
    Dec  9 07:14:18.611: INFO: Breadth first check of 10.2.166.80 on host 10.0.17.108...
    Dec  9 07:14:18.614: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.2.136.117:9080/dial?request=hostname&protocol=http&host=10.2.166.80&port=8083&tries=1'] Namespace:pod-network-test-1383 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Dec  9 07:14:18.614: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    Dec  9 07:14:18.615: INFO: ExecWithOptions: Clientset creation
    Dec  9 07:14:18.616: INFO: ExecWithOptions: execute(POST https://10.3.0.1:443/api/v1/namespaces/pod-network-test-1383/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.2.136.117%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.2.166.80%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Dec  9 07:14:18.692: INFO: Waiting for responses: map[]
    Dec  9 07:14:18.693: INFO: reached 10.2.166.80 after 0/1 tries
    Dec  9 07:14:18.693: INFO: Going to retry 0 out of 2 pods....
    [AfterEach] [sig-network] Networking
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:14:18.693: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Networking
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Networking
      tear down framework | framework.go:193
    STEP: Destroying namespace "pod-network-test-1383" for this suite. 12/09/22 07:14:18.697
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:205
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:14:18.706
Dec  9 07:14:18.706: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename secrets 12/09/22 07:14:18.71
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:14:18.726
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:14:18.729
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:205
STEP: Creating secret with name s-test-opt-del-d75eb56a-eed7-471d-9191-1b974956da95 12/09/22 07:14:18.735
STEP: Creating secret with name s-test-opt-upd-bd46f4eb-2793-4c10-8a18-c6d5d6df8271 12/09/22 07:14:18.74
STEP: Creating the pod 12/09/22 07:14:18.744
Dec  9 07:14:18.752: INFO: Waiting up to 5m0s for pod "pod-secrets-edd1ca2f-192d-46c2-b34c-85d91935eac0" in namespace "secrets-1991" to be "running and ready"
Dec  9 07:14:18.757: INFO: Pod "pod-secrets-edd1ca2f-192d-46c2-b34c-85d91935eac0": Phase="Pending", Reason="", readiness=false. Elapsed: 5.288777ms
Dec  9 07:14:18.757: INFO: The phase of Pod pod-secrets-edd1ca2f-192d-46c2-b34c-85d91935eac0 is Pending, waiting for it to be Running (with Ready = true)
Dec  9 07:14:20.762: INFO: Pod "pod-secrets-edd1ca2f-192d-46c2-b34c-85d91935eac0": Phase="Running", Reason="", readiness=true. Elapsed: 2.010092066s
Dec  9 07:14:20.762: INFO: The phase of Pod pod-secrets-edd1ca2f-192d-46c2-b34c-85d91935eac0 is Running (Ready = true)
Dec  9 07:14:20.762: INFO: Pod "pod-secrets-edd1ca2f-192d-46c2-b34c-85d91935eac0" satisfied condition "running and ready"
STEP: Deleting secret s-test-opt-del-d75eb56a-eed7-471d-9191-1b974956da95 12/09/22 07:14:20.788
STEP: Updating secret s-test-opt-upd-bd46f4eb-2793-4c10-8a18-c6d5d6df8271 12/09/22 07:14:20.794
STEP: Creating secret with name s-test-opt-create-5f2bbe02-0736-4a05-89a9-77b7b67b7bad 12/09/22 07:14:20.8
STEP: waiting to observe update in volume 12/09/22 07:14:20.804
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Dec  9 07:14:22.835: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-1991" for this suite. 12/09/22 07:14:22.838
------------------------------
â€¢ [4.138 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:205

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:14:18.706
    Dec  9 07:14:18.706: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename secrets 12/09/22 07:14:18.71
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:14:18.726
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:14:18.729
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:205
    STEP: Creating secret with name s-test-opt-del-d75eb56a-eed7-471d-9191-1b974956da95 12/09/22 07:14:18.735
    STEP: Creating secret with name s-test-opt-upd-bd46f4eb-2793-4c10-8a18-c6d5d6df8271 12/09/22 07:14:18.74
    STEP: Creating the pod 12/09/22 07:14:18.744
    Dec  9 07:14:18.752: INFO: Waiting up to 5m0s for pod "pod-secrets-edd1ca2f-192d-46c2-b34c-85d91935eac0" in namespace "secrets-1991" to be "running and ready"
    Dec  9 07:14:18.757: INFO: Pod "pod-secrets-edd1ca2f-192d-46c2-b34c-85d91935eac0": Phase="Pending", Reason="", readiness=false. Elapsed: 5.288777ms
    Dec  9 07:14:18.757: INFO: The phase of Pod pod-secrets-edd1ca2f-192d-46c2-b34c-85d91935eac0 is Pending, waiting for it to be Running (with Ready = true)
    Dec  9 07:14:20.762: INFO: Pod "pod-secrets-edd1ca2f-192d-46c2-b34c-85d91935eac0": Phase="Running", Reason="", readiness=true. Elapsed: 2.010092066s
    Dec  9 07:14:20.762: INFO: The phase of Pod pod-secrets-edd1ca2f-192d-46c2-b34c-85d91935eac0 is Running (Ready = true)
    Dec  9 07:14:20.762: INFO: Pod "pod-secrets-edd1ca2f-192d-46c2-b34c-85d91935eac0" satisfied condition "running and ready"
    STEP: Deleting secret s-test-opt-del-d75eb56a-eed7-471d-9191-1b974956da95 12/09/22 07:14:20.788
    STEP: Updating secret s-test-opt-upd-bd46f4eb-2793-4c10-8a18-c6d5d6df8271 12/09/22 07:14:20.794
    STEP: Creating secret with name s-test-opt-create-5f2bbe02-0736-4a05-89a9-77b7b67b7bad 12/09/22 07:14:20.8
    STEP: waiting to observe update in volume 12/09/22 07:14:20.804
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:14:22.835: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-1991" for this suite. 12/09/22 07:14:22.838
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] ResourceQuota
  should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:803
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:14:22.846
Dec  9 07:14:22.846: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename resourcequota 12/09/22 07:14:22.847
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:14:22.915
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:14:22.934
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:803
STEP: Creating a ResourceQuota with best effort scope 12/09/22 07:14:22.938
STEP: Ensuring ResourceQuota status is calculated 12/09/22 07:14:22.952
STEP: Creating a ResourceQuota with not best effort scope 12/09/22 07:14:24.955
STEP: Ensuring ResourceQuota status is calculated 12/09/22 07:14:24.962
STEP: Creating a best-effort pod 12/09/22 07:14:26.967
STEP: Ensuring resource quota with best effort scope captures the pod usage 12/09/22 07:14:26.977
STEP: Ensuring resource quota with not best effort ignored the pod usage 12/09/22 07:14:28.981
STEP: Deleting the pod 12/09/22 07:14:30.986
STEP: Ensuring resource quota status released the pod usage 12/09/22 07:14:31
STEP: Creating a not best-effort pod 12/09/22 07:14:33.005
STEP: Ensuring resource quota with not best effort scope captures the pod usage 12/09/22 07:14:33.02
STEP: Ensuring resource quota with best effort scope ignored the pod usage 12/09/22 07:14:35.024
STEP: Deleting the pod 12/09/22 07:14:37.028
STEP: Ensuring resource quota status released the pod usage 12/09/22 07:14:37.038
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Dec  9 07:14:39.042: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-8769" for this suite. 12/09/22 07:14:39.046
------------------------------
â€¢ [SLOW TEST] [16.207 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:803

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:14:22.846
    Dec  9 07:14:22.846: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename resourcequota 12/09/22 07:14:22.847
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:14:22.915
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:14:22.934
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should verify ResourceQuota with best effort scope. [Conformance]
      test/e2e/apimachinery/resource_quota.go:803
    STEP: Creating a ResourceQuota with best effort scope 12/09/22 07:14:22.938
    STEP: Ensuring ResourceQuota status is calculated 12/09/22 07:14:22.952
    STEP: Creating a ResourceQuota with not best effort scope 12/09/22 07:14:24.955
    STEP: Ensuring ResourceQuota status is calculated 12/09/22 07:14:24.962
    STEP: Creating a best-effort pod 12/09/22 07:14:26.967
    STEP: Ensuring resource quota with best effort scope captures the pod usage 12/09/22 07:14:26.977
    STEP: Ensuring resource quota with not best effort ignored the pod usage 12/09/22 07:14:28.981
    STEP: Deleting the pod 12/09/22 07:14:30.986
    STEP: Ensuring resource quota status released the pod usage 12/09/22 07:14:31
    STEP: Creating a not best-effort pod 12/09/22 07:14:33.005
    STEP: Ensuring resource quota with not best effort scope captures the pod usage 12/09/22 07:14:33.02
    STEP: Ensuring resource quota with best effort scope ignored the pod usage 12/09/22 07:14:35.024
    STEP: Deleting the pod 12/09/22 07:14:37.028
    STEP: Ensuring resource quota status released the pod usage 12/09/22 07:14:37.038
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:14:39.042: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-8769" for this suite. 12/09/22 07:14:39.046
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:108
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:14:39.06
Dec  9 07:14:39.061: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename disruption 12/09/22 07:14:39.061
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:14:39.087
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:14:39.091
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[It] should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:108
STEP: creating the pdb 12/09/22 07:14:39.096
STEP: Waiting for the pdb to be processed 12/09/22 07:14:39.101
STEP: updating the pdb 12/09/22 07:14:41.112
STEP: Waiting for the pdb to be processed 12/09/22 07:14:41.119
STEP: patching the pdb 12/09/22 07:14:43.126
STEP: Waiting for the pdb to be processed 12/09/22 07:14:43.139
STEP: Waiting for the pdb to be deleted 12/09/22 07:14:43.169
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Dec  9 07:14:43.180: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-9985" for this suite. 12/09/22 07:14:43.187
------------------------------
â€¢ [4.141 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:108

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:14:39.06
    Dec  9 07:14:39.061: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename disruption 12/09/22 07:14:39.061
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:14:39.087
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:14:39.091
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [It] should create a PodDisruptionBudget [Conformance]
      test/e2e/apps/disruption.go:108
    STEP: creating the pdb 12/09/22 07:14:39.096
    STEP: Waiting for the pdb to be processed 12/09/22 07:14:39.101
    STEP: updating the pdb 12/09/22 07:14:41.112
    STEP: Waiting for the pdb to be processed 12/09/22 07:14:41.119
    STEP: patching the pdb 12/09/22 07:14:43.126
    STEP: Waiting for the pdb to be processed 12/09/22 07:14:43.139
    STEP: Waiting for the pdb to be deleted 12/09/22 07:14:43.169
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:14:43.180: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-9985" for this suite. 12/09/22 07:14:43.187
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:209
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:14:43.207
Dec  9 07:14:43.207: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename webhook 12/09/22 07:14:43.208
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:14:43.239
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:14:43.246
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 12/09/22 07:14:43.264
STEP: Create role binding to let webhook read extension-apiserver-authentication 12/09/22 07:14:43.827
STEP: Deploying the webhook pod 12/09/22 07:14:43.834
STEP: Wait for the deployment to be ready 12/09/22 07:14:43.845
Dec  9 07:14:43.871: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 12/09/22 07:14:45.881
STEP: Verifying the service has paired with the endpoint 12/09/22 07:14:45.891
Dec  9 07:14:46.891: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:209
STEP: Registering the webhook via the AdmissionRegistration API 12/09/22 07:14:46.894
STEP: create a pod 12/09/22 07:14:46.909
Dec  9 07:14:46.915: INFO: Waiting up to 5m0s for pod "to-be-attached-pod" in namespace "webhook-8578" to be "running"
Dec  9 07:14:46.923: INFO: Pod "to-be-attached-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 7.649235ms
Dec  9 07:14:48.927: INFO: Pod "to-be-attached-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.011516102s
Dec  9 07:14:48.927: INFO: Pod "to-be-attached-pod" satisfied condition "running"
STEP: 'kubectl attach' the pod, should be denied by the webhook 12/09/22 07:14:48.927
Dec  9 07:14:48.927: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=webhook-8578 attach --namespace=webhook-8578 to-be-attached-pod -i -c=container1'
Dec  9 07:14:49.276: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Dec  9 07:14:49.282: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-8578" for this suite. 12/09/22 07:14:49.336
STEP: Destroying namespace "webhook-8578-markers" for this suite. 12/09/22 07:14:49.343
------------------------------
â€¢ [SLOW TEST] [6.146 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:209

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:14:43.207
    Dec  9 07:14:43.207: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename webhook 12/09/22 07:14:43.208
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:14:43.239
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:14:43.246
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 12/09/22 07:14:43.264
    STEP: Create role binding to let webhook read extension-apiserver-authentication 12/09/22 07:14:43.827
    STEP: Deploying the webhook pod 12/09/22 07:14:43.834
    STEP: Wait for the deployment to be ready 12/09/22 07:14:43.845
    Dec  9 07:14:43.871: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 12/09/22 07:14:45.881
    STEP: Verifying the service has paired with the endpoint 12/09/22 07:14:45.891
    Dec  9 07:14:46.891: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny attaching pod [Conformance]
      test/e2e/apimachinery/webhook.go:209
    STEP: Registering the webhook via the AdmissionRegistration API 12/09/22 07:14:46.894
    STEP: create a pod 12/09/22 07:14:46.909
    Dec  9 07:14:46.915: INFO: Waiting up to 5m0s for pod "to-be-attached-pod" in namespace "webhook-8578" to be "running"
    Dec  9 07:14:46.923: INFO: Pod "to-be-attached-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 7.649235ms
    Dec  9 07:14:48.927: INFO: Pod "to-be-attached-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.011516102s
    Dec  9 07:14:48.927: INFO: Pod "to-be-attached-pod" satisfied condition "running"
    STEP: 'kubectl attach' the pod, should be denied by the webhook 12/09/22 07:14:48.927
    Dec  9 07:14:48.927: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=webhook-8578 attach --namespace=webhook-8578 to-be-attached-pod -i -c=container1'
    Dec  9 07:14:49.276: INFO: rc: 1
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:14:49.282: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-8578" for this suite. 12/09/22 07:14:49.336
    STEP: Destroying namespace "webhook-8578-markers" for this suite. 12/09/22 07:14:49.343
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
  should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:14:49.353
Dec  9 07:14:49.353: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename runtimeclass 12/09/22 07:14:49.354
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:14:49.389
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:14:49.399
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It] should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129
Dec  9 07:14:49.419: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-5712 to be scheduled
Dec  9 07:14:49.424: INFO: 1 pods are not scheduled: [runtimeclass-5712/test-runtimeclass-runtimeclass-5712-preconfigured-handler-f8m84(2c2a3f0b-6452-402c-9e96-5834ca71dd59)]
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Dec  9 07:14:51.435: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-5712" for this suite. 12/09/22 07:14:51.439
------------------------------
â€¢ [2.091 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:14:49.353
    Dec  9 07:14:49.353: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename runtimeclass 12/09/22 07:14:49.354
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:14:49.389
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:14:49.399
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It] should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:129
    Dec  9 07:14:49.419: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-5712 to be scheduled
    Dec  9 07:14:49.424: INFO: 1 pods are not scheduled: [runtimeclass-5712/test-runtimeclass-runtimeclass-5712-preconfigured-handler-f8m84(2c2a3f0b-6452-402c-9e96-5834ca71dd59)]
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:14:51.435: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-5712" for this suite. 12/09/22 07:14:51.439
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-apps] ReplicationController
  should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:110
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:14:51.445
Dec  9 07:14:51.445: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename replication-controller 12/09/22 07:14:51.447
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:14:51.465
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:14:51.469
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:110
STEP: creating a ReplicationController 12/09/22 07:14:51.482
STEP: waiting for RC to be added 12/09/22 07:14:51.493
STEP: waiting for available Replicas 12/09/22 07:14:51.493
STEP: patching ReplicationController 12/09/22 07:14:52.423
STEP: waiting for RC to be modified 12/09/22 07:14:52.431
STEP: patching ReplicationController status 12/09/22 07:14:52.431
STEP: waiting for RC to be modified 12/09/22 07:14:52.436
STEP: waiting for available Replicas 12/09/22 07:14:52.436
STEP: fetching ReplicationController status 12/09/22 07:14:52.441
STEP: patching ReplicationController scale 12/09/22 07:14:52.444
STEP: waiting for RC to be modified 12/09/22 07:14:52.452
STEP: waiting for ReplicationController's scale to be the max amount 12/09/22 07:14:52.453
STEP: fetching ReplicationController; ensuring that it's patched 12/09/22 07:14:54.233
STEP: updating ReplicationController status 12/09/22 07:14:54.237
STEP: waiting for RC to be modified 12/09/22 07:14:54.25
STEP: listing all ReplicationControllers 12/09/22 07:14:54.25
STEP: checking that ReplicationController has expected values 12/09/22 07:14:54.282
STEP: deleting ReplicationControllers by collection 12/09/22 07:14:54.283
STEP: waiting for ReplicationController to have a DELETED watchEvent 12/09/22 07:14:54.293
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Dec  9 07:14:54.524: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-8837" for this suite. 12/09/22 07:14:54.54
------------------------------
â€¢ [3.146 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:110

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:14:51.445
    Dec  9 07:14:51.445: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename replication-controller 12/09/22 07:14:51.447
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:14:51.465
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:14:51.469
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should test the lifecycle of a ReplicationController [Conformance]
      test/e2e/apps/rc.go:110
    STEP: creating a ReplicationController 12/09/22 07:14:51.482
    STEP: waiting for RC to be added 12/09/22 07:14:51.493
    STEP: waiting for available Replicas 12/09/22 07:14:51.493
    STEP: patching ReplicationController 12/09/22 07:14:52.423
    STEP: waiting for RC to be modified 12/09/22 07:14:52.431
    STEP: patching ReplicationController status 12/09/22 07:14:52.431
    STEP: waiting for RC to be modified 12/09/22 07:14:52.436
    STEP: waiting for available Replicas 12/09/22 07:14:52.436
    STEP: fetching ReplicationController status 12/09/22 07:14:52.441
    STEP: patching ReplicationController scale 12/09/22 07:14:52.444
    STEP: waiting for RC to be modified 12/09/22 07:14:52.452
    STEP: waiting for ReplicationController's scale to be the max amount 12/09/22 07:14:52.453
    STEP: fetching ReplicationController; ensuring that it's patched 12/09/22 07:14:54.233
    STEP: updating ReplicationController status 12/09/22 07:14:54.237
    STEP: waiting for RC to be modified 12/09/22 07:14:54.25
    STEP: listing all ReplicationControllers 12/09/22 07:14:54.25
    STEP: checking that ReplicationController has expected values 12/09/22 07:14:54.282
    STEP: deleting ReplicationControllers by collection 12/09/22 07:14:54.283
    STEP: waiting for ReplicationController to have a DELETED watchEvent 12/09/22 07:14:54.293
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:14:54.524: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-8837" for this suite. 12/09/22 07:14:54.54
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:215
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:14:54.591
Dec  9 07:14:54.592: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename container-probe 12/09/22 07:14:54.593
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:14:54.68
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:14:54.689
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:215
STEP: Creating pod test-webserver-f02aa55c-8067-4026-9b3a-a8d2f5de002b in namespace container-probe-9607 12/09/22 07:14:54.695
Dec  9 07:14:54.710: INFO: Waiting up to 5m0s for pod "test-webserver-f02aa55c-8067-4026-9b3a-a8d2f5de002b" in namespace "container-probe-9607" to be "not pending"
Dec  9 07:14:54.734: INFO: Pod "test-webserver-f02aa55c-8067-4026-9b3a-a8d2f5de002b": Phase="Pending", Reason="", readiness=false. Elapsed: 23.411038ms
Dec  9 07:14:56.738: INFO: Pod "test-webserver-f02aa55c-8067-4026-9b3a-a8d2f5de002b": Phase="Running", Reason="", readiness=true. Elapsed: 2.028317638s
Dec  9 07:14:56.739: INFO: Pod "test-webserver-f02aa55c-8067-4026-9b3a-a8d2f5de002b" satisfied condition "not pending"
Dec  9 07:14:56.739: INFO: Started pod test-webserver-f02aa55c-8067-4026-9b3a-a8d2f5de002b in namespace container-probe-9607
STEP: checking the pod's current state and verifying that restartCount is present 12/09/22 07:14:56.739
Dec  9 07:14:56.742: INFO: Initial restart count of pod test-webserver-f02aa55c-8067-4026-9b3a-a8d2f5de002b is 0
STEP: deleting the pod 12/09/22 07:18:57.324
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Dec  9 07:18:57.346: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-9607" for this suite. 12/09/22 07:18:57.355
------------------------------
â€¢ [SLOW TEST] [242.771 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:215

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:14:54.591
    Dec  9 07:14:54.592: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename container-probe 12/09/22 07:14:54.593
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:14:54.68
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:14:54.689
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:215
    STEP: Creating pod test-webserver-f02aa55c-8067-4026-9b3a-a8d2f5de002b in namespace container-probe-9607 12/09/22 07:14:54.695
    Dec  9 07:14:54.710: INFO: Waiting up to 5m0s for pod "test-webserver-f02aa55c-8067-4026-9b3a-a8d2f5de002b" in namespace "container-probe-9607" to be "not pending"
    Dec  9 07:14:54.734: INFO: Pod "test-webserver-f02aa55c-8067-4026-9b3a-a8d2f5de002b": Phase="Pending", Reason="", readiness=false. Elapsed: 23.411038ms
    Dec  9 07:14:56.738: INFO: Pod "test-webserver-f02aa55c-8067-4026-9b3a-a8d2f5de002b": Phase="Running", Reason="", readiness=true. Elapsed: 2.028317638s
    Dec  9 07:14:56.739: INFO: Pod "test-webserver-f02aa55c-8067-4026-9b3a-a8d2f5de002b" satisfied condition "not pending"
    Dec  9 07:14:56.739: INFO: Started pod test-webserver-f02aa55c-8067-4026-9b3a-a8d2f5de002b in namespace container-probe-9607
    STEP: checking the pod's current state and verifying that restartCount is present 12/09/22 07:14:56.739
    Dec  9 07:14:56.742: INFO: Initial restart count of pod test-webserver-f02aa55c-8067-4026-9b3a-a8d2f5de002b is 0
    STEP: deleting the pod 12/09/22 07:18:57.324
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:18:57.346: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-9607" for this suite. 12/09/22 07:18:57.355
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:68
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:18:57.368
Dec  9 07:18:57.368: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename projected 12/09/22 07:18:57.369
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:18:57.394
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:18:57.403
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:68
STEP: Creating a pod to test downward API volume plugin 12/09/22 07:18:57.423
Dec  9 07:18:57.434: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f1f2497e-f1b4-4972-adaa-2b2e98485e70" in namespace "projected-4035" to be "Succeeded or Failed"
Dec  9 07:18:57.440: INFO: Pod "downwardapi-volume-f1f2497e-f1b4-4972-adaa-2b2e98485e70": Phase="Pending", Reason="", readiness=false. Elapsed: 5.30006ms
Dec  9 07:18:59.444: INFO: Pod "downwardapi-volume-f1f2497e-f1b4-4972-adaa-2b2e98485e70": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009238178s
Dec  9 07:19:01.457: INFO: Pod "downwardapi-volume-f1f2497e-f1b4-4972-adaa-2b2e98485e70": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02218799s
STEP: Saw pod success 12/09/22 07:19:01.457
Dec  9 07:19:01.457: INFO: Pod "downwardapi-volume-f1f2497e-f1b4-4972-adaa-2b2e98485e70" satisfied condition "Succeeded or Failed"
Dec  9 07:19:01.461: INFO: Trying to get logs from node ip-10-0-10-179 pod downwardapi-volume-f1f2497e-f1b4-4972-adaa-2b2e98485e70 container client-container: <nil>
STEP: delete the pod 12/09/22 07:19:01.476
Dec  9 07:19:01.516: INFO: Waiting for pod downwardapi-volume-f1f2497e-f1b4-4972-adaa-2b2e98485e70 to disappear
Dec  9 07:19:01.528: INFO: Pod downwardapi-volume-f1f2497e-f1b4-4972-adaa-2b2e98485e70 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Dec  9 07:19:01.529: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-4035" for this suite. 12/09/22 07:19:01.537
------------------------------
â€¢ [4.188 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:18:57.368
    Dec  9 07:18:57.368: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename projected 12/09/22 07:18:57.369
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:18:57.394
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:18:57.403
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:68
    STEP: Creating a pod to test downward API volume plugin 12/09/22 07:18:57.423
    Dec  9 07:18:57.434: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f1f2497e-f1b4-4972-adaa-2b2e98485e70" in namespace "projected-4035" to be "Succeeded or Failed"
    Dec  9 07:18:57.440: INFO: Pod "downwardapi-volume-f1f2497e-f1b4-4972-adaa-2b2e98485e70": Phase="Pending", Reason="", readiness=false. Elapsed: 5.30006ms
    Dec  9 07:18:59.444: INFO: Pod "downwardapi-volume-f1f2497e-f1b4-4972-adaa-2b2e98485e70": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009238178s
    Dec  9 07:19:01.457: INFO: Pod "downwardapi-volume-f1f2497e-f1b4-4972-adaa-2b2e98485e70": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02218799s
    STEP: Saw pod success 12/09/22 07:19:01.457
    Dec  9 07:19:01.457: INFO: Pod "downwardapi-volume-f1f2497e-f1b4-4972-adaa-2b2e98485e70" satisfied condition "Succeeded or Failed"
    Dec  9 07:19:01.461: INFO: Trying to get logs from node ip-10-0-10-179 pod downwardapi-volume-f1f2497e-f1b4-4972-adaa-2b2e98485e70 container client-container: <nil>
    STEP: delete the pod 12/09/22 07:19:01.476
    Dec  9 07:19:01.516: INFO: Waiting for pod downwardapi-volume-f1f2497e-f1b4-4972-adaa-2b2e98485e70 to disappear
    Dec  9 07:19:01.528: INFO: Pod downwardapi-volume-f1f2497e-f1b4-4972-adaa-2b2e98485e70 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:19:01.529: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-4035" for this suite. 12/09/22 07:19:01.537
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-apps] ReplicaSet
  Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:19:01.558
Dec  9 07:19:01.558: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename replicaset 12/09/22 07:19:01.559
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:19:01.599
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:19:01.606
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143
STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota 12/09/22 07:19:01.613
Dec  9 07:19:01.628: INFO: Pod name sample-pod: Found 0 pods out of 1
Dec  9 07:19:06.632: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 12/09/22 07:19:06.632
STEP: getting scale subresource 12/09/22 07:19:06.632
STEP: updating a scale subresource 12/09/22 07:19:06.649
STEP: verifying the replicaset Spec.Replicas was modified 12/09/22 07:19:06.677
STEP: Patch a scale subresource 12/09/22 07:19:06.729
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Dec  9 07:19:06.780: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-6204" for this suite. 12/09/22 07:19:06.79
------------------------------
â€¢ [SLOW TEST] [5.241 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:19:01.558
    Dec  9 07:19:01.558: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename replicaset 12/09/22 07:19:01.559
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:19:01.599
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:19:01.606
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] Replicaset should have a working scale subresource [Conformance]
      test/e2e/apps/replica_set.go:143
    STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota 12/09/22 07:19:01.613
    Dec  9 07:19:01.628: INFO: Pod name sample-pod: Found 0 pods out of 1
    Dec  9 07:19:06.632: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 12/09/22 07:19:06.632
    STEP: getting scale subresource 12/09/22 07:19:06.632
    STEP: updating a scale subresource 12/09/22 07:19:06.649
    STEP: verifying the replicaset Spec.Replicas was modified 12/09/22 07:19:06.677
    STEP: Patch a scale subresource 12/09/22 07:19:06.729
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:19:06.780: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-6204" for this suite. 12/09/22 07:19:06.79
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:704
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:19:06.8
Dec  9 07:19:06.800: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename sched-pred 12/09/22 07:19:06.802
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:19:06.849
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:19:06.855
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:97
Dec  9 07:19:06.870: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Dec  9 07:19:06.880: INFO: Waiting for terminating namespaces to be deleted...
Dec  9 07:19:06.885: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-10-179 before test
Dec  9 07:19:06.901: INFO: calico-node-9zhz5 from kube-system started at 2022-12-09 06:56:15 +0000 UTC (1 container statuses recorded)
Dec  9 07:19:06.901: INFO: 	Container calico-node ready: true, restart count 0
Dec  9 07:19:06.901: INFO: coredns-844594ffdd-q2bzk from kube-system started at 2022-12-09 06:56:25 +0000 UTC (1 container statuses recorded)
Dec  9 07:19:06.901: INFO: 	Container coredns ready: true, restart count 0
Dec  9 07:19:06.901: INFO: coredns-844594ffdd-rls2f from kube-system started at 2022-12-09 06:56:25 +0000 UTC (1 container statuses recorded)
Dec  9 07:19:06.901: INFO: 	Container coredns ready: true, restart count 0
Dec  9 07:19:06.901: INFO: kube-proxy-dv66w from kube-system started at 2022-12-09 06:56:15 +0000 UTC (1 container statuses recorded)
Dec  9 07:19:06.901: INFO: 	Container kube-proxy ready: true, restart count 0
Dec  9 07:19:06.901: INFO: test-rs-7hpjr from replicaset-6204 started at 2022-12-09 07:19:01 +0000 UTC (1 container statuses recorded)
Dec  9 07:19:06.901: INFO: 	Container httpd ready: true, restart count 0
Dec  9 07:19:06.901: INFO: sonobuoy-systemd-logs-daemon-set-8dafb6c50b3e4797-scljx from sonobuoy started at 2022-12-09 06:58:30 +0000 UTC (2 container statuses recorded)
Dec  9 07:19:06.901: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec  9 07:19:06.901: INFO: 	Container systemd-logs ready: true, restart count 0
Dec  9 07:19:06.901: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-17-108 before test
Dec  9 07:19:06.915: INFO: calico-node-flv5c from kube-system started at 2022-12-09 06:56:14 +0000 UTC (1 container statuses recorded)
Dec  9 07:19:06.916: INFO: 	Container calico-node ready: true, restart count 0
Dec  9 07:19:06.916: INFO: kube-proxy-r75k5 from kube-system started at 2022-12-09 06:56:14 +0000 UTC (1 container statuses recorded)
Dec  9 07:19:06.916: INFO: 	Container kube-proxy ready: true, restart count 0
Dec  9 07:19:06.916: INFO: test-rs-4ng25 from replicaset-6204 started at 2022-12-09 07:19:06 +0000 UTC (1 container statuses recorded)
Dec  9 07:19:06.916: INFO: 	Container httpd ready: false, restart count 0
Dec  9 07:19:06.916: INFO: sonobuoy from sonobuoy started at 2022-12-09 06:58:27 +0000 UTC (1 container statuses recorded)
Dec  9 07:19:06.916: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Dec  9 07:19:06.916: INFO: sonobuoy-e2e-job-023adc8283f34c8c from sonobuoy started at 2022-12-09 06:58:30 +0000 UTC (2 container statuses recorded)
Dec  9 07:19:06.916: INFO: 	Container e2e ready: true, restart count 0
Dec  9 07:19:06.916: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec  9 07:19:06.916: INFO: sonobuoy-systemd-logs-daemon-set-8dafb6c50b3e4797-jg2s9 from sonobuoy started at 2022-12-09 06:58:30 +0000 UTC (2 container statuses recorded)
Dec  9 07:19:06.916: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec  9 07:19:06.916: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:704
STEP: Trying to launch a pod without a label to get a node which can launch it. 12/09/22 07:19:06.916
Dec  9 07:19:06.925: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-2459" to be "running"
Dec  9 07:19:06.930: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 5.408539ms
Dec  9 07:19:08.934: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.008949669s
Dec  9 07:19:08.934: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 12/09/22 07:19:08.937
STEP: Trying to apply a random label on the found node. 12/09/22 07:19:08.952
STEP: verifying the node has the label kubernetes.io/e2e-5ff34025-9a73-4676-aaf6-98bb08ee620e 95 12/09/22 07:19:08.962
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled 12/09/22 07:19:08.969
Dec  9 07:19:08.977: INFO: Waiting up to 5m0s for pod "pod4" in namespace "sched-pred-2459" to be "not pending"
Dec  9 07:19:08.983: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 6.232221ms
Dec  9 07:19:10.987: INFO: Pod "pod4": Phase="Running", Reason="", readiness=true. Elapsed: 2.010402444s
Dec  9 07:19:10.987: INFO: Pod "pod4" satisfied condition "not pending"
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 10.0.10.179 on the node which pod4 resides and expect not scheduled 12/09/22 07:19:10.987
Dec  9 07:19:10.993: INFO: Waiting up to 5m0s for pod "pod5" in namespace "sched-pred-2459" to be "not pending"
Dec  9 07:19:10.997: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3.5706ms
Dec  9 07:19:13.009: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01577335s
Dec  9 07:19:15.006: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012401862s
Dec  9 07:19:17.007: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.013753519s
Dec  9 07:19:19.003: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.009833268s
Dec  9 07:19:21.003: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.010203394s
Dec  9 07:19:23.002: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.008423078s
Dec  9 07:19:25.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.010742782s
Dec  9 07:19:27.001: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 16.007493035s
Dec  9 07:19:29.001: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 18.007741249s
Dec  9 07:19:31.010: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 20.017132273s
Dec  9 07:19:33.002: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 22.00875408s
Dec  9 07:19:35.003: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 24.009711803s
Dec  9 07:19:37.001: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 26.00784425s
Dec  9 07:19:39.005: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 28.011490605s
Dec  9 07:19:41.002: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 30.009079433s
Dec  9 07:19:43.015: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 32.021330478s
Dec  9 07:19:45.002: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 34.008769883s
Dec  9 07:19:47.002: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 36.008520731s
Dec  9 07:19:49.003: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 38.009773924s
Dec  9 07:19:51.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 40.010482305s
Dec  9 07:19:53.000: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 42.006882195s
Dec  9 07:19:55.000: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 44.00711342s
Dec  9 07:19:57.002: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 46.009194638s
Dec  9 07:19:59.002: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 48.008494859s
Dec  9 07:20:01.001: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 50.008151995s
Dec  9 07:20:03.002: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 52.00930365s
Dec  9 07:20:05.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 54.010998017s
Dec  9 07:20:07.002: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 56.008986205s
Dec  9 07:20:09.002: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 58.008521712s
Dec  9 07:20:11.001: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.007693953s
Dec  9 07:20:13.002: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.008329576s
Dec  9 07:20:15.002: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.008672642s
Dec  9 07:20:17.001: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.007589996s
Dec  9 07:20:19.001: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.007770204s
Dec  9 07:20:21.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.010842763s
Dec  9 07:20:23.002: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.008366798s
Dec  9 07:20:25.002: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.009174984s
Dec  9 07:20:27.001: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.0078587s
Dec  9 07:20:29.001: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.008286641s
Dec  9 07:20:31.001: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.0080813s
Dec  9 07:20:33.003: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.009424709s
Dec  9 07:20:35.003: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.010305338s
Dec  9 07:20:37.006: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.013038991s
Dec  9 07:20:39.001: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.007335422s
Dec  9 07:20:41.002: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.008904324s
Dec  9 07:20:43.003: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.009618545s
Dec  9 07:20:45.003: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.010014574s
Dec  9 07:20:47.002: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.008406288s
Dec  9 07:20:49.002: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.008405348s
Dec  9 07:20:51.002: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.008803891s
Dec  9 07:20:53.001: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.007671953s
Dec  9 07:20:55.001: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.00793341s
Dec  9 07:20:57.000: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.007206304s
Dec  9 07:20:59.001: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.008248352s
Dec  9 07:21:01.001: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.008090338s
Dec  9 07:21:03.002: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.008530896s
Dec  9 07:21:05.003: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.010034557s
Dec  9 07:21:07.001: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.00812699s
Dec  9 07:21:09.002: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.008922766s
Dec  9 07:21:11.001: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.00781718s
Dec  9 07:21:13.002: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m2.008374793s
Dec  9 07:21:15.013: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m4.020084993s
Dec  9 07:21:17.000: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m6.00701356s
Dec  9 07:21:19.001: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m8.007820365s
Dec  9 07:21:21.001: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m10.008253222s
Dec  9 07:21:23.012: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m12.018827693s
Dec  9 07:21:25.001: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m14.008086767s
Dec  9 07:21:27.002: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m16.009145862s
Dec  9 07:21:29.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m18.010864347s
Dec  9 07:21:31.001: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m20.007610545s
Dec  9 07:21:33.001: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m22.007318721s
Dec  9 07:21:35.001: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m24.007693615s
Dec  9 07:21:37.002: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m26.008534332s
Dec  9 07:21:39.000: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m28.007026471s
Dec  9 07:21:41.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m30.0113032s
Dec  9 07:21:43.002: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m32.008855762s
Dec  9 07:21:45.001: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m34.007465158s
Dec  9 07:21:47.007: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m36.013551985s
Dec  9 07:21:49.003: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m38.010159849s
Dec  9 07:21:51.003: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m40.009411603s
Dec  9 07:21:53.002: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m42.008424987s
Dec  9 07:21:55.002: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m44.008610285s
Dec  9 07:21:57.000: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m46.006959457s
Dec  9 07:21:59.000: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m48.007185085s
Dec  9 07:22:01.001: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m50.007543914s
Dec  9 07:22:03.003: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m52.009631572s
Dec  9 07:22:05.006: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m54.013212966s
Dec  9 07:22:07.001: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m56.007862146s
Dec  9 07:22:09.002: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m58.008531264s
Dec  9 07:22:11.002: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m0.008691489s
Dec  9 07:22:13.000: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m2.006638229s
Dec  9 07:22:15.006: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m4.012858319s
Dec  9 07:22:17.001: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m6.008301909s
Dec  9 07:22:19.003: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m8.009391807s
Dec  9 07:22:21.002: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m10.009116056s
Dec  9 07:22:23.002: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m12.008717642s
Dec  9 07:22:25.001: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m14.007358541s
Dec  9 07:22:27.001: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m16.007366493s
Dec  9 07:22:29.002: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m18.009122542s
Dec  9 07:22:31.007: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m20.014084481s
Dec  9 07:22:33.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m22.010436776s
Dec  9 07:22:35.000: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m24.006846244s
Dec  9 07:22:37.000: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m26.007220201s
Dec  9 07:22:39.008: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m28.015265593s
Dec  9 07:22:41.000: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m30.007204976s
Dec  9 07:22:43.002: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m32.008428543s
Dec  9 07:22:45.002: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m34.008344766s
Dec  9 07:22:47.001: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m36.007509622s
Dec  9 07:22:49.003: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m38.00940912s
Dec  9 07:22:51.003: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m40.00987532s
Dec  9 07:22:53.002: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m42.008458229s
Dec  9 07:22:55.000: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m44.007196274s
Dec  9 07:22:57.001: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m46.007567634s
Dec  9 07:22:59.003: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m48.010254574s
Dec  9 07:23:01.001: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m50.007509196s
Dec  9 07:23:03.002: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m52.008943653s
Dec  9 07:23:05.001: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m54.007341295s
Dec  9 07:23:07.001: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m56.007989298s
Dec  9 07:23:09.001: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m58.007368178s
Dec  9 07:23:11.001: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m0.007917629s
Dec  9 07:23:13.000: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m2.007028087s
Dec  9 07:23:15.002: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m4.008367881s
Dec  9 07:23:17.000: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m6.007089926s
Dec  9 07:23:19.002: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m8.009291643s
Dec  9 07:23:21.001: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m10.007956269s
Dec  9 07:23:23.001: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m12.008043435s
Dec  9 07:23:25.007: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m14.013714558s
Dec  9 07:23:27.001: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m16.007778536s
Dec  9 07:23:29.001: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m18.007995527s
Dec  9 07:23:31.001: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m20.00753892s
Dec  9 07:23:33.003: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m22.009528182s
Dec  9 07:23:35.009: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m24.015363761s
Dec  9 07:23:37.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m26.010515585s
Dec  9 07:23:39.001: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m28.007398516s
Dec  9 07:23:41.000: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m30.007178214s
Dec  9 07:23:43.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m32.010628988s
Dec  9 07:23:45.001: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m34.008232549s
Dec  9 07:23:47.002: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m36.009035076s
Dec  9 07:23:49.001: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m38.007868307s
Dec  9 07:23:51.003: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m40.009445653s
Dec  9 07:23:53.002: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m42.008572871s
Dec  9 07:23:55.008: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m44.014492609s
Dec  9 07:23:57.003: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m46.00942064s
Dec  9 07:23:59.000: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m48.006982218s
Dec  9 07:24:01.001: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m50.007712675s
Dec  9 07:24:03.001: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m52.007897639s
Dec  9 07:24:05.002: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m54.009170273s
Dec  9 07:24:07.003: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m56.009952242s
Dec  9 07:24:09.000: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m58.007165083s
Dec  9 07:24:11.003: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.00986938s
Dec  9 07:24:11.007: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.014203739s
STEP: removing the label kubernetes.io/e2e-5ff34025-9a73-4676-aaf6-98bb08ee620e off the node ip-10-0-10-179 12/09/22 07:24:11.008
STEP: verifying the node doesn't have the label kubernetes.io/e2e-5ff34025-9a73-4676-aaf6-98bb08ee620e 12/09/22 07:24:11.023
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/node/init/init.go:32
Dec  9 07:24:11.027: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:88
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-pred-2459" for this suite. 12/09/22 07:24:11.032
------------------------------
â€¢ [SLOW TEST] [304.238 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:704

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:19:06.8
    Dec  9 07:19:06.800: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename sched-pred 12/09/22 07:19:06.802
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:19:06.849
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:19:06.855
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:97
    Dec  9 07:19:06.870: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Dec  9 07:19:06.880: INFO: Waiting for terminating namespaces to be deleted...
    Dec  9 07:19:06.885: INFO: 
    Logging pods the apiserver thinks is on node ip-10-0-10-179 before test
    Dec  9 07:19:06.901: INFO: calico-node-9zhz5 from kube-system started at 2022-12-09 06:56:15 +0000 UTC (1 container statuses recorded)
    Dec  9 07:19:06.901: INFO: 	Container calico-node ready: true, restart count 0
    Dec  9 07:19:06.901: INFO: coredns-844594ffdd-q2bzk from kube-system started at 2022-12-09 06:56:25 +0000 UTC (1 container statuses recorded)
    Dec  9 07:19:06.901: INFO: 	Container coredns ready: true, restart count 0
    Dec  9 07:19:06.901: INFO: coredns-844594ffdd-rls2f from kube-system started at 2022-12-09 06:56:25 +0000 UTC (1 container statuses recorded)
    Dec  9 07:19:06.901: INFO: 	Container coredns ready: true, restart count 0
    Dec  9 07:19:06.901: INFO: kube-proxy-dv66w from kube-system started at 2022-12-09 06:56:15 +0000 UTC (1 container statuses recorded)
    Dec  9 07:19:06.901: INFO: 	Container kube-proxy ready: true, restart count 0
    Dec  9 07:19:06.901: INFO: test-rs-7hpjr from replicaset-6204 started at 2022-12-09 07:19:01 +0000 UTC (1 container statuses recorded)
    Dec  9 07:19:06.901: INFO: 	Container httpd ready: true, restart count 0
    Dec  9 07:19:06.901: INFO: sonobuoy-systemd-logs-daemon-set-8dafb6c50b3e4797-scljx from sonobuoy started at 2022-12-09 06:58:30 +0000 UTC (2 container statuses recorded)
    Dec  9 07:19:06.901: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Dec  9 07:19:06.901: INFO: 	Container systemd-logs ready: true, restart count 0
    Dec  9 07:19:06.901: INFO: 
    Logging pods the apiserver thinks is on node ip-10-0-17-108 before test
    Dec  9 07:19:06.915: INFO: calico-node-flv5c from kube-system started at 2022-12-09 06:56:14 +0000 UTC (1 container statuses recorded)
    Dec  9 07:19:06.916: INFO: 	Container calico-node ready: true, restart count 0
    Dec  9 07:19:06.916: INFO: kube-proxy-r75k5 from kube-system started at 2022-12-09 06:56:14 +0000 UTC (1 container statuses recorded)
    Dec  9 07:19:06.916: INFO: 	Container kube-proxy ready: true, restart count 0
    Dec  9 07:19:06.916: INFO: test-rs-4ng25 from replicaset-6204 started at 2022-12-09 07:19:06 +0000 UTC (1 container statuses recorded)
    Dec  9 07:19:06.916: INFO: 	Container httpd ready: false, restart count 0
    Dec  9 07:19:06.916: INFO: sonobuoy from sonobuoy started at 2022-12-09 06:58:27 +0000 UTC (1 container statuses recorded)
    Dec  9 07:19:06.916: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Dec  9 07:19:06.916: INFO: sonobuoy-e2e-job-023adc8283f34c8c from sonobuoy started at 2022-12-09 06:58:30 +0000 UTC (2 container statuses recorded)
    Dec  9 07:19:06.916: INFO: 	Container e2e ready: true, restart count 0
    Dec  9 07:19:06.916: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Dec  9 07:19:06.916: INFO: sonobuoy-systemd-logs-daemon-set-8dafb6c50b3e4797-jg2s9 from sonobuoy started at 2022-12-09 06:58:30 +0000 UTC (2 container statuses recorded)
    Dec  9 07:19:06.916: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Dec  9 07:19:06.916: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
      test/e2e/scheduling/predicates.go:704
    STEP: Trying to launch a pod without a label to get a node which can launch it. 12/09/22 07:19:06.916
    Dec  9 07:19:06.925: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-2459" to be "running"
    Dec  9 07:19:06.930: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 5.408539ms
    Dec  9 07:19:08.934: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.008949669s
    Dec  9 07:19:08.934: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 12/09/22 07:19:08.937
    STEP: Trying to apply a random label on the found node. 12/09/22 07:19:08.952
    STEP: verifying the node has the label kubernetes.io/e2e-5ff34025-9a73-4676-aaf6-98bb08ee620e 95 12/09/22 07:19:08.962
    STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled 12/09/22 07:19:08.969
    Dec  9 07:19:08.977: INFO: Waiting up to 5m0s for pod "pod4" in namespace "sched-pred-2459" to be "not pending"
    Dec  9 07:19:08.983: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 6.232221ms
    Dec  9 07:19:10.987: INFO: Pod "pod4": Phase="Running", Reason="", readiness=true. Elapsed: 2.010402444s
    Dec  9 07:19:10.987: INFO: Pod "pod4" satisfied condition "not pending"
    STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 10.0.10.179 on the node which pod4 resides and expect not scheduled 12/09/22 07:19:10.987
    Dec  9 07:19:10.993: INFO: Waiting up to 5m0s for pod "pod5" in namespace "sched-pred-2459" to be "not pending"
    Dec  9 07:19:10.997: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3.5706ms
    Dec  9 07:19:13.009: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01577335s
    Dec  9 07:19:15.006: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012401862s
    Dec  9 07:19:17.007: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.013753519s
    Dec  9 07:19:19.003: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.009833268s
    Dec  9 07:19:21.003: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.010203394s
    Dec  9 07:19:23.002: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.008423078s
    Dec  9 07:19:25.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.010742782s
    Dec  9 07:19:27.001: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 16.007493035s
    Dec  9 07:19:29.001: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 18.007741249s
    Dec  9 07:19:31.010: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 20.017132273s
    Dec  9 07:19:33.002: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 22.00875408s
    Dec  9 07:19:35.003: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 24.009711803s
    Dec  9 07:19:37.001: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 26.00784425s
    Dec  9 07:19:39.005: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 28.011490605s
    Dec  9 07:19:41.002: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 30.009079433s
    Dec  9 07:19:43.015: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 32.021330478s
    Dec  9 07:19:45.002: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 34.008769883s
    Dec  9 07:19:47.002: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 36.008520731s
    Dec  9 07:19:49.003: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 38.009773924s
    Dec  9 07:19:51.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 40.010482305s
    Dec  9 07:19:53.000: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 42.006882195s
    Dec  9 07:19:55.000: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 44.00711342s
    Dec  9 07:19:57.002: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 46.009194638s
    Dec  9 07:19:59.002: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 48.008494859s
    Dec  9 07:20:01.001: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 50.008151995s
    Dec  9 07:20:03.002: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 52.00930365s
    Dec  9 07:20:05.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 54.010998017s
    Dec  9 07:20:07.002: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 56.008986205s
    Dec  9 07:20:09.002: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 58.008521712s
    Dec  9 07:20:11.001: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.007693953s
    Dec  9 07:20:13.002: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.008329576s
    Dec  9 07:20:15.002: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.008672642s
    Dec  9 07:20:17.001: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.007589996s
    Dec  9 07:20:19.001: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.007770204s
    Dec  9 07:20:21.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.010842763s
    Dec  9 07:20:23.002: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.008366798s
    Dec  9 07:20:25.002: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.009174984s
    Dec  9 07:20:27.001: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.0078587s
    Dec  9 07:20:29.001: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.008286641s
    Dec  9 07:20:31.001: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.0080813s
    Dec  9 07:20:33.003: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.009424709s
    Dec  9 07:20:35.003: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.010305338s
    Dec  9 07:20:37.006: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.013038991s
    Dec  9 07:20:39.001: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.007335422s
    Dec  9 07:20:41.002: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.008904324s
    Dec  9 07:20:43.003: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.009618545s
    Dec  9 07:20:45.003: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.010014574s
    Dec  9 07:20:47.002: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.008406288s
    Dec  9 07:20:49.002: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.008405348s
    Dec  9 07:20:51.002: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.008803891s
    Dec  9 07:20:53.001: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.007671953s
    Dec  9 07:20:55.001: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.00793341s
    Dec  9 07:20:57.000: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.007206304s
    Dec  9 07:20:59.001: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.008248352s
    Dec  9 07:21:01.001: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.008090338s
    Dec  9 07:21:03.002: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.008530896s
    Dec  9 07:21:05.003: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.010034557s
    Dec  9 07:21:07.001: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.00812699s
    Dec  9 07:21:09.002: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.008922766s
    Dec  9 07:21:11.001: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.00781718s
    Dec  9 07:21:13.002: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m2.008374793s
    Dec  9 07:21:15.013: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m4.020084993s
    Dec  9 07:21:17.000: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m6.00701356s
    Dec  9 07:21:19.001: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m8.007820365s
    Dec  9 07:21:21.001: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m10.008253222s
    Dec  9 07:21:23.012: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m12.018827693s
    Dec  9 07:21:25.001: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m14.008086767s
    Dec  9 07:21:27.002: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m16.009145862s
    Dec  9 07:21:29.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m18.010864347s
    Dec  9 07:21:31.001: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m20.007610545s
    Dec  9 07:21:33.001: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m22.007318721s
    Dec  9 07:21:35.001: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m24.007693615s
    Dec  9 07:21:37.002: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m26.008534332s
    Dec  9 07:21:39.000: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m28.007026471s
    Dec  9 07:21:41.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m30.0113032s
    Dec  9 07:21:43.002: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m32.008855762s
    Dec  9 07:21:45.001: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m34.007465158s
    Dec  9 07:21:47.007: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m36.013551985s
    Dec  9 07:21:49.003: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m38.010159849s
    Dec  9 07:21:51.003: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m40.009411603s
    Dec  9 07:21:53.002: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m42.008424987s
    Dec  9 07:21:55.002: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m44.008610285s
    Dec  9 07:21:57.000: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m46.006959457s
    Dec  9 07:21:59.000: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m48.007185085s
    Dec  9 07:22:01.001: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m50.007543914s
    Dec  9 07:22:03.003: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m52.009631572s
    Dec  9 07:22:05.006: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m54.013212966s
    Dec  9 07:22:07.001: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m56.007862146s
    Dec  9 07:22:09.002: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m58.008531264s
    Dec  9 07:22:11.002: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m0.008691489s
    Dec  9 07:22:13.000: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m2.006638229s
    Dec  9 07:22:15.006: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m4.012858319s
    Dec  9 07:22:17.001: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m6.008301909s
    Dec  9 07:22:19.003: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m8.009391807s
    Dec  9 07:22:21.002: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m10.009116056s
    Dec  9 07:22:23.002: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m12.008717642s
    Dec  9 07:22:25.001: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m14.007358541s
    Dec  9 07:22:27.001: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m16.007366493s
    Dec  9 07:22:29.002: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m18.009122542s
    Dec  9 07:22:31.007: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m20.014084481s
    Dec  9 07:22:33.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m22.010436776s
    Dec  9 07:22:35.000: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m24.006846244s
    Dec  9 07:22:37.000: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m26.007220201s
    Dec  9 07:22:39.008: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m28.015265593s
    Dec  9 07:22:41.000: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m30.007204976s
    Dec  9 07:22:43.002: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m32.008428543s
    Dec  9 07:22:45.002: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m34.008344766s
    Dec  9 07:22:47.001: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m36.007509622s
    Dec  9 07:22:49.003: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m38.00940912s
    Dec  9 07:22:51.003: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m40.00987532s
    Dec  9 07:22:53.002: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m42.008458229s
    Dec  9 07:22:55.000: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m44.007196274s
    Dec  9 07:22:57.001: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m46.007567634s
    Dec  9 07:22:59.003: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m48.010254574s
    Dec  9 07:23:01.001: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m50.007509196s
    Dec  9 07:23:03.002: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m52.008943653s
    Dec  9 07:23:05.001: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m54.007341295s
    Dec  9 07:23:07.001: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m56.007989298s
    Dec  9 07:23:09.001: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m58.007368178s
    Dec  9 07:23:11.001: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m0.007917629s
    Dec  9 07:23:13.000: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m2.007028087s
    Dec  9 07:23:15.002: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m4.008367881s
    Dec  9 07:23:17.000: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m6.007089926s
    Dec  9 07:23:19.002: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m8.009291643s
    Dec  9 07:23:21.001: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m10.007956269s
    Dec  9 07:23:23.001: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m12.008043435s
    Dec  9 07:23:25.007: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m14.013714558s
    Dec  9 07:23:27.001: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m16.007778536s
    Dec  9 07:23:29.001: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m18.007995527s
    Dec  9 07:23:31.001: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m20.00753892s
    Dec  9 07:23:33.003: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m22.009528182s
    Dec  9 07:23:35.009: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m24.015363761s
    Dec  9 07:23:37.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m26.010515585s
    Dec  9 07:23:39.001: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m28.007398516s
    Dec  9 07:23:41.000: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m30.007178214s
    Dec  9 07:23:43.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m32.010628988s
    Dec  9 07:23:45.001: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m34.008232549s
    Dec  9 07:23:47.002: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m36.009035076s
    Dec  9 07:23:49.001: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m38.007868307s
    Dec  9 07:23:51.003: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m40.009445653s
    Dec  9 07:23:53.002: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m42.008572871s
    Dec  9 07:23:55.008: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m44.014492609s
    Dec  9 07:23:57.003: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m46.00942064s
    Dec  9 07:23:59.000: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m48.006982218s
    Dec  9 07:24:01.001: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m50.007712675s
    Dec  9 07:24:03.001: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m52.007897639s
    Dec  9 07:24:05.002: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m54.009170273s
    Dec  9 07:24:07.003: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m56.009952242s
    Dec  9 07:24:09.000: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m58.007165083s
    Dec  9 07:24:11.003: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.00986938s
    Dec  9 07:24:11.007: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.014203739s
    STEP: removing the label kubernetes.io/e2e-5ff34025-9a73-4676-aaf6-98bb08ee620e off the node ip-10-0-10-179 12/09/22 07:24:11.008
    STEP: verifying the node doesn't have the label kubernetes.io/e2e-5ff34025-9a73-4676-aaf6-98bb08ee620e 12/09/22 07:24:11.023
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:24:11.027: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:88
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-pred-2459" for this suite. 12/09/22 07:24:11.032
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:68
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:24:11.044
Dec  9 07:24:11.044: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename secrets 12/09/22 07:24:11.045
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:24:11.076
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:24:11.081
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:68
STEP: Creating secret with name secret-test-b21e61d5-ae65-4e77-bb6f-ba8181c5bf28 12/09/22 07:24:11.086
STEP: Creating a pod to test consume secrets 12/09/22 07:24:11.111
Dec  9 07:24:11.144: INFO: Waiting up to 5m0s for pod "pod-secrets-50e9d6af-6500-4960-9aad-2910a47f09a8" in namespace "secrets-6604" to be "Succeeded or Failed"
Dec  9 07:24:11.173: INFO: Pod "pod-secrets-50e9d6af-6500-4960-9aad-2910a47f09a8": Phase="Pending", Reason="", readiness=false. Elapsed: 28.534214ms
Dec  9 07:24:13.178: INFO: Pod "pod-secrets-50e9d6af-6500-4960-9aad-2910a47f09a8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033498461s
Dec  9 07:24:15.178: INFO: Pod "pod-secrets-50e9d6af-6500-4960-9aad-2910a47f09a8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.03347585s
STEP: Saw pod success 12/09/22 07:24:15.178
Dec  9 07:24:15.179: INFO: Pod "pod-secrets-50e9d6af-6500-4960-9aad-2910a47f09a8" satisfied condition "Succeeded or Failed"
Dec  9 07:24:15.181: INFO: Trying to get logs from node ip-10-0-10-179 pod pod-secrets-50e9d6af-6500-4960-9aad-2910a47f09a8 container secret-volume-test: <nil>
STEP: delete the pod 12/09/22 07:24:15.198
Dec  9 07:24:15.212: INFO: Waiting for pod pod-secrets-50e9d6af-6500-4960-9aad-2910a47f09a8 to disappear
Dec  9 07:24:15.215: INFO: Pod pod-secrets-50e9d6af-6500-4960-9aad-2910a47f09a8 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Dec  9 07:24:15.215: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-6604" for this suite. 12/09/22 07:24:15.219
------------------------------
â€¢ [4.182 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:24:11.044
    Dec  9 07:24:11.044: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename secrets 12/09/22 07:24:11.045
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:24:11.076
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:24:11.081
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:68
    STEP: Creating secret with name secret-test-b21e61d5-ae65-4e77-bb6f-ba8181c5bf28 12/09/22 07:24:11.086
    STEP: Creating a pod to test consume secrets 12/09/22 07:24:11.111
    Dec  9 07:24:11.144: INFO: Waiting up to 5m0s for pod "pod-secrets-50e9d6af-6500-4960-9aad-2910a47f09a8" in namespace "secrets-6604" to be "Succeeded or Failed"
    Dec  9 07:24:11.173: INFO: Pod "pod-secrets-50e9d6af-6500-4960-9aad-2910a47f09a8": Phase="Pending", Reason="", readiness=false. Elapsed: 28.534214ms
    Dec  9 07:24:13.178: INFO: Pod "pod-secrets-50e9d6af-6500-4960-9aad-2910a47f09a8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033498461s
    Dec  9 07:24:15.178: INFO: Pod "pod-secrets-50e9d6af-6500-4960-9aad-2910a47f09a8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.03347585s
    STEP: Saw pod success 12/09/22 07:24:15.178
    Dec  9 07:24:15.179: INFO: Pod "pod-secrets-50e9d6af-6500-4960-9aad-2910a47f09a8" satisfied condition "Succeeded or Failed"
    Dec  9 07:24:15.181: INFO: Trying to get logs from node ip-10-0-10-179 pod pod-secrets-50e9d6af-6500-4960-9aad-2910a47f09a8 container secret-volume-test: <nil>
    STEP: delete the pod 12/09/22 07:24:15.198
    Dec  9 07:24:15.212: INFO: Waiting for pod pod-secrets-50e9d6af-6500-4960-9aad-2910a47f09a8 to disappear
    Dec  9 07:24:15.215: INFO: Pod pod-secrets-50e9d6af-6500-4960-9aad-2910a47f09a8 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:24:15.215: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-6604" for this suite. 12/09/22 07:24:15.219
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:92
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:24:15.227
Dec  9 07:24:15.227: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename var-expansion 12/09/22 07:24:15.228
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:24:15.243
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:24:15.251
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:92
STEP: Creating a pod to test substitution in container's args 12/09/22 07:24:15.255
Dec  9 07:24:15.263: INFO: Waiting up to 5m0s for pod "var-expansion-a1487dd9-777c-4cb7-b957-d57b98eef596" in namespace "var-expansion-6592" to be "Succeeded or Failed"
Dec  9 07:24:15.268: INFO: Pod "var-expansion-a1487dd9-777c-4cb7-b957-d57b98eef596": Phase="Pending", Reason="", readiness=false. Elapsed: 4.428497ms
Dec  9 07:24:17.272: INFO: Pod "var-expansion-a1487dd9-777c-4cb7-b957-d57b98eef596": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008553292s
Dec  9 07:24:19.272: INFO: Pod "var-expansion-a1487dd9-777c-4cb7-b957-d57b98eef596": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00877235s
STEP: Saw pod success 12/09/22 07:24:19.272
Dec  9 07:24:19.272: INFO: Pod "var-expansion-a1487dd9-777c-4cb7-b957-d57b98eef596" satisfied condition "Succeeded or Failed"
Dec  9 07:24:19.276: INFO: Trying to get logs from node ip-10-0-10-179 pod var-expansion-a1487dd9-777c-4cb7-b957-d57b98eef596 container dapi-container: <nil>
STEP: delete the pod 12/09/22 07:24:19.282
Dec  9 07:24:19.297: INFO: Waiting for pod var-expansion-a1487dd9-777c-4cb7-b957-d57b98eef596 to disappear
Dec  9 07:24:19.303: INFO: Pod var-expansion-a1487dd9-777c-4cb7-b957-d57b98eef596 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Dec  9 07:24:19.303: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-6592" for this suite. 12/09/22 07:24:19.307
------------------------------
â€¢ [4.086 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:24:15.227
    Dec  9 07:24:15.227: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename var-expansion 12/09/22 07:24:15.228
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:24:15.243
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:24:15.251
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow substituting values in a container's args [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:92
    STEP: Creating a pod to test substitution in container's args 12/09/22 07:24:15.255
    Dec  9 07:24:15.263: INFO: Waiting up to 5m0s for pod "var-expansion-a1487dd9-777c-4cb7-b957-d57b98eef596" in namespace "var-expansion-6592" to be "Succeeded or Failed"
    Dec  9 07:24:15.268: INFO: Pod "var-expansion-a1487dd9-777c-4cb7-b957-d57b98eef596": Phase="Pending", Reason="", readiness=false. Elapsed: 4.428497ms
    Dec  9 07:24:17.272: INFO: Pod "var-expansion-a1487dd9-777c-4cb7-b957-d57b98eef596": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008553292s
    Dec  9 07:24:19.272: INFO: Pod "var-expansion-a1487dd9-777c-4cb7-b957-d57b98eef596": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00877235s
    STEP: Saw pod success 12/09/22 07:24:19.272
    Dec  9 07:24:19.272: INFO: Pod "var-expansion-a1487dd9-777c-4cb7-b957-d57b98eef596" satisfied condition "Succeeded or Failed"
    Dec  9 07:24:19.276: INFO: Trying to get logs from node ip-10-0-10-179 pod var-expansion-a1487dd9-777c-4cb7-b957-d57b98eef596 container dapi-container: <nil>
    STEP: delete the pod 12/09/22 07:24:19.282
    Dec  9 07:24:19.297: INFO: Waiting for pod var-expansion-a1487dd9-777c-4cb7-b957-d57b98eef596 to disappear
    Dec  9 07:24:19.303: INFO: Pod var-expansion-a1487dd9-777c-4cb7-b957-d57b98eef596 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:24:19.303: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-6592" for this suite. 12/09/22 07:24:19.307
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] ConfigMap
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:93
[BeforeEach] [sig-node] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:24:19.315
Dec  9 07:24:19.315: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename configmap 12/09/22 07:24:19.317
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:24:19.336
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:24:19.341
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:93
STEP: Creating configMap configmap-1312/configmap-test-b478d9cb-0619-4158-9120-870c273f9596 12/09/22 07:24:19.352
STEP: Creating a pod to test consume configMaps 12/09/22 07:24:19.357
Dec  9 07:24:19.365: INFO: Waiting up to 5m0s for pod "pod-configmaps-a46e2a3f-5ecd-4ef8-9bf8-30dbd88e516b" in namespace "configmap-1312" to be "Succeeded or Failed"
Dec  9 07:24:19.369: INFO: Pod "pod-configmaps-a46e2a3f-5ecd-4ef8-9bf8-30dbd88e516b": Phase="Pending", Reason="", readiness=false. Elapsed: 3.81133ms
Dec  9 07:24:21.374: INFO: Pod "pod-configmaps-a46e2a3f-5ecd-4ef8-9bf8-30dbd88e516b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008635483s
Dec  9 07:24:23.375: INFO: Pod "pod-configmaps-a46e2a3f-5ecd-4ef8-9bf8-30dbd88e516b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009478252s
STEP: Saw pod success 12/09/22 07:24:23.375
Dec  9 07:24:23.375: INFO: Pod "pod-configmaps-a46e2a3f-5ecd-4ef8-9bf8-30dbd88e516b" satisfied condition "Succeeded or Failed"
Dec  9 07:24:23.380: INFO: Trying to get logs from node ip-10-0-10-179 pod pod-configmaps-a46e2a3f-5ecd-4ef8-9bf8-30dbd88e516b container env-test: <nil>
STEP: delete the pod 12/09/22 07:24:23.387
Dec  9 07:24:23.398: INFO: Waiting for pod pod-configmaps-a46e2a3f-5ecd-4ef8-9bf8-30dbd88e516b to disappear
Dec  9 07:24:23.418: INFO: Pod pod-configmaps-a46e2a3f-5ecd-4ef8-9bf8-30dbd88e516b no longer exists
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/node/init/init.go:32
Dec  9 07:24:23.418: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-1312" for this suite. 12/09/22 07:24:23.428
------------------------------
â€¢ [4.122 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:93

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:24:19.315
    Dec  9 07:24:19.315: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename configmap 12/09/22 07:24:19.317
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:24:19.336
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:24:19.341
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable via the environment [NodeConformance] [Conformance]
      test/e2e/common/node/configmap.go:93
    STEP: Creating configMap configmap-1312/configmap-test-b478d9cb-0619-4158-9120-870c273f9596 12/09/22 07:24:19.352
    STEP: Creating a pod to test consume configMaps 12/09/22 07:24:19.357
    Dec  9 07:24:19.365: INFO: Waiting up to 5m0s for pod "pod-configmaps-a46e2a3f-5ecd-4ef8-9bf8-30dbd88e516b" in namespace "configmap-1312" to be "Succeeded or Failed"
    Dec  9 07:24:19.369: INFO: Pod "pod-configmaps-a46e2a3f-5ecd-4ef8-9bf8-30dbd88e516b": Phase="Pending", Reason="", readiness=false. Elapsed: 3.81133ms
    Dec  9 07:24:21.374: INFO: Pod "pod-configmaps-a46e2a3f-5ecd-4ef8-9bf8-30dbd88e516b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008635483s
    Dec  9 07:24:23.375: INFO: Pod "pod-configmaps-a46e2a3f-5ecd-4ef8-9bf8-30dbd88e516b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009478252s
    STEP: Saw pod success 12/09/22 07:24:23.375
    Dec  9 07:24:23.375: INFO: Pod "pod-configmaps-a46e2a3f-5ecd-4ef8-9bf8-30dbd88e516b" satisfied condition "Succeeded or Failed"
    Dec  9 07:24:23.380: INFO: Trying to get logs from node ip-10-0-10-179 pod pod-configmaps-a46e2a3f-5ecd-4ef8-9bf8-30dbd88e516b container env-test: <nil>
    STEP: delete the pod 12/09/22 07:24:23.387
    Dec  9 07:24:23.398: INFO: Waiting for pod pod-configmaps-a46e2a3f-5ecd-4ef8-9bf8-30dbd88e516b to disappear
    Dec  9 07:24:23.418: INFO: Pod pod-configmaps-a46e2a3f-5ecd-4ef8-9bf8-30dbd88e516b no longer exists
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:24:23.418: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-1312" for this suite. 12/09/22 07:24:23.428
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:326
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:24:23.439
Dec  9 07:24:23.439: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename resourcequota 12/09/22 07:24:23.44
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:24:23.484
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:24:23.492
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:326
STEP: Counting existing ResourceQuota 12/09/22 07:24:40.502
STEP: Creating a ResourceQuota 12/09/22 07:24:45.505
STEP: Ensuring resource quota status is calculated 12/09/22 07:24:45.511
STEP: Creating a ConfigMap 12/09/22 07:24:47.515
STEP: Ensuring resource quota status captures configMap creation 12/09/22 07:24:47.525
STEP: Deleting a ConfigMap 12/09/22 07:24:49.535
STEP: Ensuring resource quota status released usage 12/09/22 07:24:49.543
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Dec  9 07:24:51.547: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-7067" for this suite. 12/09/22 07:24:51.551
------------------------------
â€¢ [SLOW TEST] [28.120 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:326

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:24:23.439
    Dec  9 07:24:23.439: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename resourcequota 12/09/22 07:24:23.44
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:24:23.484
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:24:23.492
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
      test/e2e/apimachinery/resource_quota.go:326
    STEP: Counting existing ResourceQuota 12/09/22 07:24:40.502
    STEP: Creating a ResourceQuota 12/09/22 07:24:45.505
    STEP: Ensuring resource quota status is calculated 12/09/22 07:24:45.511
    STEP: Creating a ConfigMap 12/09/22 07:24:47.515
    STEP: Ensuring resource quota status captures configMap creation 12/09/22 07:24:47.525
    STEP: Deleting a ConfigMap 12/09/22 07:24:49.535
    STEP: Ensuring resource quota status released usage 12/09/22 07:24:49.543
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:24:51.547: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-7067" for this suite. 12/09/22 07:24:51.551
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2191
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:24:51.561
Dec  9 07:24:51.561: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename services 12/09/22 07:24:51.562
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:24:51.6
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:24:51.606
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2191
STEP: creating service in namespace services-1036 12/09/22 07:24:51.612
STEP: creating service affinity-clusterip in namespace services-1036 12/09/22 07:24:51.612
STEP: creating replication controller affinity-clusterip in namespace services-1036 12/09/22 07:24:51.63
I1209 07:24:51.640242      20 runners.go:193] Created replication controller with name: affinity-clusterip, namespace: services-1036, replica count: 3
I1209 07:24:54.691791      20 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Dec  9 07:24:54.703: INFO: Creating new exec pod
Dec  9 07:24:54.713: INFO: Waiting up to 5m0s for pod "execpod-affinityxxnhp" in namespace "services-1036" to be "running"
Dec  9 07:24:54.727: INFO: Pod "execpod-affinityxxnhp": Phase="Pending", Reason="", readiness=false. Elapsed: 13.8035ms
Dec  9 07:24:56.730: INFO: Pod "execpod-affinityxxnhp": Phase="Running", Reason="", readiness=true. Elapsed: 2.017163553s
Dec  9 07:24:56.730: INFO: Pod "execpod-affinityxxnhp" satisfied condition "running"
Dec  9 07:24:57.732: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=services-1036 exec execpod-affinityxxnhp -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip 80'
Dec  9 07:24:57.889: INFO: stderr: "+ nc -v -z -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
Dec  9 07:24:57.889: INFO: stdout: ""
Dec  9 07:24:57.889: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=services-1036 exec execpod-affinityxxnhp -- /bin/sh -x -c nc -v -z -w 2 10.3.24.55 80'
Dec  9 07:24:58.089: INFO: stderr: "+ nc -v -z -w 2 10.3.24.55 80\nConnection to 10.3.24.55 80 port [tcp/http] succeeded!\n"
Dec  9 07:24:58.089: INFO: stdout: ""
Dec  9 07:24:58.089: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=services-1036 exec execpod-affinityxxnhp -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.3.24.55:80/ ; done'
Dec  9 07:24:58.414: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.24.55:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.24.55:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.24.55:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.24.55:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.24.55:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.24.55:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.24.55:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.24.55:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.24.55:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.24.55:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.24.55:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.24.55:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.24.55:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.24.55:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.24.55:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.24.55:80/\n"
Dec  9 07:24:58.414: INFO: stdout: "\naffinity-clusterip-z5tmz\naffinity-clusterip-z5tmz\naffinity-clusterip-z5tmz\naffinity-clusterip-z5tmz\naffinity-clusterip-z5tmz\naffinity-clusterip-z5tmz\naffinity-clusterip-z5tmz\naffinity-clusterip-z5tmz\naffinity-clusterip-z5tmz\naffinity-clusterip-z5tmz\naffinity-clusterip-z5tmz\naffinity-clusterip-z5tmz\naffinity-clusterip-z5tmz\naffinity-clusterip-z5tmz\naffinity-clusterip-z5tmz\naffinity-clusterip-z5tmz"
Dec  9 07:24:58.414: INFO: Received response from host: affinity-clusterip-z5tmz
Dec  9 07:24:58.414: INFO: Received response from host: affinity-clusterip-z5tmz
Dec  9 07:24:58.414: INFO: Received response from host: affinity-clusterip-z5tmz
Dec  9 07:24:58.414: INFO: Received response from host: affinity-clusterip-z5tmz
Dec  9 07:24:58.414: INFO: Received response from host: affinity-clusterip-z5tmz
Dec  9 07:24:58.414: INFO: Received response from host: affinity-clusterip-z5tmz
Dec  9 07:24:58.414: INFO: Received response from host: affinity-clusterip-z5tmz
Dec  9 07:24:58.414: INFO: Received response from host: affinity-clusterip-z5tmz
Dec  9 07:24:58.414: INFO: Received response from host: affinity-clusterip-z5tmz
Dec  9 07:24:58.414: INFO: Received response from host: affinity-clusterip-z5tmz
Dec  9 07:24:58.414: INFO: Received response from host: affinity-clusterip-z5tmz
Dec  9 07:24:58.414: INFO: Received response from host: affinity-clusterip-z5tmz
Dec  9 07:24:58.414: INFO: Received response from host: affinity-clusterip-z5tmz
Dec  9 07:24:58.414: INFO: Received response from host: affinity-clusterip-z5tmz
Dec  9 07:24:58.414: INFO: Received response from host: affinity-clusterip-z5tmz
Dec  9 07:24:58.414: INFO: Received response from host: affinity-clusterip-z5tmz
Dec  9 07:24:58.414: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip in namespace services-1036, will wait for the garbage collector to delete the pods 12/09/22 07:24:58.426
Dec  9 07:24:58.488: INFO: Deleting ReplicationController affinity-clusterip took: 7.758563ms
Dec  9 07:24:58.588: INFO: Terminating ReplicationController affinity-clusterip pods took: 100.565783ms
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Dec  9 07:25:00.828: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-1036" for this suite. 12/09/22 07:25:00.837
------------------------------
â€¢ [SLOW TEST] [9.282 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2191

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:24:51.561
    Dec  9 07:24:51.561: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename services 12/09/22 07:24:51.562
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:24:51.6
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:24:51.606
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2191
    STEP: creating service in namespace services-1036 12/09/22 07:24:51.612
    STEP: creating service affinity-clusterip in namespace services-1036 12/09/22 07:24:51.612
    STEP: creating replication controller affinity-clusterip in namespace services-1036 12/09/22 07:24:51.63
    I1209 07:24:51.640242      20 runners.go:193] Created replication controller with name: affinity-clusterip, namespace: services-1036, replica count: 3
    I1209 07:24:54.691791      20 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Dec  9 07:24:54.703: INFO: Creating new exec pod
    Dec  9 07:24:54.713: INFO: Waiting up to 5m0s for pod "execpod-affinityxxnhp" in namespace "services-1036" to be "running"
    Dec  9 07:24:54.727: INFO: Pod "execpod-affinityxxnhp": Phase="Pending", Reason="", readiness=false. Elapsed: 13.8035ms
    Dec  9 07:24:56.730: INFO: Pod "execpod-affinityxxnhp": Phase="Running", Reason="", readiness=true. Elapsed: 2.017163553s
    Dec  9 07:24:56.730: INFO: Pod "execpod-affinityxxnhp" satisfied condition "running"
    Dec  9 07:24:57.732: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=services-1036 exec execpod-affinityxxnhp -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip 80'
    Dec  9 07:24:57.889: INFO: stderr: "+ nc -v -z -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
    Dec  9 07:24:57.889: INFO: stdout: ""
    Dec  9 07:24:57.889: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=services-1036 exec execpod-affinityxxnhp -- /bin/sh -x -c nc -v -z -w 2 10.3.24.55 80'
    Dec  9 07:24:58.089: INFO: stderr: "+ nc -v -z -w 2 10.3.24.55 80\nConnection to 10.3.24.55 80 port [tcp/http] succeeded!\n"
    Dec  9 07:24:58.089: INFO: stdout: ""
    Dec  9 07:24:58.089: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=services-1036 exec execpod-affinityxxnhp -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.3.24.55:80/ ; done'
    Dec  9 07:24:58.414: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.24.55:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.24.55:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.24.55:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.24.55:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.24.55:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.24.55:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.24.55:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.24.55:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.24.55:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.24.55:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.24.55:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.24.55:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.24.55:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.24.55:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.24.55:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.24.55:80/\n"
    Dec  9 07:24:58.414: INFO: stdout: "\naffinity-clusterip-z5tmz\naffinity-clusterip-z5tmz\naffinity-clusterip-z5tmz\naffinity-clusterip-z5tmz\naffinity-clusterip-z5tmz\naffinity-clusterip-z5tmz\naffinity-clusterip-z5tmz\naffinity-clusterip-z5tmz\naffinity-clusterip-z5tmz\naffinity-clusterip-z5tmz\naffinity-clusterip-z5tmz\naffinity-clusterip-z5tmz\naffinity-clusterip-z5tmz\naffinity-clusterip-z5tmz\naffinity-clusterip-z5tmz\naffinity-clusterip-z5tmz"
    Dec  9 07:24:58.414: INFO: Received response from host: affinity-clusterip-z5tmz
    Dec  9 07:24:58.414: INFO: Received response from host: affinity-clusterip-z5tmz
    Dec  9 07:24:58.414: INFO: Received response from host: affinity-clusterip-z5tmz
    Dec  9 07:24:58.414: INFO: Received response from host: affinity-clusterip-z5tmz
    Dec  9 07:24:58.414: INFO: Received response from host: affinity-clusterip-z5tmz
    Dec  9 07:24:58.414: INFO: Received response from host: affinity-clusterip-z5tmz
    Dec  9 07:24:58.414: INFO: Received response from host: affinity-clusterip-z5tmz
    Dec  9 07:24:58.414: INFO: Received response from host: affinity-clusterip-z5tmz
    Dec  9 07:24:58.414: INFO: Received response from host: affinity-clusterip-z5tmz
    Dec  9 07:24:58.414: INFO: Received response from host: affinity-clusterip-z5tmz
    Dec  9 07:24:58.414: INFO: Received response from host: affinity-clusterip-z5tmz
    Dec  9 07:24:58.414: INFO: Received response from host: affinity-clusterip-z5tmz
    Dec  9 07:24:58.414: INFO: Received response from host: affinity-clusterip-z5tmz
    Dec  9 07:24:58.414: INFO: Received response from host: affinity-clusterip-z5tmz
    Dec  9 07:24:58.414: INFO: Received response from host: affinity-clusterip-z5tmz
    Dec  9 07:24:58.414: INFO: Received response from host: affinity-clusterip-z5tmz
    Dec  9 07:24:58.414: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-clusterip in namespace services-1036, will wait for the garbage collector to delete the pods 12/09/22 07:24:58.426
    Dec  9 07:24:58.488: INFO: Deleting ReplicationController affinity-clusterip took: 7.758563ms
    Dec  9 07:24:58.588: INFO: Terminating ReplicationController affinity-clusterip pods took: 100.565783ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:25:00.828: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-1036" for this suite. 12/09/22 07:25:00.837
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Containers
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:87
[BeforeEach] [sig-node] Containers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:25:00.846
Dec  9 07:25:00.847: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename containers 12/09/22 07:25:00.848
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:25:00.874
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:25:00.879
[BeforeEach] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:87
STEP: Creating a pod to test override all 12/09/22 07:25:00.886
Dec  9 07:25:00.896: INFO: Waiting up to 5m0s for pod "client-containers-30729bdb-033c-4848-8dc8-86631bf49b88" in namespace "containers-1278" to be "Succeeded or Failed"
Dec  9 07:25:00.900: INFO: Pod "client-containers-30729bdb-033c-4848-8dc8-86631bf49b88": Phase="Pending", Reason="", readiness=false. Elapsed: 3.924972ms
Dec  9 07:25:02.904: INFO: Pod "client-containers-30729bdb-033c-4848-8dc8-86631bf49b88": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008397811s
Dec  9 07:25:04.905: INFO: Pod "client-containers-30729bdb-033c-4848-8dc8-86631bf49b88": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008877s
STEP: Saw pod success 12/09/22 07:25:04.905
Dec  9 07:25:04.905: INFO: Pod "client-containers-30729bdb-033c-4848-8dc8-86631bf49b88" satisfied condition "Succeeded or Failed"
Dec  9 07:25:04.908: INFO: Trying to get logs from node ip-10-0-10-179 pod client-containers-30729bdb-033c-4848-8dc8-86631bf49b88 container agnhost-container: <nil>
STEP: delete the pod 12/09/22 07:25:04.914
Dec  9 07:25:04.927: INFO: Waiting for pod client-containers-30729bdb-033c-4848-8dc8-86631bf49b88 to disappear
Dec  9 07:25:04.931: INFO: Pod client-containers-30729bdb-033c-4848-8dc8-86631bf49b88 no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/node/init/init.go:32
Dec  9 07:25:04.931: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Containers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Containers
  tear down framework | framework.go:193
STEP: Destroying namespace "containers-1278" for this suite. 12/09/22 07:25:04.936
------------------------------
â€¢ [4.095 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:87

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:25:00.846
    Dec  9 07:25:00.847: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename containers 12/09/22 07:25:00.848
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:25:00.874
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:25:00.879
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:87
    STEP: Creating a pod to test override all 12/09/22 07:25:00.886
    Dec  9 07:25:00.896: INFO: Waiting up to 5m0s for pod "client-containers-30729bdb-033c-4848-8dc8-86631bf49b88" in namespace "containers-1278" to be "Succeeded or Failed"
    Dec  9 07:25:00.900: INFO: Pod "client-containers-30729bdb-033c-4848-8dc8-86631bf49b88": Phase="Pending", Reason="", readiness=false. Elapsed: 3.924972ms
    Dec  9 07:25:02.904: INFO: Pod "client-containers-30729bdb-033c-4848-8dc8-86631bf49b88": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008397811s
    Dec  9 07:25:04.905: INFO: Pod "client-containers-30729bdb-033c-4848-8dc8-86631bf49b88": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008877s
    STEP: Saw pod success 12/09/22 07:25:04.905
    Dec  9 07:25:04.905: INFO: Pod "client-containers-30729bdb-033c-4848-8dc8-86631bf49b88" satisfied condition "Succeeded or Failed"
    Dec  9 07:25:04.908: INFO: Trying to get logs from node ip-10-0-10-179 pod client-containers-30729bdb-033c-4848-8dc8-86631bf49b88 container agnhost-container: <nil>
    STEP: delete the pod 12/09/22 07:25:04.914
    Dec  9 07:25:04.927: INFO: Waiting for pod client-containers-30729bdb-033c-4848-8dc8-86631bf49b88 to disappear
    Dec  9 07:25:04.931: INFO: Pod client-containers-30729bdb-033c-4848-8dc8-86631bf49b88 no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:25:04.931: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Containers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Containers
      tear down framework | framework.go:193
    STEP: Destroying namespace "containers-1278" for this suite. 12/09/22 07:25:04.936
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:25:04.943
Dec  9 07:25:04.944: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename gc 12/09/22 07:25:04.946
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:25:04.976
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:25:04.98
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491
STEP: create the deployment 12/09/22 07:25:04.984
STEP: Wait for the Deployment to create new ReplicaSet 12/09/22 07:25:04.991
STEP: delete the deployment 12/09/22 07:25:05.505
STEP: wait for all rs to be garbage collected 12/09/22 07:25:05.513
STEP: expected 0 rs, got 1 rs 12/09/22 07:25:05.524
STEP: expected 0 pods, got 2 pods 12/09/22 07:25:05.542
STEP: Gathering metrics 12/09/22 07:25:06.051
Dec  9 07:25:06.072: INFO: Waiting up to 5m0s for pod "kube-controller-manager-ip-10-0-7-84" in namespace "kube-system" to be "running and ready"
Dec  9 07:25:06.077: INFO: Pod "kube-controller-manager-ip-10-0-7-84": Phase="Running", Reason="", readiness=true. Elapsed: 5.237102ms
Dec  9 07:25:06.077: INFO: The phase of Pod kube-controller-manager-ip-10-0-7-84 is Running (Ready = true)
Dec  9 07:25:06.078: INFO: Pod "kube-controller-manager-ip-10-0-7-84" satisfied condition "running and ready"
Dec  9 07:25:06.160: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Dec  9 07:25:06.160: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-9335" for this suite. 12/09/22 07:25:06.164
------------------------------
â€¢ [1.234 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:25:04.943
    Dec  9 07:25:04.944: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename gc 12/09/22 07:25:04.946
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:25:04.976
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:25:04.98
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete RS created by deployment when not orphaning [Conformance]
      test/e2e/apimachinery/garbage_collector.go:491
    STEP: create the deployment 12/09/22 07:25:04.984
    STEP: Wait for the Deployment to create new ReplicaSet 12/09/22 07:25:04.991
    STEP: delete the deployment 12/09/22 07:25:05.505
    STEP: wait for all rs to be garbage collected 12/09/22 07:25:05.513
    STEP: expected 0 rs, got 1 rs 12/09/22 07:25:05.524
    STEP: expected 0 pods, got 2 pods 12/09/22 07:25:05.542
    STEP: Gathering metrics 12/09/22 07:25:06.051
    Dec  9 07:25:06.072: INFO: Waiting up to 5m0s for pod "kube-controller-manager-ip-10-0-7-84" in namespace "kube-system" to be "running and ready"
    Dec  9 07:25:06.077: INFO: Pod "kube-controller-manager-ip-10-0-7-84": Phase="Running", Reason="", readiness=true. Elapsed: 5.237102ms
    Dec  9 07:25:06.077: INFO: The phase of Pod kube-controller-manager-ip-10-0-7-84 is Running (Ready = true)
    Dec  9 07:25:06.078: INFO: Pod "kube-controller-manager-ip-10-0-7-84" satisfied condition "running and ready"
    Dec  9 07:25:06.160: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:25:06.160: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-9335" for this suite. 12/09/22 07:25:06.164
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Probing container
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:135
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:25:06.179
Dec  9 07:25:06.179: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename container-probe 12/09/22 07:25:06.179
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:25:06.243
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:25:06.263
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:135
STEP: Creating pod busybox-98293db5-56f6-460d-ab5e-7f494ca606a0 in namespace container-probe-6474 12/09/22 07:25:06.268
Dec  9 07:25:06.278: INFO: Waiting up to 5m0s for pod "busybox-98293db5-56f6-460d-ab5e-7f494ca606a0" in namespace "container-probe-6474" to be "not pending"
Dec  9 07:25:06.284: INFO: Pod "busybox-98293db5-56f6-460d-ab5e-7f494ca606a0": Phase="Pending", Reason="", readiness=false. Elapsed: 5.949436ms
Dec  9 07:25:08.289: INFO: Pod "busybox-98293db5-56f6-460d-ab5e-7f494ca606a0": Phase="Running", Reason="", readiness=true. Elapsed: 2.011297399s
Dec  9 07:25:08.289: INFO: Pod "busybox-98293db5-56f6-460d-ab5e-7f494ca606a0" satisfied condition "not pending"
Dec  9 07:25:08.289: INFO: Started pod busybox-98293db5-56f6-460d-ab5e-7f494ca606a0 in namespace container-probe-6474
STEP: checking the pod's current state and verifying that restartCount is present 12/09/22 07:25:08.289
Dec  9 07:25:08.293: INFO: Initial restart count of pod busybox-98293db5-56f6-460d-ab5e-7f494ca606a0 is 0
Dec  9 07:25:58.434: INFO: Restart count of pod container-probe-6474/busybox-98293db5-56f6-460d-ab5e-7f494ca606a0 is now 1 (50.140965889s elapsed)
STEP: deleting the pod 12/09/22 07:25:58.434
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Dec  9 07:25:58.447: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-6474" for this suite. 12/09/22 07:25:58.452
------------------------------
â€¢ [SLOW TEST] [52.282 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:135

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:25:06.179
    Dec  9 07:25:06.179: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename container-probe 12/09/22 07:25:06.179
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:25:06.243
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:25:06.263
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:135
    STEP: Creating pod busybox-98293db5-56f6-460d-ab5e-7f494ca606a0 in namespace container-probe-6474 12/09/22 07:25:06.268
    Dec  9 07:25:06.278: INFO: Waiting up to 5m0s for pod "busybox-98293db5-56f6-460d-ab5e-7f494ca606a0" in namespace "container-probe-6474" to be "not pending"
    Dec  9 07:25:06.284: INFO: Pod "busybox-98293db5-56f6-460d-ab5e-7f494ca606a0": Phase="Pending", Reason="", readiness=false. Elapsed: 5.949436ms
    Dec  9 07:25:08.289: INFO: Pod "busybox-98293db5-56f6-460d-ab5e-7f494ca606a0": Phase="Running", Reason="", readiness=true. Elapsed: 2.011297399s
    Dec  9 07:25:08.289: INFO: Pod "busybox-98293db5-56f6-460d-ab5e-7f494ca606a0" satisfied condition "not pending"
    Dec  9 07:25:08.289: INFO: Started pod busybox-98293db5-56f6-460d-ab5e-7f494ca606a0 in namespace container-probe-6474
    STEP: checking the pod's current state and verifying that restartCount is present 12/09/22 07:25:08.289
    Dec  9 07:25:08.293: INFO: Initial restart count of pod busybox-98293db5-56f6-460d-ab5e-7f494ca606a0 is 0
    Dec  9 07:25:58.434: INFO: Restart count of pod container-probe-6474/busybox-98293db5-56f6-460d-ab5e-7f494ca606a0 is now 1 (50.140965889s elapsed)
    STEP: deleting the pod 12/09/22 07:25:58.434
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:25:58.447: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-6474" for this suite. 12/09/22 07:25:58.452
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should get and update a ReplicationController scale [Conformance]
  test/e2e/apps/rc.go:402
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:25:58.463
Dec  9 07:25:58.463: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename replication-controller 12/09/22 07:25:58.464
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:25:58.498
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:25:58.508
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should get and update a ReplicationController scale [Conformance]
  test/e2e/apps/rc.go:402
STEP: Creating ReplicationController "e2e-rc-nvhv2" 12/09/22 07:25:58.513
Dec  9 07:25:58.520: INFO: Get Replication Controller "e2e-rc-nvhv2" to confirm replicas
Dec  9 07:25:59.559: INFO: Get Replication Controller "e2e-rc-nvhv2" to confirm replicas
Dec  9 07:25:59.563: INFO: Found 1 replicas for "e2e-rc-nvhv2" replication controller
STEP: Getting scale subresource for ReplicationController "e2e-rc-nvhv2" 12/09/22 07:25:59.563
STEP: Updating a scale subresource 12/09/22 07:25:59.566
STEP: Verifying replicas where modified for replication controller "e2e-rc-nvhv2" 12/09/22 07:25:59.572
Dec  9 07:25:59.572: INFO: Get Replication Controller "e2e-rc-nvhv2" to confirm replicas
Dec  9 07:26:00.576: INFO: Get Replication Controller "e2e-rc-nvhv2" to confirm replicas
Dec  9 07:26:00.583: INFO: Found 2 replicas for "e2e-rc-nvhv2" replication controller
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Dec  9 07:26:00.583: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-2955" for this suite. 12/09/22 07:26:00.592
------------------------------
â€¢ [2.146 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should get and update a ReplicationController scale [Conformance]
  test/e2e/apps/rc.go:402

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:25:58.463
    Dec  9 07:25:58.463: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename replication-controller 12/09/22 07:25:58.464
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:25:58.498
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:25:58.508
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should get and update a ReplicationController scale [Conformance]
      test/e2e/apps/rc.go:402
    STEP: Creating ReplicationController "e2e-rc-nvhv2" 12/09/22 07:25:58.513
    Dec  9 07:25:58.520: INFO: Get Replication Controller "e2e-rc-nvhv2" to confirm replicas
    Dec  9 07:25:59.559: INFO: Get Replication Controller "e2e-rc-nvhv2" to confirm replicas
    Dec  9 07:25:59.563: INFO: Found 1 replicas for "e2e-rc-nvhv2" replication controller
    STEP: Getting scale subresource for ReplicationController "e2e-rc-nvhv2" 12/09/22 07:25:59.563
    STEP: Updating a scale subresource 12/09/22 07:25:59.566
    STEP: Verifying replicas where modified for replication controller "e2e-rc-nvhv2" 12/09/22 07:25:59.572
    Dec  9 07:25:59.572: INFO: Get Replication Controller "e2e-rc-nvhv2" to confirm replicas
    Dec  9 07:26:00.576: INFO: Get Replication Controller "e2e-rc-nvhv2" to confirm replicas
    Dec  9 07:26:00.583: INFO: Found 2 replicas for "e2e-rc-nvhv2" replication controller
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:26:00.583: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-2955" for this suite. 12/09/22 07:26:00.592
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:101
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:26:00.617
Dec  9 07:26:00.617: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename replication-controller 12/09/22 07:26:00.618
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:26:00.667
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:26:00.68
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:101
STEP: Given a ReplicationController is created 12/09/22 07:26:00.688
STEP: When the matched label of one of its pods change 12/09/22 07:26:00.696
Dec  9 07:26:00.705: INFO: Pod name pod-release: Found 0 pods out of 1
Dec  9 07:26:05.718: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released 12/09/22 07:26:05.735
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Dec  9 07:26:06.745: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-5708" for this suite. 12/09/22 07:26:06.753
------------------------------
â€¢ [SLOW TEST] [6.145 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:101

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:26:00.617
    Dec  9 07:26:00.617: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename replication-controller 12/09/22 07:26:00.618
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:26:00.667
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:26:00.68
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should release no longer matching pods [Conformance]
      test/e2e/apps/rc.go:101
    STEP: Given a ReplicationController is created 12/09/22 07:26:00.688
    STEP: When the matched label of one of its pods change 12/09/22 07:26:00.696
    Dec  9 07:26:00.705: INFO: Pod name pod-release: Found 0 pods out of 1
    Dec  9 07:26:05.718: INFO: Pod name pod-release: Found 1 pods out of 1
    STEP: Then the pod is released 12/09/22 07:26:05.735
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:26:06.745: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-5708" for this suite. 12/09/22 07:26:06.753
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should have a working scale subresource [Conformance]
  test/e2e/apps/statefulset.go:848
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:26:06.761
Dec  9 07:26:06.762: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename statefulset 12/09/22 07:26:06.763
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:26:06.793
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:26:06.81
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-8065 12/09/22 07:26:06.816
[It] should have a working scale subresource [Conformance]
  test/e2e/apps/statefulset.go:848
STEP: Creating statefulset ss in namespace statefulset-8065 12/09/22 07:26:06.827
Dec  9 07:26:06.857: INFO: Found 0 stateful pods, waiting for 1
Dec  9 07:26:16.861: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource 12/09/22 07:26:16.866
STEP: updating a scale subresource 12/09/22 07:26:16.869
STEP: verifying the statefulset Spec.Replicas was modified 12/09/22 07:26:16.874
STEP: Patch a scale subresource 12/09/22 07:26:16.877
STEP: verifying the statefulset Spec.Replicas was modified 12/09/22 07:26:16.885
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Dec  9 07:26:16.900: INFO: Deleting all statefulset in ns statefulset-8065
Dec  9 07:26:16.907: INFO: Scaling statefulset ss to 0
Dec  9 07:26:26.923: INFO: Waiting for statefulset status.replicas updated to 0
Dec  9 07:26:26.926: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Dec  9 07:26:26.936: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-8065" for this suite. 12/09/22 07:26:26.942
------------------------------
â€¢ [SLOW TEST] [20.186 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should have a working scale subresource [Conformance]
    test/e2e/apps/statefulset.go:848

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:26:06.761
    Dec  9 07:26:06.762: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename statefulset 12/09/22 07:26:06.763
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:26:06.793
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:26:06.81
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-8065 12/09/22 07:26:06.816
    [It] should have a working scale subresource [Conformance]
      test/e2e/apps/statefulset.go:848
    STEP: Creating statefulset ss in namespace statefulset-8065 12/09/22 07:26:06.827
    Dec  9 07:26:06.857: INFO: Found 0 stateful pods, waiting for 1
    Dec  9 07:26:16.861: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: getting scale subresource 12/09/22 07:26:16.866
    STEP: updating a scale subresource 12/09/22 07:26:16.869
    STEP: verifying the statefulset Spec.Replicas was modified 12/09/22 07:26:16.874
    STEP: Patch a scale subresource 12/09/22 07:26:16.877
    STEP: verifying the statefulset Spec.Replicas was modified 12/09/22 07:26:16.885
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Dec  9 07:26:16.900: INFO: Deleting all statefulset in ns statefulset-8065
    Dec  9 07:26:16.907: INFO: Scaling statefulset ss to 0
    Dec  9 07:26:26.923: INFO: Waiting for statefulset status.replicas updated to 0
    Dec  9 07:26:26.926: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:26:26.936: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-8065" for this suite. 12/09/22 07:26:26.942
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:124
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:26:26.947
Dec  9 07:26:26.948: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename projected 12/09/22 07:26:26.949
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:26:26.962
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:26:26.968
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:124
STEP: Creating projection with configMap that has name projected-configmap-test-upd-3a2c6a6d-9364-4ea3-a572-ba3ae844ffe9 12/09/22 07:26:26.975
STEP: Creating the pod 12/09/22 07:26:26.978
Dec  9 07:26:26.985: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-79a6b5ce-86f9-4daa-b30b-d531be5e07bb" in namespace "projected-892" to be "running and ready"
Dec  9 07:26:26.989: INFO: Pod "pod-projected-configmaps-79a6b5ce-86f9-4daa-b30b-d531be5e07bb": Phase="Pending", Reason="", readiness=false. Elapsed: 3.415132ms
Dec  9 07:26:26.989: INFO: The phase of Pod pod-projected-configmaps-79a6b5ce-86f9-4daa-b30b-d531be5e07bb is Pending, waiting for it to be Running (with Ready = true)
Dec  9 07:26:28.993: INFO: Pod "pod-projected-configmaps-79a6b5ce-86f9-4daa-b30b-d531be5e07bb": Phase="Running", Reason="", readiness=true. Elapsed: 2.007868708s
Dec  9 07:26:28.993: INFO: The phase of Pod pod-projected-configmaps-79a6b5ce-86f9-4daa-b30b-d531be5e07bb is Running (Ready = true)
Dec  9 07:26:28.993: INFO: Pod "pod-projected-configmaps-79a6b5ce-86f9-4daa-b30b-d531be5e07bb" satisfied condition "running and ready"
STEP: Updating configmap projected-configmap-test-upd-3a2c6a6d-9364-4ea3-a572-ba3ae844ffe9 12/09/22 07:26:29.004
STEP: waiting to observe update in volume 12/09/22 07:26:29.015
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Dec  9 07:26:31.029: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-892" for this suite. 12/09/22 07:26:31.034
------------------------------
â€¢ [4.096 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:26:26.947
    Dec  9 07:26:26.948: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename projected 12/09/22 07:26:26.949
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:26:26.962
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:26:26.968
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:124
    STEP: Creating projection with configMap that has name projected-configmap-test-upd-3a2c6a6d-9364-4ea3-a572-ba3ae844ffe9 12/09/22 07:26:26.975
    STEP: Creating the pod 12/09/22 07:26:26.978
    Dec  9 07:26:26.985: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-79a6b5ce-86f9-4daa-b30b-d531be5e07bb" in namespace "projected-892" to be "running and ready"
    Dec  9 07:26:26.989: INFO: Pod "pod-projected-configmaps-79a6b5ce-86f9-4daa-b30b-d531be5e07bb": Phase="Pending", Reason="", readiness=false. Elapsed: 3.415132ms
    Dec  9 07:26:26.989: INFO: The phase of Pod pod-projected-configmaps-79a6b5ce-86f9-4daa-b30b-d531be5e07bb is Pending, waiting for it to be Running (with Ready = true)
    Dec  9 07:26:28.993: INFO: Pod "pod-projected-configmaps-79a6b5ce-86f9-4daa-b30b-d531be5e07bb": Phase="Running", Reason="", readiness=true. Elapsed: 2.007868708s
    Dec  9 07:26:28.993: INFO: The phase of Pod pod-projected-configmaps-79a6b5ce-86f9-4daa-b30b-d531be5e07bb is Running (Ready = true)
    Dec  9 07:26:28.993: INFO: Pod "pod-projected-configmaps-79a6b5ce-86f9-4daa-b30b-d531be5e07bb" satisfied condition "running and ready"
    STEP: Updating configmap projected-configmap-test-upd-3a2c6a6d-9364-4ea3-a572-ba3ae844ffe9 12/09/22 07:26:29.004
    STEP: waiting to observe update in volume 12/09/22 07:26:29.015
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:26:31.029: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-892" for this suite. 12/09/22 07:26:31.034
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:225
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:26:31.044
Dec  9 07:26:31.044: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename var-expansion 12/09/22 07:26:31.045
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:26:31.095
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:26:31.103
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:225
STEP: creating the pod with failed condition 12/09/22 07:26:31.109
Dec  9 07:26:31.118: INFO: Waiting up to 2m0s for pod "var-expansion-dbc922db-fd03-42ae-9077-149957eddc1f" in namespace "var-expansion-4161" to be "running"
Dec  9 07:26:31.123: INFO: Pod "var-expansion-dbc922db-fd03-42ae-9077-149957eddc1f": Phase="Pending", Reason="", readiness=false. Elapsed: 5.079497ms
Dec  9 07:26:33.127: INFO: Pod "var-expansion-dbc922db-fd03-42ae-9077-149957eddc1f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008799977s
Dec  9 07:26:35.127: INFO: Pod "var-expansion-dbc922db-fd03-42ae-9077-149957eddc1f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009220451s
Dec  9 07:26:37.127: INFO: Pod "var-expansion-dbc922db-fd03-42ae-9077-149957eddc1f": Phase="Pending", Reason="", readiness=false. Elapsed: 6.009032749s
Dec  9 07:26:39.130: INFO: Pod "var-expansion-dbc922db-fd03-42ae-9077-149957eddc1f": Phase="Pending", Reason="", readiness=false. Elapsed: 8.011799283s
Dec  9 07:26:41.127: INFO: Pod "var-expansion-dbc922db-fd03-42ae-9077-149957eddc1f": Phase="Pending", Reason="", readiness=false. Elapsed: 10.009124498s
Dec  9 07:26:43.133: INFO: Pod "var-expansion-dbc922db-fd03-42ae-9077-149957eddc1f": Phase="Pending", Reason="", readiness=false. Elapsed: 12.014354399s
Dec  9 07:26:45.129: INFO: Pod "var-expansion-dbc922db-fd03-42ae-9077-149957eddc1f": Phase="Pending", Reason="", readiness=false. Elapsed: 14.011102284s
Dec  9 07:26:47.128: INFO: Pod "var-expansion-dbc922db-fd03-42ae-9077-149957eddc1f": Phase="Pending", Reason="", readiness=false. Elapsed: 16.00963081s
Dec  9 07:26:49.129: INFO: Pod "var-expansion-dbc922db-fd03-42ae-9077-149957eddc1f": Phase="Pending", Reason="", readiness=false. Elapsed: 18.010726491s
Dec  9 07:26:51.128: INFO: Pod "var-expansion-dbc922db-fd03-42ae-9077-149957eddc1f": Phase="Pending", Reason="", readiness=false. Elapsed: 20.00948151s
Dec  9 07:26:53.128: INFO: Pod "var-expansion-dbc922db-fd03-42ae-9077-149957eddc1f": Phase="Pending", Reason="", readiness=false. Elapsed: 22.010228381s
Dec  9 07:26:55.127: INFO: Pod "var-expansion-dbc922db-fd03-42ae-9077-149957eddc1f": Phase="Pending", Reason="", readiness=false. Elapsed: 24.009281506s
Dec  9 07:26:57.127: INFO: Pod "var-expansion-dbc922db-fd03-42ae-9077-149957eddc1f": Phase="Pending", Reason="", readiness=false. Elapsed: 26.009201375s
Dec  9 07:26:59.130: INFO: Pod "var-expansion-dbc922db-fd03-42ae-9077-149957eddc1f": Phase="Pending", Reason="", readiness=false. Elapsed: 28.011463658s
Dec  9 07:27:01.129: INFO: Pod "var-expansion-dbc922db-fd03-42ae-9077-149957eddc1f": Phase="Pending", Reason="", readiness=false. Elapsed: 30.010795367s
Dec  9 07:27:03.127: INFO: Pod "var-expansion-dbc922db-fd03-42ae-9077-149957eddc1f": Phase="Pending", Reason="", readiness=false. Elapsed: 32.00887422s
Dec  9 07:27:05.127: INFO: Pod "var-expansion-dbc922db-fd03-42ae-9077-149957eddc1f": Phase="Pending", Reason="", readiness=false. Elapsed: 34.008863765s
Dec  9 07:27:07.128: INFO: Pod "var-expansion-dbc922db-fd03-42ae-9077-149957eddc1f": Phase="Pending", Reason="", readiness=false. Elapsed: 36.009623053s
Dec  9 07:27:09.128: INFO: Pod "var-expansion-dbc922db-fd03-42ae-9077-149957eddc1f": Phase="Pending", Reason="", readiness=false. Elapsed: 38.00978259s
Dec  9 07:27:11.129: INFO: Pod "var-expansion-dbc922db-fd03-42ae-9077-149957eddc1f": Phase="Pending", Reason="", readiness=false. Elapsed: 40.011256134s
Dec  9 07:27:13.129: INFO: Pod "var-expansion-dbc922db-fd03-42ae-9077-149957eddc1f": Phase="Pending", Reason="", readiness=false. Elapsed: 42.010522633s
Dec  9 07:27:15.128: INFO: Pod "var-expansion-dbc922db-fd03-42ae-9077-149957eddc1f": Phase="Pending", Reason="", readiness=false. Elapsed: 44.009591832s
Dec  9 07:27:17.129: INFO: Pod "var-expansion-dbc922db-fd03-42ae-9077-149957eddc1f": Phase="Pending", Reason="", readiness=false. Elapsed: 46.011292182s
Dec  9 07:27:19.127: INFO: Pod "var-expansion-dbc922db-fd03-42ae-9077-149957eddc1f": Phase="Pending", Reason="", readiness=false. Elapsed: 48.009227237s
Dec  9 07:27:21.128: INFO: Pod "var-expansion-dbc922db-fd03-42ae-9077-149957eddc1f": Phase="Pending", Reason="", readiness=false. Elapsed: 50.009783681s
Dec  9 07:27:23.127: INFO: Pod "var-expansion-dbc922db-fd03-42ae-9077-149957eddc1f": Phase="Pending", Reason="", readiness=false. Elapsed: 52.009012628s
Dec  9 07:27:25.129: INFO: Pod "var-expansion-dbc922db-fd03-42ae-9077-149957eddc1f": Phase="Pending", Reason="", readiness=false. Elapsed: 54.010505923s
Dec  9 07:27:27.127: INFO: Pod "var-expansion-dbc922db-fd03-42ae-9077-149957eddc1f": Phase="Pending", Reason="", readiness=false. Elapsed: 56.008882082s
Dec  9 07:27:29.127: INFO: Pod "var-expansion-dbc922db-fd03-42ae-9077-149957eddc1f": Phase="Pending", Reason="", readiness=false. Elapsed: 58.009304366s
Dec  9 07:27:31.127: INFO: Pod "var-expansion-dbc922db-fd03-42ae-9077-149957eddc1f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.008820735s
Dec  9 07:27:33.127: INFO: Pod "var-expansion-dbc922db-fd03-42ae-9077-149957eddc1f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.009239819s
Dec  9 07:27:35.128: INFO: Pod "var-expansion-dbc922db-fd03-42ae-9077-149957eddc1f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.009964685s
Dec  9 07:27:37.128: INFO: Pod "var-expansion-dbc922db-fd03-42ae-9077-149957eddc1f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.009675127s
Dec  9 07:27:39.129: INFO: Pod "var-expansion-dbc922db-fd03-42ae-9077-149957eddc1f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.010931567s
Dec  9 07:27:41.129: INFO: Pod "var-expansion-dbc922db-fd03-42ae-9077-149957eddc1f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.010719382s
Dec  9 07:27:43.129: INFO: Pod "var-expansion-dbc922db-fd03-42ae-9077-149957eddc1f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.010322s
Dec  9 07:27:45.129: INFO: Pod "var-expansion-dbc922db-fd03-42ae-9077-149957eddc1f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.010597969s
Dec  9 07:27:47.129: INFO: Pod "var-expansion-dbc922db-fd03-42ae-9077-149957eddc1f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.010865469s
Dec  9 07:27:49.128: INFO: Pod "var-expansion-dbc922db-fd03-42ae-9077-149957eddc1f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.010121242s
Dec  9 07:27:51.128: INFO: Pod "var-expansion-dbc922db-fd03-42ae-9077-149957eddc1f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.009484148s
Dec  9 07:27:53.128: INFO: Pod "var-expansion-dbc922db-fd03-42ae-9077-149957eddc1f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.010219354s
Dec  9 07:27:55.130: INFO: Pod "var-expansion-dbc922db-fd03-42ae-9077-149957eddc1f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.011683281s
Dec  9 07:27:57.132: INFO: Pod "var-expansion-dbc922db-fd03-42ae-9077-149957eddc1f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.01376838s
Dec  9 07:27:59.127: INFO: Pod "var-expansion-dbc922db-fd03-42ae-9077-149957eddc1f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.008999756s
Dec  9 07:28:01.129: INFO: Pod "var-expansion-dbc922db-fd03-42ae-9077-149957eddc1f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.010535978s
Dec  9 07:28:03.128: INFO: Pod "var-expansion-dbc922db-fd03-42ae-9077-149957eddc1f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.009683518s
Dec  9 07:28:05.138: INFO: Pod "var-expansion-dbc922db-fd03-42ae-9077-149957eddc1f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.019802074s
Dec  9 07:28:07.129: INFO: Pod "var-expansion-dbc922db-fd03-42ae-9077-149957eddc1f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.010378508s
Dec  9 07:28:09.127: INFO: Pod "var-expansion-dbc922db-fd03-42ae-9077-149957eddc1f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.009178887s
Dec  9 07:28:11.128: INFO: Pod "var-expansion-dbc922db-fd03-42ae-9077-149957eddc1f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.009836069s
Dec  9 07:28:13.128: INFO: Pod "var-expansion-dbc922db-fd03-42ae-9077-149957eddc1f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.00990941s
Dec  9 07:28:15.128: INFO: Pod "var-expansion-dbc922db-fd03-42ae-9077-149957eddc1f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.009722222s
Dec  9 07:28:17.127: INFO: Pod "var-expansion-dbc922db-fd03-42ae-9077-149957eddc1f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.008755364s
Dec  9 07:28:19.128: INFO: Pod "var-expansion-dbc922db-fd03-42ae-9077-149957eddc1f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.01030532s
Dec  9 07:28:21.128: INFO: Pod "var-expansion-dbc922db-fd03-42ae-9077-149957eddc1f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.009477468s
Dec  9 07:28:23.132: INFO: Pod "var-expansion-dbc922db-fd03-42ae-9077-149957eddc1f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.013825414s
Dec  9 07:28:25.134: INFO: Pod "var-expansion-dbc922db-fd03-42ae-9077-149957eddc1f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.016213892s
Dec  9 07:28:27.128: INFO: Pod "var-expansion-dbc922db-fd03-42ae-9077-149957eddc1f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.009791873s
Dec  9 07:28:29.127: INFO: Pod "var-expansion-dbc922db-fd03-42ae-9077-149957eddc1f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.009118231s
Dec  9 07:28:31.128: INFO: Pod "var-expansion-dbc922db-fd03-42ae-9077-149957eddc1f": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.009541887s
Dec  9 07:28:31.131: INFO: Pod "var-expansion-dbc922db-fd03-42ae-9077-149957eddc1f": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.0125322s
STEP: updating the pod 12/09/22 07:28:31.131
Dec  9 07:28:31.644: INFO: Successfully updated pod "var-expansion-dbc922db-fd03-42ae-9077-149957eddc1f"
STEP: waiting for pod running 12/09/22 07:28:31.644
Dec  9 07:28:31.644: INFO: Waiting up to 2m0s for pod "var-expansion-dbc922db-fd03-42ae-9077-149957eddc1f" in namespace "var-expansion-4161" to be "running"
Dec  9 07:28:31.648: INFO: Pod "var-expansion-dbc922db-fd03-42ae-9077-149957eddc1f": Phase="Pending", Reason="", readiness=false. Elapsed: 3.593249ms
Dec  9 07:28:33.651: INFO: Pod "var-expansion-dbc922db-fd03-42ae-9077-149957eddc1f": Phase="Running", Reason="", readiness=true. Elapsed: 2.007424444s
Dec  9 07:28:33.652: INFO: Pod "var-expansion-dbc922db-fd03-42ae-9077-149957eddc1f" satisfied condition "running"
STEP: deleting the pod gracefully 12/09/22 07:28:33.652
Dec  9 07:28:33.652: INFO: Deleting pod "var-expansion-dbc922db-fd03-42ae-9077-149957eddc1f" in namespace "var-expansion-4161"
Dec  9 07:28:33.659: INFO: Wait up to 5m0s for pod "var-expansion-dbc922db-fd03-42ae-9077-149957eddc1f" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Dec  9 07:29:05.673: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-4161" for this suite. 12/09/22 07:29:05.677
------------------------------
â€¢ [SLOW TEST] [154.643 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:225

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:26:31.044
    Dec  9 07:26:31.044: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename var-expansion 12/09/22 07:26:31.045
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:26:31.095
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:26:31.103
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
      test/e2e/common/node/expansion.go:225
    STEP: creating the pod with failed condition 12/09/22 07:26:31.109
    Dec  9 07:26:31.118: INFO: Waiting up to 2m0s for pod "var-expansion-dbc922db-fd03-42ae-9077-149957eddc1f" in namespace "var-expansion-4161" to be "running"
    Dec  9 07:26:31.123: INFO: Pod "var-expansion-dbc922db-fd03-42ae-9077-149957eddc1f": Phase="Pending", Reason="", readiness=false. Elapsed: 5.079497ms
    Dec  9 07:26:33.127: INFO: Pod "var-expansion-dbc922db-fd03-42ae-9077-149957eddc1f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008799977s
    Dec  9 07:26:35.127: INFO: Pod "var-expansion-dbc922db-fd03-42ae-9077-149957eddc1f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009220451s
    Dec  9 07:26:37.127: INFO: Pod "var-expansion-dbc922db-fd03-42ae-9077-149957eddc1f": Phase="Pending", Reason="", readiness=false. Elapsed: 6.009032749s
    Dec  9 07:26:39.130: INFO: Pod "var-expansion-dbc922db-fd03-42ae-9077-149957eddc1f": Phase="Pending", Reason="", readiness=false. Elapsed: 8.011799283s
    Dec  9 07:26:41.127: INFO: Pod "var-expansion-dbc922db-fd03-42ae-9077-149957eddc1f": Phase="Pending", Reason="", readiness=false. Elapsed: 10.009124498s
    Dec  9 07:26:43.133: INFO: Pod "var-expansion-dbc922db-fd03-42ae-9077-149957eddc1f": Phase="Pending", Reason="", readiness=false. Elapsed: 12.014354399s
    Dec  9 07:26:45.129: INFO: Pod "var-expansion-dbc922db-fd03-42ae-9077-149957eddc1f": Phase="Pending", Reason="", readiness=false. Elapsed: 14.011102284s
    Dec  9 07:26:47.128: INFO: Pod "var-expansion-dbc922db-fd03-42ae-9077-149957eddc1f": Phase="Pending", Reason="", readiness=false. Elapsed: 16.00963081s
    Dec  9 07:26:49.129: INFO: Pod "var-expansion-dbc922db-fd03-42ae-9077-149957eddc1f": Phase="Pending", Reason="", readiness=false. Elapsed: 18.010726491s
    Dec  9 07:26:51.128: INFO: Pod "var-expansion-dbc922db-fd03-42ae-9077-149957eddc1f": Phase="Pending", Reason="", readiness=false. Elapsed: 20.00948151s
    Dec  9 07:26:53.128: INFO: Pod "var-expansion-dbc922db-fd03-42ae-9077-149957eddc1f": Phase="Pending", Reason="", readiness=false. Elapsed: 22.010228381s
    Dec  9 07:26:55.127: INFO: Pod "var-expansion-dbc922db-fd03-42ae-9077-149957eddc1f": Phase="Pending", Reason="", readiness=false. Elapsed: 24.009281506s
    Dec  9 07:26:57.127: INFO: Pod "var-expansion-dbc922db-fd03-42ae-9077-149957eddc1f": Phase="Pending", Reason="", readiness=false. Elapsed: 26.009201375s
    Dec  9 07:26:59.130: INFO: Pod "var-expansion-dbc922db-fd03-42ae-9077-149957eddc1f": Phase="Pending", Reason="", readiness=false. Elapsed: 28.011463658s
    Dec  9 07:27:01.129: INFO: Pod "var-expansion-dbc922db-fd03-42ae-9077-149957eddc1f": Phase="Pending", Reason="", readiness=false. Elapsed: 30.010795367s
    Dec  9 07:27:03.127: INFO: Pod "var-expansion-dbc922db-fd03-42ae-9077-149957eddc1f": Phase="Pending", Reason="", readiness=false. Elapsed: 32.00887422s
    Dec  9 07:27:05.127: INFO: Pod "var-expansion-dbc922db-fd03-42ae-9077-149957eddc1f": Phase="Pending", Reason="", readiness=false. Elapsed: 34.008863765s
    Dec  9 07:27:07.128: INFO: Pod "var-expansion-dbc922db-fd03-42ae-9077-149957eddc1f": Phase="Pending", Reason="", readiness=false. Elapsed: 36.009623053s
    Dec  9 07:27:09.128: INFO: Pod "var-expansion-dbc922db-fd03-42ae-9077-149957eddc1f": Phase="Pending", Reason="", readiness=false. Elapsed: 38.00978259s
    Dec  9 07:27:11.129: INFO: Pod "var-expansion-dbc922db-fd03-42ae-9077-149957eddc1f": Phase="Pending", Reason="", readiness=false. Elapsed: 40.011256134s
    Dec  9 07:27:13.129: INFO: Pod "var-expansion-dbc922db-fd03-42ae-9077-149957eddc1f": Phase="Pending", Reason="", readiness=false. Elapsed: 42.010522633s
    Dec  9 07:27:15.128: INFO: Pod "var-expansion-dbc922db-fd03-42ae-9077-149957eddc1f": Phase="Pending", Reason="", readiness=false. Elapsed: 44.009591832s
    Dec  9 07:27:17.129: INFO: Pod "var-expansion-dbc922db-fd03-42ae-9077-149957eddc1f": Phase="Pending", Reason="", readiness=false. Elapsed: 46.011292182s
    Dec  9 07:27:19.127: INFO: Pod "var-expansion-dbc922db-fd03-42ae-9077-149957eddc1f": Phase="Pending", Reason="", readiness=false. Elapsed: 48.009227237s
    Dec  9 07:27:21.128: INFO: Pod "var-expansion-dbc922db-fd03-42ae-9077-149957eddc1f": Phase="Pending", Reason="", readiness=false. Elapsed: 50.009783681s
    Dec  9 07:27:23.127: INFO: Pod "var-expansion-dbc922db-fd03-42ae-9077-149957eddc1f": Phase="Pending", Reason="", readiness=false. Elapsed: 52.009012628s
    Dec  9 07:27:25.129: INFO: Pod "var-expansion-dbc922db-fd03-42ae-9077-149957eddc1f": Phase="Pending", Reason="", readiness=false. Elapsed: 54.010505923s
    Dec  9 07:27:27.127: INFO: Pod "var-expansion-dbc922db-fd03-42ae-9077-149957eddc1f": Phase="Pending", Reason="", readiness=false. Elapsed: 56.008882082s
    Dec  9 07:27:29.127: INFO: Pod "var-expansion-dbc922db-fd03-42ae-9077-149957eddc1f": Phase="Pending", Reason="", readiness=false. Elapsed: 58.009304366s
    Dec  9 07:27:31.127: INFO: Pod "var-expansion-dbc922db-fd03-42ae-9077-149957eddc1f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.008820735s
    Dec  9 07:27:33.127: INFO: Pod "var-expansion-dbc922db-fd03-42ae-9077-149957eddc1f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.009239819s
    Dec  9 07:27:35.128: INFO: Pod "var-expansion-dbc922db-fd03-42ae-9077-149957eddc1f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.009964685s
    Dec  9 07:27:37.128: INFO: Pod "var-expansion-dbc922db-fd03-42ae-9077-149957eddc1f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.009675127s
    Dec  9 07:27:39.129: INFO: Pod "var-expansion-dbc922db-fd03-42ae-9077-149957eddc1f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.010931567s
    Dec  9 07:27:41.129: INFO: Pod "var-expansion-dbc922db-fd03-42ae-9077-149957eddc1f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.010719382s
    Dec  9 07:27:43.129: INFO: Pod "var-expansion-dbc922db-fd03-42ae-9077-149957eddc1f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.010322s
    Dec  9 07:27:45.129: INFO: Pod "var-expansion-dbc922db-fd03-42ae-9077-149957eddc1f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.010597969s
    Dec  9 07:27:47.129: INFO: Pod "var-expansion-dbc922db-fd03-42ae-9077-149957eddc1f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.010865469s
    Dec  9 07:27:49.128: INFO: Pod "var-expansion-dbc922db-fd03-42ae-9077-149957eddc1f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.010121242s
    Dec  9 07:27:51.128: INFO: Pod "var-expansion-dbc922db-fd03-42ae-9077-149957eddc1f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.009484148s
    Dec  9 07:27:53.128: INFO: Pod "var-expansion-dbc922db-fd03-42ae-9077-149957eddc1f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.010219354s
    Dec  9 07:27:55.130: INFO: Pod "var-expansion-dbc922db-fd03-42ae-9077-149957eddc1f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.011683281s
    Dec  9 07:27:57.132: INFO: Pod "var-expansion-dbc922db-fd03-42ae-9077-149957eddc1f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.01376838s
    Dec  9 07:27:59.127: INFO: Pod "var-expansion-dbc922db-fd03-42ae-9077-149957eddc1f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.008999756s
    Dec  9 07:28:01.129: INFO: Pod "var-expansion-dbc922db-fd03-42ae-9077-149957eddc1f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.010535978s
    Dec  9 07:28:03.128: INFO: Pod "var-expansion-dbc922db-fd03-42ae-9077-149957eddc1f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.009683518s
    Dec  9 07:28:05.138: INFO: Pod "var-expansion-dbc922db-fd03-42ae-9077-149957eddc1f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.019802074s
    Dec  9 07:28:07.129: INFO: Pod "var-expansion-dbc922db-fd03-42ae-9077-149957eddc1f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.010378508s
    Dec  9 07:28:09.127: INFO: Pod "var-expansion-dbc922db-fd03-42ae-9077-149957eddc1f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.009178887s
    Dec  9 07:28:11.128: INFO: Pod "var-expansion-dbc922db-fd03-42ae-9077-149957eddc1f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.009836069s
    Dec  9 07:28:13.128: INFO: Pod "var-expansion-dbc922db-fd03-42ae-9077-149957eddc1f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.00990941s
    Dec  9 07:28:15.128: INFO: Pod "var-expansion-dbc922db-fd03-42ae-9077-149957eddc1f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.009722222s
    Dec  9 07:28:17.127: INFO: Pod "var-expansion-dbc922db-fd03-42ae-9077-149957eddc1f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.008755364s
    Dec  9 07:28:19.128: INFO: Pod "var-expansion-dbc922db-fd03-42ae-9077-149957eddc1f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.01030532s
    Dec  9 07:28:21.128: INFO: Pod "var-expansion-dbc922db-fd03-42ae-9077-149957eddc1f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.009477468s
    Dec  9 07:28:23.132: INFO: Pod "var-expansion-dbc922db-fd03-42ae-9077-149957eddc1f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.013825414s
    Dec  9 07:28:25.134: INFO: Pod "var-expansion-dbc922db-fd03-42ae-9077-149957eddc1f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.016213892s
    Dec  9 07:28:27.128: INFO: Pod "var-expansion-dbc922db-fd03-42ae-9077-149957eddc1f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.009791873s
    Dec  9 07:28:29.127: INFO: Pod "var-expansion-dbc922db-fd03-42ae-9077-149957eddc1f": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.009118231s
    Dec  9 07:28:31.128: INFO: Pod "var-expansion-dbc922db-fd03-42ae-9077-149957eddc1f": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.009541887s
    Dec  9 07:28:31.131: INFO: Pod "var-expansion-dbc922db-fd03-42ae-9077-149957eddc1f": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.0125322s
    STEP: updating the pod 12/09/22 07:28:31.131
    Dec  9 07:28:31.644: INFO: Successfully updated pod "var-expansion-dbc922db-fd03-42ae-9077-149957eddc1f"
    STEP: waiting for pod running 12/09/22 07:28:31.644
    Dec  9 07:28:31.644: INFO: Waiting up to 2m0s for pod "var-expansion-dbc922db-fd03-42ae-9077-149957eddc1f" in namespace "var-expansion-4161" to be "running"
    Dec  9 07:28:31.648: INFO: Pod "var-expansion-dbc922db-fd03-42ae-9077-149957eddc1f": Phase="Pending", Reason="", readiness=false. Elapsed: 3.593249ms
    Dec  9 07:28:33.651: INFO: Pod "var-expansion-dbc922db-fd03-42ae-9077-149957eddc1f": Phase="Running", Reason="", readiness=true. Elapsed: 2.007424444s
    Dec  9 07:28:33.652: INFO: Pod "var-expansion-dbc922db-fd03-42ae-9077-149957eddc1f" satisfied condition "running"
    STEP: deleting the pod gracefully 12/09/22 07:28:33.652
    Dec  9 07:28:33.652: INFO: Deleting pod "var-expansion-dbc922db-fd03-42ae-9077-149957eddc1f" in namespace "var-expansion-4161"
    Dec  9 07:28:33.659: INFO: Wait up to 5m0s for pod "var-expansion-dbc922db-fd03-42ae-9077-149957eddc1f" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:29:05.673: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-4161" for this suite. 12/09/22 07:29:05.677
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:193
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:29:05.693
Dec  9 07:29:05.693: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename projected 12/09/22 07:29:05.694
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:29:05.713
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:29:05.717
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:193
STEP: Creating a pod to test downward API volume plugin 12/09/22 07:29:05.721
Dec  9 07:29:05.731: INFO: Waiting up to 5m0s for pod "downwardapi-volume-34988a32-1e32-4762-9ed4-aad208b0efea" in namespace "projected-6712" to be "Succeeded or Failed"
Dec  9 07:29:05.737: INFO: Pod "downwardapi-volume-34988a32-1e32-4762-9ed4-aad208b0efea": Phase="Pending", Reason="", readiness=false. Elapsed: 5.945328ms
Dec  9 07:29:07.741: INFO: Pod "downwardapi-volume-34988a32-1e32-4762-9ed4-aad208b0efea": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009312736s
Dec  9 07:29:09.742: INFO: Pod "downwardapi-volume-34988a32-1e32-4762-9ed4-aad208b0efea": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010866742s
STEP: Saw pod success 12/09/22 07:29:09.742
Dec  9 07:29:09.743: INFO: Pod "downwardapi-volume-34988a32-1e32-4762-9ed4-aad208b0efea" satisfied condition "Succeeded or Failed"
Dec  9 07:29:09.746: INFO: Trying to get logs from node ip-10-0-10-179 pod downwardapi-volume-34988a32-1e32-4762-9ed4-aad208b0efea container client-container: <nil>
STEP: delete the pod 12/09/22 07:29:09.757
Dec  9 07:29:09.767: INFO: Waiting for pod downwardapi-volume-34988a32-1e32-4762-9ed4-aad208b0efea to disappear
Dec  9 07:29:09.770: INFO: Pod downwardapi-volume-34988a32-1e32-4762-9ed4-aad208b0efea no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Dec  9 07:29:09.771: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-6712" for this suite. 12/09/22 07:29:09.774
------------------------------
â€¢ [4.095 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:29:05.693
    Dec  9 07:29:05.693: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename projected 12/09/22 07:29:05.694
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:29:05.713
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:29:05.717
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide container's cpu limit [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:193
    STEP: Creating a pod to test downward API volume plugin 12/09/22 07:29:05.721
    Dec  9 07:29:05.731: INFO: Waiting up to 5m0s for pod "downwardapi-volume-34988a32-1e32-4762-9ed4-aad208b0efea" in namespace "projected-6712" to be "Succeeded or Failed"
    Dec  9 07:29:05.737: INFO: Pod "downwardapi-volume-34988a32-1e32-4762-9ed4-aad208b0efea": Phase="Pending", Reason="", readiness=false. Elapsed: 5.945328ms
    Dec  9 07:29:07.741: INFO: Pod "downwardapi-volume-34988a32-1e32-4762-9ed4-aad208b0efea": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009312736s
    Dec  9 07:29:09.742: INFO: Pod "downwardapi-volume-34988a32-1e32-4762-9ed4-aad208b0efea": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010866742s
    STEP: Saw pod success 12/09/22 07:29:09.742
    Dec  9 07:29:09.743: INFO: Pod "downwardapi-volume-34988a32-1e32-4762-9ed4-aad208b0efea" satisfied condition "Succeeded or Failed"
    Dec  9 07:29:09.746: INFO: Trying to get logs from node ip-10-0-10-179 pod downwardapi-volume-34988a32-1e32-4762-9ed4-aad208b0efea container client-container: <nil>
    STEP: delete the pod 12/09/22 07:29:09.757
    Dec  9 07:29:09.767: INFO: Waiting for pod downwardapi-volume-34988a32-1e32-4762-9ed4-aad208b0efea to disappear
    Dec  9 07:29:09.770: INFO: Pod downwardapi-volume-34988a32-1e32-4762-9ed4-aad208b0efea no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:29:09.771: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-6712" for this suite. 12/09/22 07:29:09.774
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:392
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:29:09.79
Dec  9 07:29:09.790: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename resourcequota 12/09/22 07:29:09.791
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:29:09.815
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:29:09.821
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:392
STEP: Counting existing ResourceQuota 12/09/22 07:29:09.825
STEP: Creating a ResourceQuota 12/09/22 07:29:14.828
STEP: Ensuring resource quota status is calculated 12/09/22 07:29:14.836
STEP: Creating a ReplicationController 12/09/22 07:29:16.843
STEP: Ensuring resource quota status captures replication controller creation 12/09/22 07:29:16.856
STEP: Deleting a ReplicationController 12/09/22 07:29:18.861
STEP: Ensuring resource quota status released usage 12/09/22 07:29:18.868
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Dec  9 07:29:20.872: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-7341" for this suite. 12/09/22 07:29:20.878
------------------------------
â€¢ [SLOW TEST] [11.105 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:392

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:29:09.79
    Dec  9 07:29:09.790: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename resourcequota 12/09/22 07:29:09.791
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:29:09.815
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:29:09.821
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
      test/e2e/apimachinery/resource_quota.go:392
    STEP: Counting existing ResourceQuota 12/09/22 07:29:09.825
    STEP: Creating a ResourceQuota 12/09/22 07:29:14.828
    STEP: Ensuring resource quota status is calculated 12/09/22 07:29:14.836
    STEP: Creating a ReplicationController 12/09/22 07:29:16.843
    STEP: Ensuring resource quota status captures replication controller creation 12/09/22 07:29:16.856
    STEP: Deleting a ReplicationController 12/09/22 07:29:18.861
    STEP: Ensuring resource quota status released usage 12/09/22 07:29:18.868
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:29:20.872: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-7341" for this suite. 12/09/22 07:29:20.878
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:74
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:29:20.895
Dec  9 07:29:20.895: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename configmap 12/09/22 07:29:20.896
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:29:20.928
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:29:20.934
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:74
STEP: Creating configMap with name configmap-test-volume-cf6fad6a-25c5-4570-9d7d-a9512baa5cbd 12/09/22 07:29:20.939
STEP: Creating a pod to test consume configMaps 12/09/22 07:29:20.954
Dec  9 07:29:20.964: INFO: Waiting up to 5m0s for pod "pod-configmaps-83fff5ed-b3e0-4aba-8850-2c9777d24142" in namespace "configmap-3219" to be "Succeeded or Failed"
Dec  9 07:29:20.970: INFO: Pod "pod-configmaps-83fff5ed-b3e0-4aba-8850-2c9777d24142": Phase="Pending", Reason="", readiness=false. Elapsed: 6.788291ms
Dec  9 07:29:22.975: INFO: Pod "pod-configmaps-83fff5ed-b3e0-4aba-8850-2c9777d24142": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011465388s
Dec  9 07:29:24.976: INFO: Pod "pod-configmaps-83fff5ed-b3e0-4aba-8850-2c9777d24142": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012310457s
STEP: Saw pod success 12/09/22 07:29:24.976
Dec  9 07:29:24.976: INFO: Pod "pod-configmaps-83fff5ed-b3e0-4aba-8850-2c9777d24142" satisfied condition "Succeeded or Failed"
Dec  9 07:29:24.979: INFO: Trying to get logs from node ip-10-0-10-179 pod pod-configmaps-83fff5ed-b3e0-4aba-8850-2c9777d24142 container agnhost-container: <nil>
STEP: delete the pod 12/09/22 07:29:24.986
Dec  9 07:29:24.997: INFO: Waiting for pod pod-configmaps-83fff5ed-b3e0-4aba-8850-2c9777d24142 to disappear
Dec  9 07:29:25.000: INFO: Pod pod-configmaps-83fff5ed-b3e0-4aba-8850-2c9777d24142 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Dec  9 07:29:25.000: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-3219" for this suite. 12/09/22 07:29:25.004
------------------------------
â€¢ [4.122 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:74

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:29:20.895
    Dec  9 07:29:20.895: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename configmap 12/09/22 07:29:20.896
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:29:20.928
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:29:20.934
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:74
    STEP: Creating configMap with name configmap-test-volume-cf6fad6a-25c5-4570-9d7d-a9512baa5cbd 12/09/22 07:29:20.939
    STEP: Creating a pod to test consume configMaps 12/09/22 07:29:20.954
    Dec  9 07:29:20.964: INFO: Waiting up to 5m0s for pod "pod-configmaps-83fff5ed-b3e0-4aba-8850-2c9777d24142" in namespace "configmap-3219" to be "Succeeded or Failed"
    Dec  9 07:29:20.970: INFO: Pod "pod-configmaps-83fff5ed-b3e0-4aba-8850-2c9777d24142": Phase="Pending", Reason="", readiness=false. Elapsed: 6.788291ms
    Dec  9 07:29:22.975: INFO: Pod "pod-configmaps-83fff5ed-b3e0-4aba-8850-2c9777d24142": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011465388s
    Dec  9 07:29:24.976: INFO: Pod "pod-configmaps-83fff5ed-b3e0-4aba-8850-2c9777d24142": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012310457s
    STEP: Saw pod success 12/09/22 07:29:24.976
    Dec  9 07:29:24.976: INFO: Pod "pod-configmaps-83fff5ed-b3e0-4aba-8850-2c9777d24142" satisfied condition "Succeeded or Failed"
    Dec  9 07:29:24.979: INFO: Trying to get logs from node ip-10-0-10-179 pod pod-configmaps-83fff5ed-b3e0-4aba-8850-2c9777d24142 container agnhost-container: <nil>
    STEP: delete the pod 12/09/22 07:29:24.986
    Dec  9 07:29:24.997: INFO: Waiting for pod pod-configmaps-83fff5ed-b3e0-4aba-8850-2c9777d24142 to disappear
    Dec  9 07:29:25.000: INFO: Pod pod-configmaps-83fff5ed-b3e0-4aba-8850-2c9777d24142 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:29:25.000: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-3219" for this suite. 12/09/22 07:29:25.004
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:398
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:29:25.02
Dec  9 07:29:25.021: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename pods 12/09/22 07:29:25.022
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:29:25.068
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:29:25.095
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:398
STEP: creating the pod 12/09/22 07:29:25.1
STEP: submitting the pod to kubernetes 12/09/22 07:29:25.1
Dec  9 07:29:25.118: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-2ae24b1c-014a-430d-953e-663b31c1e1e0" in namespace "pods-5637" to be "running and ready"
Dec  9 07:29:25.128: INFO: Pod "pod-update-activedeadlineseconds-2ae24b1c-014a-430d-953e-663b31c1e1e0": Phase="Pending", Reason="", readiness=false. Elapsed: 10.291631ms
Dec  9 07:29:25.128: INFO: The phase of Pod pod-update-activedeadlineseconds-2ae24b1c-014a-430d-953e-663b31c1e1e0 is Pending, waiting for it to be Running (with Ready = true)
Dec  9 07:29:27.132: INFO: Pod "pod-update-activedeadlineseconds-2ae24b1c-014a-430d-953e-663b31c1e1e0": Phase="Running", Reason="", readiness=true. Elapsed: 2.013725547s
Dec  9 07:29:27.132: INFO: The phase of Pod pod-update-activedeadlineseconds-2ae24b1c-014a-430d-953e-663b31c1e1e0 is Running (Ready = true)
Dec  9 07:29:27.132: INFO: Pod "pod-update-activedeadlineseconds-2ae24b1c-014a-430d-953e-663b31c1e1e0" satisfied condition "running and ready"
STEP: verifying the pod is in kubernetes 12/09/22 07:29:27.135
STEP: updating the pod 12/09/22 07:29:27.138
Dec  9 07:29:27.650: INFO: Successfully updated pod "pod-update-activedeadlineseconds-2ae24b1c-014a-430d-953e-663b31c1e1e0"
Dec  9 07:29:27.650: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-2ae24b1c-014a-430d-953e-663b31c1e1e0" in namespace "pods-5637" to be "terminated with reason DeadlineExceeded"
Dec  9 07:29:27.653: INFO: Pod "pod-update-activedeadlineseconds-2ae24b1c-014a-430d-953e-663b31c1e1e0": Phase="Running", Reason="", readiness=true. Elapsed: 2.770058ms
Dec  9 07:29:29.657: INFO: Pod "pod-update-activedeadlineseconds-2ae24b1c-014a-430d-953e-663b31c1e1e0": Phase="Running", Reason="", readiness=true. Elapsed: 2.00704124s
Dec  9 07:29:31.657: INFO: Pod "pod-update-activedeadlineseconds-2ae24b1c-014a-430d-953e-663b31c1e1e0": Phase="Running", Reason="", readiness=false. Elapsed: 4.006641065s
Dec  9 07:29:33.659: INFO: Pod "pod-update-activedeadlineseconds-2ae24b1c-014a-430d-953e-663b31c1e1e0": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 6.00810349s
Dec  9 07:29:33.659: INFO: Pod "pod-update-activedeadlineseconds-2ae24b1c-014a-430d-953e-663b31c1e1e0" satisfied condition "terminated with reason DeadlineExceeded"
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Dec  9 07:29:33.659: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-5637" for this suite. 12/09/22 07:29:33.663
------------------------------
â€¢ [SLOW TEST] [8.652 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:398

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:29:25.02
    Dec  9 07:29:25.021: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename pods 12/09/22 07:29:25.022
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:29:25.068
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:29:25.095
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:398
    STEP: creating the pod 12/09/22 07:29:25.1
    STEP: submitting the pod to kubernetes 12/09/22 07:29:25.1
    Dec  9 07:29:25.118: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-2ae24b1c-014a-430d-953e-663b31c1e1e0" in namespace "pods-5637" to be "running and ready"
    Dec  9 07:29:25.128: INFO: Pod "pod-update-activedeadlineseconds-2ae24b1c-014a-430d-953e-663b31c1e1e0": Phase="Pending", Reason="", readiness=false. Elapsed: 10.291631ms
    Dec  9 07:29:25.128: INFO: The phase of Pod pod-update-activedeadlineseconds-2ae24b1c-014a-430d-953e-663b31c1e1e0 is Pending, waiting for it to be Running (with Ready = true)
    Dec  9 07:29:27.132: INFO: Pod "pod-update-activedeadlineseconds-2ae24b1c-014a-430d-953e-663b31c1e1e0": Phase="Running", Reason="", readiness=true. Elapsed: 2.013725547s
    Dec  9 07:29:27.132: INFO: The phase of Pod pod-update-activedeadlineseconds-2ae24b1c-014a-430d-953e-663b31c1e1e0 is Running (Ready = true)
    Dec  9 07:29:27.132: INFO: Pod "pod-update-activedeadlineseconds-2ae24b1c-014a-430d-953e-663b31c1e1e0" satisfied condition "running and ready"
    STEP: verifying the pod is in kubernetes 12/09/22 07:29:27.135
    STEP: updating the pod 12/09/22 07:29:27.138
    Dec  9 07:29:27.650: INFO: Successfully updated pod "pod-update-activedeadlineseconds-2ae24b1c-014a-430d-953e-663b31c1e1e0"
    Dec  9 07:29:27.650: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-2ae24b1c-014a-430d-953e-663b31c1e1e0" in namespace "pods-5637" to be "terminated with reason DeadlineExceeded"
    Dec  9 07:29:27.653: INFO: Pod "pod-update-activedeadlineseconds-2ae24b1c-014a-430d-953e-663b31c1e1e0": Phase="Running", Reason="", readiness=true. Elapsed: 2.770058ms
    Dec  9 07:29:29.657: INFO: Pod "pod-update-activedeadlineseconds-2ae24b1c-014a-430d-953e-663b31c1e1e0": Phase="Running", Reason="", readiness=true. Elapsed: 2.00704124s
    Dec  9 07:29:31.657: INFO: Pod "pod-update-activedeadlineseconds-2ae24b1c-014a-430d-953e-663b31c1e1e0": Phase="Running", Reason="", readiness=false. Elapsed: 4.006641065s
    Dec  9 07:29:33.659: INFO: Pod "pod-update-activedeadlineseconds-2ae24b1c-014a-430d-953e-663b31c1e1e0": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 6.00810349s
    Dec  9 07:29:33.659: INFO: Pod "pod-update-activedeadlineseconds-2ae24b1c-014a-430d-953e-663b31c1e1e0" satisfied condition "terminated with reason DeadlineExceeded"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:29:33.659: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-5637" for this suite. 12/09/22 07:29:33.663
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1302
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:29:33.677
Dec  9 07:29:33.677: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename services 12/09/22 07:29:33.677
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:29:33.716
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:29:33.724
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1302
STEP: creating service nodeport-test with type=NodePort in namespace services-1864 12/09/22 07:29:33.732
STEP: creating replication controller nodeport-test in namespace services-1864 12/09/22 07:29:33.747
I1209 07:29:33.756845      20 runners.go:193] Created replication controller with name: nodeport-test, namespace: services-1864, replica count: 2
I1209 07:29:36.808319      20 runners.go:193] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Dec  9 07:29:36.808: INFO: Creating new exec pod
Dec  9 07:29:36.815: INFO: Waiting up to 5m0s for pod "execpodl6vd7" in namespace "services-1864" to be "running"
Dec  9 07:29:36.820: INFO: Pod "execpodl6vd7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.92945ms
Dec  9 07:29:38.828: INFO: Pod "execpodl6vd7": Phase="Running", Reason="", readiness=true. Elapsed: 2.012845957s
Dec  9 07:29:38.828: INFO: Pod "execpodl6vd7" satisfied condition "running"
Dec  9 07:29:39.834: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=services-1864 exec execpodl6vd7 -- /bin/sh -x -c nc -v -z -w 2 nodeport-test 80'
Dec  9 07:29:39.991: INFO: stderr: "+ nc -v -z -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Dec  9 07:29:39.991: INFO: stdout: ""
Dec  9 07:29:39.991: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=services-1864 exec execpodl6vd7 -- /bin/sh -x -c nc -v -z -w 2 10.3.104.221 80'
Dec  9 07:29:40.159: INFO: stderr: "+ nc -v -z -w 2 10.3.104.221 80\nConnection to 10.3.104.221 80 port [tcp/http] succeeded!\n"
Dec  9 07:29:40.159: INFO: stdout: ""
Dec  9 07:29:40.159: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=services-1864 exec execpodl6vd7 -- /bin/sh -x -c nc -v -z -w 2 10.0.10.179 30857'
Dec  9 07:29:40.307: INFO: stderr: "+ nc -v -z -w 2 10.0.10.179 30857\nConnection to 10.0.10.179 30857 port [tcp/*] succeeded!\n"
Dec  9 07:29:40.307: INFO: stdout: ""
Dec  9 07:29:40.307: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=services-1864 exec execpodl6vd7 -- /bin/sh -x -c nc -v -z -w 2 10.0.17.108 30857'
Dec  9 07:29:40.464: INFO: stderr: "+ nc -v -z -w 2 10.0.17.108 30857\nConnection to 10.0.17.108 30857 port [tcp/*] succeeded!\n"
Dec  9 07:29:40.464: INFO: stdout: ""
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Dec  9 07:29:40.464: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-1864" for this suite. 12/09/22 07:29:40.468
------------------------------
â€¢ [SLOW TEST] [6.798 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1302

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:29:33.677
    Dec  9 07:29:33.677: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename services 12/09/22 07:29:33.677
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:29:33.716
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:29:33.724
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to create a functioning NodePort service [Conformance]
      test/e2e/network/service.go:1302
    STEP: creating service nodeport-test with type=NodePort in namespace services-1864 12/09/22 07:29:33.732
    STEP: creating replication controller nodeport-test in namespace services-1864 12/09/22 07:29:33.747
    I1209 07:29:33.756845      20 runners.go:193] Created replication controller with name: nodeport-test, namespace: services-1864, replica count: 2
    I1209 07:29:36.808319      20 runners.go:193] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Dec  9 07:29:36.808: INFO: Creating new exec pod
    Dec  9 07:29:36.815: INFO: Waiting up to 5m0s for pod "execpodl6vd7" in namespace "services-1864" to be "running"
    Dec  9 07:29:36.820: INFO: Pod "execpodl6vd7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.92945ms
    Dec  9 07:29:38.828: INFO: Pod "execpodl6vd7": Phase="Running", Reason="", readiness=true. Elapsed: 2.012845957s
    Dec  9 07:29:38.828: INFO: Pod "execpodl6vd7" satisfied condition "running"
    Dec  9 07:29:39.834: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=services-1864 exec execpodl6vd7 -- /bin/sh -x -c nc -v -z -w 2 nodeport-test 80'
    Dec  9 07:29:39.991: INFO: stderr: "+ nc -v -z -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
    Dec  9 07:29:39.991: INFO: stdout: ""
    Dec  9 07:29:39.991: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=services-1864 exec execpodl6vd7 -- /bin/sh -x -c nc -v -z -w 2 10.3.104.221 80'
    Dec  9 07:29:40.159: INFO: stderr: "+ nc -v -z -w 2 10.3.104.221 80\nConnection to 10.3.104.221 80 port [tcp/http] succeeded!\n"
    Dec  9 07:29:40.159: INFO: stdout: ""
    Dec  9 07:29:40.159: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=services-1864 exec execpodl6vd7 -- /bin/sh -x -c nc -v -z -w 2 10.0.10.179 30857'
    Dec  9 07:29:40.307: INFO: stderr: "+ nc -v -z -w 2 10.0.10.179 30857\nConnection to 10.0.10.179 30857 port [tcp/*] succeeded!\n"
    Dec  9 07:29:40.307: INFO: stdout: ""
    Dec  9 07:29:40.307: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=services-1864 exec execpodl6vd7 -- /bin/sh -x -c nc -v -z -w 2 10.0.17.108 30857'
    Dec  9 07:29:40.464: INFO: stderr: "+ nc -v -z -w 2 10.0.17.108 30857\nConnection to 10.0.17.108 30857 port [tcp/*] succeeded!\n"
    Dec  9 07:29:40.464: INFO: stdout: ""
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:29:40.464: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-1864" for this suite. 12/09/22 07:29:40.468
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:267
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:29:40.476
Dec  9 07:29:40.476: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename downward-api 12/09/22 07:29:40.477
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:29:40.505
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:29:40.524
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:267
STEP: Creating a pod to test downward api env vars 12/09/22 07:29:40.531
Dec  9 07:29:40.559: INFO: Waiting up to 5m0s for pod "downward-api-fd5966d7-9b93-453a-a9d5-3c5b07b33d02" in namespace "downward-api-2020" to be "Succeeded or Failed"
Dec  9 07:29:40.575: INFO: Pod "downward-api-fd5966d7-9b93-453a-a9d5-3c5b07b33d02": Phase="Pending", Reason="", readiness=false. Elapsed: 15.613875ms
Dec  9 07:29:42.578: INFO: Pod "downward-api-fd5966d7-9b93-453a-a9d5-3c5b07b33d02": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019329646s
Dec  9 07:29:44.578: INFO: Pod "downward-api-fd5966d7-9b93-453a-a9d5-3c5b07b33d02": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019128483s
STEP: Saw pod success 12/09/22 07:29:44.578
Dec  9 07:29:44.579: INFO: Pod "downward-api-fd5966d7-9b93-453a-a9d5-3c5b07b33d02" satisfied condition "Succeeded or Failed"
Dec  9 07:29:44.581: INFO: Trying to get logs from node ip-10-0-10-179 pod downward-api-fd5966d7-9b93-453a-a9d5-3c5b07b33d02 container dapi-container: <nil>
STEP: delete the pod 12/09/22 07:29:44.587
Dec  9 07:29:44.603: INFO: Waiting for pod downward-api-fd5966d7-9b93-453a-a9d5-3c5b07b33d02 to disappear
Dec  9 07:29:44.606: INFO: Pod downward-api-fd5966d7-9b93-453a-a9d5-3c5b07b33d02 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Dec  9 07:29:44.606: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-2020" for this suite. 12/09/22 07:29:44.61
------------------------------
â€¢ [4.139 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:267

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:29:40.476
    Dec  9 07:29:40.476: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename downward-api 12/09/22 07:29:40.477
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:29:40.505
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:29:40.524
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide pod UID as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:267
    STEP: Creating a pod to test downward api env vars 12/09/22 07:29:40.531
    Dec  9 07:29:40.559: INFO: Waiting up to 5m0s for pod "downward-api-fd5966d7-9b93-453a-a9d5-3c5b07b33d02" in namespace "downward-api-2020" to be "Succeeded or Failed"
    Dec  9 07:29:40.575: INFO: Pod "downward-api-fd5966d7-9b93-453a-a9d5-3c5b07b33d02": Phase="Pending", Reason="", readiness=false. Elapsed: 15.613875ms
    Dec  9 07:29:42.578: INFO: Pod "downward-api-fd5966d7-9b93-453a-a9d5-3c5b07b33d02": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019329646s
    Dec  9 07:29:44.578: INFO: Pod "downward-api-fd5966d7-9b93-453a-a9d5-3c5b07b33d02": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019128483s
    STEP: Saw pod success 12/09/22 07:29:44.578
    Dec  9 07:29:44.579: INFO: Pod "downward-api-fd5966d7-9b93-453a-a9d5-3c5b07b33d02" satisfied condition "Succeeded or Failed"
    Dec  9 07:29:44.581: INFO: Trying to get logs from node ip-10-0-10-179 pod downward-api-fd5966d7-9b93-453a-a9d5-3c5b07b33d02 container dapi-container: <nil>
    STEP: delete the pod 12/09/22 07:29:44.587
    Dec  9 07:29:44.603: INFO: Waiting for pod downward-api-fd5966d7-9b93-453a-a9d5-3c5b07b33d02 to disappear
    Dec  9 07:29:44.606: INFO: Pod downward-api-fd5966d7-9b93-453a-a9d5-3c5b07b33d02 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:29:44.606: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-2020" for this suite. 12/09/22 07:29:44.61
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  listing custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:85
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:29:44.618
Dec  9 07:29:44.618: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename custom-resource-definition 12/09/22 07:29:44.619
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:29:44.64
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:29:44.645
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] listing custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:85
Dec  9 07:29:44.650: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Dec  9 07:29:51.238: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-6627" for this suite. 12/09/22 07:29:51.243
------------------------------
â€¢ [SLOW TEST] [6.633 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    listing custom resource definition objects works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:85

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:29:44.618
    Dec  9 07:29:44.618: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename custom-resource-definition 12/09/22 07:29:44.619
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:29:44.64
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:29:44.645
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] listing custom resource definition objects works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:85
    Dec  9 07:29:44.650: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:29:51.238: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-6627" for this suite. 12/09/22 07:29:51.243
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:29:51.252
Dec  9 07:29:51.252: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename custom-resource-definition 12/09/22 07:29:51.253
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:29:51.272
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:29:51.276
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198
STEP: fetching the /apis discovery document 12/09/22 07:29:51.28
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document 12/09/22 07:29:51.282
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document 12/09/22 07:29:51.282
STEP: fetching the /apis/apiextensions.k8s.io discovery document 12/09/22 07:29:51.282
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document 12/09/22 07:29:51.284
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document 12/09/22 07:29:51.284
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document 12/09/22 07:29:51.285
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Dec  9 07:29:51.285: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-1774" for this suite. 12/09/22 07:29:51.29
------------------------------
â€¢ [0.047 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:29:51.252
    Dec  9 07:29:51.252: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename custom-resource-definition 12/09/22 07:29:51.253
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:29:51.272
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:29:51.276
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] should include custom resource definition resources in discovery documents [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:198
    STEP: fetching the /apis discovery document 12/09/22 07:29:51.28
    STEP: finding the apiextensions.k8s.io API group in the /apis discovery document 12/09/22 07:29:51.282
    STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document 12/09/22 07:29:51.282
    STEP: fetching the /apis/apiextensions.k8s.io discovery document 12/09/22 07:29:51.282
    STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document 12/09/22 07:29:51.284
    STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document 12/09/22 07:29:51.284
    STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document 12/09/22 07:29:51.285
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:29:51.285: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-1774" for this suite. 12/09/22 07:29:51.29
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:29:51.301
Dec  9 07:29:51.301: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename deployment 12/09/22 07:29:51.302
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:29:51.334
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:29:51.339
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113
Dec  9 07:29:51.344: INFO: Creating deployment "test-recreate-deployment"
Dec  9 07:29:51.349: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Dec  9 07:29:51.359: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Dec  9 07:29:53.375: INFO: Waiting deployment "test-recreate-deployment" to complete
Dec  9 07:29:53.378: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.December, 9, 7, 29, 51, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 9, 7, 29, 51, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 9, 7, 29, 51, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 9, 7, 29, 51, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-795566c5cb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  9 07:29:55.381: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Dec  9 07:29:55.390: INFO: Updating deployment test-recreate-deployment
Dec  9 07:29:55.390: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Dec  9 07:29:55.517: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-3030  27a8ec61-c4b5-442f-be03-7a8e157ca49a 11372 2 2022-12-09 07:29:51 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2022-12-09 07:29:55 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-09 07:29:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0029dae08 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2022-12-09 07:29:55 +0000 UTC,LastTransitionTime:2022-12-09 07:29:55 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-cff6dc657" is progressing.,LastUpdateTime:2022-12-09 07:29:55 +0000 UTC,LastTransitionTime:2022-12-09 07:29:51 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Dec  9 07:29:55.529: INFO: New ReplicaSet "test-recreate-deployment-cff6dc657" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-cff6dc657  deployment-3030  02f75e93-d512-4bf9-adfa-ea2f1f1c600a 11370 1 2022-12-09 07:29:55 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 27a8ec61-c4b5-442f-be03-7a8e157ca49a 0xc0029db340 0xc0029db341}] [] [{kube-controller-manager Update apps/v1 2022-12-09 07:29:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"27a8ec61-c4b5-442f-be03-7a8e157ca49a\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-09 07:29:55 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: cff6dc657,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0029db3e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Dec  9 07:29:55.530: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Dec  9 07:29:55.530: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-795566c5cb  deployment-3030  d50e208a-dce2-4b5c-ba47-be14afa16acf 11360 2 2022-12-09 07:29:51 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:795566c5cb] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 27a8ec61-c4b5-442f-be03-7a8e157ca49a 0xc0029db1f7 0xc0029db1f8}] [] [{kube-controller-manager Update apps/v1 2022-12-09 07:29:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"27a8ec61-c4b5-442f-be03-7a8e157ca49a\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-09 07:29:55 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 795566c5cb,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:795566c5cb] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0029db2c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Dec  9 07:29:55.548: INFO: Pod "test-recreate-deployment-cff6dc657-ttxcc" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-cff6dc657-ttxcc test-recreate-deployment-cff6dc657- deployment-3030  ee239c6c-d78f-4e41-8be9-023a44a66629 11371 0 2022-12-09 07:29:55 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[] [{apps/v1 ReplicaSet test-recreate-deployment-cff6dc657 02f75e93-d512-4bf9-adfa-ea2f1f1c600a 0xc00375a180 0xc00375a181}] [] [{kube-controller-manager Update v1 2022-12-09 07:29:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"02f75e93-d512-4bf9-adfa-ea2f1f1c600a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-09 07:29:55 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-kfqht,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-kfqht,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-10-179,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 07:29:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 07:29:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 07:29:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 07:29:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.10.179,PodIP:,StartTime:2022-12-09 07:29:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Dec  9 07:29:55.548: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-3030" for this suite. 12/09/22 07:29:55.555
------------------------------
â€¢ [4.262 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:29:51.301
    Dec  9 07:29:51.301: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename deployment 12/09/22 07:29:51.302
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:29:51.334
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:29:51.339
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] RecreateDeployment should delete old pods and create new ones [Conformance]
      test/e2e/apps/deployment.go:113
    Dec  9 07:29:51.344: INFO: Creating deployment "test-recreate-deployment"
    Dec  9 07:29:51.349: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
    Dec  9 07:29:51.359: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
    Dec  9 07:29:53.375: INFO: Waiting deployment "test-recreate-deployment" to complete
    Dec  9 07:29:53.378: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.December, 9, 7, 29, 51, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 9, 7, 29, 51, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 9, 7, 29, 51, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 9, 7, 29, 51, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-795566c5cb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Dec  9 07:29:55.381: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
    Dec  9 07:29:55.390: INFO: Updating deployment test-recreate-deployment
    Dec  9 07:29:55.390: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Dec  9 07:29:55.517: INFO: Deployment "test-recreate-deployment":
    &Deployment{ObjectMeta:{test-recreate-deployment  deployment-3030  27a8ec61-c4b5-442f-be03-7a8e157ca49a 11372 2 2022-12-09 07:29:51 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2022-12-09 07:29:55 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-09 07:29:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0029dae08 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2022-12-09 07:29:55 +0000 UTC,LastTransitionTime:2022-12-09 07:29:55 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-cff6dc657" is progressing.,LastUpdateTime:2022-12-09 07:29:55 +0000 UTC,LastTransitionTime:2022-12-09 07:29:51 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

    Dec  9 07:29:55.529: INFO: New ReplicaSet "test-recreate-deployment-cff6dc657" of Deployment "test-recreate-deployment":
    &ReplicaSet{ObjectMeta:{test-recreate-deployment-cff6dc657  deployment-3030  02f75e93-d512-4bf9-adfa-ea2f1f1c600a 11370 1 2022-12-09 07:29:55 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 27a8ec61-c4b5-442f-be03-7a8e157ca49a 0xc0029db340 0xc0029db341}] [] [{kube-controller-manager Update apps/v1 2022-12-09 07:29:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"27a8ec61-c4b5-442f-be03-7a8e157ca49a\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-09 07:29:55 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: cff6dc657,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0029db3e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Dec  9 07:29:55.530: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
    Dec  9 07:29:55.530: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-795566c5cb  deployment-3030  d50e208a-dce2-4b5c-ba47-be14afa16acf 11360 2 2022-12-09 07:29:51 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:795566c5cb] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 27a8ec61-c4b5-442f-be03-7a8e157ca49a 0xc0029db1f7 0xc0029db1f8}] [] [{kube-controller-manager Update apps/v1 2022-12-09 07:29:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"27a8ec61-c4b5-442f-be03-7a8e157ca49a\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-09 07:29:55 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 795566c5cb,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:795566c5cb] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0029db2c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Dec  9 07:29:55.548: INFO: Pod "test-recreate-deployment-cff6dc657-ttxcc" is not available:
    &Pod{ObjectMeta:{test-recreate-deployment-cff6dc657-ttxcc test-recreate-deployment-cff6dc657- deployment-3030  ee239c6c-d78f-4e41-8be9-023a44a66629 11371 0 2022-12-09 07:29:55 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[] [{apps/v1 ReplicaSet test-recreate-deployment-cff6dc657 02f75e93-d512-4bf9-adfa-ea2f1f1c600a 0xc00375a180 0xc00375a181}] [] [{kube-controller-manager Update v1 2022-12-09 07:29:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"02f75e93-d512-4bf9-adfa-ea2f1f1c600a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-09 07:29:55 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-kfqht,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-kfqht,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-10-179,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 07:29:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 07:29:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 07:29:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 07:29:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.10.179,PodIP:,StartTime:2022-12-09 07:29:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:29:55.548: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-3030" for this suite. 12/09/22 07:29:55.555
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] Projected downwardAPI
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:162
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:29:55.563
Dec  9 07:29:55.563: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename projected 12/09/22 07:29:55.564
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:29:55.591
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:29:55.597
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:162
STEP: Creating the pod 12/09/22 07:29:55.6
Dec  9 07:29:55.610: INFO: Waiting up to 5m0s for pod "annotationupdate5ff08e20-9f4d-40f8-bf70-fef7857e1894" in namespace "projected-4565" to be "running and ready"
Dec  9 07:29:55.615: INFO: Pod "annotationupdate5ff08e20-9f4d-40f8-bf70-fef7857e1894": Phase="Pending", Reason="", readiness=false. Elapsed: 4.652877ms
Dec  9 07:29:55.615: INFO: The phase of Pod annotationupdate5ff08e20-9f4d-40f8-bf70-fef7857e1894 is Pending, waiting for it to be Running (with Ready = true)
Dec  9 07:29:57.618: INFO: Pod "annotationupdate5ff08e20-9f4d-40f8-bf70-fef7857e1894": Phase="Running", Reason="", readiness=true. Elapsed: 2.007937555s
Dec  9 07:29:57.618: INFO: The phase of Pod annotationupdate5ff08e20-9f4d-40f8-bf70-fef7857e1894 is Running (Ready = true)
Dec  9 07:29:57.618: INFO: Pod "annotationupdate5ff08e20-9f4d-40f8-bf70-fef7857e1894" satisfied condition "running and ready"
Dec  9 07:29:58.138: INFO: Successfully updated pod "annotationupdate5ff08e20-9f4d-40f8-bf70-fef7857e1894"
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Dec  9 07:30:00.152: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-4565" for this suite. 12/09/22 07:30:00.157
------------------------------
â€¢ [4.599 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:162

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:29:55.563
    Dec  9 07:29:55.563: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename projected 12/09/22 07:29:55.564
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:29:55.591
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:29:55.597
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should update annotations on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:162
    STEP: Creating the pod 12/09/22 07:29:55.6
    Dec  9 07:29:55.610: INFO: Waiting up to 5m0s for pod "annotationupdate5ff08e20-9f4d-40f8-bf70-fef7857e1894" in namespace "projected-4565" to be "running and ready"
    Dec  9 07:29:55.615: INFO: Pod "annotationupdate5ff08e20-9f4d-40f8-bf70-fef7857e1894": Phase="Pending", Reason="", readiness=false. Elapsed: 4.652877ms
    Dec  9 07:29:55.615: INFO: The phase of Pod annotationupdate5ff08e20-9f4d-40f8-bf70-fef7857e1894 is Pending, waiting for it to be Running (with Ready = true)
    Dec  9 07:29:57.618: INFO: Pod "annotationupdate5ff08e20-9f4d-40f8-bf70-fef7857e1894": Phase="Running", Reason="", readiness=true. Elapsed: 2.007937555s
    Dec  9 07:29:57.618: INFO: The phase of Pod annotationupdate5ff08e20-9f4d-40f8-bf70-fef7857e1894 is Running (Ready = true)
    Dec  9 07:29:57.618: INFO: Pod "annotationupdate5ff08e20-9f4d-40f8-bf70-fef7857e1894" satisfied condition "running and ready"
    Dec  9 07:29:58.138: INFO: Successfully updated pod "annotationupdate5ff08e20-9f4d-40f8-bf70-fef7857e1894"
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:30:00.152: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-4565" for this suite. 12/09/22 07:30:00.157
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:109
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:30:00.163
Dec  9 07:30:00.163: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename projected 12/09/22 07:30:00.164
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:30:00.186
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:30:00.189
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:109
STEP: Creating configMap with name projected-configmap-test-volume-map-f5f31fea-43ce-4646-a9bd-5f2d731b6ab3 12/09/22 07:30:00.194
STEP: Creating a pod to test consume configMaps 12/09/22 07:30:00.2
Dec  9 07:30:00.209: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-e69439de-0b08-44e2-b778-04c929e160a9" in namespace "projected-7911" to be "Succeeded or Failed"
Dec  9 07:30:00.221: INFO: Pod "pod-projected-configmaps-e69439de-0b08-44e2-b778-04c929e160a9": Phase="Pending", Reason="", readiness=false. Elapsed: 11.636339ms
Dec  9 07:30:02.231: INFO: Pod "pod-projected-configmaps-e69439de-0b08-44e2-b778-04c929e160a9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021628083s
Dec  9 07:30:04.227: INFO: Pod "pod-projected-configmaps-e69439de-0b08-44e2-b778-04c929e160a9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017302062s
STEP: Saw pod success 12/09/22 07:30:04.227
Dec  9 07:30:04.227: INFO: Pod "pod-projected-configmaps-e69439de-0b08-44e2-b778-04c929e160a9" satisfied condition "Succeeded or Failed"
Dec  9 07:30:04.231: INFO: Trying to get logs from node ip-10-0-17-108 pod pod-projected-configmaps-e69439de-0b08-44e2-b778-04c929e160a9 container agnhost-container: <nil>
STEP: delete the pod 12/09/22 07:30:04.278
Dec  9 07:30:04.313: INFO: Waiting for pod pod-projected-configmaps-e69439de-0b08-44e2-b778-04c929e160a9 to disappear
Dec  9 07:30:04.333: INFO: Pod pod-projected-configmaps-e69439de-0b08-44e2-b778-04c929e160a9 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Dec  9 07:30:04.333: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-7911" for this suite. 12/09/22 07:30:04.345
------------------------------
â€¢ [4.193 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:109

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:30:00.163
    Dec  9 07:30:00.163: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename projected 12/09/22 07:30:00.164
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:30:00.186
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:30:00.189
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:109
    STEP: Creating configMap with name projected-configmap-test-volume-map-f5f31fea-43ce-4646-a9bd-5f2d731b6ab3 12/09/22 07:30:00.194
    STEP: Creating a pod to test consume configMaps 12/09/22 07:30:00.2
    Dec  9 07:30:00.209: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-e69439de-0b08-44e2-b778-04c929e160a9" in namespace "projected-7911" to be "Succeeded or Failed"
    Dec  9 07:30:00.221: INFO: Pod "pod-projected-configmaps-e69439de-0b08-44e2-b778-04c929e160a9": Phase="Pending", Reason="", readiness=false. Elapsed: 11.636339ms
    Dec  9 07:30:02.231: INFO: Pod "pod-projected-configmaps-e69439de-0b08-44e2-b778-04c929e160a9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021628083s
    Dec  9 07:30:04.227: INFO: Pod "pod-projected-configmaps-e69439de-0b08-44e2-b778-04c929e160a9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017302062s
    STEP: Saw pod success 12/09/22 07:30:04.227
    Dec  9 07:30:04.227: INFO: Pod "pod-projected-configmaps-e69439de-0b08-44e2-b778-04c929e160a9" satisfied condition "Succeeded or Failed"
    Dec  9 07:30:04.231: INFO: Trying to get logs from node ip-10-0-17-108 pod pod-projected-configmaps-e69439de-0b08-44e2-b778-04c929e160a9 container agnhost-container: <nil>
    STEP: delete the pod 12/09/22 07:30:04.278
    Dec  9 07:30:04.313: INFO: Waiting for pod pod-projected-configmaps-e69439de-0b08-44e2-b778-04c929e160a9 to disappear
    Dec  9 07:30:04.333: INFO: Pod pod-projected-configmaps-e69439de-0b08-44e2-b778-04c929e160a9 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:30:04.333: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-7911" for this suite. 12/09/22 07:30:04.345
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ControllerRevision [Serial]
  should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:30:04.361
Dec  9 07:30:04.361: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename controllerrevisions 12/09/22 07:30:04.362
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:30:04.442
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:30:04.457
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/apps/controller_revision.go:93
[It] should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124
STEP: Creating DaemonSet "e2e-mvtz8-daemon-set" 12/09/22 07:30:04.532
STEP: Check that daemon pods launch on every node of the cluster. 12/09/22 07:30:04.552
Dec  9 07:30:04.559: INFO: DaemonSet pods can't tolerate node ip-10-0-7-84 with taints [{Key:node-role.kubernetes.io/controller Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 07:30:04.577: INFO: Number of nodes with available pods controlled by daemonset e2e-mvtz8-daemon-set: 0
Dec  9 07:30:04.577: INFO: Node ip-10-0-10-179 is running 0 daemon pod, expected 1
Dec  9 07:30:05.594: INFO: DaemonSet pods can't tolerate node ip-10-0-7-84 with taints [{Key:node-role.kubernetes.io/controller Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 07:30:05.603: INFO: Number of nodes with available pods controlled by daemonset e2e-mvtz8-daemon-set: 0
Dec  9 07:30:05.603: INFO: Node ip-10-0-10-179 is running 0 daemon pod, expected 1
Dec  9 07:30:06.580: INFO: DaemonSet pods can't tolerate node ip-10-0-7-84 with taints [{Key:node-role.kubernetes.io/controller Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 07:30:06.583: INFO: Number of nodes with available pods controlled by daemonset e2e-mvtz8-daemon-set: 1
Dec  9 07:30:06.583: INFO: Node ip-10-0-10-179 is running 0 daemon pod, expected 1
Dec  9 07:30:07.581: INFO: DaemonSet pods can't tolerate node ip-10-0-7-84 with taints [{Key:node-role.kubernetes.io/controller Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 07:30:07.584: INFO: Number of nodes with available pods controlled by daemonset e2e-mvtz8-daemon-set: 2
Dec  9 07:30:07.584: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset e2e-mvtz8-daemon-set
STEP: Confirm DaemonSet "e2e-mvtz8-daemon-set" successfully created with "daemonset-name=e2e-mvtz8-daemon-set" label 12/09/22 07:30:07.587
STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-mvtz8-daemon-set" 12/09/22 07:30:07.594
Dec  9 07:30:07.598: INFO: Located ControllerRevision: "e2e-mvtz8-daemon-set-7bc8f7f5b8"
STEP: Patching ControllerRevision "e2e-mvtz8-daemon-set-7bc8f7f5b8" 12/09/22 07:30:07.602
Dec  9 07:30:07.608: INFO: e2e-mvtz8-daemon-set-7bc8f7f5b8 has been patched
STEP: Create a new ControllerRevision 12/09/22 07:30:07.608
Dec  9 07:30:07.623: INFO: Created ControllerRevision: e2e-mvtz8-daemon-set-677cc5798
STEP: Confirm that there are two ControllerRevisions 12/09/22 07:30:07.623
Dec  9 07:30:07.623: INFO: Requesting list of ControllerRevisions to confirm quantity
Dec  9 07:30:07.627: INFO: Found 2 ControllerRevisions
STEP: Deleting ControllerRevision "e2e-mvtz8-daemon-set-7bc8f7f5b8" 12/09/22 07:30:07.627
STEP: Confirm that there is only one ControllerRevision 12/09/22 07:30:07.633
Dec  9 07:30:07.633: INFO: Requesting list of ControllerRevisions to confirm quantity
Dec  9 07:30:07.636: INFO: Found 1 ControllerRevisions
STEP: Updating ControllerRevision "e2e-mvtz8-daemon-set-677cc5798" 12/09/22 07:30:07.639
Dec  9 07:30:07.648: INFO: e2e-mvtz8-daemon-set-677cc5798 has been updated
STEP: Generate another ControllerRevision by patching the Daemonset 12/09/22 07:30:07.648
W1209 07:30:07.655499      20 warnings.go:70] unknown field "updateStrategy"
STEP: Confirm that there are two ControllerRevisions 12/09/22 07:30:07.655
Dec  9 07:30:07.655: INFO: Requesting list of ControllerRevisions to confirm quantity
Dec  9 07:30:08.659: INFO: Requesting list of ControllerRevisions to confirm quantity
Dec  9 07:30:08.663: INFO: Found 2 ControllerRevisions
STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-mvtz8-daemon-set-677cc5798=updated" 12/09/22 07:30:08.663
STEP: Confirm that there is only one ControllerRevision 12/09/22 07:30:08.677
Dec  9 07:30:08.677: INFO: Requesting list of ControllerRevisions to confirm quantity
Dec  9 07:30:08.681: INFO: Found 1 ControllerRevisions
Dec  9 07:30:08.684: INFO: ControllerRevision "e2e-mvtz8-daemon-set-b77587c7f" has revision 3
[AfterEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/apps/controller_revision.go:58
STEP: Deleting DaemonSet "e2e-mvtz8-daemon-set" 12/09/22 07:30:08.687
STEP: deleting DaemonSet.extensions e2e-mvtz8-daemon-set in namespace controllerrevisions-7868, will wait for the garbage collector to delete the pods 12/09/22 07:30:08.688
Dec  9 07:30:08.747: INFO: Deleting DaemonSet.extensions e2e-mvtz8-daemon-set took: 5.987275ms
Dec  9 07:30:08.849: INFO: Terminating DaemonSet.extensions e2e-mvtz8-daemon-set pods took: 101.506389ms
Dec  9 07:30:10.252: INFO: Number of nodes with available pods controlled by daemonset e2e-mvtz8-daemon-set: 0
Dec  9 07:30:10.252: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset e2e-mvtz8-daemon-set
Dec  9 07:30:10.254: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"11553"},"items":null}

Dec  9 07:30:10.257: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"11553"},"items":null}

[AfterEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/node/init/init.go:32
Dec  9 07:30:10.266: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "controllerrevisions-7868" for this suite. 12/09/22 07:30:10.269
------------------------------
â€¢ [SLOW TEST] [5.915 seconds]
[sig-apps] ControllerRevision [Serial]
test/e2e/apps/framework.go:23
  should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:30:04.361
    Dec  9 07:30:04.361: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename controllerrevisions 12/09/22 07:30:04.362
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:30:04.442
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:30:04.457
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/apps/controller_revision.go:93
    [It] should manage the lifecycle of a ControllerRevision [Conformance]
      test/e2e/apps/controller_revision.go:124
    STEP: Creating DaemonSet "e2e-mvtz8-daemon-set" 12/09/22 07:30:04.532
    STEP: Check that daemon pods launch on every node of the cluster. 12/09/22 07:30:04.552
    Dec  9 07:30:04.559: INFO: DaemonSet pods can't tolerate node ip-10-0-7-84 with taints [{Key:node-role.kubernetes.io/controller Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Dec  9 07:30:04.577: INFO: Number of nodes with available pods controlled by daemonset e2e-mvtz8-daemon-set: 0
    Dec  9 07:30:04.577: INFO: Node ip-10-0-10-179 is running 0 daemon pod, expected 1
    Dec  9 07:30:05.594: INFO: DaemonSet pods can't tolerate node ip-10-0-7-84 with taints [{Key:node-role.kubernetes.io/controller Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Dec  9 07:30:05.603: INFO: Number of nodes with available pods controlled by daemonset e2e-mvtz8-daemon-set: 0
    Dec  9 07:30:05.603: INFO: Node ip-10-0-10-179 is running 0 daemon pod, expected 1
    Dec  9 07:30:06.580: INFO: DaemonSet pods can't tolerate node ip-10-0-7-84 with taints [{Key:node-role.kubernetes.io/controller Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Dec  9 07:30:06.583: INFO: Number of nodes with available pods controlled by daemonset e2e-mvtz8-daemon-set: 1
    Dec  9 07:30:06.583: INFO: Node ip-10-0-10-179 is running 0 daemon pod, expected 1
    Dec  9 07:30:07.581: INFO: DaemonSet pods can't tolerate node ip-10-0-7-84 with taints [{Key:node-role.kubernetes.io/controller Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Dec  9 07:30:07.584: INFO: Number of nodes with available pods controlled by daemonset e2e-mvtz8-daemon-set: 2
    Dec  9 07:30:07.584: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset e2e-mvtz8-daemon-set
    STEP: Confirm DaemonSet "e2e-mvtz8-daemon-set" successfully created with "daemonset-name=e2e-mvtz8-daemon-set" label 12/09/22 07:30:07.587
    STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-mvtz8-daemon-set" 12/09/22 07:30:07.594
    Dec  9 07:30:07.598: INFO: Located ControllerRevision: "e2e-mvtz8-daemon-set-7bc8f7f5b8"
    STEP: Patching ControllerRevision "e2e-mvtz8-daemon-set-7bc8f7f5b8" 12/09/22 07:30:07.602
    Dec  9 07:30:07.608: INFO: e2e-mvtz8-daemon-set-7bc8f7f5b8 has been patched
    STEP: Create a new ControllerRevision 12/09/22 07:30:07.608
    Dec  9 07:30:07.623: INFO: Created ControllerRevision: e2e-mvtz8-daemon-set-677cc5798
    STEP: Confirm that there are two ControllerRevisions 12/09/22 07:30:07.623
    Dec  9 07:30:07.623: INFO: Requesting list of ControllerRevisions to confirm quantity
    Dec  9 07:30:07.627: INFO: Found 2 ControllerRevisions
    STEP: Deleting ControllerRevision "e2e-mvtz8-daemon-set-7bc8f7f5b8" 12/09/22 07:30:07.627
    STEP: Confirm that there is only one ControllerRevision 12/09/22 07:30:07.633
    Dec  9 07:30:07.633: INFO: Requesting list of ControllerRevisions to confirm quantity
    Dec  9 07:30:07.636: INFO: Found 1 ControllerRevisions
    STEP: Updating ControllerRevision "e2e-mvtz8-daemon-set-677cc5798" 12/09/22 07:30:07.639
    Dec  9 07:30:07.648: INFO: e2e-mvtz8-daemon-set-677cc5798 has been updated
    STEP: Generate another ControllerRevision by patching the Daemonset 12/09/22 07:30:07.648
    W1209 07:30:07.655499      20 warnings.go:70] unknown field "updateStrategy"
    STEP: Confirm that there are two ControllerRevisions 12/09/22 07:30:07.655
    Dec  9 07:30:07.655: INFO: Requesting list of ControllerRevisions to confirm quantity
    Dec  9 07:30:08.659: INFO: Requesting list of ControllerRevisions to confirm quantity
    Dec  9 07:30:08.663: INFO: Found 2 ControllerRevisions
    STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-mvtz8-daemon-set-677cc5798=updated" 12/09/22 07:30:08.663
    STEP: Confirm that there is only one ControllerRevision 12/09/22 07:30:08.677
    Dec  9 07:30:08.677: INFO: Requesting list of ControllerRevisions to confirm quantity
    Dec  9 07:30:08.681: INFO: Found 1 ControllerRevisions
    Dec  9 07:30:08.684: INFO: ControllerRevision "e2e-mvtz8-daemon-set-b77587c7f" has revision 3
    [AfterEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/apps/controller_revision.go:58
    STEP: Deleting DaemonSet "e2e-mvtz8-daemon-set" 12/09/22 07:30:08.687
    STEP: deleting DaemonSet.extensions e2e-mvtz8-daemon-set in namespace controllerrevisions-7868, will wait for the garbage collector to delete the pods 12/09/22 07:30:08.688
    Dec  9 07:30:08.747: INFO: Deleting DaemonSet.extensions e2e-mvtz8-daemon-set took: 5.987275ms
    Dec  9 07:30:08.849: INFO: Terminating DaemonSet.extensions e2e-mvtz8-daemon-set pods took: 101.506389ms
    Dec  9 07:30:10.252: INFO: Number of nodes with available pods controlled by daemonset e2e-mvtz8-daemon-set: 0
    Dec  9 07:30:10.252: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset e2e-mvtz8-daemon-set
    Dec  9 07:30:10.254: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"11553"},"items":null}

    Dec  9 07:30:10.257: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"11553"},"items":null}

    [AfterEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:30:10.266: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "controllerrevisions-7868" for this suite. 12/09/22 07:30:10.269
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] Aggregator
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100
[BeforeEach] [sig-api-machinery] Aggregator
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:30:10.279
Dec  9 07:30:10.279: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename aggregator 12/09/22 07:30:10.28
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:30:10.297
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:30:10.301
[BeforeEach] [sig-api-machinery] Aggregator
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] Aggregator
  test/e2e/apimachinery/aggregator.go:78
Dec  9 07:30:10.305: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
[It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100
STEP: Registering the sample API server. 12/09/22 07:30:10.306
Dec  9 07:30:11.172: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
Dec  9 07:30:13.242: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.December, 9, 7, 30, 11, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 9, 7, 30, 11, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 9, 7, 30, 11, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 9, 7, 30, 11, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  9 07:30:15.253: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.December, 9, 7, 30, 11, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 9, 7, 30, 11, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 9, 7, 30, 11, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 9, 7, 30, 11, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  9 07:30:17.247: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.December, 9, 7, 30, 11, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 9, 7, 30, 11, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 9, 7, 30, 11, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 9, 7, 30, 11, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  9 07:30:19.247: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.December, 9, 7, 30, 11, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 9, 7, 30, 11, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 9, 7, 30, 11, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 9, 7, 30, 11, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  9 07:30:21.245: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.December, 9, 7, 30, 11, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 9, 7, 30, 11, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 9, 7, 30, 11, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 9, 7, 30, 11, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  9 07:30:23.247: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.December, 9, 7, 30, 11, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 9, 7, 30, 11, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 9, 7, 30, 11, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 9, 7, 30, 11, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  9 07:30:25.248: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.December, 9, 7, 30, 11, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 9, 7, 30, 11, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 9, 7, 30, 11, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 9, 7, 30, 11, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  9 07:30:27.246: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.December, 9, 7, 30, 11, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 9, 7, 30, 11, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 9, 7, 30, 11, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 9, 7, 30, 11, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  9 07:30:29.247: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.December, 9, 7, 30, 11, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 9, 7, 30, 11, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 9, 7, 30, 11, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 9, 7, 30, 11, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  9 07:30:31.248: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.December, 9, 7, 30, 11, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 9, 7, 30, 11, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 9, 7, 30, 11, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 9, 7, 30, 11, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  9 07:30:33.248: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.December, 9, 7, 30, 11, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 9, 7, 30, 11, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 9, 7, 30, 11, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 9, 7, 30, 11, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  9 07:30:35.484: INFO: Waited 222.749251ms for the sample-apiserver to be ready to handle requests.
STEP: Read Status for v1alpha1.wardle.example.com 12/09/22 07:30:35.542
STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}' 12/09/22 07:30:35.545
STEP: List APIServices 12/09/22 07:30:35.553
Dec  9 07:30:35.560: INFO: Found v1alpha1.wardle.example.com in APIServiceList
[AfterEach] [sig-api-machinery] Aggregator
  test/e2e/apimachinery/aggregator.go:68
[AfterEach] [sig-api-machinery] Aggregator
  test/e2e/framework/node/init/init.go:32
Dec  9 07:30:35.980: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Aggregator
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Aggregator
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Aggregator
  tear down framework | framework.go:193
STEP: Destroying namespace "aggregator-1835" for this suite. 12/09/22 07:30:35.987
------------------------------
â€¢ [SLOW TEST] [25.718 seconds]
[sig-api-machinery] Aggregator
test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Aggregator
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:30:10.279
    Dec  9 07:30:10.279: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename aggregator 12/09/22 07:30:10.28
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:30:10.297
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:30:10.301
    [BeforeEach] [sig-api-machinery] Aggregator
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] Aggregator
      test/e2e/apimachinery/aggregator.go:78
    Dec  9 07:30:10.305: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    [It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
      test/e2e/apimachinery/aggregator.go:100
    STEP: Registering the sample API server. 12/09/22 07:30:10.306
    Dec  9 07:30:11.172: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
    Dec  9 07:30:13.242: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.December, 9, 7, 30, 11, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 9, 7, 30, 11, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 9, 7, 30, 11, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 9, 7, 30, 11, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Dec  9 07:30:15.253: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.December, 9, 7, 30, 11, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 9, 7, 30, 11, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 9, 7, 30, 11, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 9, 7, 30, 11, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Dec  9 07:30:17.247: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.December, 9, 7, 30, 11, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 9, 7, 30, 11, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 9, 7, 30, 11, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 9, 7, 30, 11, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Dec  9 07:30:19.247: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.December, 9, 7, 30, 11, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 9, 7, 30, 11, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 9, 7, 30, 11, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 9, 7, 30, 11, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Dec  9 07:30:21.245: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.December, 9, 7, 30, 11, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 9, 7, 30, 11, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 9, 7, 30, 11, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 9, 7, 30, 11, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Dec  9 07:30:23.247: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.December, 9, 7, 30, 11, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 9, 7, 30, 11, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 9, 7, 30, 11, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 9, 7, 30, 11, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Dec  9 07:30:25.248: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.December, 9, 7, 30, 11, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 9, 7, 30, 11, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 9, 7, 30, 11, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 9, 7, 30, 11, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Dec  9 07:30:27.246: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.December, 9, 7, 30, 11, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 9, 7, 30, 11, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 9, 7, 30, 11, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 9, 7, 30, 11, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Dec  9 07:30:29.247: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.December, 9, 7, 30, 11, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 9, 7, 30, 11, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 9, 7, 30, 11, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 9, 7, 30, 11, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Dec  9 07:30:31.248: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.December, 9, 7, 30, 11, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 9, 7, 30, 11, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 9, 7, 30, 11, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 9, 7, 30, 11, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Dec  9 07:30:33.248: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.December, 9, 7, 30, 11, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 9, 7, 30, 11, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 9, 7, 30, 11, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 9, 7, 30, 11, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Dec  9 07:30:35.484: INFO: Waited 222.749251ms for the sample-apiserver to be ready to handle requests.
    STEP: Read Status for v1alpha1.wardle.example.com 12/09/22 07:30:35.542
    STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}' 12/09/22 07:30:35.545
    STEP: List APIServices 12/09/22 07:30:35.553
    Dec  9 07:30:35.560: INFO: Found v1alpha1.wardle.example.com in APIServiceList
    [AfterEach] [sig-api-machinery] Aggregator
      test/e2e/apimachinery/aggregator.go:68
    [AfterEach] [sig-api-machinery] Aggregator
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:30:35.980: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Aggregator
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Aggregator
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Aggregator
      tear down framework | framework.go:193
    STEP: Destroying namespace "aggregator-1835" for this suite. 12/09/22 07:30:35.987
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:122
[BeforeEach] [sig-network] Networking
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:30:36.001
Dec  9 07:30:36.001: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename pod-network-test 12/09/22 07:30:36.002
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:30:36.037
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:30:36.043
[BeforeEach] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:31
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:122
STEP: Performing setup for networking test in namespace pod-network-test-279 12/09/22 07:30:36.048
STEP: creating a selector 12/09/22 07:30:36.048
STEP: Creating the service pods in kubernetes 12/09/22 07:30:36.048
Dec  9 07:30:36.049: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Dec  9 07:30:36.093: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-279" to be "running and ready"
Dec  9 07:30:36.113: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 20.396701ms
Dec  9 07:30:36.114: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Dec  9 07:30:38.118: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02476344s
Dec  9 07:30:38.118: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Dec  9 07:30:40.119: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.026376041s
Dec  9 07:30:40.119: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Dec  9 07:30:42.117: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.024020654s
Dec  9 07:30:42.117: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Dec  9 07:30:44.125: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.031599428s
Dec  9 07:30:44.125: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Dec  9 07:30:46.120: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.02662781s
Dec  9 07:30:46.120: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Dec  9 07:30:48.120: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.026958202s
Dec  9 07:30:48.120: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Dec  9 07:30:48.120: INFO: Pod "netserver-0" satisfied condition "running and ready"
Dec  9 07:30:48.123: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-279" to be "running and ready"
Dec  9 07:30:48.126: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 3.148706ms
Dec  9 07:30:48.127: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Dec  9 07:30:48.127: INFO: Pod "netserver-1" satisfied condition "running and ready"
STEP: Creating test pods 12/09/22 07:30:48.13
Dec  9 07:30:48.154: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-279" to be "running"
Dec  9 07:30:48.184: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 29.440692ms
Dec  9 07:30:50.189: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.034850434s
Dec  9 07:30:50.189: INFO: Pod "test-container-pod" satisfied condition "running"
Dec  9 07:30:50.193: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-279" to be "running"
Dec  9 07:30:50.197: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 3.523632ms
Dec  9 07:30:50.197: INFO: Pod "host-test-container-pod" satisfied condition "running"
Dec  9 07:30:50.201: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
Dec  9 07:30:50.202: INFO: Going to poll 10.2.136.150 on port 8081 at least 0 times, with a maximum of 34 tries before failing
Dec  9 07:30:50.205: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.2.136.150 8081 | grep -v '^\s*$'] Namespace:pod-network-test-279 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec  9 07:30:50.206: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
Dec  9 07:30:50.206: INFO: ExecWithOptions: Clientset creation
Dec  9 07:30:50.207: INFO: ExecWithOptions: execute(POST https://10.3.0.1:443/api/v1/namespaces/pod-network-test-279/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.2.136.150+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Dec  9 07:30:51.269: INFO: Found all 1 expected endpoints: [netserver-0]
Dec  9 07:30:51.269: INFO: Going to poll 10.2.166.91 on port 8081 at least 0 times, with a maximum of 34 tries before failing
Dec  9 07:30:51.272: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.2.166.91 8081 | grep -v '^\s*$'] Namespace:pod-network-test-279 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec  9 07:30:51.272: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
Dec  9 07:30:51.273: INFO: ExecWithOptions: Clientset creation
Dec  9 07:30:51.273: INFO: ExecWithOptions: execute(POST https://10.3.0.1:443/api/v1/namespaces/pod-network-test-279/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.2.166.91+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Dec  9 07:30:52.355: INFO: Found all 1 expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  test/e2e/framework/node/init/init.go:32
Dec  9 07:30:52.355: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Networking
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Networking
  tear down framework | framework.go:193
STEP: Destroying namespace "pod-network-test-279" for this suite. 12/09/22 07:30:52.36
------------------------------
â€¢ [SLOW TEST] [16.365 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:30:36.001
    Dec  9 07:30:36.001: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename pod-network-test 12/09/22 07:30:36.002
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:30:36.037
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:30:36.043
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:31
    [It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:122
    STEP: Performing setup for networking test in namespace pod-network-test-279 12/09/22 07:30:36.048
    STEP: creating a selector 12/09/22 07:30:36.048
    STEP: Creating the service pods in kubernetes 12/09/22 07:30:36.048
    Dec  9 07:30:36.049: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Dec  9 07:30:36.093: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-279" to be "running and ready"
    Dec  9 07:30:36.113: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 20.396701ms
    Dec  9 07:30:36.114: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Dec  9 07:30:38.118: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02476344s
    Dec  9 07:30:38.118: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Dec  9 07:30:40.119: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.026376041s
    Dec  9 07:30:40.119: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Dec  9 07:30:42.117: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.024020654s
    Dec  9 07:30:42.117: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Dec  9 07:30:44.125: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.031599428s
    Dec  9 07:30:44.125: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Dec  9 07:30:46.120: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.02662781s
    Dec  9 07:30:46.120: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Dec  9 07:30:48.120: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.026958202s
    Dec  9 07:30:48.120: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Dec  9 07:30:48.120: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Dec  9 07:30:48.123: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-279" to be "running and ready"
    Dec  9 07:30:48.126: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 3.148706ms
    Dec  9 07:30:48.127: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Dec  9 07:30:48.127: INFO: Pod "netserver-1" satisfied condition "running and ready"
    STEP: Creating test pods 12/09/22 07:30:48.13
    Dec  9 07:30:48.154: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-279" to be "running"
    Dec  9 07:30:48.184: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 29.440692ms
    Dec  9 07:30:50.189: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.034850434s
    Dec  9 07:30:50.189: INFO: Pod "test-container-pod" satisfied condition "running"
    Dec  9 07:30:50.193: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-279" to be "running"
    Dec  9 07:30:50.197: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 3.523632ms
    Dec  9 07:30:50.197: INFO: Pod "host-test-container-pod" satisfied condition "running"
    Dec  9 07:30:50.201: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
    Dec  9 07:30:50.202: INFO: Going to poll 10.2.136.150 on port 8081 at least 0 times, with a maximum of 34 tries before failing
    Dec  9 07:30:50.205: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.2.136.150 8081 | grep -v '^\s*$'] Namespace:pod-network-test-279 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Dec  9 07:30:50.206: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    Dec  9 07:30:50.206: INFO: ExecWithOptions: Clientset creation
    Dec  9 07:30:50.207: INFO: ExecWithOptions: execute(POST https://10.3.0.1:443/api/v1/namespaces/pod-network-test-279/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.2.136.150+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Dec  9 07:30:51.269: INFO: Found all 1 expected endpoints: [netserver-0]
    Dec  9 07:30:51.269: INFO: Going to poll 10.2.166.91 on port 8081 at least 0 times, with a maximum of 34 tries before failing
    Dec  9 07:30:51.272: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.2.166.91 8081 | grep -v '^\s*$'] Namespace:pod-network-test-279 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Dec  9 07:30:51.272: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    Dec  9 07:30:51.273: INFO: ExecWithOptions: Clientset creation
    Dec  9 07:30:51.273: INFO: ExecWithOptions: execute(POST https://10.3.0.1:443/api/v1/namespaces/pod-network-test-279/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.2.166.91+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Dec  9 07:30:52.355: INFO: Found all 1 expected endpoints: [netserver-1]
    [AfterEach] [sig-network] Networking
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:30:52.355: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Networking
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Networking
      tear down framework | framework.go:193
    STEP: Destroying namespace "pod-network-test-279" for this suite. 12/09/22 07:30:52.36
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:47
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:30:52.367
Dec  9 07:30:52.367: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename projected 12/09/22 07:30:52.368
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:30:52.394
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:30:52.402
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:47
STEP: Creating configMap with name projected-configmap-test-volume-faf0562b-7b41-4a47-bfcb-94c68607439d 12/09/22 07:30:52.406
STEP: Creating a pod to test consume configMaps 12/09/22 07:30:52.412
Dec  9 07:30:52.425: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-05c362b7-f1da-4290-bab4-0a6d5e80936f" in namespace "projected-3965" to be "Succeeded or Failed"
Dec  9 07:30:52.442: INFO: Pod "pod-projected-configmaps-05c362b7-f1da-4290-bab4-0a6d5e80936f": Phase="Pending", Reason="", readiness=false. Elapsed: 16.972492ms
Dec  9 07:30:54.460: INFO: Pod "pod-projected-configmaps-05c362b7-f1da-4290-bab4-0a6d5e80936f": Phase="Running", Reason="", readiness=true. Elapsed: 2.034733735s
Dec  9 07:30:56.453: INFO: Pod "pod-projected-configmaps-05c362b7-f1da-4290-bab4-0a6d5e80936f": Phase="Running", Reason="", readiness=false. Elapsed: 4.027787036s
Dec  9 07:30:58.446: INFO: Pod "pod-projected-configmaps-05c362b7-f1da-4290-bab4-0a6d5e80936f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.020256469s
STEP: Saw pod success 12/09/22 07:30:58.446
Dec  9 07:30:58.446: INFO: Pod "pod-projected-configmaps-05c362b7-f1da-4290-bab4-0a6d5e80936f" satisfied condition "Succeeded or Failed"
Dec  9 07:30:58.449: INFO: Trying to get logs from node ip-10-0-17-108 pod pod-projected-configmaps-05c362b7-f1da-4290-bab4-0a6d5e80936f container agnhost-container: <nil>
STEP: delete the pod 12/09/22 07:30:58.454
Dec  9 07:30:58.473: INFO: Waiting for pod pod-projected-configmaps-05c362b7-f1da-4290-bab4-0a6d5e80936f to disappear
Dec  9 07:30:58.481: INFO: Pod pod-projected-configmaps-05c362b7-f1da-4290-bab4-0a6d5e80936f no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Dec  9 07:30:58.481: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-3965" for this suite. 12/09/22 07:30:58.486
------------------------------
â€¢ [SLOW TEST] [6.130 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:47

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:30:52.367
    Dec  9 07:30:52.367: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename projected 12/09/22 07:30:52.368
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:30:52.394
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:30:52.402
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:47
    STEP: Creating configMap with name projected-configmap-test-volume-faf0562b-7b41-4a47-bfcb-94c68607439d 12/09/22 07:30:52.406
    STEP: Creating a pod to test consume configMaps 12/09/22 07:30:52.412
    Dec  9 07:30:52.425: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-05c362b7-f1da-4290-bab4-0a6d5e80936f" in namespace "projected-3965" to be "Succeeded or Failed"
    Dec  9 07:30:52.442: INFO: Pod "pod-projected-configmaps-05c362b7-f1da-4290-bab4-0a6d5e80936f": Phase="Pending", Reason="", readiness=false. Elapsed: 16.972492ms
    Dec  9 07:30:54.460: INFO: Pod "pod-projected-configmaps-05c362b7-f1da-4290-bab4-0a6d5e80936f": Phase="Running", Reason="", readiness=true. Elapsed: 2.034733735s
    Dec  9 07:30:56.453: INFO: Pod "pod-projected-configmaps-05c362b7-f1da-4290-bab4-0a6d5e80936f": Phase="Running", Reason="", readiness=false. Elapsed: 4.027787036s
    Dec  9 07:30:58.446: INFO: Pod "pod-projected-configmaps-05c362b7-f1da-4290-bab4-0a6d5e80936f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.020256469s
    STEP: Saw pod success 12/09/22 07:30:58.446
    Dec  9 07:30:58.446: INFO: Pod "pod-projected-configmaps-05c362b7-f1da-4290-bab4-0a6d5e80936f" satisfied condition "Succeeded or Failed"
    Dec  9 07:30:58.449: INFO: Trying to get logs from node ip-10-0-17-108 pod pod-projected-configmaps-05c362b7-f1da-4290-bab4-0a6d5e80936f container agnhost-container: <nil>
    STEP: delete the pod 12/09/22 07:30:58.454
    Dec  9 07:30:58.473: INFO: Waiting for pod pod-projected-configmaps-05c362b7-f1da-4290-bab4-0a6d5e80936f to disappear
    Dec  9 07:30:58.481: INFO: Pod pod-projected-configmaps-05c362b7-f1da-4290-bab4-0a6d5e80936f no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:30:58.481: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-3965" for this suite. 12/09/22 07:30:58.486
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-network] Services
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2250
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:30:58.498
Dec  9 07:30:58.498: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename services 12/09/22 07:30:58.499
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:30:58.516
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:30:58.521
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2250
STEP: creating service in namespace services-2526 12/09/22 07:30:58.524
STEP: creating service affinity-nodeport-transition in namespace services-2526 12/09/22 07:30:58.525
STEP: creating replication controller affinity-nodeport-transition in namespace services-2526 12/09/22 07:30:58.536
I1209 07:30:58.546730      20 runners.go:193] Created replication controller with name: affinity-nodeport-transition, namespace: services-2526, replica count: 3
I1209 07:31:01.598161      20 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Dec  9 07:31:01.618: INFO: Creating new exec pod
Dec  9 07:31:01.641: INFO: Waiting up to 5m0s for pod "execpod-affinityc87vv" in namespace "services-2526" to be "running"
Dec  9 07:31:01.671: INFO: Pod "execpod-affinityc87vv": Phase="Pending", Reason="", readiness=false. Elapsed: 29.537099ms
Dec  9 07:31:03.675: INFO: Pod "execpod-affinityc87vv": Phase="Running", Reason="", readiness=true. Elapsed: 2.03374312s
Dec  9 07:31:03.675: INFO: Pod "execpod-affinityc87vv" satisfied condition "running"
Dec  9 07:31:04.682: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=services-2526 exec execpod-affinityc87vv -- /bin/sh -x -c nc -v -z -w 2 affinity-nodeport-transition 80'
Dec  9 07:31:04.862: INFO: stderr: "+ nc -v -z -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
Dec  9 07:31:04.862: INFO: stdout: ""
Dec  9 07:31:04.862: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=services-2526 exec execpod-affinityc87vv -- /bin/sh -x -c nc -v -z -w 2 10.3.185.142 80'
Dec  9 07:31:05.055: INFO: stderr: "+ nc -v -z -w 2 10.3.185.142 80\nConnection to 10.3.185.142 80 port [tcp/http] succeeded!\n"
Dec  9 07:31:05.055: INFO: stdout: ""
Dec  9 07:31:05.055: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=services-2526 exec execpod-affinityc87vv -- /bin/sh -x -c nc -v -z -w 2 10.0.10.179 32254'
Dec  9 07:31:05.268: INFO: stderr: "+ nc -v -z -w 2 10.0.10.179 32254\nConnection to 10.0.10.179 32254 port [tcp/*] succeeded!\n"
Dec  9 07:31:05.268: INFO: stdout: ""
Dec  9 07:31:05.268: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=services-2526 exec execpod-affinityc87vv -- /bin/sh -x -c nc -v -z -w 2 10.0.17.108 32254'
Dec  9 07:31:05.454: INFO: stderr: "+ nc -v -z -w 2 10.0.17.108 32254\nConnection to 10.0.17.108 32254 port [tcp/*] succeeded!\n"
Dec  9 07:31:05.454: INFO: stdout: ""
Dec  9 07:31:05.463: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=services-2526 exec execpod-affinityc87vv -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.0.10.179:32254/ ; done'
Dec  9 07:31:05.751: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.179:32254/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.179:32254/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.179:32254/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.179:32254/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.179:32254/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.179:32254/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.179:32254/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.179:32254/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.179:32254/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.179:32254/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.179:32254/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.179:32254/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.179:32254/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.179:32254/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.179:32254/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.179:32254/\n"
Dec  9 07:31:05.751: INFO: stdout: "\naffinity-nodeport-transition-4fvlm\naffinity-nodeport-transition-b4vh9\naffinity-nodeport-transition-b5c6b\naffinity-nodeport-transition-4fvlm\naffinity-nodeport-transition-b4vh9\naffinity-nodeport-transition-b5c6b\naffinity-nodeport-transition-4fvlm\naffinity-nodeport-transition-b4vh9\naffinity-nodeport-transition-b5c6b\naffinity-nodeport-transition-4fvlm\naffinity-nodeport-transition-b4vh9\naffinity-nodeport-transition-b5c6b\naffinity-nodeport-transition-4fvlm\naffinity-nodeport-transition-b4vh9\naffinity-nodeport-transition-b5c6b\naffinity-nodeport-transition-4fvlm"
Dec  9 07:31:05.751: INFO: Received response from host: affinity-nodeport-transition-4fvlm
Dec  9 07:31:05.751: INFO: Received response from host: affinity-nodeport-transition-b4vh9
Dec  9 07:31:05.751: INFO: Received response from host: affinity-nodeport-transition-b5c6b
Dec  9 07:31:05.751: INFO: Received response from host: affinity-nodeport-transition-4fvlm
Dec  9 07:31:05.751: INFO: Received response from host: affinity-nodeport-transition-b4vh9
Dec  9 07:31:05.751: INFO: Received response from host: affinity-nodeport-transition-b5c6b
Dec  9 07:31:05.751: INFO: Received response from host: affinity-nodeport-transition-4fvlm
Dec  9 07:31:05.751: INFO: Received response from host: affinity-nodeport-transition-b4vh9
Dec  9 07:31:05.751: INFO: Received response from host: affinity-nodeport-transition-b5c6b
Dec  9 07:31:05.751: INFO: Received response from host: affinity-nodeport-transition-4fvlm
Dec  9 07:31:05.751: INFO: Received response from host: affinity-nodeport-transition-b4vh9
Dec  9 07:31:05.751: INFO: Received response from host: affinity-nodeport-transition-b5c6b
Dec  9 07:31:05.751: INFO: Received response from host: affinity-nodeport-transition-4fvlm
Dec  9 07:31:05.751: INFO: Received response from host: affinity-nodeport-transition-b4vh9
Dec  9 07:31:05.751: INFO: Received response from host: affinity-nodeport-transition-b5c6b
Dec  9 07:31:05.751: INFO: Received response from host: affinity-nodeport-transition-4fvlm
Dec  9 07:31:05.761: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=services-2526 exec execpod-affinityc87vv -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.0.10.179:32254/ ; done'
Dec  9 07:31:06.059: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.179:32254/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.179:32254/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.179:32254/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.179:32254/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.179:32254/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.179:32254/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.179:32254/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.179:32254/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.179:32254/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.179:32254/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.179:32254/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.179:32254/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.179:32254/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.179:32254/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.179:32254/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.179:32254/\n"
Dec  9 07:31:06.059: INFO: stdout: "\naffinity-nodeport-transition-b5c6b\naffinity-nodeport-transition-b5c6b\naffinity-nodeport-transition-b5c6b\naffinity-nodeport-transition-b5c6b\naffinity-nodeport-transition-b5c6b\naffinity-nodeport-transition-b5c6b\naffinity-nodeport-transition-b5c6b\naffinity-nodeport-transition-b5c6b\naffinity-nodeport-transition-b5c6b\naffinity-nodeport-transition-b5c6b\naffinity-nodeport-transition-b5c6b\naffinity-nodeport-transition-b5c6b\naffinity-nodeport-transition-b5c6b\naffinity-nodeport-transition-b5c6b\naffinity-nodeport-transition-b5c6b\naffinity-nodeport-transition-b5c6b"
Dec  9 07:31:06.059: INFO: Received response from host: affinity-nodeport-transition-b5c6b
Dec  9 07:31:06.059: INFO: Received response from host: affinity-nodeport-transition-b5c6b
Dec  9 07:31:06.059: INFO: Received response from host: affinity-nodeport-transition-b5c6b
Dec  9 07:31:06.059: INFO: Received response from host: affinity-nodeport-transition-b5c6b
Dec  9 07:31:06.059: INFO: Received response from host: affinity-nodeport-transition-b5c6b
Dec  9 07:31:06.059: INFO: Received response from host: affinity-nodeport-transition-b5c6b
Dec  9 07:31:06.059: INFO: Received response from host: affinity-nodeport-transition-b5c6b
Dec  9 07:31:06.059: INFO: Received response from host: affinity-nodeport-transition-b5c6b
Dec  9 07:31:06.059: INFO: Received response from host: affinity-nodeport-transition-b5c6b
Dec  9 07:31:06.059: INFO: Received response from host: affinity-nodeport-transition-b5c6b
Dec  9 07:31:06.059: INFO: Received response from host: affinity-nodeport-transition-b5c6b
Dec  9 07:31:06.059: INFO: Received response from host: affinity-nodeport-transition-b5c6b
Dec  9 07:31:06.059: INFO: Received response from host: affinity-nodeport-transition-b5c6b
Dec  9 07:31:06.059: INFO: Received response from host: affinity-nodeport-transition-b5c6b
Dec  9 07:31:06.059: INFO: Received response from host: affinity-nodeport-transition-b5c6b
Dec  9 07:31:06.059: INFO: Received response from host: affinity-nodeport-transition-b5c6b
Dec  9 07:31:06.059: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-2526, will wait for the garbage collector to delete the pods 12/09/22 07:31:06.072
Dec  9 07:31:06.139: INFO: Deleting ReplicationController affinity-nodeport-transition took: 7.969988ms
Dec  9 07:31:06.440: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 301.192496ms
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Dec  9 07:31:08.692: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-2526" for this suite. 12/09/22 07:31:08.71
------------------------------
â€¢ [SLOW TEST] [10.229 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2250

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:30:58.498
    Dec  9 07:30:58.498: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename services 12/09/22 07:30:58.499
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:30:58.516
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:30:58.521
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2250
    STEP: creating service in namespace services-2526 12/09/22 07:30:58.524
    STEP: creating service affinity-nodeport-transition in namespace services-2526 12/09/22 07:30:58.525
    STEP: creating replication controller affinity-nodeport-transition in namespace services-2526 12/09/22 07:30:58.536
    I1209 07:30:58.546730      20 runners.go:193] Created replication controller with name: affinity-nodeport-transition, namespace: services-2526, replica count: 3
    I1209 07:31:01.598161      20 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Dec  9 07:31:01.618: INFO: Creating new exec pod
    Dec  9 07:31:01.641: INFO: Waiting up to 5m0s for pod "execpod-affinityc87vv" in namespace "services-2526" to be "running"
    Dec  9 07:31:01.671: INFO: Pod "execpod-affinityc87vv": Phase="Pending", Reason="", readiness=false. Elapsed: 29.537099ms
    Dec  9 07:31:03.675: INFO: Pod "execpod-affinityc87vv": Phase="Running", Reason="", readiness=true. Elapsed: 2.03374312s
    Dec  9 07:31:03.675: INFO: Pod "execpod-affinityc87vv" satisfied condition "running"
    Dec  9 07:31:04.682: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=services-2526 exec execpod-affinityc87vv -- /bin/sh -x -c nc -v -z -w 2 affinity-nodeport-transition 80'
    Dec  9 07:31:04.862: INFO: stderr: "+ nc -v -z -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
    Dec  9 07:31:04.862: INFO: stdout: ""
    Dec  9 07:31:04.862: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=services-2526 exec execpod-affinityc87vv -- /bin/sh -x -c nc -v -z -w 2 10.3.185.142 80'
    Dec  9 07:31:05.055: INFO: stderr: "+ nc -v -z -w 2 10.3.185.142 80\nConnection to 10.3.185.142 80 port [tcp/http] succeeded!\n"
    Dec  9 07:31:05.055: INFO: stdout: ""
    Dec  9 07:31:05.055: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=services-2526 exec execpod-affinityc87vv -- /bin/sh -x -c nc -v -z -w 2 10.0.10.179 32254'
    Dec  9 07:31:05.268: INFO: stderr: "+ nc -v -z -w 2 10.0.10.179 32254\nConnection to 10.0.10.179 32254 port [tcp/*] succeeded!\n"
    Dec  9 07:31:05.268: INFO: stdout: ""
    Dec  9 07:31:05.268: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=services-2526 exec execpod-affinityc87vv -- /bin/sh -x -c nc -v -z -w 2 10.0.17.108 32254'
    Dec  9 07:31:05.454: INFO: stderr: "+ nc -v -z -w 2 10.0.17.108 32254\nConnection to 10.0.17.108 32254 port [tcp/*] succeeded!\n"
    Dec  9 07:31:05.454: INFO: stdout: ""
    Dec  9 07:31:05.463: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=services-2526 exec execpod-affinityc87vv -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.0.10.179:32254/ ; done'
    Dec  9 07:31:05.751: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.179:32254/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.179:32254/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.179:32254/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.179:32254/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.179:32254/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.179:32254/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.179:32254/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.179:32254/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.179:32254/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.179:32254/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.179:32254/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.179:32254/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.179:32254/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.179:32254/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.179:32254/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.179:32254/\n"
    Dec  9 07:31:05.751: INFO: stdout: "\naffinity-nodeport-transition-4fvlm\naffinity-nodeport-transition-b4vh9\naffinity-nodeport-transition-b5c6b\naffinity-nodeport-transition-4fvlm\naffinity-nodeport-transition-b4vh9\naffinity-nodeport-transition-b5c6b\naffinity-nodeport-transition-4fvlm\naffinity-nodeport-transition-b4vh9\naffinity-nodeport-transition-b5c6b\naffinity-nodeport-transition-4fvlm\naffinity-nodeport-transition-b4vh9\naffinity-nodeport-transition-b5c6b\naffinity-nodeport-transition-4fvlm\naffinity-nodeport-transition-b4vh9\naffinity-nodeport-transition-b5c6b\naffinity-nodeport-transition-4fvlm"
    Dec  9 07:31:05.751: INFO: Received response from host: affinity-nodeport-transition-4fvlm
    Dec  9 07:31:05.751: INFO: Received response from host: affinity-nodeport-transition-b4vh9
    Dec  9 07:31:05.751: INFO: Received response from host: affinity-nodeport-transition-b5c6b
    Dec  9 07:31:05.751: INFO: Received response from host: affinity-nodeport-transition-4fvlm
    Dec  9 07:31:05.751: INFO: Received response from host: affinity-nodeport-transition-b4vh9
    Dec  9 07:31:05.751: INFO: Received response from host: affinity-nodeport-transition-b5c6b
    Dec  9 07:31:05.751: INFO: Received response from host: affinity-nodeport-transition-4fvlm
    Dec  9 07:31:05.751: INFO: Received response from host: affinity-nodeport-transition-b4vh9
    Dec  9 07:31:05.751: INFO: Received response from host: affinity-nodeport-transition-b5c6b
    Dec  9 07:31:05.751: INFO: Received response from host: affinity-nodeport-transition-4fvlm
    Dec  9 07:31:05.751: INFO: Received response from host: affinity-nodeport-transition-b4vh9
    Dec  9 07:31:05.751: INFO: Received response from host: affinity-nodeport-transition-b5c6b
    Dec  9 07:31:05.751: INFO: Received response from host: affinity-nodeport-transition-4fvlm
    Dec  9 07:31:05.751: INFO: Received response from host: affinity-nodeport-transition-b4vh9
    Dec  9 07:31:05.751: INFO: Received response from host: affinity-nodeport-transition-b5c6b
    Dec  9 07:31:05.751: INFO: Received response from host: affinity-nodeport-transition-4fvlm
    Dec  9 07:31:05.761: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=services-2526 exec execpod-affinityc87vv -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.0.10.179:32254/ ; done'
    Dec  9 07:31:06.059: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.179:32254/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.179:32254/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.179:32254/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.179:32254/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.179:32254/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.179:32254/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.179:32254/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.179:32254/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.179:32254/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.179:32254/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.179:32254/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.179:32254/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.179:32254/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.179:32254/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.179:32254/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.179:32254/\n"
    Dec  9 07:31:06.059: INFO: stdout: "\naffinity-nodeport-transition-b5c6b\naffinity-nodeport-transition-b5c6b\naffinity-nodeport-transition-b5c6b\naffinity-nodeport-transition-b5c6b\naffinity-nodeport-transition-b5c6b\naffinity-nodeport-transition-b5c6b\naffinity-nodeport-transition-b5c6b\naffinity-nodeport-transition-b5c6b\naffinity-nodeport-transition-b5c6b\naffinity-nodeport-transition-b5c6b\naffinity-nodeport-transition-b5c6b\naffinity-nodeport-transition-b5c6b\naffinity-nodeport-transition-b5c6b\naffinity-nodeport-transition-b5c6b\naffinity-nodeport-transition-b5c6b\naffinity-nodeport-transition-b5c6b"
    Dec  9 07:31:06.059: INFO: Received response from host: affinity-nodeport-transition-b5c6b
    Dec  9 07:31:06.059: INFO: Received response from host: affinity-nodeport-transition-b5c6b
    Dec  9 07:31:06.059: INFO: Received response from host: affinity-nodeport-transition-b5c6b
    Dec  9 07:31:06.059: INFO: Received response from host: affinity-nodeport-transition-b5c6b
    Dec  9 07:31:06.059: INFO: Received response from host: affinity-nodeport-transition-b5c6b
    Dec  9 07:31:06.059: INFO: Received response from host: affinity-nodeport-transition-b5c6b
    Dec  9 07:31:06.059: INFO: Received response from host: affinity-nodeport-transition-b5c6b
    Dec  9 07:31:06.059: INFO: Received response from host: affinity-nodeport-transition-b5c6b
    Dec  9 07:31:06.059: INFO: Received response from host: affinity-nodeport-transition-b5c6b
    Dec  9 07:31:06.059: INFO: Received response from host: affinity-nodeport-transition-b5c6b
    Dec  9 07:31:06.059: INFO: Received response from host: affinity-nodeport-transition-b5c6b
    Dec  9 07:31:06.059: INFO: Received response from host: affinity-nodeport-transition-b5c6b
    Dec  9 07:31:06.059: INFO: Received response from host: affinity-nodeport-transition-b5c6b
    Dec  9 07:31:06.059: INFO: Received response from host: affinity-nodeport-transition-b5c6b
    Dec  9 07:31:06.059: INFO: Received response from host: affinity-nodeport-transition-b5c6b
    Dec  9 07:31:06.059: INFO: Received response from host: affinity-nodeport-transition-b5c6b
    Dec  9 07:31:06.059: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-2526, will wait for the garbage collector to delete the pods 12/09/22 07:31:06.072
    Dec  9 07:31:06.139: INFO: Deleting ReplicationController affinity-nodeport-transition took: 7.969988ms
    Dec  9 07:31:06.440: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 301.192496ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:31:08.692: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-2526" for this suite. 12/09/22 07:31:08.71
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  creating/deleting custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:58
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:31:08.734
Dec  9 07:31:08.734: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename custom-resource-definition 12/09/22 07:31:08.735
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:31:08.756
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:31:08.762
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] creating/deleting custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:58
Dec  9 07:31:08.770: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Dec  9 07:31:09.878: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-5646" for this suite. 12/09/22 07:31:09.882
------------------------------
â€¢ [1.155 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    creating/deleting custom resource definition objects works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:58

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:31:08.734
    Dec  9 07:31:08.734: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename custom-resource-definition 12/09/22 07:31:08.735
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:31:08.756
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:31:08.762
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] creating/deleting custom resource definition objects works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:58
    Dec  9 07:31:08.770: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:31:09.878: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-5646" for this suite. 12/09/22 07:31:09.882
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] Garbage collector
  should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:31:09.889
Dec  9 07:31:09.889: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename gc 12/09/22 07:31:09.89
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:31:09.913
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:31:09.917
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849
Dec  9 07:31:09.966: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"9746e952-2c35-401b-b6b1-15aff90dcc62", Controller:(*bool)(0xc0041277ce), BlockOwnerDeletion:(*bool)(0xc0041277cf)}}
Dec  9 07:31:09.982: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"36d3cd42-6947-43c9-8b49-fae7b123ff03", Controller:(*bool)(0xc0041279fe), BlockOwnerDeletion:(*bool)(0xc0041279ff)}}
Dec  9 07:31:10.017: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"b93ec4e9-ac48-4bba-9421-a7059b728113", Controller:(*bool)(0xc005183b16), BlockOwnerDeletion:(*bool)(0xc005183b17)}}
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Dec  9 07:31:15.044: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-9207" for this suite. 12/09/22 07:31:15.049
------------------------------
â€¢ [SLOW TEST] [5.169 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:31:09.889
    Dec  9 07:31:09.889: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename gc 12/09/22 07:31:09.89
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:31:09.913
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:31:09.917
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should not be blocked by dependency circle [Conformance]
      test/e2e/apimachinery/garbage_collector.go:849
    Dec  9 07:31:09.966: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"9746e952-2c35-401b-b6b1-15aff90dcc62", Controller:(*bool)(0xc0041277ce), BlockOwnerDeletion:(*bool)(0xc0041277cf)}}
    Dec  9 07:31:09.982: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"36d3cd42-6947-43c9-8b49-fae7b123ff03", Controller:(*bool)(0xc0041279fe), BlockOwnerDeletion:(*bool)(0xc0041279ff)}}
    Dec  9 07:31:10.017: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"b93ec4e9-ac48-4bba-9421-a7059b728113", Controller:(*bool)(0xc005183b16), BlockOwnerDeletion:(*bool)(0xc005183b17)}}
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:31:15.044: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-9207" for this suite. 12/09/22 07:31:15.049
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-node] KubeletManagedEtcHosts
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63
[BeforeEach] [sig-node] KubeletManagedEtcHosts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:31:15.06
Dec  9 07:31:15.060: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts 12/09/22 07:31:15.061
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:31:15.079
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:31:15.085
[BeforeEach] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/metrics/init/init.go:31
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63
STEP: Setting up the test 12/09/22 07:31:15.089
STEP: Creating hostNetwork=false pod 12/09/22 07:31:15.09
Dec  9 07:31:15.102: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "e2e-kubelet-etc-hosts-3628" to be "running and ready"
Dec  9 07:31:15.111: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 9.696791ms
Dec  9 07:31:15.112: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
Dec  9 07:31:17.116: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.014805991s
Dec  9 07:31:17.117: INFO: The phase of Pod test-pod is Running (Ready = true)
Dec  9 07:31:17.117: INFO: Pod "test-pod" satisfied condition "running and ready"
STEP: Creating hostNetwork=true pod 12/09/22 07:31:17.12
Dec  9 07:31:17.126: INFO: Waiting up to 5m0s for pod "test-host-network-pod" in namespace "e2e-kubelet-etc-hosts-3628" to be "running and ready"
Dec  9 07:31:17.131: INFO: Pod "test-host-network-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.709168ms
Dec  9 07:31:17.131: INFO: The phase of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
Dec  9 07:31:19.135: INFO: Pod "test-host-network-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.009175652s
Dec  9 07:31:19.135: INFO: The phase of Pod test-host-network-pod is Running (Ready = true)
Dec  9 07:31:19.135: INFO: Pod "test-host-network-pod" satisfied condition "running and ready"
STEP: Running the test 12/09/22 07:31:19.139
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false 12/09/22 07:31:19.139
Dec  9 07:31:19.139: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-3628 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec  9 07:31:19.139: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
Dec  9 07:31:19.140: INFO: ExecWithOptions: Clientset creation
Dec  9 07:31:19.140: INFO: ExecWithOptions: execute(POST https://10.3.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-3628/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Dec  9 07:31:19.232: INFO: Exec stderr: ""
Dec  9 07:31:19.232: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-3628 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec  9 07:31:19.232: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
Dec  9 07:31:19.232: INFO: ExecWithOptions: Clientset creation
Dec  9 07:31:19.232: INFO: ExecWithOptions: execute(POST https://10.3.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-3628/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Dec  9 07:31:19.337: INFO: Exec stderr: ""
Dec  9 07:31:19.337: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-3628 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec  9 07:31:19.338: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
Dec  9 07:31:19.338: INFO: ExecWithOptions: Clientset creation
Dec  9 07:31:19.339: INFO: ExecWithOptions: execute(POST https://10.3.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-3628/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Dec  9 07:31:19.440: INFO: Exec stderr: ""
Dec  9 07:31:19.440: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-3628 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec  9 07:31:19.440: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
Dec  9 07:31:19.442: INFO: ExecWithOptions: Clientset creation
Dec  9 07:31:19.442: INFO: ExecWithOptions: execute(POST https://10.3.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-3628/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Dec  9 07:31:19.510: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount 12/09/22 07:31:19.51
Dec  9 07:31:19.510: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-3628 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec  9 07:31:19.511: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
Dec  9 07:31:19.511: INFO: ExecWithOptions: Clientset creation
Dec  9 07:31:19.511: INFO: ExecWithOptions: execute(POST https://10.3.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-3628/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
Dec  9 07:31:19.593: INFO: Exec stderr: ""
Dec  9 07:31:19.593: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-3628 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec  9 07:31:19.593: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
Dec  9 07:31:19.594: INFO: ExecWithOptions: Clientset creation
Dec  9 07:31:19.594: INFO: ExecWithOptions: execute(POST https://10.3.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-3628/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
Dec  9 07:31:19.665: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true 12/09/22 07:31:19.665
Dec  9 07:31:19.665: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-3628 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec  9 07:31:19.665: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
Dec  9 07:31:19.666: INFO: ExecWithOptions: Clientset creation
Dec  9 07:31:19.666: INFO: ExecWithOptions: execute(POST https://10.3.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-3628/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Dec  9 07:31:19.757: INFO: Exec stderr: ""
Dec  9 07:31:19.757: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-3628 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec  9 07:31:19.757: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
Dec  9 07:31:19.759: INFO: ExecWithOptions: Clientset creation
Dec  9 07:31:19.759: INFO: ExecWithOptions: execute(POST https://10.3.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-3628/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Dec  9 07:31:19.862: INFO: Exec stderr: ""
Dec  9 07:31:19.862: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-3628 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec  9 07:31:19.862: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
Dec  9 07:31:19.863: INFO: ExecWithOptions: Clientset creation
Dec  9 07:31:19.863: INFO: ExecWithOptions: execute(POST https://10.3.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-3628/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Dec  9 07:31:19.965: INFO: Exec stderr: ""
Dec  9 07:31:19.965: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-3628 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec  9 07:31:19.965: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
Dec  9 07:31:19.967: INFO: ExecWithOptions: Clientset creation
Dec  9 07:31:19.968: INFO: ExecWithOptions: execute(POST https://10.3.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-3628/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Dec  9 07:31:20.060: INFO: Exec stderr: ""
[AfterEach] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/node/init/init.go:32
Dec  9 07:31:20.060: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
  tear down framework | framework.go:193
STEP: Destroying namespace "e2e-kubelet-etc-hosts-3628" for this suite. 12/09/22 07:31:20.066
------------------------------
â€¢ [SLOW TEST] [5.013 seconds]
[sig-node] KubeletManagedEtcHosts
test/e2e/common/node/framework.go:23
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] KubeletManagedEtcHosts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:31:15.06
    Dec  9 07:31:15.060: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts 12/09/22 07:31:15.061
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:31:15.079
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:31:15.085
    [BeforeEach] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/metrics/init/init.go:31
    [It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet_etc_hosts.go:63
    STEP: Setting up the test 12/09/22 07:31:15.089
    STEP: Creating hostNetwork=false pod 12/09/22 07:31:15.09
    Dec  9 07:31:15.102: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "e2e-kubelet-etc-hosts-3628" to be "running and ready"
    Dec  9 07:31:15.111: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 9.696791ms
    Dec  9 07:31:15.112: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
    Dec  9 07:31:17.116: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.014805991s
    Dec  9 07:31:17.117: INFO: The phase of Pod test-pod is Running (Ready = true)
    Dec  9 07:31:17.117: INFO: Pod "test-pod" satisfied condition "running and ready"
    STEP: Creating hostNetwork=true pod 12/09/22 07:31:17.12
    Dec  9 07:31:17.126: INFO: Waiting up to 5m0s for pod "test-host-network-pod" in namespace "e2e-kubelet-etc-hosts-3628" to be "running and ready"
    Dec  9 07:31:17.131: INFO: Pod "test-host-network-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.709168ms
    Dec  9 07:31:17.131: INFO: The phase of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
    Dec  9 07:31:19.135: INFO: Pod "test-host-network-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.009175652s
    Dec  9 07:31:19.135: INFO: The phase of Pod test-host-network-pod is Running (Ready = true)
    Dec  9 07:31:19.135: INFO: Pod "test-host-network-pod" satisfied condition "running and ready"
    STEP: Running the test 12/09/22 07:31:19.139
    STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false 12/09/22 07:31:19.139
    Dec  9 07:31:19.139: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-3628 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Dec  9 07:31:19.139: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    Dec  9 07:31:19.140: INFO: ExecWithOptions: Clientset creation
    Dec  9 07:31:19.140: INFO: ExecWithOptions: execute(POST https://10.3.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-3628/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Dec  9 07:31:19.232: INFO: Exec stderr: ""
    Dec  9 07:31:19.232: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-3628 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Dec  9 07:31:19.232: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    Dec  9 07:31:19.232: INFO: ExecWithOptions: Clientset creation
    Dec  9 07:31:19.232: INFO: ExecWithOptions: execute(POST https://10.3.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-3628/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Dec  9 07:31:19.337: INFO: Exec stderr: ""
    Dec  9 07:31:19.337: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-3628 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Dec  9 07:31:19.338: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    Dec  9 07:31:19.338: INFO: ExecWithOptions: Clientset creation
    Dec  9 07:31:19.339: INFO: ExecWithOptions: execute(POST https://10.3.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-3628/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Dec  9 07:31:19.440: INFO: Exec stderr: ""
    Dec  9 07:31:19.440: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-3628 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Dec  9 07:31:19.440: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    Dec  9 07:31:19.442: INFO: ExecWithOptions: Clientset creation
    Dec  9 07:31:19.442: INFO: ExecWithOptions: execute(POST https://10.3.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-3628/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Dec  9 07:31:19.510: INFO: Exec stderr: ""
    STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount 12/09/22 07:31:19.51
    Dec  9 07:31:19.510: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-3628 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Dec  9 07:31:19.511: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    Dec  9 07:31:19.511: INFO: ExecWithOptions: Clientset creation
    Dec  9 07:31:19.511: INFO: ExecWithOptions: execute(POST https://10.3.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-3628/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
    Dec  9 07:31:19.593: INFO: Exec stderr: ""
    Dec  9 07:31:19.593: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-3628 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Dec  9 07:31:19.593: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    Dec  9 07:31:19.594: INFO: ExecWithOptions: Clientset creation
    Dec  9 07:31:19.594: INFO: ExecWithOptions: execute(POST https://10.3.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-3628/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
    Dec  9 07:31:19.665: INFO: Exec stderr: ""
    STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true 12/09/22 07:31:19.665
    Dec  9 07:31:19.665: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-3628 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Dec  9 07:31:19.665: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    Dec  9 07:31:19.666: INFO: ExecWithOptions: Clientset creation
    Dec  9 07:31:19.666: INFO: ExecWithOptions: execute(POST https://10.3.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-3628/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Dec  9 07:31:19.757: INFO: Exec stderr: ""
    Dec  9 07:31:19.757: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-3628 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Dec  9 07:31:19.757: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    Dec  9 07:31:19.759: INFO: ExecWithOptions: Clientset creation
    Dec  9 07:31:19.759: INFO: ExecWithOptions: execute(POST https://10.3.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-3628/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Dec  9 07:31:19.862: INFO: Exec stderr: ""
    Dec  9 07:31:19.862: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-3628 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Dec  9 07:31:19.862: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    Dec  9 07:31:19.863: INFO: ExecWithOptions: Clientset creation
    Dec  9 07:31:19.863: INFO: ExecWithOptions: execute(POST https://10.3.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-3628/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Dec  9 07:31:19.965: INFO: Exec stderr: ""
    Dec  9 07:31:19.965: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-3628 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Dec  9 07:31:19.965: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    Dec  9 07:31:19.967: INFO: ExecWithOptions: Clientset creation
    Dec  9 07:31:19.968: INFO: ExecWithOptions: execute(POST https://10.3.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-3628/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Dec  9 07:31:20.060: INFO: Exec stderr: ""
    [AfterEach] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:31:20.060: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
      tear down framework | framework.go:193
    STEP: Destroying namespace "e2e-kubelet-etc-hosts-3628" for this suite. 12/09/22 07:31:20.066
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context when creating containers with AllowPrivilegeEscalation
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:609
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:31:20.074
Dec  9 07:31:20.074: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename security-context-test 12/09/22 07:31:20.075
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:31:20.113
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:31:20.121
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:50
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:609
Dec  9 07:31:20.140: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-1bce531d-79c3-46c1-96f4-77311c4a1e6d" in namespace "security-context-test-3417" to be "Succeeded or Failed"
Dec  9 07:31:20.150: INFO: Pod "alpine-nnp-false-1bce531d-79c3-46c1-96f4-77311c4a1e6d": Phase="Pending", Reason="", readiness=false. Elapsed: 9.854518ms
Dec  9 07:31:22.154: INFO: Pod "alpine-nnp-false-1bce531d-79c3-46c1-96f4-77311c4a1e6d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013675751s
Dec  9 07:31:24.154: INFO: Pod "alpine-nnp-false-1bce531d-79c3-46c1-96f4-77311c4a1e6d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013754306s
Dec  9 07:31:26.153: INFO: Pod "alpine-nnp-false-1bce531d-79c3-46c1-96f4-77311c4a1e6d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.013536993s
Dec  9 07:31:26.154: INFO: Pod "alpine-nnp-false-1bce531d-79c3-46c1-96f4-77311c4a1e6d" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Dec  9 07:31:26.160: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-test-3417" for this suite. 12/09/22 07:31:26.163
------------------------------
â€¢ [SLOW TEST] [6.098 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  when creating containers with AllowPrivilegeEscalation
  test/e2e/common/node/security_context.go:555
    should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:609

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:31:20.074
    Dec  9 07:31:20.074: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename security-context-test 12/09/22 07:31:20.075
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:31:20.113
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:31:20.121
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:50
    [It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:609
    Dec  9 07:31:20.140: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-1bce531d-79c3-46c1-96f4-77311c4a1e6d" in namespace "security-context-test-3417" to be "Succeeded or Failed"
    Dec  9 07:31:20.150: INFO: Pod "alpine-nnp-false-1bce531d-79c3-46c1-96f4-77311c4a1e6d": Phase="Pending", Reason="", readiness=false. Elapsed: 9.854518ms
    Dec  9 07:31:22.154: INFO: Pod "alpine-nnp-false-1bce531d-79c3-46c1-96f4-77311c4a1e6d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013675751s
    Dec  9 07:31:24.154: INFO: Pod "alpine-nnp-false-1bce531d-79c3-46c1-96f4-77311c4a1e6d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013754306s
    Dec  9 07:31:26.153: INFO: Pod "alpine-nnp-false-1bce531d-79c3-46c1-96f4-77311c4a1e6d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.013536993s
    Dec  9 07:31:26.154: INFO: Pod "alpine-nnp-false-1bce531d-79c3-46c1-96f4-77311c4a1e6d" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:31:26.160: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-test-3417" for this suite. 12/09/22 07:31:26.163
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:656
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:31:26.172
Dec  9 07:31:26.173: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename webhook 12/09/22 07:31:26.174
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:31:26.218
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:31:26.229
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 12/09/22 07:31:26.275
STEP: Create role binding to let webhook read extension-apiserver-authentication 12/09/22 07:31:26.849
STEP: Deploying the webhook pod 12/09/22 07:31:26.856
STEP: Wait for the deployment to be ready 12/09/22 07:31:26.866
Dec  9 07:31:26.875: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 12/09/22 07:31:28.893
STEP: Verifying the service has paired with the endpoint 12/09/22 07:31:28.909
Dec  9 07:31:29.910: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:656
STEP: Listing all of the created validation webhooks 12/09/22 07:31:29.98
STEP: Creating a configMap that should be mutated 12/09/22 07:31:29.992
STEP: Deleting the collection of validation webhooks 12/09/22 07:31:30.024
STEP: Creating a configMap that should not be mutated 12/09/22 07:31:30.06
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Dec  9 07:31:30.070: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-6094" for this suite. 12/09/22 07:31:30.131
STEP: Destroying namespace "webhook-6094-markers" for this suite. 12/09/22 07:31:30.154
------------------------------
â€¢ [3.988 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:656

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:31:26.172
    Dec  9 07:31:26.173: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename webhook 12/09/22 07:31:26.174
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:31:26.218
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:31:26.229
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 12/09/22 07:31:26.275
    STEP: Create role binding to let webhook read extension-apiserver-authentication 12/09/22 07:31:26.849
    STEP: Deploying the webhook pod 12/09/22 07:31:26.856
    STEP: Wait for the deployment to be ready 12/09/22 07:31:26.866
    Dec  9 07:31:26.875: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 12/09/22 07:31:28.893
    STEP: Verifying the service has paired with the endpoint 12/09/22 07:31:28.909
    Dec  9 07:31:29.910: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] listing mutating webhooks should work [Conformance]
      test/e2e/apimachinery/webhook.go:656
    STEP: Listing all of the created validation webhooks 12/09/22 07:31:29.98
    STEP: Creating a configMap that should be mutated 12/09/22 07:31:29.992
    STEP: Deleting the collection of validation webhooks 12/09/22 07:31:30.024
    STEP: Creating a configMap that should not be mutated 12/09/22 07:31:30.06
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:31:30.070: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-6094" for this suite. 12/09/22 07:31:30.131
    STEP: Destroying namespace "webhook-6094-markers" for this suite. 12/09/22 07:31:30.154
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:138
[BeforeEach] [sig-node] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:31:30.184
Dec  9 07:31:30.184: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename configmap 12/09/22 07:31:30.189
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:31:30.218
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:31:30.223
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:138
STEP: Creating configMap that has name configmap-test-emptyKey-ed0b2c73-6012-4459-9468-5f837dc4916f 12/09/22 07:31:30.229
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/node/init/init.go:32
Dec  9 07:31:30.232: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-3930" for this suite. 12/09/22 07:31:30.236
------------------------------
â€¢ [0.061 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:138

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:31:30.184
    Dec  9 07:31:30.184: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename configmap 12/09/22 07:31:30.189
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:31:30.218
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:31:30.223
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should fail to create ConfigMap with empty key [Conformance]
      test/e2e/common/node/configmap.go:138
    STEP: Creating configMap that has name configmap-test-emptyKey-ed0b2c73-6012-4459-9468-5f837dc4916f 12/09/22 07:31:30.229
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:31:30.232: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-3930" for this suite. 12/09/22 07:31:30.236
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:862
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:31:30.252
Dec  9 07:31:30.253: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename daemonsets 12/09/22 07:31:30.253
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:31:30.273
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:31:30.279
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:862
STEP: Creating simple DaemonSet "daemon-set" 12/09/22 07:31:30.316
STEP: Check that daemon pods launch on every node of the cluster. 12/09/22 07:31:30.323
Dec  9 07:31:30.328: INFO: DaemonSet pods can't tolerate node ip-10-0-7-84 with taints [{Key:node-role.kubernetes.io/controller Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 07:31:30.333: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Dec  9 07:31:30.333: INFO: Node ip-10-0-10-179 is running 0 daemon pod, expected 1
Dec  9 07:31:31.338: INFO: DaemonSet pods can't tolerate node ip-10-0-7-84 with taints [{Key:node-role.kubernetes.io/controller Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 07:31:31.341: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Dec  9 07:31:31.341: INFO: Node ip-10-0-10-179 is running 0 daemon pod, expected 1
Dec  9 07:31:32.337: INFO: DaemonSet pods can't tolerate node ip-10-0-7-84 with taints [{Key:node-role.kubernetes.io/controller Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 07:31:32.340: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Dec  9 07:31:32.340: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
STEP: Getting /status 12/09/22 07:31:32.343
Dec  9 07:31:32.346: INFO: Daemon Set daemon-set has Conditions: []
STEP: updating the DaemonSet Status 12/09/22 07:31:32.346
Dec  9 07:31:32.356: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the daemon set status to be updated 12/09/22 07:31:32.356
Dec  9 07:31:32.358: INFO: Observed &DaemonSet event: ADDED
Dec  9 07:31:32.359: INFO: Observed &DaemonSet event: MODIFIED
Dec  9 07:31:32.359: INFO: Observed &DaemonSet event: MODIFIED
Dec  9 07:31:32.359: INFO: Observed &DaemonSet event: MODIFIED
Dec  9 07:31:32.359: INFO: Found daemon set daemon-set in namespace daemonsets-3830 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Dec  9 07:31:32.359: INFO: Daemon set daemon-set has an updated status
STEP: patching the DaemonSet Status 12/09/22 07:31:32.359
STEP: watching for the daemon set status to be patched 12/09/22 07:31:32.366
Dec  9 07:31:32.370: INFO: Observed &DaemonSet event: ADDED
Dec  9 07:31:32.370: INFO: Observed &DaemonSet event: MODIFIED
Dec  9 07:31:32.370: INFO: Observed &DaemonSet event: MODIFIED
Dec  9 07:31:32.371: INFO: Observed &DaemonSet event: MODIFIED
Dec  9 07:31:32.371: INFO: Observed daemon set daemon-set in namespace daemonsets-3830 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Dec  9 07:31:32.371: INFO: Observed &DaemonSet event: MODIFIED
Dec  9 07:31:32.371: INFO: Found daemon set daemon-set in namespace daemonsets-3830 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
Dec  9 07:31:32.371: INFO: Daemon set daemon-set has a patched status
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 12/09/22 07:31:32.376
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3830, will wait for the garbage collector to delete the pods 12/09/22 07:31:32.377
Dec  9 07:31:32.439: INFO: Deleting DaemonSet.extensions daemon-set took: 8.485869ms
Dec  9 07:31:32.539: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.218792ms
Dec  9 07:31:34.643: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Dec  9 07:31:34.643: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Dec  9 07:31:34.646: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"12365"},"items":null}

Dec  9 07:31:34.649: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"12365"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Dec  9 07:31:34.657: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-3830" for this suite. 12/09/22 07:31:34.661
------------------------------
â€¢ [4.416 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:862

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:31:30.252
    Dec  9 07:31:30.253: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename daemonsets 12/09/22 07:31:30.253
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:31:30.273
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:31:30.279
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should verify changes to a daemon set status [Conformance]
      test/e2e/apps/daemon_set.go:862
    STEP: Creating simple DaemonSet "daemon-set" 12/09/22 07:31:30.316
    STEP: Check that daemon pods launch on every node of the cluster. 12/09/22 07:31:30.323
    Dec  9 07:31:30.328: INFO: DaemonSet pods can't tolerate node ip-10-0-7-84 with taints [{Key:node-role.kubernetes.io/controller Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Dec  9 07:31:30.333: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Dec  9 07:31:30.333: INFO: Node ip-10-0-10-179 is running 0 daemon pod, expected 1
    Dec  9 07:31:31.338: INFO: DaemonSet pods can't tolerate node ip-10-0-7-84 with taints [{Key:node-role.kubernetes.io/controller Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Dec  9 07:31:31.341: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Dec  9 07:31:31.341: INFO: Node ip-10-0-10-179 is running 0 daemon pod, expected 1
    Dec  9 07:31:32.337: INFO: DaemonSet pods can't tolerate node ip-10-0-7-84 with taints [{Key:node-role.kubernetes.io/controller Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Dec  9 07:31:32.340: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Dec  9 07:31:32.340: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    STEP: Getting /status 12/09/22 07:31:32.343
    Dec  9 07:31:32.346: INFO: Daemon Set daemon-set has Conditions: []
    STEP: updating the DaemonSet Status 12/09/22 07:31:32.346
    Dec  9 07:31:32.356: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the daemon set status to be updated 12/09/22 07:31:32.356
    Dec  9 07:31:32.358: INFO: Observed &DaemonSet event: ADDED
    Dec  9 07:31:32.359: INFO: Observed &DaemonSet event: MODIFIED
    Dec  9 07:31:32.359: INFO: Observed &DaemonSet event: MODIFIED
    Dec  9 07:31:32.359: INFO: Observed &DaemonSet event: MODIFIED
    Dec  9 07:31:32.359: INFO: Found daemon set daemon-set in namespace daemonsets-3830 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Dec  9 07:31:32.359: INFO: Daemon set daemon-set has an updated status
    STEP: patching the DaemonSet Status 12/09/22 07:31:32.359
    STEP: watching for the daemon set status to be patched 12/09/22 07:31:32.366
    Dec  9 07:31:32.370: INFO: Observed &DaemonSet event: ADDED
    Dec  9 07:31:32.370: INFO: Observed &DaemonSet event: MODIFIED
    Dec  9 07:31:32.370: INFO: Observed &DaemonSet event: MODIFIED
    Dec  9 07:31:32.371: INFO: Observed &DaemonSet event: MODIFIED
    Dec  9 07:31:32.371: INFO: Observed daemon set daemon-set in namespace daemonsets-3830 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Dec  9 07:31:32.371: INFO: Observed &DaemonSet event: MODIFIED
    Dec  9 07:31:32.371: INFO: Found daemon set daemon-set in namespace daemonsets-3830 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
    Dec  9 07:31:32.371: INFO: Daemon set daemon-set has a patched status
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 12/09/22 07:31:32.376
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3830, will wait for the garbage collector to delete the pods 12/09/22 07:31:32.377
    Dec  9 07:31:32.439: INFO: Deleting DaemonSet.extensions daemon-set took: 8.485869ms
    Dec  9 07:31:32.539: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.218792ms
    Dec  9 07:31:34.643: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Dec  9 07:31:34.643: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Dec  9 07:31:34.646: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"12365"},"items":null}

    Dec  9 07:31:34.649: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"12365"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:31:34.657: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-3830" for this suite. 12/09/22 07:31:34.661
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Secrets
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:46
[BeforeEach] [sig-node] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:31:34.671
Dec  9 07:31:34.671: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename secrets 12/09/22 07:31:34.672
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:31:34.69
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:31:34.694
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:46
STEP: Creating secret with name secret-test-d785c714-64e8-43a0-b6ca-98560ff2b7c1 12/09/22 07:31:34.697
STEP: Creating a pod to test consume secrets 12/09/22 07:31:34.701
Dec  9 07:31:34.715: INFO: Waiting up to 5m0s for pod "pod-secrets-bee90946-7fc3-4b60-a250-3013cc968359" in namespace "secrets-7839" to be "Succeeded or Failed"
Dec  9 07:31:34.722: INFO: Pod "pod-secrets-bee90946-7fc3-4b60-a250-3013cc968359": Phase="Pending", Reason="", readiness=false. Elapsed: 6.788892ms
Dec  9 07:31:36.725: INFO: Pod "pod-secrets-bee90946-7fc3-4b60-a250-3013cc968359": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010183272s
Dec  9 07:31:38.727: INFO: Pod "pod-secrets-bee90946-7fc3-4b60-a250-3013cc968359": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.0117047s
STEP: Saw pod success 12/09/22 07:31:38.727
Dec  9 07:31:38.727: INFO: Pod "pod-secrets-bee90946-7fc3-4b60-a250-3013cc968359" satisfied condition "Succeeded or Failed"
Dec  9 07:31:38.730: INFO: Trying to get logs from node ip-10-0-10-179 pod pod-secrets-bee90946-7fc3-4b60-a250-3013cc968359 container secret-env-test: <nil>
STEP: delete the pod 12/09/22 07:31:38.735
Dec  9 07:31:38.745: INFO: Waiting for pod pod-secrets-bee90946-7fc3-4b60-a250-3013cc968359 to disappear
Dec  9 07:31:38.748: INFO: Pod pod-secrets-bee90946-7fc3-4b60-a250-3013cc968359 no longer exists
[AfterEach] [sig-node] Secrets
  test/e2e/framework/node/init/init.go:32
Dec  9 07:31:38.748: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-7839" for this suite. 12/09/22 07:31:38.753
------------------------------
â€¢ [4.091 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:31:34.671
    Dec  9 07:31:34.671: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename secrets 12/09/22 07:31:34.672
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:31:34.69
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:31:34.694
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in env vars [NodeConformance] [Conformance]
      test/e2e/common/node/secrets.go:46
    STEP: Creating secret with name secret-test-d785c714-64e8-43a0-b6ca-98560ff2b7c1 12/09/22 07:31:34.697
    STEP: Creating a pod to test consume secrets 12/09/22 07:31:34.701
    Dec  9 07:31:34.715: INFO: Waiting up to 5m0s for pod "pod-secrets-bee90946-7fc3-4b60-a250-3013cc968359" in namespace "secrets-7839" to be "Succeeded or Failed"
    Dec  9 07:31:34.722: INFO: Pod "pod-secrets-bee90946-7fc3-4b60-a250-3013cc968359": Phase="Pending", Reason="", readiness=false. Elapsed: 6.788892ms
    Dec  9 07:31:36.725: INFO: Pod "pod-secrets-bee90946-7fc3-4b60-a250-3013cc968359": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010183272s
    Dec  9 07:31:38.727: INFO: Pod "pod-secrets-bee90946-7fc3-4b60-a250-3013cc968359": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.0117047s
    STEP: Saw pod success 12/09/22 07:31:38.727
    Dec  9 07:31:38.727: INFO: Pod "pod-secrets-bee90946-7fc3-4b60-a250-3013cc968359" satisfied condition "Succeeded or Failed"
    Dec  9 07:31:38.730: INFO: Trying to get logs from node ip-10-0-10-179 pod pod-secrets-bee90946-7fc3-4b60-a250-3013cc968359 container secret-env-test: <nil>
    STEP: delete the pod 12/09/22 07:31:38.735
    Dec  9 07:31:38.745: INFO: Waiting for pod pod-secrets-bee90946-7fc3-4b60-a250-3013cc968359 to disappear
    Dec  9 07:31:38.748: INFO: Pod pod-secrets-bee90946-7fc3-4b60-a250-3013cc968359 no longer exists
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:31:38.748: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-7839" for this suite. 12/09/22 07:31:38.753
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:194
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:31:38.764
Dec  9 07:31:38.766: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename crd-publish-openapi 12/09/22 07:31:38.768
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:31:38.807
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:31:38.813
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:194
Dec  9 07:31:38.825: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 12/09/22 07:31:40.339
Dec  9 07:31:40.339: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=crd-publish-openapi-617 --namespace=crd-publish-openapi-617 create -f -'
Dec  9 07:31:41.192: INFO: stderr: ""
Dec  9 07:31:41.192: INFO: stdout: "e2e-test-crd-publish-openapi-2470-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Dec  9 07:31:41.192: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=crd-publish-openapi-617 --namespace=crd-publish-openapi-617 delete e2e-test-crd-publish-openapi-2470-crds test-cr'
Dec  9 07:31:41.272: INFO: stderr: ""
Dec  9 07:31:41.272: INFO: stdout: "e2e-test-crd-publish-openapi-2470-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Dec  9 07:31:41.272: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=crd-publish-openapi-617 --namespace=crd-publish-openapi-617 apply -f -'
Dec  9 07:31:41.501: INFO: stderr: ""
Dec  9 07:31:41.501: INFO: stdout: "e2e-test-crd-publish-openapi-2470-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Dec  9 07:31:41.501: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=crd-publish-openapi-617 --namespace=crd-publish-openapi-617 delete e2e-test-crd-publish-openapi-2470-crds test-cr'
Dec  9 07:31:41.587: INFO: stderr: ""
Dec  9 07:31:41.587: INFO: stdout: "e2e-test-crd-publish-openapi-2470-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR 12/09/22 07:31:41.587
Dec  9 07:31:41.588: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=crd-publish-openapi-617 explain e2e-test-crd-publish-openapi-2470-crds'
Dec  9 07:31:41.809: INFO: stderr: ""
Dec  9 07:31:41.809: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-2470-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Dec  9 07:31:43.356: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-617" for this suite. 12/09/22 07:31:43.369
------------------------------
â€¢ [4.616 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:194

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:31:38.764
    Dec  9 07:31:38.766: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename crd-publish-openapi 12/09/22 07:31:38.768
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:31:38.807
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:31:38.813
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for CRD preserving unknown fields at the schema root [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:194
    Dec  9 07:31:38.825: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 12/09/22 07:31:40.339
    Dec  9 07:31:40.339: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=crd-publish-openapi-617 --namespace=crd-publish-openapi-617 create -f -'
    Dec  9 07:31:41.192: INFO: stderr: ""
    Dec  9 07:31:41.192: INFO: stdout: "e2e-test-crd-publish-openapi-2470-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
    Dec  9 07:31:41.192: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=crd-publish-openapi-617 --namespace=crd-publish-openapi-617 delete e2e-test-crd-publish-openapi-2470-crds test-cr'
    Dec  9 07:31:41.272: INFO: stderr: ""
    Dec  9 07:31:41.272: INFO: stdout: "e2e-test-crd-publish-openapi-2470-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
    Dec  9 07:31:41.272: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=crd-publish-openapi-617 --namespace=crd-publish-openapi-617 apply -f -'
    Dec  9 07:31:41.501: INFO: stderr: ""
    Dec  9 07:31:41.501: INFO: stdout: "e2e-test-crd-publish-openapi-2470-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
    Dec  9 07:31:41.501: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=crd-publish-openapi-617 --namespace=crd-publish-openapi-617 delete e2e-test-crd-publish-openapi-2470-crds test-cr'
    Dec  9 07:31:41.587: INFO: stderr: ""
    Dec  9 07:31:41.587: INFO: stdout: "e2e-test-crd-publish-openapi-2470-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR 12/09/22 07:31:41.587
    Dec  9 07:31:41.588: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=crd-publish-openapi-617 explain e2e-test-crd-publish-openapi-2470-crds'
    Dec  9 07:31:41.809: INFO: stderr: ""
    Dec  9 07:31:41.809: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-2470-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:31:43.356: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-617" for this suite. 12/09/22 07:31:43.369
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl version
  should check is all data is printed  [Conformance]
  test/e2e/kubectl/kubectl.go:1685
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:31:43.382
Dec  9 07:31:43.382: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename kubectl 12/09/22 07:31:43.383
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:31:43.403
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:31:43.408
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check is all data is printed  [Conformance]
  test/e2e/kubectl/kubectl.go:1685
Dec  9 07:31:43.412: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=kubectl-4137 version'
Dec  9 07:31:43.492: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
Dec  9 07:31:43.493: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"26\", GitVersion:\"v1.26.0\", GitCommit:\"b46a3f887ca979b1a5d14fd39cb1af43e7e5d12d\", GitTreeState:\"clean\", BuildDate:\"2022-12-08T19:58:30Z\", GoVersion:\"go1.19.4\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v4.5.7\nServer Version: version.Info{Major:\"1\", Minor:\"26\", GitVersion:\"v1.26.0\", GitCommit:\"b46a3f887ca979b1a5d14fd39cb1af43e7e5d12d\", GitTreeState:\"clean\", BuildDate:\"2022-12-08T19:51:45Z\", GoVersion:\"go1.19.4\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Dec  9 07:31:43.493: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-4137" for this suite. 12/09/22 07:31:43.497
------------------------------
â€¢ [0.121 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl version
  test/e2e/kubectl/kubectl.go:1679
    should check is all data is printed  [Conformance]
    test/e2e/kubectl/kubectl.go:1685

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:31:43.382
    Dec  9 07:31:43.382: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename kubectl 12/09/22 07:31:43.383
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:31:43.403
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:31:43.408
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check is all data is printed  [Conformance]
      test/e2e/kubectl/kubectl.go:1685
    Dec  9 07:31:43.412: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=kubectl-4137 version'
    Dec  9 07:31:43.492: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
    Dec  9 07:31:43.493: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"26\", GitVersion:\"v1.26.0\", GitCommit:\"b46a3f887ca979b1a5d14fd39cb1af43e7e5d12d\", GitTreeState:\"clean\", BuildDate:\"2022-12-08T19:58:30Z\", GoVersion:\"go1.19.4\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v4.5.7\nServer Version: version.Info{Major:\"1\", Minor:\"26\", GitVersion:\"v1.26.0\", GitCommit:\"b46a3f887ca979b1a5d14fd39cb1af43e7e5d12d\", GitTreeState:\"clean\", BuildDate:\"2022-12-08T19:51:45Z\", GoVersion:\"go1.19.4\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:31:43.493: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-4137" for this suite. 12/09/22 07:31:43.497
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Security Context When creating a pod with readOnlyRootFilesystem
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:486
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:31:43.503
Dec  9 07:31:43.503: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename security-context-test 12/09/22 07:31:43.504
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:31:43.523
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:31:43.526
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:50
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:486
Dec  9 07:31:43.538: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-f446e67a-9c13-4515-9e52-d4b4c65aa131" in namespace "security-context-test-3322" to be "Succeeded or Failed"
Dec  9 07:31:43.543: INFO: Pod "busybox-readonly-false-f446e67a-9c13-4515-9e52-d4b4c65aa131": Phase="Pending", Reason="", readiness=false. Elapsed: 5.217294ms
Dec  9 07:31:45.549: INFO: Pod "busybox-readonly-false-f446e67a-9c13-4515-9e52-d4b4c65aa131": Phase="Running", Reason="", readiness=true. Elapsed: 2.011765189s
Dec  9 07:31:47.546: INFO: Pod "busybox-readonly-false-f446e67a-9c13-4515-9e52-d4b4c65aa131": Phase="Running", Reason="", readiness=false. Elapsed: 4.008611381s
Dec  9 07:31:49.548: INFO: Pod "busybox-readonly-false-f446e67a-9c13-4515-9e52-d4b4c65aa131": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.009774045s
Dec  9 07:31:49.548: INFO: Pod "busybox-readonly-false-f446e67a-9c13-4515-9e52-d4b4c65aa131" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Dec  9 07:31:49.548: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-test-3322" for this suite. 12/09/22 07:31:49.551
------------------------------
â€¢ [SLOW TEST] [6.053 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a pod with readOnlyRootFilesystem
  test/e2e/common/node/security_context.go:430
    should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:486

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:31:43.503
    Dec  9 07:31:43.503: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename security-context-test 12/09/22 07:31:43.504
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:31:43.523
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:31:43.526
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:50
    [It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:486
    Dec  9 07:31:43.538: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-f446e67a-9c13-4515-9e52-d4b4c65aa131" in namespace "security-context-test-3322" to be "Succeeded or Failed"
    Dec  9 07:31:43.543: INFO: Pod "busybox-readonly-false-f446e67a-9c13-4515-9e52-d4b4c65aa131": Phase="Pending", Reason="", readiness=false. Elapsed: 5.217294ms
    Dec  9 07:31:45.549: INFO: Pod "busybox-readonly-false-f446e67a-9c13-4515-9e52-d4b4c65aa131": Phase="Running", Reason="", readiness=true. Elapsed: 2.011765189s
    Dec  9 07:31:47.546: INFO: Pod "busybox-readonly-false-f446e67a-9c13-4515-9e52-d4b4c65aa131": Phase="Running", Reason="", readiness=false. Elapsed: 4.008611381s
    Dec  9 07:31:49.548: INFO: Pod "busybox-readonly-false-f446e67a-9c13-4515-9e52-d4b4c65aa131": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.009774045s
    Dec  9 07:31:49.548: INFO: Pod "busybox-readonly-false-f446e67a-9c13-4515-9e52-d4b4c65aa131" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:31:49.548: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-test-3322" for this suite. 12/09/22 07:31:49.551
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-network] Services
  should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:848
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:31:49.556
Dec  9 07:31:49.557: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename services 12/09/22 07:31:49.557
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:31:49.574
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:31:49.58
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:848
STEP: creating service multi-endpoint-test in namespace services-7672 12/09/22 07:31:49.584
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7672 to expose endpoints map[] 12/09/22 07:31:49.596
Dec  9 07:31:49.609: INFO: successfully validated that service multi-endpoint-test in namespace services-7672 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-7672 12/09/22 07:31:49.609
Dec  9 07:31:49.617: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-7672" to be "running and ready"
Dec  9 07:31:49.621: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.829053ms
Dec  9 07:31:49.622: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Dec  9 07:31:51.627: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.008996266s
Dec  9 07:31:51.627: INFO: The phase of Pod pod1 is Running (Ready = true)
Dec  9 07:31:51.627: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7672 to expose endpoints map[pod1:[100]] 12/09/22 07:31:51.63
Dec  9 07:31:51.639: INFO: successfully validated that service multi-endpoint-test in namespace services-7672 exposes endpoints map[pod1:[100]]
STEP: Creating pod pod2 in namespace services-7672 12/09/22 07:31:51.64
Dec  9 07:31:51.646: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-7672" to be "running and ready"
Dec  9 07:31:51.669: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 22.99301ms
Dec  9 07:31:51.669: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Dec  9 07:31:53.674: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.027134143s
Dec  9 07:31:53.674: INFO: The phase of Pod pod2 is Running (Ready = true)
Dec  9 07:31:53.674: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7672 to expose endpoints map[pod1:[100] pod2:[101]] 12/09/22 07:31:53.676
Dec  9 07:31:53.688: INFO: successfully validated that service multi-endpoint-test in namespace services-7672 exposes endpoints map[pod1:[100] pod2:[101]]
STEP: Checking if the Service forwards traffic to pods 12/09/22 07:31:53.688
Dec  9 07:31:53.688: INFO: Creating new exec pod
Dec  9 07:31:53.694: INFO: Waiting up to 5m0s for pod "execpodlhwc8" in namespace "services-7672" to be "running"
Dec  9 07:31:53.697: INFO: Pod "execpodlhwc8": Phase="Pending", Reason="", readiness=false. Elapsed: 3.357756ms
Dec  9 07:31:55.702: INFO: Pod "execpodlhwc8": Phase="Running", Reason="", readiness=true. Elapsed: 2.008657773s
Dec  9 07:31:55.703: INFO: Pod "execpodlhwc8" satisfied condition "running"
Dec  9 07:31:56.704: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=services-7672 exec execpodlhwc8 -- /bin/sh -x -c nc -v -z -w 2 multi-endpoint-test 80'
Dec  9 07:31:56.872: INFO: stderr: "+ nc -v -z -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
Dec  9 07:31:56.872: INFO: stdout: ""
Dec  9 07:31:56.872: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=services-7672 exec execpodlhwc8 -- /bin/sh -x -c nc -v -z -w 2 10.3.63.169 80'
Dec  9 07:31:57.085: INFO: stderr: "+ nc -v -z -w 2 10.3.63.169 80\nConnection to 10.3.63.169 80 port [tcp/http] succeeded!\n"
Dec  9 07:31:57.086: INFO: stdout: ""
Dec  9 07:31:57.086: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=services-7672 exec execpodlhwc8 -- /bin/sh -x -c nc -v -z -w 2 multi-endpoint-test 81'
Dec  9 07:31:57.303: INFO: stderr: "+ nc -v -z -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
Dec  9 07:31:57.303: INFO: stdout: ""
Dec  9 07:31:57.303: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=services-7672 exec execpodlhwc8 -- /bin/sh -x -c nc -v -z -w 2 10.3.63.169 81'
Dec  9 07:31:57.438: INFO: stderr: "+ nc -v -z -w 2 10.3.63.169 81\nConnection to 10.3.63.169 81 port [tcp/*] succeeded!\n"
Dec  9 07:31:57.439: INFO: stdout: ""
STEP: Deleting pod pod1 in namespace services-7672 12/09/22 07:31:57.439
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7672 to expose endpoints map[pod2:[101]] 12/09/22 07:31:57.464
Dec  9 07:31:57.495: INFO: successfully validated that service multi-endpoint-test in namespace services-7672 exposes endpoints map[pod2:[101]]
STEP: Deleting pod pod2 in namespace services-7672 12/09/22 07:31:57.495
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7672 to expose endpoints map[] 12/09/22 07:31:57.517
Dec  9 07:31:58.542: INFO: successfully validated that service multi-endpoint-test in namespace services-7672 exposes endpoints map[]
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Dec  9 07:31:58.560: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-7672" for this suite. 12/09/22 07:31:58.581
------------------------------
â€¢ [SLOW TEST] [9.036 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:848

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:31:49.556
    Dec  9 07:31:49.557: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename services 12/09/22 07:31:49.557
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:31:49.574
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:31:49.58
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should serve multiport endpoints from pods  [Conformance]
      test/e2e/network/service.go:848
    STEP: creating service multi-endpoint-test in namespace services-7672 12/09/22 07:31:49.584
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7672 to expose endpoints map[] 12/09/22 07:31:49.596
    Dec  9 07:31:49.609: INFO: successfully validated that service multi-endpoint-test in namespace services-7672 exposes endpoints map[]
    STEP: Creating pod pod1 in namespace services-7672 12/09/22 07:31:49.609
    Dec  9 07:31:49.617: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-7672" to be "running and ready"
    Dec  9 07:31:49.621: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.829053ms
    Dec  9 07:31:49.622: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Dec  9 07:31:51.627: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.008996266s
    Dec  9 07:31:51.627: INFO: The phase of Pod pod1 is Running (Ready = true)
    Dec  9 07:31:51.627: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7672 to expose endpoints map[pod1:[100]] 12/09/22 07:31:51.63
    Dec  9 07:31:51.639: INFO: successfully validated that service multi-endpoint-test in namespace services-7672 exposes endpoints map[pod1:[100]]
    STEP: Creating pod pod2 in namespace services-7672 12/09/22 07:31:51.64
    Dec  9 07:31:51.646: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-7672" to be "running and ready"
    Dec  9 07:31:51.669: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 22.99301ms
    Dec  9 07:31:51.669: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Dec  9 07:31:53.674: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.027134143s
    Dec  9 07:31:53.674: INFO: The phase of Pod pod2 is Running (Ready = true)
    Dec  9 07:31:53.674: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7672 to expose endpoints map[pod1:[100] pod2:[101]] 12/09/22 07:31:53.676
    Dec  9 07:31:53.688: INFO: successfully validated that service multi-endpoint-test in namespace services-7672 exposes endpoints map[pod1:[100] pod2:[101]]
    STEP: Checking if the Service forwards traffic to pods 12/09/22 07:31:53.688
    Dec  9 07:31:53.688: INFO: Creating new exec pod
    Dec  9 07:31:53.694: INFO: Waiting up to 5m0s for pod "execpodlhwc8" in namespace "services-7672" to be "running"
    Dec  9 07:31:53.697: INFO: Pod "execpodlhwc8": Phase="Pending", Reason="", readiness=false. Elapsed: 3.357756ms
    Dec  9 07:31:55.702: INFO: Pod "execpodlhwc8": Phase="Running", Reason="", readiness=true. Elapsed: 2.008657773s
    Dec  9 07:31:55.703: INFO: Pod "execpodlhwc8" satisfied condition "running"
    Dec  9 07:31:56.704: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=services-7672 exec execpodlhwc8 -- /bin/sh -x -c nc -v -z -w 2 multi-endpoint-test 80'
    Dec  9 07:31:56.872: INFO: stderr: "+ nc -v -z -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
    Dec  9 07:31:56.872: INFO: stdout: ""
    Dec  9 07:31:56.872: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=services-7672 exec execpodlhwc8 -- /bin/sh -x -c nc -v -z -w 2 10.3.63.169 80'
    Dec  9 07:31:57.085: INFO: stderr: "+ nc -v -z -w 2 10.3.63.169 80\nConnection to 10.3.63.169 80 port [tcp/http] succeeded!\n"
    Dec  9 07:31:57.086: INFO: stdout: ""
    Dec  9 07:31:57.086: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=services-7672 exec execpodlhwc8 -- /bin/sh -x -c nc -v -z -w 2 multi-endpoint-test 81'
    Dec  9 07:31:57.303: INFO: stderr: "+ nc -v -z -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
    Dec  9 07:31:57.303: INFO: stdout: ""
    Dec  9 07:31:57.303: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=services-7672 exec execpodlhwc8 -- /bin/sh -x -c nc -v -z -w 2 10.3.63.169 81'
    Dec  9 07:31:57.438: INFO: stderr: "+ nc -v -z -w 2 10.3.63.169 81\nConnection to 10.3.63.169 81 port [tcp/*] succeeded!\n"
    Dec  9 07:31:57.439: INFO: stdout: ""
    STEP: Deleting pod pod1 in namespace services-7672 12/09/22 07:31:57.439
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7672 to expose endpoints map[pod2:[101]] 12/09/22 07:31:57.464
    Dec  9 07:31:57.495: INFO: successfully validated that service multi-endpoint-test in namespace services-7672 exposes endpoints map[pod2:[101]]
    STEP: Deleting pod pod2 in namespace services-7672 12/09/22 07:31:57.495
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7672 to expose endpoints map[] 12/09/22 07:31:57.517
    Dec  9 07:31:58.542: INFO: successfully validated that service multi-endpoint-test in namespace services-7672 exposes endpoints map[]
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:31:58.560: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-7672" for this suite. 12/09/22 07:31:58.581
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:97
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:31:58.593
Dec  9 07:31:58.594: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename emptydir 12/09/22 07:31:58.595
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:31:58.612
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:31:58.616
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:97
STEP: Creating a pod to test emptydir 0644 on tmpfs 12/09/22 07:31:58.621
Dec  9 07:31:58.628: INFO: Waiting up to 5m0s for pod "pod-0d580546-1c1d-448a-9119-65396e93dae0" in namespace "emptydir-5143" to be "Succeeded or Failed"
Dec  9 07:31:58.633: INFO: Pod "pod-0d580546-1c1d-448a-9119-65396e93dae0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.607656ms
Dec  9 07:32:00.638: INFO: Pod "pod-0d580546-1c1d-448a-9119-65396e93dae0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009354695s
Dec  9 07:32:02.639: INFO: Pod "pod-0d580546-1c1d-448a-9119-65396e93dae0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01031451s
STEP: Saw pod success 12/09/22 07:32:02.639
Dec  9 07:32:02.639: INFO: Pod "pod-0d580546-1c1d-448a-9119-65396e93dae0" satisfied condition "Succeeded or Failed"
Dec  9 07:32:02.642: INFO: Trying to get logs from node ip-10-0-10-179 pod pod-0d580546-1c1d-448a-9119-65396e93dae0 container test-container: <nil>
STEP: delete the pod 12/09/22 07:32:02.654
Dec  9 07:32:02.682: INFO: Waiting for pod pod-0d580546-1c1d-448a-9119-65396e93dae0 to disappear
Dec  9 07:32:02.694: INFO: Pod pod-0d580546-1c1d-448a-9119-65396e93dae0 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Dec  9 07:32:02.694: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-5143" for this suite. 12/09/22 07:32:02.699
------------------------------
â€¢ [4.116 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:97

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:31:58.593
    Dec  9 07:31:58.594: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename emptydir 12/09/22 07:31:58.595
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:31:58.612
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:31:58.616
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:97
    STEP: Creating a pod to test emptydir 0644 on tmpfs 12/09/22 07:31:58.621
    Dec  9 07:31:58.628: INFO: Waiting up to 5m0s for pod "pod-0d580546-1c1d-448a-9119-65396e93dae0" in namespace "emptydir-5143" to be "Succeeded or Failed"
    Dec  9 07:31:58.633: INFO: Pod "pod-0d580546-1c1d-448a-9119-65396e93dae0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.607656ms
    Dec  9 07:32:00.638: INFO: Pod "pod-0d580546-1c1d-448a-9119-65396e93dae0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009354695s
    Dec  9 07:32:02.639: INFO: Pod "pod-0d580546-1c1d-448a-9119-65396e93dae0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01031451s
    STEP: Saw pod success 12/09/22 07:32:02.639
    Dec  9 07:32:02.639: INFO: Pod "pod-0d580546-1c1d-448a-9119-65396e93dae0" satisfied condition "Succeeded or Failed"
    Dec  9 07:32:02.642: INFO: Trying to get logs from node ip-10-0-10-179 pod pod-0d580546-1c1d-448a-9119-65396e93dae0 container test-container: <nil>
    STEP: delete the pod 12/09/22 07:32:02.654
    Dec  9 07:32:02.682: INFO: Waiting for pod pod-0d580546-1c1d-448a-9119-65396e93dae0 to disappear
    Dec  9 07:32:02.694: INFO: Pod pod-0d580546-1c1d-448a-9119-65396e93dae0 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:32:02.694: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-5143" for this suite. 12/09/22 07:32:02.699
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:162
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:32:02.711
Dec  9 07:32:02.711: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename downward-api 12/09/22 07:32:02.713
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:32:02.752
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:32:02.761
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:162
STEP: Creating the pod 12/09/22 07:32:02.766
Dec  9 07:32:02.780: INFO: Waiting up to 5m0s for pod "annotationupdatea717e83b-7fec-430e-a15f-83a0016e788c" in namespace "downward-api-1171" to be "running and ready"
Dec  9 07:32:02.790: INFO: Pod "annotationupdatea717e83b-7fec-430e-a15f-83a0016e788c": Phase="Pending", Reason="", readiness=false. Elapsed: 9.596737ms
Dec  9 07:32:02.790: INFO: The phase of Pod annotationupdatea717e83b-7fec-430e-a15f-83a0016e788c is Pending, waiting for it to be Running (with Ready = true)
Dec  9 07:32:04.793: INFO: Pod "annotationupdatea717e83b-7fec-430e-a15f-83a0016e788c": Phase="Running", Reason="", readiness=true. Elapsed: 2.013042672s
Dec  9 07:32:04.793: INFO: The phase of Pod annotationupdatea717e83b-7fec-430e-a15f-83a0016e788c is Running (Ready = true)
Dec  9 07:32:04.793: INFO: Pod "annotationupdatea717e83b-7fec-430e-a15f-83a0016e788c" satisfied condition "running and ready"
Dec  9 07:32:05.315: INFO: Successfully updated pod "annotationupdatea717e83b-7fec-430e-a15f-83a0016e788c"
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Dec  9 07:32:09.338: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-1171" for this suite. 12/09/22 07:32:09.344
------------------------------
â€¢ [SLOW TEST] [6.660 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:162

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:32:02.711
    Dec  9 07:32:02.711: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename downward-api 12/09/22 07:32:02.713
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:32:02.752
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:32:02.761
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should update annotations on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:162
    STEP: Creating the pod 12/09/22 07:32:02.766
    Dec  9 07:32:02.780: INFO: Waiting up to 5m0s for pod "annotationupdatea717e83b-7fec-430e-a15f-83a0016e788c" in namespace "downward-api-1171" to be "running and ready"
    Dec  9 07:32:02.790: INFO: Pod "annotationupdatea717e83b-7fec-430e-a15f-83a0016e788c": Phase="Pending", Reason="", readiness=false. Elapsed: 9.596737ms
    Dec  9 07:32:02.790: INFO: The phase of Pod annotationupdatea717e83b-7fec-430e-a15f-83a0016e788c is Pending, waiting for it to be Running (with Ready = true)
    Dec  9 07:32:04.793: INFO: Pod "annotationupdatea717e83b-7fec-430e-a15f-83a0016e788c": Phase="Running", Reason="", readiness=true. Elapsed: 2.013042672s
    Dec  9 07:32:04.793: INFO: The phase of Pod annotationupdatea717e83b-7fec-430e-a15f-83a0016e788c is Running (Ready = true)
    Dec  9 07:32:04.793: INFO: Pod "annotationupdatea717e83b-7fec-430e-a15f-83a0016e788c" satisfied condition "running and ready"
    Dec  9 07:32:05.315: INFO: Successfully updated pod "annotationupdatea717e83b-7fec-430e-a15f-83a0016e788c"
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:32:09.338: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-1171" for this suite. 12/09/22 07:32:09.344
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:221
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:32:09.371
Dec  9 07:32:09.372: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename projected 12/09/22 07:32:09.372
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:32:09.472
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:32:09.528
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:221
STEP: Creating a pod to test downward API volume plugin 12/09/22 07:32:09.548
Dec  9 07:32:09.591: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f137f2bf-af78-406d-9748-720ce2eae781" in namespace "projected-1884" to be "Succeeded or Failed"
Dec  9 07:32:09.632: INFO: Pod "downwardapi-volume-f137f2bf-af78-406d-9748-720ce2eae781": Phase="Pending", Reason="", readiness=false. Elapsed: 41.216356ms
Dec  9 07:32:11.637: INFO: Pod "downwardapi-volume-f137f2bf-af78-406d-9748-720ce2eae781": Phase="Pending", Reason="", readiness=false. Elapsed: 2.04549515s
Dec  9 07:32:13.642: INFO: Pod "downwardapi-volume-f137f2bf-af78-406d-9748-720ce2eae781": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.050595666s
STEP: Saw pod success 12/09/22 07:32:13.642
Dec  9 07:32:13.642: INFO: Pod "downwardapi-volume-f137f2bf-af78-406d-9748-720ce2eae781" satisfied condition "Succeeded or Failed"
Dec  9 07:32:13.645: INFO: Trying to get logs from node ip-10-0-10-179 pod downwardapi-volume-f137f2bf-af78-406d-9748-720ce2eae781 container client-container: <nil>
STEP: delete the pod 12/09/22 07:32:13.651
Dec  9 07:32:13.664: INFO: Waiting for pod downwardapi-volume-f137f2bf-af78-406d-9748-720ce2eae781 to disappear
Dec  9 07:32:13.672: INFO: Pod downwardapi-volume-f137f2bf-af78-406d-9748-720ce2eae781 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Dec  9 07:32:13.672: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-1884" for this suite. 12/09/22 07:32:13.676
------------------------------
â€¢ [4.312 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:221

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:32:09.371
    Dec  9 07:32:09.372: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename projected 12/09/22 07:32:09.372
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:32:09.472
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:32:09.528
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide container's cpu request [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:221
    STEP: Creating a pod to test downward API volume plugin 12/09/22 07:32:09.548
    Dec  9 07:32:09.591: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f137f2bf-af78-406d-9748-720ce2eae781" in namespace "projected-1884" to be "Succeeded or Failed"
    Dec  9 07:32:09.632: INFO: Pod "downwardapi-volume-f137f2bf-af78-406d-9748-720ce2eae781": Phase="Pending", Reason="", readiness=false. Elapsed: 41.216356ms
    Dec  9 07:32:11.637: INFO: Pod "downwardapi-volume-f137f2bf-af78-406d-9748-720ce2eae781": Phase="Pending", Reason="", readiness=false. Elapsed: 2.04549515s
    Dec  9 07:32:13.642: INFO: Pod "downwardapi-volume-f137f2bf-af78-406d-9748-720ce2eae781": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.050595666s
    STEP: Saw pod success 12/09/22 07:32:13.642
    Dec  9 07:32:13.642: INFO: Pod "downwardapi-volume-f137f2bf-af78-406d-9748-720ce2eae781" satisfied condition "Succeeded or Failed"
    Dec  9 07:32:13.645: INFO: Trying to get logs from node ip-10-0-10-179 pod downwardapi-volume-f137f2bf-af78-406d-9748-720ce2eae781 container client-container: <nil>
    STEP: delete the pod 12/09/22 07:32:13.651
    Dec  9 07:32:13.664: INFO: Waiting for pod downwardapi-volume-f137f2bf-af78-406d-9748-720ce2eae781 to disappear
    Dec  9 07:32:13.672: INFO: Pod downwardapi-volume-f137f2bf-af78-406d-9748-720ce2eae781 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:32:13.672: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-1884" for this suite. 12/09/22 07:32:13.676
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-apps] Job
  should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:636
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:32:13.685
Dec  9 07:32:13.686: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename job 12/09/22 07:32:13.686
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:32:13.709
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:32:13.713
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:636
STEP: Creating a job 12/09/22 07:32:13.717
STEP: Ensure pods equal to parallelism count is attached to the job 12/09/22 07:32:13.724
STEP: patching /status 12/09/22 07:32:17.733
STEP: updating /status 12/09/22 07:32:17.747
STEP: get /status 12/09/22 07:32:17.756
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Dec  9 07:32:17.759: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-2913" for this suite. 12/09/22 07:32:17.763
------------------------------
â€¢ [4.087 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:636

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:32:13.685
    Dec  9 07:32:13.686: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename job 12/09/22 07:32:13.686
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:32:13.709
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:32:13.713
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply changes to a job status [Conformance]
      test/e2e/apps/job.go:636
    STEP: Creating a job 12/09/22 07:32:13.717
    STEP: Ensure pods equal to parallelism count is attached to the job 12/09/22 07:32:13.724
    STEP: patching /status 12/09/22 07:32:17.733
    STEP: updating /status 12/09/22 07:32:17.747
    STEP: get /status 12/09/22 07:32:17.756
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:32:17.759: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-2913" for this suite. 12/09/22 07:32:17.763
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with downward pod [Conformance]
  test/e2e/storage/subpath.go:92
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:32:17.774
Dec  9 07:32:17.774: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename subpath 12/09/22 07:32:17.775
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:32:17.799
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:32:17.805
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 12/09/22 07:32:17.808
[It] should support subpaths with downward pod [Conformance]
  test/e2e/storage/subpath.go:92
STEP: Creating pod pod-subpath-test-downwardapi-h4fx 12/09/22 07:32:17.82
STEP: Creating a pod to test atomic-volume-subpath 12/09/22 07:32:17.821
Dec  9 07:32:17.831: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-h4fx" in namespace "subpath-9595" to be "Succeeded or Failed"
Dec  9 07:32:17.837: INFO: Pod "pod-subpath-test-downwardapi-h4fx": Phase="Pending", Reason="", readiness=false. Elapsed: 6.035484ms
Dec  9 07:32:19.849: INFO: Pod "pod-subpath-test-downwardapi-h4fx": Phase="Running", Reason="", readiness=true. Elapsed: 2.018777843s
Dec  9 07:32:21.840: INFO: Pod "pod-subpath-test-downwardapi-h4fx": Phase="Running", Reason="", readiness=true. Elapsed: 4.009890703s
Dec  9 07:32:23.840: INFO: Pod "pod-subpath-test-downwardapi-h4fx": Phase="Running", Reason="", readiness=true. Elapsed: 6.009623088s
Dec  9 07:32:25.840: INFO: Pod "pod-subpath-test-downwardapi-h4fx": Phase="Running", Reason="", readiness=true. Elapsed: 8.009618906s
Dec  9 07:32:27.840: INFO: Pod "pod-subpath-test-downwardapi-h4fx": Phase="Running", Reason="", readiness=true. Elapsed: 10.009228112s
Dec  9 07:32:29.841: INFO: Pod "pod-subpath-test-downwardapi-h4fx": Phase="Running", Reason="", readiness=true. Elapsed: 12.010501788s
Dec  9 07:32:31.842: INFO: Pod "pod-subpath-test-downwardapi-h4fx": Phase="Running", Reason="", readiness=true. Elapsed: 14.011201731s
Dec  9 07:32:33.840: INFO: Pod "pod-subpath-test-downwardapi-h4fx": Phase="Running", Reason="", readiness=true. Elapsed: 16.009493491s
Dec  9 07:32:35.842: INFO: Pod "pod-subpath-test-downwardapi-h4fx": Phase="Running", Reason="", readiness=true. Elapsed: 18.011617582s
Dec  9 07:32:37.841: INFO: Pod "pod-subpath-test-downwardapi-h4fx": Phase="Running", Reason="", readiness=true. Elapsed: 20.010929696s
Dec  9 07:32:39.842: INFO: Pod "pod-subpath-test-downwardapi-h4fx": Phase="Running", Reason="", readiness=false. Elapsed: 22.011364201s
Dec  9 07:32:41.842: INFO: Pod "pod-subpath-test-downwardapi-h4fx": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.011035695s
STEP: Saw pod success 12/09/22 07:32:41.842
Dec  9 07:32:41.842: INFO: Pod "pod-subpath-test-downwardapi-h4fx" satisfied condition "Succeeded or Failed"
Dec  9 07:32:41.845: INFO: Trying to get logs from node ip-10-0-10-179 pod pod-subpath-test-downwardapi-h4fx container test-container-subpath-downwardapi-h4fx: <nil>
STEP: delete the pod 12/09/22 07:32:41.852
Dec  9 07:32:41.864: INFO: Waiting for pod pod-subpath-test-downwardapi-h4fx to disappear
Dec  9 07:32:41.868: INFO: Pod pod-subpath-test-downwardapi-h4fx no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-h4fx 12/09/22 07:32:41.868
Dec  9 07:32:41.868: INFO: Deleting pod "pod-subpath-test-downwardapi-h4fx" in namespace "subpath-9595"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Dec  9 07:32:41.871: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-9595" for this suite. 12/09/22 07:32:41.875
------------------------------
â€¢ [SLOW TEST] [24.106 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with downward pod [Conformance]
    test/e2e/storage/subpath.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:32:17.774
    Dec  9 07:32:17.774: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename subpath 12/09/22 07:32:17.775
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:32:17.799
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:32:17.805
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 12/09/22 07:32:17.808
    [It] should support subpaths with downward pod [Conformance]
      test/e2e/storage/subpath.go:92
    STEP: Creating pod pod-subpath-test-downwardapi-h4fx 12/09/22 07:32:17.82
    STEP: Creating a pod to test atomic-volume-subpath 12/09/22 07:32:17.821
    Dec  9 07:32:17.831: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-h4fx" in namespace "subpath-9595" to be "Succeeded or Failed"
    Dec  9 07:32:17.837: INFO: Pod "pod-subpath-test-downwardapi-h4fx": Phase="Pending", Reason="", readiness=false. Elapsed: 6.035484ms
    Dec  9 07:32:19.849: INFO: Pod "pod-subpath-test-downwardapi-h4fx": Phase="Running", Reason="", readiness=true. Elapsed: 2.018777843s
    Dec  9 07:32:21.840: INFO: Pod "pod-subpath-test-downwardapi-h4fx": Phase="Running", Reason="", readiness=true. Elapsed: 4.009890703s
    Dec  9 07:32:23.840: INFO: Pod "pod-subpath-test-downwardapi-h4fx": Phase="Running", Reason="", readiness=true. Elapsed: 6.009623088s
    Dec  9 07:32:25.840: INFO: Pod "pod-subpath-test-downwardapi-h4fx": Phase="Running", Reason="", readiness=true. Elapsed: 8.009618906s
    Dec  9 07:32:27.840: INFO: Pod "pod-subpath-test-downwardapi-h4fx": Phase="Running", Reason="", readiness=true. Elapsed: 10.009228112s
    Dec  9 07:32:29.841: INFO: Pod "pod-subpath-test-downwardapi-h4fx": Phase="Running", Reason="", readiness=true. Elapsed: 12.010501788s
    Dec  9 07:32:31.842: INFO: Pod "pod-subpath-test-downwardapi-h4fx": Phase="Running", Reason="", readiness=true. Elapsed: 14.011201731s
    Dec  9 07:32:33.840: INFO: Pod "pod-subpath-test-downwardapi-h4fx": Phase="Running", Reason="", readiness=true. Elapsed: 16.009493491s
    Dec  9 07:32:35.842: INFO: Pod "pod-subpath-test-downwardapi-h4fx": Phase="Running", Reason="", readiness=true. Elapsed: 18.011617582s
    Dec  9 07:32:37.841: INFO: Pod "pod-subpath-test-downwardapi-h4fx": Phase="Running", Reason="", readiness=true. Elapsed: 20.010929696s
    Dec  9 07:32:39.842: INFO: Pod "pod-subpath-test-downwardapi-h4fx": Phase="Running", Reason="", readiness=false. Elapsed: 22.011364201s
    Dec  9 07:32:41.842: INFO: Pod "pod-subpath-test-downwardapi-h4fx": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.011035695s
    STEP: Saw pod success 12/09/22 07:32:41.842
    Dec  9 07:32:41.842: INFO: Pod "pod-subpath-test-downwardapi-h4fx" satisfied condition "Succeeded or Failed"
    Dec  9 07:32:41.845: INFO: Trying to get logs from node ip-10-0-10-179 pod pod-subpath-test-downwardapi-h4fx container test-container-subpath-downwardapi-h4fx: <nil>
    STEP: delete the pod 12/09/22 07:32:41.852
    Dec  9 07:32:41.864: INFO: Waiting for pod pod-subpath-test-downwardapi-h4fx to disappear
    Dec  9 07:32:41.868: INFO: Pod pod-subpath-test-downwardapi-h4fx no longer exists
    STEP: Deleting pod pod-subpath-test-downwardapi-h4fx 12/09/22 07:32:41.868
    Dec  9 07:32:41.868: INFO: Deleting pod "pod-subpath-test-downwardapi-h4fx" in namespace "subpath-9595"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:32:41.871: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-9595" for this suite. 12/09/22 07:32:41.875
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server
  should support proxy with --port 0  [Conformance]
  test/e2e/kubectl/kubectl.go:1787
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:32:41.884
Dec  9 07:32:41.884: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename kubectl 12/09/22 07:32:41.885
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:32:41.903
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:32:41.911
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should support proxy with --port 0  [Conformance]
  test/e2e/kubectl/kubectl.go:1787
STEP: starting the proxy server 12/09/22 07:32:41.917
Dec  9 07:32:41.918: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=kubectl-577 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output 12/09/22 07:32:41.977
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Dec  9 07:32:41.989: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-577" for this suite. 12/09/22 07:32:41.995
------------------------------
â€¢ [0.116 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Proxy server
  test/e2e/kubectl/kubectl.go:1780
    should support proxy with --port 0  [Conformance]
    test/e2e/kubectl/kubectl.go:1787

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:32:41.884
    Dec  9 07:32:41.884: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename kubectl 12/09/22 07:32:41.885
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:32:41.903
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:32:41.911
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should support proxy with --port 0  [Conformance]
      test/e2e/kubectl/kubectl.go:1787
    STEP: starting the proxy server 12/09/22 07:32:41.917
    Dec  9 07:32:41.918: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=kubectl-577 proxy -p 0 --disable-filter'
    STEP: curling proxy /api/ output 12/09/22 07:32:41.977
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:32:41.989: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-577" for this suite. 12/09/22 07:32:41.995
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:331
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:32:42.001
Dec  9 07:32:42.001: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename sched-pred 12/09/22 07:32:42.002
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:32:42.02
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:32:42.027
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:97
Dec  9 07:32:42.031: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Dec  9 07:32:42.040: INFO: Waiting for terminating namespaces to be deleted...
Dec  9 07:32:42.045: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-10-179 before test
Dec  9 07:32:42.050: INFO: suspend-false-to-true-7592r from job-2913 started at 2022-12-09 07:32:13 +0000 UTC (1 container statuses recorded)
Dec  9 07:32:42.050: INFO: 	Container c ready: true, restart count 0
Dec  9 07:32:42.050: INFO: calico-node-9zhz5 from kube-system started at 2022-12-09 06:56:15 +0000 UTC (1 container statuses recorded)
Dec  9 07:32:42.050: INFO: 	Container calico-node ready: true, restart count 0
Dec  9 07:32:42.050: INFO: coredns-844594ffdd-q2bzk from kube-system started at 2022-12-09 06:56:25 +0000 UTC (1 container statuses recorded)
Dec  9 07:32:42.050: INFO: 	Container coredns ready: true, restart count 0
Dec  9 07:32:42.050: INFO: coredns-844594ffdd-rls2f from kube-system started at 2022-12-09 06:56:25 +0000 UTC (1 container statuses recorded)
Dec  9 07:32:42.050: INFO: 	Container coredns ready: true, restart count 0
Dec  9 07:32:42.050: INFO: kube-proxy-dv66w from kube-system started at 2022-12-09 06:56:15 +0000 UTC (1 container statuses recorded)
Dec  9 07:32:42.050: INFO: 	Container kube-proxy ready: true, restart count 0
Dec  9 07:32:42.050: INFO: sonobuoy-systemd-logs-daemon-set-8dafb6c50b3e4797-scljx from sonobuoy started at 2022-12-09 06:58:30 +0000 UTC (2 container statuses recorded)
Dec  9 07:32:42.050: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec  9 07:32:42.051: INFO: 	Container systemd-logs ready: true, restart count 0
Dec  9 07:32:42.051: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-17-108 before test
Dec  9 07:32:42.055: INFO: suspend-false-to-true-jm6dg from job-2913 started at 2022-12-09 07:32:13 +0000 UTC (1 container statuses recorded)
Dec  9 07:32:42.055: INFO: 	Container c ready: true, restart count 0
Dec  9 07:32:42.055: INFO: calico-node-flv5c from kube-system started at 2022-12-09 06:56:14 +0000 UTC (1 container statuses recorded)
Dec  9 07:32:42.055: INFO: 	Container calico-node ready: true, restart count 0
Dec  9 07:32:42.055: INFO: kube-proxy-r75k5 from kube-system started at 2022-12-09 06:56:14 +0000 UTC (1 container statuses recorded)
Dec  9 07:32:42.055: INFO: 	Container kube-proxy ready: true, restart count 0
Dec  9 07:32:42.055: INFO: sonobuoy from sonobuoy started at 2022-12-09 06:58:27 +0000 UTC (1 container statuses recorded)
Dec  9 07:32:42.056: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Dec  9 07:32:42.056: INFO: sonobuoy-e2e-job-023adc8283f34c8c from sonobuoy started at 2022-12-09 06:58:30 +0000 UTC (2 container statuses recorded)
Dec  9 07:32:42.056: INFO: 	Container e2e ready: true, restart count 0
Dec  9 07:32:42.056: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec  9 07:32:42.056: INFO: sonobuoy-systemd-logs-daemon-set-8dafb6c50b3e4797-jg2s9 from sonobuoy started at 2022-12-09 06:58:30 +0000 UTC (2 container statuses recorded)
Dec  9 07:32:42.056: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec  9 07:32:42.056: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:331
STEP: verifying the node has the label node ip-10-0-10-179 12/09/22 07:32:42.072
STEP: verifying the node has the label node ip-10-0-17-108 12/09/22 07:32:42.087
Dec  9 07:32:42.106: INFO: Pod suspend-false-to-true-7592r requesting resource cpu=0m on Node ip-10-0-10-179
Dec  9 07:32:42.106: INFO: Pod suspend-false-to-true-jm6dg requesting resource cpu=0m on Node ip-10-0-17-108
Dec  9 07:32:42.106: INFO: Pod calico-node-9zhz5 requesting resource cpu=100m on Node ip-10-0-10-179
Dec  9 07:32:42.107: INFO: Pod calico-node-flv5c requesting resource cpu=100m on Node ip-10-0-17-108
Dec  9 07:32:42.107: INFO: Pod coredns-844594ffdd-q2bzk requesting resource cpu=100m on Node ip-10-0-10-179
Dec  9 07:32:42.107: INFO: Pod coredns-844594ffdd-rls2f requesting resource cpu=100m on Node ip-10-0-10-179
Dec  9 07:32:42.107: INFO: Pod kube-proxy-dv66w requesting resource cpu=0m on Node ip-10-0-10-179
Dec  9 07:32:42.108: INFO: Pod kube-proxy-r75k5 requesting resource cpu=0m on Node ip-10-0-17-108
Dec  9 07:32:42.108: INFO: Pod sonobuoy requesting resource cpu=0m on Node ip-10-0-17-108
Dec  9 07:32:42.108: INFO: Pod sonobuoy-e2e-job-023adc8283f34c8c requesting resource cpu=0m on Node ip-10-0-17-108
Dec  9 07:32:42.108: INFO: Pod sonobuoy-systemd-logs-daemon-set-8dafb6c50b3e4797-jg2s9 requesting resource cpu=0m on Node ip-10-0-17-108
Dec  9 07:32:42.108: INFO: Pod sonobuoy-systemd-logs-daemon-set-8dafb6c50b3e4797-scljx requesting resource cpu=0m on Node ip-10-0-10-179
STEP: Starting Pods to consume most of the cluster CPU. 12/09/22 07:32:42.109
Dec  9 07:32:42.110: INFO: Creating a pod which consumes cpu=1190m on Node ip-10-0-10-179
Dec  9 07:32:42.122: INFO: Creating a pod which consumes cpu=1330m on Node ip-10-0-17-108
Dec  9 07:32:42.135: INFO: Waiting up to 5m0s for pod "filler-pod-123c04dd-8354-484c-bb5b-8c79afbd11f1" in namespace "sched-pred-3464" to be "running"
Dec  9 07:32:42.153: INFO: Pod "filler-pod-123c04dd-8354-484c-bb5b-8c79afbd11f1": Phase="Pending", Reason="", readiness=false. Elapsed: 17.317621ms
Dec  9 07:32:44.163: INFO: Pod "filler-pod-123c04dd-8354-484c-bb5b-8c79afbd11f1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02728517s
Dec  9 07:32:46.163: INFO: Pod "filler-pod-123c04dd-8354-484c-bb5b-8c79afbd11f1": Phase="Running", Reason="", readiness=true. Elapsed: 4.027364988s
Dec  9 07:32:46.163: INFO: Pod "filler-pod-123c04dd-8354-484c-bb5b-8c79afbd11f1" satisfied condition "running"
Dec  9 07:32:46.163: INFO: Waiting up to 5m0s for pod "filler-pod-4b7f5fc5-0f7e-4a4c-a6e1-f40d72c30940" in namespace "sched-pred-3464" to be "running"
Dec  9 07:32:46.175: INFO: Pod "filler-pod-4b7f5fc5-0f7e-4a4c-a6e1-f40d72c30940": Phase="Running", Reason="", readiness=true. Elapsed: 12.04283ms
Dec  9 07:32:46.175: INFO: Pod "filler-pod-4b7f5fc5-0f7e-4a4c-a6e1-f40d72c30940" satisfied condition "running"
STEP: Creating another pod that requires unavailable amount of CPU. 12/09/22 07:32:46.175
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-123c04dd-8354-484c-bb5b-8c79afbd11f1.172f0f9ef318243d], Reason = [Scheduled], Message = [Successfully assigned sched-pred-3464/filler-pod-123c04dd-8354-484c-bb5b-8c79afbd11f1 to ip-10-0-10-179] 12/09/22 07:32:46.205
STEP: Considering event: 
Type = [Warning], Name = [filler-pod-123c04dd-8354-484c-bb5b-8c79afbd11f1.172f0f9f3a8ae420], Reason = [FailedMount], Message = [MountVolume.SetUp failed for volume "kube-api-access-56q5t" : failed to sync configmap cache: timed out waiting for the condition] 12/09/22 07:32:46.205
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-123c04dd-8354-484c-bb5b-8c79afbd11f1.172f0f9f7dff6ab1], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 12/09/22 07:32:46.205
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-123c04dd-8354-484c-bb5b-8c79afbd11f1.172f0f9f7f7df2b3], Reason = [Created], Message = [Created container filler-pod-123c04dd-8354-484c-bb5b-8c79afbd11f1] 12/09/22 07:32:46.205
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-123c04dd-8354-484c-bb5b-8c79afbd11f1.172f0f9f8b12da5a], Reason = [Started], Message = [Started container filler-pod-123c04dd-8354-484c-bb5b-8c79afbd11f1] 12/09/22 07:32:46.205
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-4b7f5fc5-0f7e-4a4c-a6e1-f40d72c30940.172f0f9ef42b1e0d], Reason = [Scheduled], Message = [Successfully assigned sched-pred-3464/filler-pod-4b7f5fc5-0f7e-4a4c-a6e1-f40d72c30940 to ip-10-0-17-108] 12/09/22 07:32:46.205
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-4b7f5fc5-0f7e-4a4c-a6e1-f40d72c30940.172f0f9f2816c2ed], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 12/09/22 07:32:46.206
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-4b7f5fc5-0f7e-4a4c-a6e1-f40d72c30940.172f0f9f29507dbe], Reason = [Created], Message = [Created container filler-pod-4b7f5fc5-0f7e-4a4c-a6e1-f40d72c30940] 12/09/22 07:32:46.206
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-4b7f5fc5-0f7e-4a4c-a6e1-f40d72c30940.172f0f9f2ec13d92], Reason = [Started], Message = [Started container filler-pod-4b7f5fc5-0f7e-4a4c-a6e1-f40d72c30940] 12/09/22 07:32:46.206
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.172f0f9feaa3d703], Reason = [FailedScheduling], Message = [0/3 nodes are available: 1 node(s) had untolerated taint {node-role.kubernetes.io/controller: }, 3 Insufficient cpu. preemption: 0/3 nodes are available: 1 Preemption is not helpful for scheduling, 2 No preemption victims found for incoming pod..] 12/09/22 07:32:46.332
STEP: removing the label node off the node ip-10-0-10-179 12/09/22 07:32:47.292
STEP: verifying the node doesn't have the label node 12/09/22 07:32:47.308
STEP: removing the label node off the node ip-10-0-17-108 12/09/22 07:32:47.313
STEP: verifying the node doesn't have the label node 12/09/22 07:32:47.325
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/node/init/init.go:32
Dec  9 07:32:47.329: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:88
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-pred-3464" for this suite. 12/09/22 07:32:47.333
------------------------------
â€¢ [SLOW TEST] [5.338 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:331

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:32:42.001
    Dec  9 07:32:42.001: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename sched-pred 12/09/22 07:32:42.002
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:32:42.02
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:32:42.027
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:97
    Dec  9 07:32:42.031: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Dec  9 07:32:42.040: INFO: Waiting for terminating namespaces to be deleted...
    Dec  9 07:32:42.045: INFO: 
    Logging pods the apiserver thinks is on node ip-10-0-10-179 before test
    Dec  9 07:32:42.050: INFO: suspend-false-to-true-7592r from job-2913 started at 2022-12-09 07:32:13 +0000 UTC (1 container statuses recorded)
    Dec  9 07:32:42.050: INFO: 	Container c ready: true, restart count 0
    Dec  9 07:32:42.050: INFO: calico-node-9zhz5 from kube-system started at 2022-12-09 06:56:15 +0000 UTC (1 container statuses recorded)
    Dec  9 07:32:42.050: INFO: 	Container calico-node ready: true, restart count 0
    Dec  9 07:32:42.050: INFO: coredns-844594ffdd-q2bzk from kube-system started at 2022-12-09 06:56:25 +0000 UTC (1 container statuses recorded)
    Dec  9 07:32:42.050: INFO: 	Container coredns ready: true, restart count 0
    Dec  9 07:32:42.050: INFO: coredns-844594ffdd-rls2f from kube-system started at 2022-12-09 06:56:25 +0000 UTC (1 container statuses recorded)
    Dec  9 07:32:42.050: INFO: 	Container coredns ready: true, restart count 0
    Dec  9 07:32:42.050: INFO: kube-proxy-dv66w from kube-system started at 2022-12-09 06:56:15 +0000 UTC (1 container statuses recorded)
    Dec  9 07:32:42.050: INFO: 	Container kube-proxy ready: true, restart count 0
    Dec  9 07:32:42.050: INFO: sonobuoy-systemd-logs-daemon-set-8dafb6c50b3e4797-scljx from sonobuoy started at 2022-12-09 06:58:30 +0000 UTC (2 container statuses recorded)
    Dec  9 07:32:42.050: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Dec  9 07:32:42.051: INFO: 	Container systemd-logs ready: true, restart count 0
    Dec  9 07:32:42.051: INFO: 
    Logging pods the apiserver thinks is on node ip-10-0-17-108 before test
    Dec  9 07:32:42.055: INFO: suspend-false-to-true-jm6dg from job-2913 started at 2022-12-09 07:32:13 +0000 UTC (1 container statuses recorded)
    Dec  9 07:32:42.055: INFO: 	Container c ready: true, restart count 0
    Dec  9 07:32:42.055: INFO: calico-node-flv5c from kube-system started at 2022-12-09 06:56:14 +0000 UTC (1 container statuses recorded)
    Dec  9 07:32:42.055: INFO: 	Container calico-node ready: true, restart count 0
    Dec  9 07:32:42.055: INFO: kube-proxy-r75k5 from kube-system started at 2022-12-09 06:56:14 +0000 UTC (1 container statuses recorded)
    Dec  9 07:32:42.055: INFO: 	Container kube-proxy ready: true, restart count 0
    Dec  9 07:32:42.055: INFO: sonobuoy from sonobuoy started at 2022-12-09 06:58:27 +0000 UTC (1 container statuses recorded)
    Dec  9 07:32:42.056: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Dec  9 07:32:42.056: INFO: sonobuoy-e2e-job-023adc8283f34c8c from sonobuoy started at 2022-12-09 06:58:30 +0000 UTC (2 container statuses recorded)
    Dec  9 07:32:42.056: INFO: 	Container e2e ready: true, restart count 0
    Dec  9 07:32:42.056: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Dec  9 07:32:42.056: INFO: sonobuoy-systemd-logs-daemon-set-8dafb6c50b3e4797-jg2s9 from sonobuoy started at 2022-12-09 06:58:30 +0000 UTC (2 container statuses recorded)
    Dec  9 07:32:42.056: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Dec  9 07:32:42.056: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates resource limits of pods that are allowed to run  [Conformance]
      test/e2e/scheduling/predicates.go:331
    STEP: verifying the node has the label node ip-10-0-10-179 12/09/22 07:32:42.072
    STEP: verifying the node has the label node ip-10-0-17-108 12/09/22 07:32:42.087
    Dec  9 07:32:42.106: INFO: Pod suspend-false-to-true-7592r requesting resource cpu=0m on Node ip-10-0-10-179
    Dec  9 07:32:42.106: INFO: Pod suspend-false-to-true-jm6dg requesting resource cpu=0m on Node ip-10-0-17-108
    Dec  9 07:32:42.106: INFO: Pod calico-node-9zhz5 requesting resource cpu=100m on Node ip-10-0-10-179
    Dec  9 07:32:42.107: INFO: Pod calico-node-flv5c requesting resource cpu=100m on Node ip-10-0-17-108
    Dec  9 07:32:42.107: INFO: Pod coredns-844594ffdd-q2bzk requesting resource cpu=100m on Node ip-10-0-10-179
    Dec  9 07:32:42.107: INFO: Pod coredns-844594ffdd-rls2f requesting resource cpu=100m on Node ip-10-0-10-179
    Dec  9 07:32:42.107: INFO: Pod kube-proxy-dv66w requesting resource cpu=0m on Node ip-10-0-10-179
    Dec  9 07:32:42.108: INFO: Pod kube-proxy-r75k5 requesting resource cpu=0m on Node ip-10-0-17-108
    Dec  9 07:32:42.108: INFO: Pod sonobuoy requesting resource cpu=0m on Node ip-10-0-17-108
    Dec  9 07:32:42.108: INFO: Pod sonobuoy-e2e-job-023adc8283f34c8c requesting resource cpu=0m on Node ip-10-0-17-108
    Dec  9 07:32:42.108: INFO: Pod sonobuoy-systemd-logs-daemon-set-8dafb6c50b3e4797-jg2s9 requesting resource cpu=0m on Node ip-10-0-17-108
    Dec  9 07:32:42.108: INFO: Pod sonobuoy-systemd-logs-daemon-set-8dafb6c50b3e4797-scljx requesting resource cpu=0m on Node ip-10-0-10-179
    STEP: Starting Pods to consume most of the cluster CPU. 12/09/22 07:32:42.109
    Dec  9 07:32:42.110: INFO: Creating a pod which consumes cpu=1190m on Node ip-10-0-10-179
    Dec  9 07:32:42.122: INFO: Creating a pod which consumes cpu=1330m on Node ip-10-0-17-108
    Dec  9 07:32:42.135: INFO: Waiting up to 5m0s for pod "filler-pod-123c04dd-8354-484c-bb5b-8c79afbd11f1" in namespace "sched-pred-3464" to be "running"
    Dec  9 07:32:42.153: INFO: Pod "filler-pod-123c04dd-8354-484c-bb5b-8c79afbd11f1": Phase="Pending", Reason="", readiness=false. Elapsed: 17.317621ms
    Dec  9 07:32:44.163: INFO: Pod "filler-pod-123c04dd-8354-484c-bb5b-8c79afbd11f1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02728517s
    Dec  9 07:32:46.163: INFO: Pod "filler-pod-123c04dd-8354-484c-bb5b-8c79afbd11f1": Phase="Running", Reason="", readiness=true. Elapsed: 4.027364988s
    Dec  9 07:32:46.163: INFO: Pod "filler-pod-123c04dd-8354-484c-bb5b-8c79afbd11f1" satisfied condition "running"
    Dec  9 07:32:46.163: INFO: Waiting up to 5m0s for pod "filler-pod-4b7f5fc5-0f7e-4a4c-a6e1-f40d72c30940" in namespace "sched-pred-3464" to be "running"
    Dec  9 07:32:46.175: INFO: Pod "filler-pod-4b7f5fc5-0f7e-4a4c-a6e1-f40d72c30940": Phase="Running", Reason="", readiness=true. Elapsed: 12.04283ms
    Dec  9 07:32:46.175: INFO: Pod "filler-pod-4b7f5fc5-0f7e-4a4c-a6e1-f40d72c30940" satisfied condition "running"
    STEP: Creating another pod that requires unavailable amount of CPU. 12/09/22 07:32:46.175
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-123c04dd-8354-484c-bb5b-8c79afbd11f1.172f0f9ef318243d], Reason = [Scheduled], Message = [Successfully assigned sched-pred-3464/filler-pod-123c04dd-8354-484c-bb5b-8c79afbd11f1 to ip-10-0-10-179] 12/09/22 07:32:46.205
    STEP: Considering event: 
    Type = [Warning], Name = [filler-pod-123c04dd-8354-484c-bb5b-8c79afbd11f1.172f0f9f3a8ae420], Reason = [FailedMount], Message = [MountVolume.SetUp failed for volume "kube-api-access-56q5t" : failed to sync configmap cache: timed out waiting for the condition] 12/09/22 07:32:46.205
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-123c04dd-8354-484c-bb5b-8c79afbd11f1.172f0f9f7dff6ab1], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 12/09/22 07:32:46.205
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-123c04dd-8354-484c-bb5b-8c79afbd11f1.172f0f9f7f7df2b3], Reason = [Created], Message = [Created container filler-pod-123c04dd-8354-484c-bb5b-8c79afbd11f1] 12/09/22 07:32:46.205
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-123c04dd-8354-484c-bb5b-8c79afbd11f1.172f0f9f8b12da5a], Reason = [Started], Message = [Started container filler-pod-123c04dd-8354-484c-bb5b-8c79afbd11f1] 12/09/22 07:32:46.205
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-4b7f5fc5-0f7e-4a4c-a6e1-f40d72c30940.172f0f9ef42b1e0d], Reason = [Scheduled], Message = [Successfully assigned sched-pred-3464/filler-pod-4b7f5fc5-0f7e-4a4c-a6e1-f40d72c30940 to ip-10-0-17-108] 12/09/22 07:32:46.205
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-4b7f5fc5-0f7e-4a4c-a6e1-f40d72c30940.172f0f9f2816c2ed], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 12/09/22 07:32:46.206
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-4b7f5fc5-0f7e-4a4c-a6e1-f40d72c30940.172f0f9f29507dbe], Reason = [Created], Message = [Created container filler-pod-4b7f5fc5-0f7e-4a4c-a6e1-f40d72c30940] 12/09/22 07:32:46.206
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-4b7f5fc5-0f7e-4a4c-a6e1-f40d72c30940.172f0f9f2ec13d92], Reason = [Started], Message = [Started container filler-pod-4b7f5fc5-0f7e-4a4c-a6e1-f40d72c30940] 12/09/22 07:32:46.206
    STEP: Considering event: 
    Type = [Warning], Name = [additional-pod.172f0f9feaa3d703], Reason = [FailedScheduling], Message = [0/3 nodes are available: 1 node(s) had untolerated taint {node-role.kubernetes.io/controller: }, 3 Insufficient cpu. preemption: 0/3 nodes are available: 1 Preemption is not helpful for scheduling, 2 No preemption victims found for incoming pod..] 12/09/22 07:32:46.332
    STEP: removing the label node off the node ip-10-0-10-179 12/09/22 07:32:47.292
    STEP: verifying the node doesn't have the label node 12/09/22 07:32:47.308
    STEP: removing the label node off the node ip-10-0-17-108 12/09/22 07:32:47.313
    STEP: verifying the node doesn't have the label node 12/09/22 07:32:47.325
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:32:47.329: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:88
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-pred-3464" for this suite. 12/09/22 07:32:47.333
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:381
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:32:47.339
Dec  9 07:32:47.339: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename webhook 12/09/22 07:32:47.34
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:32:47.36
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:32:47.364
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 12/09/22 07:32:47.383
STEP: Create role binding to let webhook read extension-apiserver-authentication 12/09/22 07:32:47.825
STEP: Deploying the webhook pod 12/09/22 07:32:47.835
STEP: Wait for the deployment to be ready 12/09/22 07:32:47.845
Dec  9 07:32:47.854: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Dec  9 07:32:49.868: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.December, 9, 7, 32, 47, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 9, 7, 32, 47, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 9, 7, 32, 47, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 9, 7, 32, 47, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 12/09/22 07:32:51.871
STEP: Verifying the service has paired with the endpoint 12/09/22 07:32:51.882
Dec  9 07:32:52.882: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:381
STEP: Setting timeout (1s) shorter than webhook latency (5s) 12/09/22 07:32:52.885
STEP: Registering slow webhook via the AdmissionRegistration API 12/09/22 07:32:52.885
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s) 12/09/22 07:32:52.905
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore 12/09/22 07:32:53.916
STEP: Registering slow webhook via the AdmissionRegistration API 12/09/22 07:32:53.916
STEP: Having no error when timeout is longer than webhook latency 12/09/22 07:32:54.945
STEP: Registering slow webhook via the AdmissionRegistration API 12/09/22 07:32:54.945
STEP: Having no error when timeout is empty (defaulted to 10s in v1) 12/09/22 07:32:59.985
STEP: Registering slow webhook via the AdmissionRegistration API 12/09/22 07:32:59.985
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Dec  9 07:33:05.055: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-6683" for this suite. 12/09/22 07:33:05.366
STEP: Destroying namespace "webhook-6683-markers" for this suite. 12/09/22 07:33:05.405
------------------------------
â€¢ [SLOW TEST] [18.108 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:381

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:32:47.339
    Dec  9 07:32:47.339: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename webhook 12/09/22 07:32:47.34
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:32:47.36
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:32:47.364
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 12/09/22 07:32:47.383
    STEP: Create role binding to let webhook read extension-apiserver-authentication 12/09/22 07:32:47.825
    STEP: Deploying the webhook pod 12/09/22 07:32:47.835
    STEP: Wait for the deployment to be ready 12/09/22 07:32:47.845
    Dec  9 07:32:47.854: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Dec  9 07:32:49.868: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.December, 9, 7, 32, 47, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 9, 7, 32, 47, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 9, 7, 32, 47, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 9, 7, 32, 47, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 12/09/22 07:32:51.871
    STEP: Verifying the service has paired with the endpoint 12/09/22 07:32:51.882
    Dec  9 07:32:52.882: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should honor timeout [Conformance]
      test/e2e/apimachinery/webhook.go:381
    STEP: Setting timeout (1s) shorter than webhook latency (5s) 12/09/22 07:32:52.885
    STEP: Registering slow webhook via the AdmissionRegistration API 12/09/22 07:32:52.885
    STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s) 12/09/22 07:32:52.905
    STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore 12/09/22 07:32:53.916
    STEP: Registering slow webhook via the AdmissionRegistration API 12/09/22 07:32:53.916
    STEP: Having no error when timeout is longer than webhook latency 12/09/22 07:32:54.945
    STEP: Registering slow webhook via the AdmissionRegistration API 12/09/22 07:32:54.945
    STEP: Having no error when timeout is empty (defaulted to 10s in v1) 12/09/22 07:32:59.985
    STEP: Registering slow webhook via the AdmissionRegistration API 12/09/22 07:32:59.985
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:33:05.055: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-6683" for this suite. 12/09/22 07:33:05.366
    STEP: Destroying namespace "webhook-6683-markers" for this suite. 12/09/22 07:33:05.405
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with configmap pod with mountPath of existing file [Conformance]
  test/e2e/storage/subpath.go:80
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:33:05.447
Dec  9 07:33:05.447: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename subpath 12/09/22 07:33:05.451
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:33:05.638
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:33:05.65
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 12/09/22 07:33:05.657
[It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
  test/e2e/storage/subpath.go:80
STEP: Creating pod pod-subpath-test-configmap-z4xp 12/09/22 07:33:05.698
STEP: Creating a pod to test atomic-volume-subpath 12/09/22 07:33:05.698
Dec  9 07:33:05.710: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-z4xp" in namespace "subpath-9933" to be "Succeeded or Failed"
Dec  9 07:33:05.717: INFO: Pod "pod-subpath-test-configmap-z4xp": Phase="Pending", Reason="", readiness=false. Elapsed: 7.513421ms
Dec  9 07:33:07.721: INFO: Pod "pod-subpath-test-configmap-z4xp": Phase="Running", Reason="", readiness=true. Elapsed: 2.011295294s
Dec  9 07:33:09.723: INFO: Pod "pod-subpath-test-configmap-z4xp": Phase="Running", Reason="", readiness=true. Elapsed: 4.013498047s
Dec  9 07:33:11.723: INFO: Pod "pod-subpath-test-configmap-z4xp": Phase="Running", Reason="", readiness=true. Elapsed: 6.01292987s
Dec  9 07:33:13.723: INFO: Pod "pod-subpath-test-configmap-z4xp": Phase="Running", Reason="", readiness=true. Elapsed: 8.013340368s
Dec  9 07:33:15.722: INFO: Pod "pod-subpath-test-configmap-z4xp": Phase="Running", Reason="", readiness=true. Elapsed: 10.011978392s
Dec  9 07:33:17.725: INFO: Pod "pod-subpath-test-configmap-z4xp": Phase="Running", Reason="", readiness=true. Elapsed: 12.014772492s
Dec  9 07:33:19.726: INFO: Pod "pod-subpath-test-configmap-z4xp": Phase="Running", Reason="", readiness=true. Elapsed: 14.015792043s
Dec  9 07:33:21.722: INFO: Pod "pod-subpath-test-configmap-z4xp": Phase="Running", Reason="", readiness=true. Elapsed: 16.012222105s
Dec  9 07:33:23.723: INFO: Pod "pod-subpath-test-configmap-z4xp": Phase="Running", Reason="", readiness=true. Elapsed: 18.013156099s
Dec  9 07:33:25.722: INFO: Pod "pod-subpath-test-configmap-z4xp": Phase="Running", Reason="", readiness=true. Elapsed: 20.011928924s
Dec  9 07:33:27.721: INFO: Pod "pod-subpath-test-configmap-z4xp": Phase="Running", Reason="", readiness=false. Elapsed: 22.011253204s
Dec  9 07:33:29.722: INFO: Pod "pod-subpath-test-configmap-z4xp": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.01184975s
STEP: Saw pod success 12/09/22 07:33:29.722
Dec  9 07:33:29.722: INFO: Pod "pod-subpath-test-configmap-z4xp" satisfied condition "Succeeded or Failed"
Dec  9 07:33:29.725: INFO: Trying to get logs from node ip-10-0-10-179 pod pod-subpath-test-configmap-z4xp container test-container-subpath-configmap-z4xp: <nil>
STEP: delete the pod 12/09/22 07:33:29.73
Dec  9 07:33:29.743: INFO: Waiting for pod pod-subpath-test-configmap-z4xp to disappear
Dec  9 07:33:29.747: INFO: Pod pod-subpath-test-configmap-z4xp no longer exists
STEP: Deleting pod pod-subpath-test-configmap-z4xp 12/09/22 07:33:29.747
Dec  9 07:33:29.747: INFO: Deleting pod "pod-subpath-test-configmap-z4xp" in namespace "subpath-9933"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Dec  9 07:33:29.750: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-9933" for this suite. 12/09/22 07:33:29.753
------------------------------
â€¢ [SLOW TEST] [24.314 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with configmap pod with mountPath of existing file [Conformance]
    test/e2e/storage/subpath.go:80

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:33:05.447
    Dec  9 07:33:05.447: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename subpath 12/09/22 07:33:05.451
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:33:05.638
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:33:05.65
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 12/09/22 07:33:05.657
    [It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
      test/e2e/storage/subpath.go:80
    STEP: Creating pod pod-subpath-test-configmap-z4xp 12/09/22 07:33:05.698
    STEP: Creating a pod to test atomic-volume-subpath 12/09/22 07:33:05.698
    Dec  9 07:33:05.710: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-z4xp" in namespace "subpath-9933" to be "Succeeded or Failed"
    Dec  9 07:33:05.717: INFO: Pod "pod-subpath-test-configmap-z4xp": Phase="Pending", Reason="", readiness=false. Elapsed: 7.513421ms
    Dec  9 07:33:07.721: INFO: Pod "pod-subpath-test-configmap-z4xp": Phase="Running", Reason="", readiness=true. Elapsed: 2.011295294s
    Dec  9 07:33:09.723: INFO: Pod "pod-subpath-test-configmap-z4xp": Phase="Running", Reason="", readiness=true. Elapsed: 4.013498047s
    Dec  9 07:33:11.723: INFO: Pod "pod-subpath-test-configmap-z4xp": Phase="Running", Reason="", readiness=true. Elapsed: 6.01292987s
    Dec  9 07:33:13.723: INFO: Pod "pod-subpath-test-configmap-z4xp": Phase="Running", Reason="", readiness=true. Elapsed: 8.013340368s
    Dec  9 07:33:15.722: INFO: Pod "pod-subpath-test-configmap-z4xp": Phase="Running", Reason="", readiness=true. Elapsed: 10.011978392s
    Dec  9 07:33:17.725: INFO: Pod "pod-subpath-test-configmap-z4xp": Phase="Running", Reason="", readiness=true. Elapsed: 12.014772492s
    Dec  9 07:33:19.726: INFO: Pod "pod-subpath-test-configmap-z4xp": Phase="Running", Reason="", readiness=true. Elapsed: 14.015792043s
    Dec  9 07:33:21.722: INFO: Pod "pod-subpath-test-configmap-z4xp": Phase="Running", Reason="", readiness=true. Elapsed: 16.012222105s
    Dec  9 07:33:23.723: INFO: Pod "pod-subpath-test-configmap-z4xp": Phase="Running", Reason="", readiness=true. Elapsed: 18.013156099s
    Dec  9 07:33:25.722: INFO: Pod "pod-subpath-test-configmap-z4xp": Phase="Running", Reason="", readiness=true. Elapsed: 20.011928924s
    Dec  9 07:33:27.721: INFO: Pod "pod-subpath-test-configmap-z4xp": Phase="Running", Reason="", readiness=false. Elapsed: 22.011253204s
    Dec  9 07:33:29.722: INFO: Pod "pod-subpath-test-configmap-z4xp": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.01184975s
    STEP: Saw pod success 12/09/22 07:33:29.722
    Dec  9 07:33:29.722: INFO: Pod "pod-subpath-test-configmap-z4xp" satisfied condition "Succeeded or Failed"
    Dec  9 07:33:29.725: INFO: Trying to get logs from node ip-10-0-10-179 pod pod-subpath-test-configmap-z4xp container test-container-subpath-configmap-z4xp: <nil>
    STEP: delete the pod 12/09/22 07:33:29.73
    Dec  9 07:33:29.743: INFO: Waiting for pod pod-subpath-test-configmap-z4xp to disappear
    Dec  9 07:33:29.747: INFO: Pod pod-subpath-test-configmap-z4xp no longer exists
    STEP: Deleting pod pod-subpath-test-configmap-z4xp 12/09/22 07:33:29.747
    Dec  9 07:33:29.747: INFO: Deleting pod "pod-subpath-test-configmap-z4xp" in namespace "subpath-9933"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:33:29.750: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-9933" for this suite. 12/09/22 07:33:29.753
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-apps] Deployment
  should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:33:29.766
Dec  9 07:33:29.766: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename deployment 12/09/22 07:33:29.767
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:33:29.798
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:33:29.802
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185
STEP: creating a Deployment 12/09/22 07:33:29.811
STEP: waiting for Deployment to be created 12/09/22 07:33:29.817
STEP: waiting for all Replicas to be Ready 12/09/22 07:33:29.819
Dec  9 07:33:29.824: INFO: observed Deployment test-deployment in namespace deployment-8870 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Dec  9 07:33:29.824: INFO: observed Deployment test-deployment in namespace deployment-8870 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Dec  9 07:33:29.834: INFO: observed Deployment test-deployment in namespace deployment-8870 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Dec  9 07:33:29.834: INFO: observed Deployment test-deployment in namespace deployment-8870 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Dec  9 07:33:29.920: INFO: observed Deployment test-deployment in namespace deployment-8870 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Dec  9 07:33:29.920: INFO: observed Deployment test-deployment in namespace deployment-8870 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Dec  9 07:33:29.963: INFO: observed Deployment test-deployment in namespace deployment-8870 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Dec  9 07:33:29.963: INFO: observed Deployment test-deployment in namespace deployment-8870 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Dec  9 07:33:30.960: INFO: observed Deployment test-deployment in namespace deployment-8870 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Dec  9 07:33:30.960: INFO: observed Deployment test-deployment in namespace deployment-8870 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Dec  9 07:33:32.856: INFO: observed Deployment test-deployment in namespace deployment-8870 with ReadyReplicas 2 and labels map[test-deployment-static:true]
STEP: patching the Deployment 12/09/22 07:33:32.856
W1209 07:33:32.869375      20 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Dec  9 07:33:32.874: INFO: observed event type ADDED
STEP: waiting for Replicas to scale 12/09/22 07:33:32.874
Dec  9 07:33:32.877: INFO: observed Deployment test-deployment in namespace deployment-8870 with ReadyReplicas 0
Dec  9 07:33:32.877: INFO: observed Deployment test-deployment in namespace deployment-8870 with ReadyReplicas 0
Dec  9 07:33:32.877: INFO: observed Deployment test-deployment in namespace deployment-8870 with ReadyReplicas 0
Dec  9 07:33:32.877: INFO: observed Deployment test-deployment in namespace deployment-8870 with ReadyReplicas 0
Dec  9 07:33:32.877: INFO: observed Deployment test-deployment in namespace deployment-8870 with ReadyReplicas 0
Dec  9 07:33:32.877: INFO: observed Deployment test-deployment in namespace deployment-8870 with ReadyReplicas 0
Dec  9 07:33:32.877: INFO: observed Deployment test-deployment in namespace deployment-8870 with ReadyReplicas 0
Dec  9 07:33:32.877: INFO: observed Deployment test-deployment in namespace deployment-8870 with ReadyReplicas 0
Dec  9 07:33:32.877: INFO: observed Deployment test-deployment in namespace deployment-8870 with ReadyReplicas 1
Dec  9 07:33:32.878: INFO: observed Deployment test-deployment in namespace deployment-8870 with ReadyReplicas 1
Dec  9 07:33:32.878: INFO: observed Deployment test-deployment in namespace deployment-8870 with ReadyReplicas 2
Dec  9 07:33:32.878: INFO: observed Deployment test-deployment in namespace deployment-8870 with ReadyReplicas 2
Dec  9 07:33:32.878: INFO: observed Deployment test-deployment in namespace deployment-8870 with ReadyReplicas 2
Dec  9 07:33:32.878: INFO: observed Deployment test-deployment in namespace deployment-8870 with ReadyReplicas 2
Dec  9 07:33:32.899: INFO: observed Deployment test-deployment in namespace deployment-8870 with ReadyReplicas 2
Dec  9 07:33:32.899: INFO: observed Deployment test-deployment in namespace deployment-8870 with ReadyReplicas 2
Dec  9 07:33:32.927: INFO: observed Deployment test-deployment in namespace deployment-8870 with ReadyReplicas 2
Dec  9 07:33:32.927: INFO: observed Deployment test-deployment in namespace deployment-8870 with ReadyReplicas 2
Dec  9 07:33:32.972: INFO: observed Deployment test-deployment in namespace deployment-8870 with ReadyReplicas 1
Dec  9 07:33:32.972: INFO: observed Deployment test-deployment in namespace deployment-8870 with ReadyReplicas 1
Dec  9 07:33:32.994: INFO: observed Deployment test-deployment in namespace deployment-8870 with ReadyReplicas 1
Dec  9 07:33:32.994: INFO: observed Deployment test-deployment in namespace deployment-8870 with ReadyReplicas 1
Dec  9 07:33:33.977: INFO: observed Deployment test-deployment in namespace deployment-8870 with ReadyReplicas 2
Dec  9 07:33:33.977: INFO: observed Deployment test-deployment in namespace deployment-8870 with ReadyReplicas 2
Dec  9 07:33:34.004: INFO: observed Deployment test-deployment in namespace deployment-8870 with ReadyReplicas 1
STEP: listing Deployments 12/09/22 07:33:34.004
Dec  9 07:33:34.007: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
STEP: updating the Deployment 12/09/22 07:33:34.007
Dec  9 07:33:34.022: INFO: observed Deployment test-deployment in namespace deployment-8870 with ReadyReplicas 1
STEP: fetching the DeploymentStatus 12/09/22 07:33:34.022
Dec  9 07:33:34.034: INFO: observed Deployment test-deployment in namespace deployment-8870 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Dec  9 07:33:34.036: INFO: observed Deployment test-deployment in namespace deployment-8870 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Dec  9 07:33:34.071: INFO: observed Deployment test-deployment in namespace deployment-8870 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Dec  9 07:33:34.131: INFO: observed Deployment test-deployment in namespace deployment-8870 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Dec  9 07:33:34.144: INFO: observed Deployment test-deployment in namespace deployment-8870 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Dec  9 07:33:35.001: INFO: observed Deployment test-deployment in namespace deployment-8870 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Dec  9 07:33:35.070: INFO: observed Deployment test-deployment in namespace deployment-8870 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Dec  9 07:33:35.113: INFO: observed Deployment test-deployment in namespace deployment-8870 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Dec  9 07:33:35.882: INFO: observed Deployment test-deployment in namespace deployment-8870 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
STEP: patching the DeploymentStatus 12/09/22 07:33:35.921
STEP: fetching the DeploymentStatus 12/09/22 07:33:35.928
Dec  9 07:33:35.933: INFO: observed Deployment test-deployment in namespace deployment-8870 with ReadyReplicas 1
Dec  9 07:33:35.933: INFO: observed Deployment test-deployment in namespace deployment-8870 with ReadyReplicas 1
Dec  9 07:33:35.933: INFO: observed Deployment test-deployment in namespace deployment-8870 with ReadyReplicas 1
Dec  9 07:33:35.933: INFO: observed Deployment test-deployment in namespace deployment-8870 with ReadyReplicas 1
Dec  9 07:33:35.934: INFO: observed Deployment test-deployment in namespace deployment-8870 with ReadyReplicas 1
Dec  9 07:33:35.934: INFO: observed Deployment test-deployment in namespace deployment-8870 with ReadyReplicas 2
Dec  9 07:33:35.934: INFO: observed Deployment test-deployment in namespace deployment-8870 with ReadyReplicas 2
Dec  9 07:33:35.934: INFO: observed Deployment test-deployment in namespace deployment-8870 with ReadyReplicas 2
Dec  9 07:33:35.934: INFO: observed Deployment test-deployment in namespace deployment-8870 with ReadyReplicas 3
STEP: deleting the Deployment 12/09/22 07:33:35.934
Dec  9 07:33:35.945: INFO: observed event type MODIFIED
Dec  9 07:33:35.946: INFO: observed event type MODIFIED
Dec  9 07:33:35.946: INFO: observed event type MODIFIED
Dec  9 07:33:35.946: INFO: observed event type MODIFIED
Dec  9 07:33:35.946: INFO: observed event type MODIFIED
Dec  9 07:33:35.946: INFO: observed event type MODIFIED
Dec  9 07:33:35.946: INFO: observed event type MODIFIED
Dec  9 07:33:35.946: INFO: observed event type MODIFIED
Dec  9 07:33:35.946: INFO: observed event type MODIFIED
Dec  9 07:33:35.947: INFO: observed event type MODIFIED
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Dec  9 07:33:35.959: INFO: Log out all the ReplicaSets if there is no deployment created
Dec  9 07:33:35.970: INFO: ReplicaSet "test-deployment-7b7876f9d6":
&ReplicaSet{ObjectMeta:{test-deployment-7b7876f9d6  deployment-8870  ad9f429e-1819-4028-9191-ee57f9368a55 13306 2 2022-12-09 07:33:34 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment 6c1e76a5-9169-4b5a-9b28-c06a87e547b4 0xc0060420a7 0xc0060420a8}] [] [{kube-controller-manager Update apps/v1 2022-12-09 07:33:35 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6c1e76a5-9169-4b5a-9b28-c06a87e547b4\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-09 07:33:35 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7b7876f9d6,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc006042130 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:2,AvailableReplicas:2,Conditions:[]ReplicaSetCondition{},},}

Dec  9 07:33:35.973: INFO: pod: "test-deployment-7b7876f9d6-frtmx":
&Pod{ObjectMeta:{test-deployment-7b7876f9d6-frtmx test-deployment-7b7876f9d6- deployment-8870  6cd89850-c516-43c8-b925-f50ac35146b7 13266 0 2022-12-09 07:33:34 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[cni.projectcalico.org/containerID:8b0f66348c5407b3aa4b7c0406d4ed22a050ac7eb698d82f6eed1c62bb0c5d7d cni.projectcalico.org/podIP:10.2.136.172/32 cni.projectcalico.org/podIPs:10.2.136.172/32] [{apps/v1 ReplicaSet test-deployment-7b7876f9d6 ad9f429e-1819-4028-9191-ee57f9368a55 0xc0060425d7 0xc0060425d8}] [] [{calico Update v1 2022-12-09 07:33:34 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2022-12-09 07:33:34 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ad9f429e-1819-4028-9191-ee57f9368a55\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-09 07:33:34 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.2.136.172\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pl4sm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pl4sm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-10-179,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 07:33:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 07:33:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 07:33:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 07:33:34 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.10.179,PodIP:10.2.136.172,StartTime:2022-12-09 07:33:34 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-12-09 07:33:34 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://4b0c828d4f2f3a33c957ca2f388e482092cad310aa8733fd6cb2caaef9ad43f3,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.2.136.172,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Dec  9 07:33:35.973: INFO: pod: "test-deployment-7b7876f9d6-lnv6f":
&Pod{ObjectMeta:{test-deployment-7b7876f9d6-lnv6f test-deployment-7b7876f9d6- deployment-8870  a26caba2-2a2e-4322-9a3c-c889d65e9b1f 13305 0 2022-12-09 07:33:35 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[cni.projectcalico.org/containerID:2875c62c07cfdbb40b18da615239a125f5313dc60417f0d156c1fd7eb704f620 cni.projectcalico.org/podIP:10.2.166.101/32 cni.projectcalico.org/podIPs:10.2.166.101/32] [{apps/v1 ReplicaSet test-deployment-7b7876f9d6 ad9f429e-1819-4028-9191-ee57f9368a55 0xc006042807 0xc006042808}] [] [{calico Update v1 2022-12-09 07:33:35 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2022-12-09 07:33:35 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ad9f429e-1819-4028-9191-ee57f9368a55\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-09 07:33:35 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.2.166.101\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-l56zg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-l56zg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-17-108,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 07:33:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 07:33:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 07:33:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 07:33:35 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.17.108,PodIP:10.2.166.101,StartTime:2022-12-09 07:33:35 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-12-09 07:33:35 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://feb233f7b0b05e1f9de568edb5d79fca75e7ee415084ec57f56842ff8cd6d396,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.2.166.101,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Dec  9 07:33:35.973: INFO: ReplicaSet "test-deployment-7df74c55ff":
&ReplicaSet{ObjectMeta:{test-deployment-7df74c55ff  deployment-8870  48dc2198-e152-45c9-90ba-e0c31b34160d 13317 4 2022-12-09 07:33:32 +0000 UTC <nil> <nil> map[pod-template-hash:7df74c55ff test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment 6c1e76a5-9169-4b5a-9b28-c06a87e547b4 0xc006042197 0xc006042198}] [] [{kube-controller-manager Update apps/v1 2022-12-09 07:33:35 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6c1e76a5-9169-4b5a-9b28-c06a87e547b4\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-09 07:33:35 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7df74c55ff,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7df74c55ff test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/pause:3.9 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc006042220 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

Dec  9 07:33:35.981: INFO: pod: "test-deployment-7df74c55ff-d2d74":
&Pod{ObjectMeta:{test-deployment-7df74c55ff-d2d74 test-deployment-7df74c55ff- deployment-8870  e013f507-d00a-4026-a567-ba165a276ae5 13307 0 2022-12-09 07:33:34 +0000 UTC 2022-12-09 07:33:35 +0000 UTC 0xc006043ba8 map[pod-template-hash:7df74c55ff test-deployment-static:true] map[cni.projectcalico.org/containerID:6c43ab55808635e6c119d0ac1c135f6652c64b781a41635eaa5324dd5d4fe263 cni.projectcalico.org/podIP:10.2.166.100/32 cni.projectcalico.org/podIPs:10.2.166.100/32] [{apps/v1 ReplicaSet test-deployment-7df74c55ff 48dc2198-e152-45c9-90ba-e0c31b34160d 0xc006043bf7 0xc006043bf8}] [] [{calico Update v1 2022-12-09 07:33:34 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2022-12-09 07:33:34 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"48dc2198-e152-45c9-90ba-e0c31b34160d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-09 07:33:35 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.2.166.100\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-c8m7v,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.9,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-c8m7v,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-17-108,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 07:33:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 07:33:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 07:33:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 07:33:34 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.17.108,PodIP:10.2.166.100,StartTime:2022-12-09 07:33:34 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-12-09 07:33:34 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.9,ImageID:registry.k8s.io/pause@sha256:7031c1b283388d2c2e09b57badb803c05ebed362dc88d84b480cc47f72a21097,ContainerID:containerd://225c5705a486ce0a24103863f546cd399ada68d30c7b539570802db904f75b5e,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.2.166.100,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Dec  9 07:33:35.982: INFO: pod: "test-deployment-7df74c55ff-hqllz":
&Pod{ObjectMeta:{test-deployment-7df74c55ff-hqllz test-deployment-7df74c55ff- deployment-8870  08590b36-5233-4a6e-b1ab-14ab9256c5c3 13312 0 2022-12-09 07:33:32 +0000 UTC 2022-12-09 07:33:36 +0000 UTC 0xc006043de0 map[pod-template-hash:7df74c55ff test-deployment-static:true] map[cni.projectcalico.org/containerID:25737b7ed1be29d9c9453168e93dced5aca4c58f85a0a90a5aa715eb48315296 cni.projectcalico.org/podIP:10.2.136.171/32 cni.projectcalico.org/podIPs:10.2.136.171/32] [{apps/v1 ReplicaSet test-deployment-7df74c55ff 48dc2198-e152-45c9-90ba-e0c31b34160d 0xc006043e37 0xc006043e38}] [] [{kube-controller-manager Update v1 2022-12-09 07:33:32 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"48dc2198-e152-45c9-90ba-e0c31b34160d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-12-09 07:33:33 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-12-09 07:33:33 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.2.136.171\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-sgtw7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.9,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-sgtw7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-10-179,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 07:33:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 07:33:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 07:33:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 07:33:32 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.10.179,PodIP:10.2.136.171,StartTime:2022-12-09 07:33:32 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-12-09 07:33:33 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.9,ImageID:registry.k8s.io/pause@sha256:7031c1b283388d2c2e09b57badb803c05ebed362dc88d84b480cc47f72a21097,ContainerID:containerd://14d42fcb02ff7cef3fd58902302eada985072dc8e33e1c0eddc9d3e83461bd3d,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.2.136.171,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Dec  9 07:33:35.982: INFO: ReplicaSet "test-deployment-f4dbc4647":
&ReplicaSet{ObjectMeta:{test-deployment-f4dbc4647  deployment-8870  47c9d805-c327-4e19-855d-4d4d913399c8 13218 3 2022-12-09 07:33:29 +0000 UTC <nil> <nil> map[pod-template-hash:f4dbc4647 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment 6c1e76a5-9169-4b5a-9b28-c06a87e547b4 0xc006042287 0xc006042288}] [] [{kube-controller-manager Update apps/v1 2022-12-09 07:33:33 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6c1e76a5-9169-4b5a-9b28-c06a87e547b4\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-09 07:33:33 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: f4dbc4647,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:f4dbc4647 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc006042310 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Dec  9 07:33:35.987: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-8870" for this suite. 12/09/22 07:33:35.997
------------------------------
â€¢ [SLOW TEST] [6.247 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:33:29.766
    Dec  9 07:33:29.766: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename deployment 12/09/22 07:33:29.767
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:33:29.798
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:33:29.802
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] should run the lifecycle of a Deployment [Conformance]
      test/e2e/apps/deployment.go:185
    STEP: creating a Deployment 12/09/22 07:33:29.811
    STEP: waiting for Deployment to be created 12/09/22 07:33:29.817
    STEP: waiting for all Replicas to be Ready 12/09/22 07:33:29.819
    Dec  9 07:33:29.824: INFO: observed Deployment test-deployment in namespace deployment-8870 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Dec  9 07:33:29.824: INFO: observed Deployment test-deployment in namespace deployment-8870 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Dec  9 07:33:29.834: INFO: observed Deployment test-deployment in namespace deployment-8870 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Dec  9 07:33:29.834: INFO: observed Deployment test-deployment in namespace deployment-8870 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Dec  9 07:33:29.920: INFO: observed Deployment test-deployment in namespace deployment-8870 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Dec  9 07:33:29.920: INFO: observed Deployment test-deployment in namespace deployment-8870 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Dec  9 07:33:29.963: INFO: observed Deployment test-deployment in namespace deployment-8870 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Dec  9 07:33:29.963: INFO: observed Deployment test-deployment in namespace deployment-8870 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Dec  9 07:33:30.960: INFO: observed Deployment test-deployment in namespace deployment-8870 with ReadyReplicas 1 and labels map[test-deployment-static:true]
    Dec  9 07:33:30.960: INFO: observed Deployment test-deployment in namespace deployment-8870 with ReadyReplicas 1 and labels map[test-deployment-static:true]
    Dec  9 07:33:32.856: INFO: observed Deployment test-deployment in namespace deployment-8870 with ReadyReplicas 2 and labels map[test-deployment-static:true]
    STEP: patching the Deployment 12/09/22 07:33:32.856
    W1209 07:33:32.869375      20 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Dec  9 07:33:32.874: INFO: observed event type ADDED
    STEP: waiting for Replicas to scale 12/09/22 07:33:32.874
    Dec  9 07:33:32.877: INFO: observed Deployment test-deployment in namespace deployment-8870 with ReadyReplicas 0
    Dec  9 07:33:32.877: INFO: observed Deployment test-deployment in namespace deployment-8870 with ReadyReplicas 0
    Dec  9 07:33:32.877: INFO: observed Deployment test-deployment in namespace deployment-8870 with ReadyReplicas 0
    Dec  9 07:33:32.877: INFO: observed Deployment test-deployment in namespace deployment-8870 with ReadyReplicas 0
    Dec  9 07:33:32.877: INFO: observed Deployment test-deployment in namespace deployment-8870 with ReadyReplicas 0
    Dec  9 07:33:32.877: INFO: observed Deployment test-deployment in namespace deployment-8870 with ReadyReplicas 0
    Dec  9 07:33:32.877: INFO: observed Deployment test-deployment in namespace deployment-8870 with ReadyReplicas 0
    Dec  9 07:33:32.877: INFO: observed Deployment test-deployment in namespace deployment-8870 with ReadyReplicas 0
    Dec  9 07:33:32.877: INFO: observed Deployment test-deployment in namespace deployment-8870 with ReadyReplicas 1
    Dec  9 07:33:32.878: INFO: observed Deployment test-deployment in namespace deployment-8870 with ReadyReplicas 1
    Dec  9 07:33:32.878: INFO: observed Deployment test-deployment in namespace deployment-8870 with ReadyReplicas 2
    Dec  9 07:33:32.878: INFO: observed Deployment test-deployment in namespace deployment-8870 with ReadyReplicas 2
    Dec  9 07:33:32.878: INFO: observed Deployment test-deployment in namespace deployment-8870 with ReadyReplicas 2
    Dec  9 07:33:32.878: INFO: observed Deployment test-deployment in namespace deployment-8870 with ReadyReplicas 2
    Dec  9 07:33:32.899: INFO: observed Deployment test-deployment in namespace deployment-8870 with ReadyReplicas 2
    Dec  9 07:33:32.899: INFO: observed Deployment test-deployment in namespace deployment-8870 with ReadyReplicas 2
    Dec  9 07:33:32.927: INFO: observed Deployment test-deployment in namespace deployment-8870 with ReadyReplicas 2
    Dec  9 07:33:32.927: INFO: observed Deployment test-deployment in namespace deployment-8870 with ReadyReplicas 2
    Dec  9 07:33:32.972: INFO: observed Deployment test-deployment in namespace deployment-8870 with ReadyReplicas 1
    Dec  9 07:33:32.972: INFO: observed Deployment test-deployment in namespace deployment-8870 with ReadyReplicas 1
    Dec  9 07:33:32.994: INFO: observed Deployment test-deployment in namespace deployment-8870 with ReadyReplicas 1
    Dec  9 07:33:32.994: INFO: observed Deployment test-deployment in namespace deployment-8870 with ReadyReplicas 1
    Dec  9 07:33:33.977: INFO: observed Deployment test-deployment in namespace deployment-8870 with ReadyReplicas 2
    Dec  9 07:33:33.977: INFO: observed Deployment test-deployment in namespace deployment-8870 with ReadyReplicas 2
    Dec  9 07:33:34.004: INFO: observed Deployment test-deployment in namespace deployment-8870 with ReadyReplicas 1
    STEP: listing Deployments 12/09/22 07:33:34.004
    Dec  9 07:33:34.007: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
    STEP: updating the Deployment 12/09/22 07:33:34.007
    Dec  9 07:33:34.022: INFO: observed Deployment test-deployment in namespace deployment-8870 with ReadyReplicas 1
    STEP: fetching the DeploymentStatus 12/09/22 07:33:34.022
    Dec  9 07:33:34.034: INFO: observed Deployment test-deployment in namespace deployment-8870 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Dec  9 07:33:34.036: INFO: observed Deployment test-deployment in namespace deployment-8870 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Dec  9 07:33:34.071: INFO: observed Deployment test-deployment in namespace deployment-8870 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Dec  9 07:33:34.131: INFO: observed Deployment test-deployment in namespace deployment-8870 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Dec  9 07:33:34.144: INFO: observed Deployment test-deployment in namespace deployment-8870 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Dec  9 07:33:35.001: INFO: observed Deployment test-deployment in namespace deployment-8870 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Dec  9 07:33:35.070: INFO: observed Deployment test-deployment in namespace deployment-8870 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Dec  9 07:33:35.113: INFO: observed Deployment test-deployment in namespace deployment-8870 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Dec  9 07:33:35.882: INFO: observed Deployment test-deployment in namespace deployment-8870 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
    STEP: patching the DeploymentStatus 12/09/22 07:33:35.921
    STEP: fetching the DeploymentStatus 12/09/22 07:33:35.928
    Dec  9 07:33:35.933: INFO: observed Deployment test-deployment in namespace deployment-8870 with ReadyReplicas 1
    Dec  9 07:33:35.933: INFO: observed Deployment test-deployment in namespace deployment-8870 with ReadyReplicas 1
    Dec  9 07:33:35.933: INFO: observed Deployment test-deployment in namespace deployment-8870 with ReadyReplicas 1
    Dec  9 07:33:35.933: INFO: observed Deployment test-deployment in namespace deployment-8870 with ReadyReplicas 1
    Dec  9 07:33:35.934: INFO: observed Deployment test-deployment in namespace deployment-8870 with ReadyReplicas 1
    Dec  9 07:33:35.934: INFO: observed Deployment test-deployment in namespace deployment-8870 with ReadyReplicas 2
    Dec  9 07:33:35.934: INFO: observed Deployment test-deployment in namespace deployment-8870 with ReadyReplicas 2
    Dec  9 07:33:35.934: INFO: observed Deployment test-deployment in namespace deployment-8870 with ReadyReplicas 2
    Dec  9 07:33:35.934: INFO: observed Deployment test-deployment in namespace deployment-8870 with ReadyReplicas 3
    STEP: deleting the Deployment 12/09/22 07:33:35.934
    Dec  9 07:33:35.945: INFO: observed event type MODIFIED
    Dec  9 07:33:35.946: INFO: observed event type MODIFIED
    Dec  9 07:33:35.946: INFO: observed event type MODIFIED
    Dec  9 07:33:35.946: INFO: observed event type MODIFIED
    Dec  9 07:33:35.946: INFO: observed event type MODIFIED
    Dec  9 07:33:35.946: INFO: observed event type MODIFIED
    Dec  9 07:33:35.946: INFO: observed event type MODIFIED
    Dec  9 07:33:35.946: INFO: observed event type MODIFIED
    Dec  9 07:33:35.946: INFO: observed event type MODIFIED
    Dec  9 07:33:35.947: INFO: observed event type MODIFIED
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Dec  9 07:33:35.959: INFO: Log out all the ReplicaSets if there is no deployment created
    Dec  9 07:33:35.970: INFO: ReplicaSet "test-deployment-7b7876f9d6":
    &ReplicaSet{ObjectMeta:{test-deployment-7b7876f9d6  deployment-8870  ad9f429e-1819-4028-9191-ee57f9368a55 13306 2 2022-12-09 07:33:34 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment 6c1e76a5-9169-4b5a-9b28-c06a87e547b4 0xc0060420a7 0xc0060420a8}] [] [{kube-controller-manager Update apps/v1 2022-12-09 07:33:35 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6c1e76a5-9169-4b5a-9b28-c06a87e547b4\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-09 07:33:35 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7b7876f9d6,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc006042130 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:2,AvailableReplicas:2,Conditions:[]ReplicaSetCondition{},},}

    Dec  9 07:33:35.973: INFO: pod: "test-deployment-7b7876f9d6-frtmx":
    &Pod{ObjectMeta:{test-deployment-7b7876f9d6-frtmx test-deployment-7b7876f9d6- deployment-8870  6cd89850-c516-43c8-b925-f50ac35146b7 13266 0 2022-12-09 07:33:34 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[cni.projectcalico.org/containerID:8b0f66348c5407b3aa4b7c0406d4ed22a050ac7eb698d82f6eed1c62bb0c5d7d cni.projectcalico.org/podIP:10.2.136.172/32 cni.projectcalico.org/podIPs:10.2.136.172/32] [{apps/v1 ReplicaSet test-deployment-7b7876f9d6 ad9f429e-1819-4028-9191-ee57f9368a55 0xc0060425d7 0xc0060425d8}] [] [{calico Update v1 2022-12-09 07:33:34 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2022-12-09 07:33:34 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ad9f429e-1819-4028-9191-ee57f9368a55\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-09 07:33:34 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.2.136.172\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pl4sm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pl4sm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-10-179,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 07:33:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 07:33:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 07:33:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 07:33:34 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.10.179,PodIP:10.2.136.172,StartTime:2022-12-09 07:33:34 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-12-09 07:33:34 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://4b0c828d4f2f3a33c957ca2f388e482092cad310aa8733fd6cb2caaef9ad43f3,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.2.136.172,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    Dec  9 07:33:35.973: INFO: pod: "test-deployment-7b7876f9d6-lnv6f":
    &Pod{ObjectMeta:{test-deployment-7b7876f9d6-lnv6f test-deployment-7b7876f9d6- deployment-8870  a26caba2-2a2e-4322-9a3c-c889d65e9b1f 13305 0 2022-12-09 07:33:35 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[cni.projectcalico.org/containerID:2875c62c07cfdbb40b18da615239a125f5313dc60417f0d156c1fd7eb704f620 cni.projectcalico.org/podIP:10.2.166.101/32 cni.projectcalico.org/podIPs:10.2.166.101/32] [{apps/v1 ReplicaSet test-deployment-7b7876f9d6 ad9f429e-1819-4028-9191-ee57f9368a55 0xc006042807 0xc006042808}] [] [{calico Update v1 2022-12-09 07:33:35 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2022-12-09 07:33:35 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ad9f429e-1819-4028-9191-ee57f9368a55\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-09 07:33:35 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.2.166.101\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-l56zg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-l56zg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-17-108,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 07:33:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 07:33:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 07:33:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 07:33:35 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.17.108,PodIP:10.2.166.101,StartTime:2022-12-09 07:33:35 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-12-09 07:33:35 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://feb233f7b0b05e1f9de568edb5d79fca75e7ee415084ec57f56842ff8cd6d396,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.2.166.101,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    Dec  9 07:33:35.973: INFO: ReplicaSet "test-deployment-7df74c55ff":
    &ReplicaSet{ObjectMeta:{test-deployment-7df74c55ff  deployment-8870  48dc2198-e152-45c9-90ba-e0c31b34160d 13317 4 2022-12-09 07:33:32 +0000 UTC <nil> <nil> map[pod-template-hash:7df74c55ff test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment 6c1e76a5-9169-4b5a-9b28-c06a87e547b4 0xc006042197 0xc006042198}] [] [{kube-controller-manager Update apps/v1 2022-12-09 07:33:35 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6c1e76a5-9169-4b5a-9b28-c06a87e547b4\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-09 07:33:35 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7df74c55ff,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7df74c55ff test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/pause:3.9 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc006042220 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

    Dec  9 07:33:35.981: INFO: pod: "test-deployment-7df74c55ff-d2d74":
    &Pod{ObjectMeta:{test-deployment-7df74c55ff-d2d74 test-deployment-7df74c55ff- deployment-8870  e013f507-d00a-4026-a567-ba165a276ae5 13307 0 2022-12-09 07:33:34 +0000 UTC 2022-12-09 07:33:35 +0000 UTC 0xc006043ba8 map[pod-template-hash:7df74c55ff test-deployment-static:true] map[cni.projectcalico.org/containerID:6c43ab55808635e6c119d0ac1c135f6652c64b781a41635eaa5324dd5d4fe263 cni.projectcalico.org/podIP:10.2.166.100/32 cni.projectcalico.org/podIPs:10.2.166.100/32] [{apps/v1 ReplicaSet test-deployment-7df74c55ff 48dc2198-e152-45c9-90ba-e0c31b34160d 0xc006043bf7 0xc006043bf8}] [] [{calico Update v1 2022-12-09 07:33:34 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2022-12-09 07:33:34 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"48dc2198-e152-45c9-90ba-e0c31b34160d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-09 07:33:35 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.2.166.100\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-c8m7v,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.9,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-c8m7v,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-17-108,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 07:33:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 07:33:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 07:33:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 07:33:34 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.17.108,PodIP:10.2.166.100,StartTime:2022-12-09 07:33:34 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-12-09 07:33:34 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.9,ImageID:registry.k8s.io/pause@sha256:7031c1b283388d2c2e09b57badb803c05ebed362dc88d84b480cc47f72a21097,ContainerID:containerd://225c5705a486ce0a24103863f546cd399ada68d30c7b539570802db904f75b5e,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.2.166.100,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    Dec  9 07:33:35.982: INFO: pod: "test-deployment-7df74c55ff-hqllz":
    &Pod{ObjectMeta:{test-deployment-7df74c55ff-hqllz test-deployment-7df74c55ff- deployment-8870  08590b36-5233-4a6e-b1ab-14ab9256c5c3 13312 0 2022-12-09 07:33:32 +0000 UTC 2022-12-09 07:33:36 +0000 UTC 0xc006043de0 map[pod-template-hash:7df74c55ff test-deployment-static:true] map[cni.projectcalico.org/containerID:25737b7ed1be29d9c9453168e93dced5aca4c58f85a0a90a5aa715eb48315296 cni.projectcalico.org/podIP:10.2.136.171/32 cni.projectcalico.org/podIPs:10.2.136.171/32] [{apps/v1 ReplicaSet test-deployment-7df74c55ff 48dc2198-e152-45c9-90ba-e0c31b34160d 0xc006043e37 0xc006043e38}] [] [{kube-controller-manager Update v1 2022-12-09 07:33:32 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"48dc2198-e152-45c9-90ba-e0c31b34160d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-12-09 07:33:33 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-12-09 07:33:33 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.2.136.171\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-sgtw7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.9,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-sgtw7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-10-179,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 07:33:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 07:33:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 07:33:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 07:33:32 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.10.179,PodIP:10.2.136.171,StartTime:2022-12-09 07:33:32 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-12-09 07:33:33 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.9,ImageID:registry.k8s.io/pause@sha256:7031c1b283388d2c2e09b57badb803c05ebed362dc88d84b480cc47f72a21097,ContainerID:containerd://14d42fcb02ff7cef3fd58902302eada985072dc8e33e1c0eddc9d3e83461bd3d,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.2.136.171,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    Dec  9 07:33:35.982: INFO: ReplicaSet "test-deployment-f4dbc4647":
    &ReplicaSet{ObjectMeta:{test-deployment-f4dbc4647  deployment-8870  47c9d805-c327-4e19-855d-4d4d913399c8 13218 3 2022-12-09 07:33:29 +0000 UTC <nil> <nil> map[pod-template-hash:f4dbc4647 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment 6c1e76a5-9169-4b5a-9b28-c06a87e547b4 0xc006042287 0xc006042288}] [] [{kube-controller-manager Update apps/v1 2022-12-09 07:33:33 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6c1e76a5-9169-4b5a-9b28-c06a87e547b4\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-09 07:33:33 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: f4dbc4647,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:f4dbc4647 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc006042310 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:33:35.987: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-8870" for this suite. 12/09/22 07:33:35.997
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:33:36.014
Dec  9 07:33:36.014: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename deployment 12/09/22 07:33:36.017
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:33:36.034
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:33:36.037
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150
Dec  9 07:33:36.041: INFO: Creating simple deployment test-new-deployment
Dec  9 07:33:36.053: INFO: deployment "test-new-deployment" doesn't have the required revision set
STEP: getting scale subresource 12/09/22 07:33:38.072
STEP: updating a scale subresource 12/09/22 07:33:38.082
STEP: verifying the deployment Spec.Replicas was modified 12/09/22 07:33:38.089
STEP: Patch a scale subresource 12/09/22 07:33:38.096
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Dec  9 07:33:38.165: INFO: Deployment "test-new-deployment":
&Deployment{ObjectMeta:{test-new-deployment  deployment-51  955c4cad-0183-4812-9724-e955551e5a80 13375 3 2022-12-09 07:33:36 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2022-12-09 07:33:36 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-09 07:33:36 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0062404c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2022-12-09 07:33:36 +0000 UTC,LastTransitionTime:2022-12-09 07:33:36 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-7f5969cbc7" has successfully progressed.,LastUpdateTime:2022-12-09 07:33:36 +0000 UTC,LastTransitionTime:2022-12-09 07:33:36 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Dec  9 07:33:38.195: INFO: New ReplicaSet "test-new-deployment-7f5969cbc7" of Deployment "test-new-deployment":
&ReplicaSet{ObjectMeta:{test-new-deployment-7f5969cbc7  deployment-51  98354cd8-70a9-49e3-879b-0a8653b074e5 13381 2 2022-12-09 07:33:36 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment 955c4cad-0183-4812-9724-e955551e5a80 0xc0062408d7 0xc0062408d8}] [] [{kube-controller-manager Update apps/v1 2022-12-09 07:33:38 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"955c4cad-0183-4812-9724-e955551e5a80\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-09 07:33:38 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7f5969cbc7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc006240968 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Dec  9 07:33:38.202: INFO: Pod "test-new-deployment-7f5969cbc7-4v7bf" is available:
&Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-4v7bf test-new-deployment-7f5969cbc7- deployment-51  c6620f07-574f-4e2a-88dd-041717b259a1 13357 0 2022-12-09 07:33:36 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:cb8dfa6453ee5c424842c2e001dd44e991f4057bdd94d9fe9a5117da70615e6e cni.projectcalico.org/podIP:10.2.136.173/32 cni.projectcalico.org/podIPs:10.2.136.173/32] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 98354cd8-70a9-49e3-879b-0a8653b074e5 0xc006240d27 0xc006240d28}] [] [{calico Update v1 2022-12-09 07:33:36 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2022-12-09 07:33:36 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"98354cd8-70a9-49e3-879b-0a8653b074e5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-09 07:33:36 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.2.136.173\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8c6fw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8c6fw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-10-179,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 07:33:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 07:33:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 07:33:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 07:33:36 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.10.179,PodIP:10.2.136.173,StartTime:2022-12-09 07:33:36 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-12-09 07:33:36 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://cf0c7f8a8c47a76ef7898113001228a6f2d7a9c2a127cb74c3bfc6d13f382f75,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.2.136.173,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec  9 07:33:38.202: INFO: Pod "test-new-deployment-7f5969cbc7-rw47g" is not available:
&Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-rw47g test-new-deployment-7f5969cbc7- deployment-51  68b78c34-06a5-4638-8787-ae764240bfdd 13382 0 2022-12-09 07:33:38 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 98354cd8-70a9-49e3-879b-0a8653b074e5 0xc006240f27 0xc006240f28}] [] [{kube-controller-manager Update v1 2022-12-09 07:33:38 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"98354cd8-70a9-49e3-879b-0a8653b074e5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-09 07:33:38 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2c8lt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2c8lt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-17-108,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 07:33:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 07:33:38 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 07:33:38 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 07:33:38 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.17.108,PodIP:,StartTime:2022-12-09 07:33:38 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Dec  9 07:33:38.202: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-51" for this suite. 12/09/22 07:33:38.237
------------------------------
â€¢ [2.258 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:33:36.014
    Dec  9 07:33:36.014: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename deployment 12/09/22 07:33:36.017
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:33:36.034
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:33:36.037
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] Deployment should have a working scale subresource [Conformance]
      test/e2e/apps/deployment.go:150
    Dec  9 07:33:36.041: INFO: Creating simple deployment test-new-deployment
    Dec  9 07:33:36.053: INFO: deployment "test-new-deployment" doesn't have the required revision set
    STEP: getting scale subresource 12/09/22 07:33:38.072
    STEP: updating a scale subresource 12/09/22 07:33:38.082
    STEP: verifying the deployment Spec.Replicas was modified 12/09/22 07:33:38.089
    STEP: Patch a scale subresource 12/09/22 07:33:38.096
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Dec  9 07:33:38.165: INFO: Deployment "test-new-deployment":
    &Deployment{ObjectMeta:{test-new-deployment  deployment-51  955c4cad-0183-4812-9724-e955551e5a80 13375 3 2022-12-09 07:33:36 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2022-12-09 07:33:36 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-09 07:33:36 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0062404c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2022-12-09 07:33:36 +0000 UTC,LastTransitionTime:2022-12-09 07:33:36 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-7f5969cbc7" has successfully progressed.,LastUpdateTime:2022-12-09 07:33:36 +0000 UTC,LastTransitionTime:2022-12-09 07:33:36 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Dec  9 07:33:38.195: INFO: New ReplicaSet "test-new-deployment-7f5969cbc7" of Deployment "test-new-deployment":
    &ReplicaSet{ObjectMeta:{test-new-deployment-7f5969cbc7  deployment-51  98354cd8-70a9-49e3-879b-0a8653b074e5 13381 2 2022-12-09 07:33:36 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment 955c4cad-0183-4812-9724-e955551e5a80 0xc0062408d7 0xc0062408d8}] [] [{kube-controller-manager Update apps/v1 2022-12-09 07:33:38 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"955c4cad-0183-4812-9724-e955551e5a80\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-09 07:33:38 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7f5969cbc7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc006240968 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Dec  9 07:33:38.202: INFO: Pod "test-new-deployment-7f5969cbc7-4v7bf" is available:
    &Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-4v7bf test-new-deployment-7f5969cbc7- deployment-51  c6620f07-574f-4e2a-88dd-041717b259a1 13357 0 2022-12-09 07:33:36 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:cb8dfa6453ee5c424842c2e001dd44e991f4057bdd94d9fe9a5117da70615e6e cni.projectcalico.org/podIP:10.2.136.173/32 cni.projectcalico.org/podIPs:10.2.136.173/32] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 98354cd8-70a9-49e3-879b-0a8653b074e5 0xc006240d27 0xc006240d28}] [] [{calico Update v1 2022-12-09 07:33:36 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2022-12-09 07:33:36 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"98354cd8-70a9-49e3-879b-0a8653b074e5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-09 07:33:36 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.2.136.173\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8c6fw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8c6fw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-10-179,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 07:33:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 07:33:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 07:33:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 07:33:36 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.10.179,PodIP:10.2.136.173,StartTime:2022-12-09 07:33:36 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-12-09 07:33:36 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://cf0c7f8a8c47a76ef7898113001228a6f2d7a9c2a127cb74c3bfc6d13f382f75,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.2.136.173,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Dec  9 07:33:38.202: INFO: Pod "test-new-deployment-7f5969cbc7-rw47g" is not available:
    &Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-rw47g test-new-deployment-7f5969cbc7- deployment-51  68b78c34-06a5-4638-8787-ae764240bfdd 13382 0 2022-12-09 07:33:38 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 98354cd8-70a9-49e3-879b-0a8653b074e5 0xc006240f27 0xc006240f28}] [] [{kube-controller-manager Update v1 2022-12-09 07:33:38 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"98354cd8-70a9-49e3-879b-0a8653b074e5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-09 07:33:38 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2c8lt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2c8lt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-17-108,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 07:33:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 07:33:38 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 07:33:38 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 07:33:38 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.17.108,PodIP:,StartTime:2022-12-09 07:33:38 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:33:38.202: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-51" for this suite. 12/09/22 07:33:38.237
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:442
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:33:38.273
Dec  9 07:33:38.273: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename crd-publish-openapi 12/09/22 07:33:38.274
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:33:38.417
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:33:38.453
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:442
STEP: set up a multi version CRD 12/09/22 07:33:38.471
Dec  9 07:33:38.472: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: mark a version not serverd 12/09/22 07:33:42.284
STEP: check the unserved version gets removed 12/09/22 07:33:42.301
STEP: check the other version is not changed 12/09/22 07:33:43.815
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Dec  9 07:33:46.759: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-5769" for this suite. 12/09/22 07:33:46.768
------------------------------
â€¢ [SLOW TEST] [8.501 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:442

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:33:38.273
    Dec  9 07:33:38.273: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename crd-publish-openapi 12/09/22 07:33:38.274
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:33:38.417
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:33:38.453
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] removes definition from spec when one version gets changed to not be served [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:442
    STEP: set up a multi version CRD 12/09/22 07:33:38.471
    Dec  9 07:33:38.472: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: mark a version not serverd 12/09/22 07:33:42.284
    STEP: check the unserved version gets removed 12/09/22 07:33:42.301
    STEP: check the other version is not changed 12/09/22 07:33:43.815
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:33:46.759: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-5769" for this suite. 12/09/22 07:33:46.768
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:33:46.776
Dec  9 07:33:46.776: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename replicaset 12/09/22 07:33:46.777
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:33:46.798
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:33:46.802
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176
STEP: Create a Replicaset 12/09/22 07:33:46.808
STEP: Verify that the required pods have come up. 12/09/22 07:33:46.813
Dec  9 07:33:46.817: INFO: Pod name sample-pod: Found 0 pods out of 1
Dec  9 07:33:51.839: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 12/09/22 07:33:51.839
STEP: Getting /status 12/09/22 07:33:51.84
Dec  9 07:33:51.852: INFO: Replicaset test-rs has Conditions: []
STEP: updating the Replicaset Status 12/09/22 07:33:51.852
Dec  9 07:33:51.878: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the ReplicaSet status to be updated 12/09/22 07:33:51.878
Dec  9 07:33:51.882: INFO: Observed &ReplicaSet event: ADDED
Dec  9 07:33:51.882: INFO: Observed &ReplicaSet event: MODIFIED
Dec  9 07:33:51.882: INFO: Observed &ReplicaSet event: MODIFIED
Dec  9 07:33:51.883: INFO: Observed &ReplicaSet event: MODIFIED
Dec  9 07:33:51.883: INFO: Found replicaset test-rs in namespace replicaset-9226 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Dec  9 07:33:51.883: INFO: Replicaset test-rs has an updated status
STEP: patching the Replicaset Status 12/09/22 07:33:51.883
Dec  9 07:33:51.883: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Dec  9 07:33:51.890: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Replicaset status to be patched 12/09/22 07:33:51.89
Dec  9 07:33:51.893: INFO: Observed &ReplicaSet event: ADDED
Dec  9 07:33:51.893: INFO: Observed &ReplicaSet event: MODIFIED
Dec  9 07:33:51.893: INFO: Observed &ReplicaSet event: MODIFIED
Dec  9 07:33:51.894: INFO: Observed &ReplicaSet event: MODIFIED
Dec  9 07:33:51.894: INFO: Observed replicaset test-rs in namespace replicaset-9226 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Dec  9 07:33:51.894: INFO: Observed &ReplicaSet event: MODIFIED
Dec  9 07:33:51.894: INFO: Found replicaset test-rs in namespace replicaset-9226 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
Dec  9 07:33:51.894: INFO: Replicaset test-rs has a patched status
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Dec  9 07:33:51.894: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-9226" for this suite. 12/09/22 07:33:51.899
------------------------------
â€¢ [SLOW TEST] [5.140 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:33:46.776
    Dec  9 07:33:46.776: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename replicaset 12/09/22 07:33:46.777
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:33:46.798
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:33:46.802
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] should validate Replicaset Status endpoints [Conformance]
      test/e2e/apps/replica_set.go:176
    STEP: Create a Replicaset 12/09/22 07:33:46.808
    STEP: Verify that the required pods have come up. 12/09/22 07:33:46.813
    Dec  9 07:33:46.817: INFO: Pod name sample-pod: Found 0 pods out of 1
    Dec  9 07:33:51.839: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 12/09/22 07:33:51.839
    STEP: Getting /status 12/09/22 07:33:51.84
    Dec  9 07:33:51.852: INFO: Replicaset test-rs has Conditions: []
    STEP: updating the Replicaset Status 12/09/22 07:33:51.852
    Dec  9 07:33:51.878: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the ReplicaSet status to be updated 12/09/22 07:33:51.878
    Dec  9 07:33:51.882: INFO: Observed &ReplicaSet event: ADDED
    Dec  9 07:33:51.882: INFO: Observed &ReplicaSet event: MODIFIED
    Dec  9 07:33:51.882: INFO: Observed &ReplicaSet event: MODIFIED
    Dec  9 07:33:51.883: INFO: Observed &ReplicaSet event: MODIFIED
    Dec  9 07:33:51.883: INFO: Found replicaset test-rs in namespace replicaset-9226 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Dec  9 07:33:51.883: INFO: Replicaset test-rs has an updated status
    STEP: patching the Replicaset Status 12/09/22 07:33:51.883
    Dec  9 07:33:51.883: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Dec  9 07:33:51.890: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Replicaset status to be patched 12/09/22 07:33:51.89
    Dec  9 07:33:51.893: INFO: Observed &ReplicaSet event: ADDED
    Dec  9 07:33:51.893: INFO: Observed &ReplicaSet event: MODIFIED
    Dec  9 07:33:51.893: INFO: Observed &ReplicaSet event: MODIFIED
    Dec  9 07:33:51.894: INFO: Observed &ReplicaSet event: MODIFIED
    Dec  9 07:33:51.894: INFO: Observed replicaset test-rs in namespace replicaset-9226 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Dec  9 07:33:51.894: INFO: Observed &ReplicaSet event: MODIFIED
    Dec  9 07:33:51.894: INFO: Found replicaset test-rs in namespace replicaset-9226 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
    Dec  9 07:33:51.894: INFO: Replicaset test-rs has a patched status
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:33:51.894: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-9226" for this suite. 12/09/22 07:33:51.899
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:33:51.918
Dec  9 07:33:51.918: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename deployment 12/09/22 07:33:51.919
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:33:51.942
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:33:51.951
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122
Dec  9 07:33:51.972: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Dec  9 07:33:56.976: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 12/09/22 07:33:56.976
Dec  9 07:33:56.976: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up 12/09/22 07:33:56.99
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Dec  9 07:33:57.010: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-2577  f561884f-74b4-46b5-8421-c231a98ccedb 13577 1 2022-12-09 07:33:56 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] [{e2e.test Update apps/v1 2022-12-09 07:33:56 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00510ded8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

Dec  9 07:33:57.015: INFO: New ReplicaSet of Deployment "test-cleanup-deployment" is nil.
Dec  9 07:33:57.015: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
Dec  9 07:33:57.016: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-2577  51075f86-6a2f-41cd-84c2-a681bf83e5ec 13580 1 2022-12-09 07:33:51 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment f561884f-74b4-46b5-8421-c231a98ccedb 0xc000a54317 0xc000a54318}] [] [{e2e.test Update apps/v1 2022-12-09 07:33:51 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-09 07:33:53 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2022-12-09 07:33:57 +0000 UTC FieldsV1 {"f:metadata":{"f:ownerReferences":{".":{},"k:{\"uid\":\"f561884f-74b4-46b5-8421-c231a98ccedb\"}":{}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc000a543f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Dec  9 07:33:57.022: INFO: Pod "test-cleanup-controller-9878w" is available:
&Pod{ObjectMeta:{test-cleanup-controller-9878w test-cleanup-controller- deployment-2577  81810f5c-c2ab-4595-b726-bbe8c95e2ce9 13569 0 2022-12-09 07:33:51 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[cni.projectcalico.org/containerID:d064bf6806ce105d28637e711096751689e72559512876e42139e5813d3c4fe7 cni.projectcalico.org/podIP:10.2.136.175/32 cni.projectcalico.org/podIPs:10.2.136.175/32] [{apps/v1 ReplicaSet test-cleanup-controller 51075f86-6a2f-41cd-84c2-a681bf83e5ec 0xc003156c67 0xc003156c68}] [] [{kube-controller-manager Update v1 2022-12-09 07:33:51 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"51075f86-6a2f-41cd-84c2-a681bf83e5ec\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-12-09 07:33:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-12-09 07:33:53 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.2.136.175\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7mm5d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7mm5d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-10-179,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 07:33:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 07:33:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 07:33:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 07:33:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.10.179,PodIP:10.2.136.175,StartTime:2022-12-09 07:33:52 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-12-09 07:33:52 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://4d7b17da02644e65f740c5512275d0cbe2b7b628f98af8fea7d65a49005b4cc0,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.2.136.175,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Dec  9 07:33:57.023: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-2577" for this suite. 12/09/22 07:33:57.049
------------------------------
â€¢ [SLOW TEST] [5.166 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:33:51.918
    Dec  9 07:33:51.918: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename deployment 12/09/22 07:33:51.919
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:33:51.942
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:33:51.951
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should delete old replica sets [Conformance]
      test/e2e/apps/deployment.go:122
    Dec  9 07:33:51.972: INFO: Pod name cleanup-pod: Found 0 pods out of 1
    Dec  9 07:33:56.976: INFO: Pod name cleanup-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 12/09/22 07:33:56.976
    Dec  9 07:33:56.976: INFO: Creating deployment test-cleanup-deployment
    STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up 12/09/22 07:33:56.99
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Dec  9 07:33:57.010: INFO: Deployment "test-cleanup-deployment":
    &Deployment{ObjectMeta:{test-cleanup-deployment  deployment-2577  f561884f-74b4-46b5-8421-c231a98ccedb 13577 1 2022-12-09 07:33:56 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] [{e2e.test Update apps/v1 2022-12-09 07:33:56 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00510ded8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

    Dec  9 07:33:57.015: INFO: New ReplicaSet of Deployment "test-cleanup-deployment" is nil.
    Dec  9 07:33:57.015: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
    Dec  9 07:33:57.016: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-2577  51075f86-6a2f-41cd-84c2-a681bf83e5ec 13580 1 2022-12-09 07:33:51 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment f561884f-74b4-46b5-8421-c231a98ccedb 0xc000a54317 0xc000a54318}] [] [{e2e.test Update apps/v1 2022-12-09 07:33:51 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-09 07:33:53 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2022-12-09 07:33:57 +0000 UTC FieldsV1 {"f:metadata":{"f:ownerReferences":{".":{},"k:{\"uid\":\"f561884f-74b4-46b5-8421-c231a98ccedb\"}":{}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc000a543f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Dec  9 07:33:57.022: INFO: Pod "test-cleanup-controller-9878w" is available:
    &Pod{ObjectMeta:{test-cleanup-controller-9878w test-cleanup-controller- deployment-2577  81810f5c-c2ab-4595-b726-bbe8c95e2ce9 13569 0 2022-12-09 07:33:51 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[cni.projectcalico.org/containerID:d064bf6806ce105d28637e711096751689e72559512876e42139e5813d3c4fe7 cni.projectcalico.org/podIP:10.2.136.175/32 cni.projectcalico.org/podIPs:10.2.136.175/32] [{apps/v1 ReplicaSet test-cleanup-controller 51075f86-6a2f-41cd-84c2-a681bf83e5ec 0xc003156c67 0xc003156c68}] [] [{kube-controller-manager Update v1 2022-12-09 07:33:51 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"51075f86-6a2f-41cd-84c2-a681bf83e5ec\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-12-09 07:33:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-12-09 07:33:53 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.2.136.175\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7mm5d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7mm5d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-10-179,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 07:33:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 07:33:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 07:33:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 07:33:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.10.179,PodIP:10.2.136.175,StartTime:2022-12-09 07:33:52 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-12-09 07:33:52 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://4d7b17da02644e65f740c5512275d0cbe2b7b628f98af8fea7d65a49005b4cc0,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.2.136.175,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:33:57.023: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-2577" for this suite. 12/09/22 07:33:57.049
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:703
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:33:57.089
Dec  9 07:33:57.089: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename job 12/09/22 07:33:57.091
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:33:57.127
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:33:57.131
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:703
STEP: Creating a suspended job 12/09/22 07:33:57.138
STEP: Patching the Job 12/09/22 07:33:57.143
STEP: Watching for Job to be patched 12/09/22 07:33:57.153
Dec  9 07:33:57.160: INFO: Event ADDED observed for Job e2e-ksdzt in namespace job-5905 with labels: map[e2e-job-label:e2e-ksdzt] and annotations: map[batch.kubernetes.io/job-tracking:]
Dec  9 07:33:57.160: INFO: Event MODIFIED found for Job e2e-ksdzt in namespace job-5905 with labels: map[e2e-job-label:e2e-ksdzt e2e-ksdzt:patched] and annotations: map[batch.kubernetes.io/job-tracking:]
STEP: Updating the job 12/09/22 07:33:57.16
STEP: Watching for Job to be updated 12/09/22 07:33:57.181
Dec  9 07:33:57.186: INFO: Event MODIFIED found for Job e2e-ksdzt in namespace job-5905 with labels: map[e2e-job-label:e2e-ksdzt e2e-ksdzt:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Dec  9 07:33:57.187: INFO: Found Job annotations: map[string]string{"batch.kubernetes.io/job-tracking":"", "updated":"true"}
STEP: Listing all Jobs with LabelSelector 12/09/22 07:33:57.187
Dec  9 07:33:57.197: INFO: Job: e2e-ksdzt as labels: map[e2e-job-label:e2e-ksdzt e2e-ksdzt:patched]
STEP: Waiting for job to complete 12/09/22 07:33:57.198
STEP: Delete a job collection with a labelselector 12/09/22 07:34:07.202
STEP: Watching for Job to be deleted 12/09/22 07:34:07.207
Dec  9 07:34:07.210: INFO: Event MODIFIED observed for Job e2e-ksdzt in namespace job-5905 with labels: map[e2e-job-label:e2e-ksdzt e2e-ksdzt:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Dec  9 07:34:07.210: INFO: Event MODIFIED observed for Job e2e-ksdzt in namespace job-5905 with labels: map[e2e-job-label:e2e-ksdzt e2e-ksdzt:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Dec  9 07:34:07.210: INFO: Event MODIFIED observed for Job e2e-ksdzt in namespace job-5905 with labels: map[e2e-job-label:e2e-ksdzt e2e-ksdzt:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Dec  9 07:34:07.210: INFO: Event MODIFIED observed for Job e2e-ksdzt in namespace job-5905 with labels: map[e2e-job-label:e2e-ksdzt e2e-ksdzt:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Dec  9 07:34:07.210: INFO: Event MODIFIED observed for Job e2e-ksdzt in namespace job-5905 with labels: map[e2e-job-label:e2e-ksdzt e2e-ksdzt:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Dec  9 07:34:07.210: INFO: Event MODIFIED observed for Job e2e-ksdzt in namespace job-5905 with labels: map[e2e-job-label:e2e-ksdzt e2e-ksdzt:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Dec  9 07:34:07.210: INFO: Event MODIFIED observed for Job e2e-ksdzt in namespace job-5905 with labels: map[e2e-job-label:e2e-ksdzt e2e-ksdzt:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Dec  9 07:34:07.211: INFO: Event MODIFIED observed for Job e2e-ksdzt in namespace job-5905 with labels: map[e2e-job-label:e2e-ksdzt e2e-ksdzt:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Dec  9 07:34:07.211: INFO: Event MODIFIED observed for Job e2e-ksdzt in namespace job-5905 with labels: map[e2e-job-label:e2e-ksdzt e2e-ksdzt:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Dec  9 07:34:07.211: INFO: Event DELETED found for Job e2e-ksdzt in namespace job-5905 with labels: map[e2e-job-label:e2e-ksdzt e2e-ksdzt:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
STEP: Relist jobs to confirm deletion 12/09/22 07:34:07.211
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Dec  9 07:34:07.222: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-5905" for this suite. 12/09/22 07:34:07.233
------------------------------
â€¢ [SLOW TEST] [10.151 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:703

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:33:57.089
    Dec  9 07:33:57.089: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename job 12/09/22 07:33:57.091
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:33:57.127
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:33:57.131
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should manage the lifecycle of a job [Conformance]
      test/e2e/apps/job.go:703
    STEP: Creating a suspended job 12/09/22 07:33:57.138
    STEP: Patching the Job 12/09/22 07:33:57.143
    STEP: Watching for Job to be patched 12/09/22 07:33:57.153
    Dec  9 07:33:57.160: INFO: Event ADDED observed for Job e2e-ksdzt in namespace job-5905 with labels: map[e2e-job-label:e2e-ksdzt] and annotations: map[batch.kubernetes.io/job-tracking:]
    Dec  9 07:33:57.160: INFO: Event MODIFIED found for Job e2e-ksdzt in namespace job-5905 with labels: map[e2e-job-label:e2e-ksdzt e2e-ksdzt:patched] and annotations: map[batch.kubernetes.io/job-tracking:]
    STEP: Updating the job 12/09/22 07:33:57.16
    STEP: Watching for Job to be updated 12/09/22 07:33:57.181
    Dec  9 07:33:57.186: INFO: Event MODIFIED found for Job e2e-ksdzt in namespace job-5905 with labels: map[e2e-job-label:e2e-ksdzt e2e-ksdzt:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Dec  9 07:33:57.187: INFO: Found Job annotations: map[string]string{"batch.kubernetes.io/job-tracking":"", "updated":"true"}
    STEP: Listing all Jobs with LabelSelector 12/09/22 07:33:57.187
    Dec  9 07:33:57.197: INFO: Job: e2e-ksdzt as labels: map[e2e-job-label:e2e-ksdzt e2e-ksdzt:patched]
    STEP: Waiting for job to complete 12/09/22 07:33:57.198
    STEP: Delete a job collection with a labelselector 12/09/22 07:34:07.202
    STEP: Watching for Job to be deleted 12/09/22 07:34:07.207
    Dec  9 07:34:07.210: INFO: Event MODIFIED observed for Job e2e-ksdzt in namespace job-5905 with labels: map[e2e-job-label:e2e-ksdzt e2e-ksdzt:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Dec  9 07:34:07.210: INFO: Event MODIFIED observed for Job e2e-ksdzt in namespace job-5905 with labels: map[e2e-job-label:e2e-ksdzt e2e-ksdzt:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Dec  9 07:34:07.210: INFO: Event MODIFIED observed for Job e2e-ksdzt in namespace job-5905 with labels: map[e2e-job-label:e2e-ksdzt e2e-ksdzt:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Dec  9 07:34:07.210: INFO: Event MODIFIED observed for Job e2e-ksdzt in namespace job-5905 with labels: map[e2e-job-label:e2e-ksdzt e2e-ksdzt:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Dec  9 07:34:07.210: INFO: Event MODIFIED observed for Job e2e-ksdzt in namespace job-5905 with labels: map[e2e-job-label:e2e-ksdzt e2e-ksdzt:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Dec  9 07:34:07.210: INFO: Event MODIFIED observed for Job e2e-ksdzt in namespace job-5905 with labels: map[e2e-job-label:e2e-ksdzt e2e-ksdzt:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Dec  9 07:34:07.210: INFO: Event MODIFIED observed for Job e2e-ksdzt in namespace job-5905 with labels: map[e2e-job-label:e2e-ksdzt e2e-ksdzt:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Dec  9 07:34:07.211: INFO: Event MODIFIED observed for Job e2e-ksdzt in namespace job-5905 with labels: map[e2e-job-label:e2e-ksdzt e2e-ksdzt:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Dec  9 07:34:07.211: INFO: Event MODIFIED observed for Job e2e-ksdzt in namespace job-5905 with labels: map[e2e-job-label:e2e-ksdzt e2e-ksdzt:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Dec  9 07:34:07.211: INFO: Event DELETED found for Job e2e-ksdzt in namespace job-5905 with labels: map[e2e-job-label:e2e-ksdzt e2e-ksdzt:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    STEP: Relist jobs to confirm deletion 12/09/22 07:34:07.211
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:34:07.222: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-5905" for this suite. 12/09/22 07:34:07.233
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] Kubelet when scheduling an agnhost Pod with hostAliases
  should write entries to /etc/hosts [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:148
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:34:07.242
Dec  9 07:34:07.242: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename kubelet-test 12/09/22 07:34:07.243
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:34:07.286
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:34:07.292
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should write entries to /etc/hosts [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:148
STEP: Waiting for pod completion 12/09/22 07:34:07.309
Dec  9 07:34:07.309: INFO: Waiting up to 3m0s for pod "agnhost-host-aliases3841f97c-4042-48f7-9b5c-1da5e1b2cd5a" in namespace "kubelet-test-6694" to be "completed"
Dec  9 07:34:07.315: INFO: Pod "agnhost-host-aliases3841f97c-4042-48f7-9b5c-1da5e1b2cd5a": Phase="Pending", Reason="", readiness=false. Elapsed: 5.645609ms
Dec  9 07:34:09.319: INFO: Pod "agnhost-host-aliases3841f97c-4042-48f7-9b5c-1da5e1b2cd5a": Phase="Running", Reason="", readiness=false. Elapsed: 2.009892857s
Dec  9 07:34:11.320: INFO: Pod "agnhost-host-aliases3841f97c-4042-48f7-9b5c-1da5e1b2cd5a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010163208s
Dec  9 07:34:11.320: INFO: Pod "agnhost-host-aliases3841f97c-4042-48f7-9b5c-1da5e1b2cd5a" satisfied condition "completed"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Dec  9 07:34:11.327: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-6694" for this suite. 12/09/22 07:34:11.332
------------------------------
â€¢ [4.096 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling an agnhost Pod with hostAliases
  test/e2e/common/node/kubelet.go:140
    should write entries to /etc/hosts [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:148

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:34:07.242
    Dec  9 07:34:07.242: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename kubelet-test 12/09/22 07:34:07.243
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:34:07.286
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:34:07.292
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should write entries to /etc/hosts [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:148
    STEP: Waiting for pod completion 12/09/22 07:34:07.309
    Dec  9 07:34:07.309: INFO: Waiting up to 3m0s for pod "agnhost-host-aliases3841f97c-4042-48f7-9b5c-1da5e1b2cd5a" in namespace "kubelet-test-6694" to be "completed"
    Dec  9 07:34:07.315: INFO: Pod "agnhost-host-aliases3841f97c-4042-48f7-9b5c-1da5e1b2cd5a": Phase="Pending", Reason="", readiness=false. Elapsed: 5.645609ms
    Dec  9 07:34:09.319: INFO: Pod "agnhost-host-aliases3841f97c-4042-48f7-9b5c-1da5e1b2cd5a": Phase="Running", Reason="", readiness=false. Elapsed: 2.009892857s
    Dec  9 07:34:11.320: INFO: Pod "agnhost-host-aliases3841f97c-4042-48f7-9b5c-1da5e1b2cd5a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010163208s
    Dec  9 07:34:11.320: INFO: Pod "agnhost-host-aliases3841f97c-4042-48f7-9b5c-1da5e1b2cd5a" satisfied condition "completed"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:34:11.327: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-6694" for this suite. 12/09/22 07:34:11.332
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:151
[BeforeEach] [sig-node] Container Lifecycle Hook
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:34:11.342
Dec  9 07:34:11.343: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename container-lifecycle-hook 12/09/22 07:34:11.345
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:34:11.364
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:34:11.37
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:77
STEP: create the container to handle the HTTPGet hook request. 12/09/22 07:34:11.378
Dec  9 07:34:11.387: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-5744" to be "running and ready"
Dec  9 07:34:11.393: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 5.045447ms
Dec  9 07:34:11.393: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Dec  9 07:34:13.397: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.009426666s
Dec  9 07:34:13.397: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Dec  9 07:34:13.397: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:151
STEP: create the pod with lifecycle hook 12/09/22 07:34:13.4
Dec  9 07:34:13.405: INFO: Waiting up to 5m0s for pod "pod-with-prestop-exec-hook" in namespace "container-lifecycle-hook-5744" to be "running and ready"
Dec  9 07:34:13.409: INFO: Pod "pod-with-prestop-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 3.616583ms
Dec  9 07:34:13.409: INFO: The phase of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Dec  9 07:34:15.413: INFO: Pod "pod-with-prestop-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.007538808s
Dec  9 07:34:15.413: INFO: The phase of Pod pod-with-prestop-exec-hook is Running (Ready = true)
Dec  9 07:34:15.413: INFO: Pod "pod-with-prestop-exec-hook" satisfied condition "running and ready"
STEP: delete the pod with lifecycle hook 12/09/22 07:34:15.415
Dec  9 07:34:15.420: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec  9 07:34:15.424: INFO: Pod pod-with-prestop-exec-hook still exists
Dec  9 07:34:17.424: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec  9 07:34:17.428: INFO: Pod pod-with-prestop-exec-hook still exists
Dec  9 07:34:19.424: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec  9 07:34:19.428: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook 12/09/22 07:34:19.428
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/node/init/init.go:32
Dec  9 07:34:19.449: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  tear down framework | framework.go:193
STEP: Destroying namespace "container-lifecycle-hook-5744" for this suite. 12/09/22 07:34:19.452
------------------------------
â€¢ [SLOW TEST] [8.116 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:151

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:34:11.342
    Dec  9 07:34:11.343: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename container-lifecycle-hook 12/09/22 07:34:11.345
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:34:11.364
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:34:11.37
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:77
    STEP: create the container to handle the HTTPGet hook request. 12/09/22 07:34:11.378
    Dec  9 07:34:11.387: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-5744" to be "running and ready"
    Dec  9 07:34:11.393: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 5.045447ms
    Dec  9 07:34:11.393: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Dec  9 07:34:13.397: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.009426666s
    Dec  9 07:34:13.397: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Dec  9 07:34:13.397: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute prestop exec hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:151
    STEP: create the pod with lifecycle hook 12/09/22 07:34:13.4
    Dec  9 07:34:13.405: INFO: Waiting up to 5m0s for pod "pod-with-prestop-exec-hook" in namespace "container-lifecycle-hook-5744" to be "running and ready"
    Dec  9 07:34:13.409: INFO: Pod "pod-with-prestop-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 3.616583ms
    Dec  9 07:34:13.409: INFO: The phase of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
    Dec  9 07:34:15.413: INFO: Pod "pod-with-prestop-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.007538808s
    Dec  9 07:34:15.413: INFO: The phase of Pod pod-with-prestop-exec-hook is Running (Ready = true)
    Dec  9 07:34:15.413: INFO: Pod "pod-with-prestop-exec-hook" satisfied condition "running and ready"
    STEP: delete the pod with lifecycle hook 12/09/22 07:34:15.415
    Dec  9 07:34:15.420: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Dec  9 07:34:15.424: INFO: Pod pod-with-prestop-exec-hook still exists
    Dec  9 07:34:17.424: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Dec  9 07:34:17.428: INFO: Pod pod-with-prestop-exec-hook still exists
    Dec  9 07:34:19.424: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Dec  9 07:34:19.428: INFO: Pod pod-with-prestop-exec-hook no longer exists
    STEP: check prestop hook 12/09/22 07:34:19.428
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:34:19.449: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-lifecycle-hook-5744" for this suite. 12/09/22 07:34:19.452
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Lease
  lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72
[BeforeEach] [sig-node] Lease
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:34:19.459
Dec  9 07:34:19.459: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename lease-test 12/09/22 07:34:19.46
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:34:19.473
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:34:19.48
[BeforeEach] [sig-node] Lease
  test/e2e/framework/metrics/init/init.go:31
[It] lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72
[AfterEach] [sig-node] Lease
  test/e2e/framework/node/init/init.go:32
Dec  9 07:34:19.546: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Lease
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Lease
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Lease
  tear down framework | framework.go:193
STEP: Destroying namespace "lease-test-853" for this suite. 12/09/22 07:34:19.55
------------------------------
â€¢ [0.096 seconds]
[sig-node] Lease
test/e2e/common/node/framework.go:23
  lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Lease
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:34:19.459
    Dec  9 07:34:19.459: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename lease-test 12/09/22 07:34:19.46
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:34:19.473
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:34:19.48
    [BeforeEach] [sig-node] Lease
      test/e2e/framework/metrics/init/init.go:31
    [It] lease API should be available [Conformance]
      test/e2e/common/node/lease.go:72
    [AfterEach] [sig-node] Lease
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:34:19.546: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Lease
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Lease
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Lease
      tear down framework | framework.go:193
    STEP: Destroying namespace "lease-test-853" for this suite. 12/09/22 07:34:19.55
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:102
[BeforeEach] [sig-network] EndpointSlice
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:34:19.557
Dec  9 07:34:19.557: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename endpointslice 12/09/22 07:34:19.559
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:34:19.574
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:34:19.578
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:52
[It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:102
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/node/init/init.go:32
Dec  9 07:34:21.640: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSlice
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSlice
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslice-8688" for this suite. 12/09/22 07:34:21.644
------------------------------
â€¢ [2.095 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:102

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:34:19.557
    Dec  9 07:34:19.557: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename endpointslice 12/09/22 07:34:19.559
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:34:19.574
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:34:19.578
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:52
    [It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
      test/e2e/network/endpointslice.go:102
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:34:21.640: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslice-8688" for this suite. 12/09/22 07:34:21.644
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:66
[BeforeEach] [sig-network] EndpointSlice
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:34:21.661
Dec  9 07:34:21.661: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename endpointslice 12/09/22 07:34:21.662
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:34:21.691
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:34:21.698
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:52
[It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:66
Dec  9 07:34:21.715: INFO: Endpoints addresses: [10.0.7.84] , ports: [6443]
Dec  9 07:34:21.715: INFO: EndpointSlices addresses: [10.0.7.84] , ports: [6443]
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/node/init/init.go:32
Dec  9 07:34:21.716: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSlice
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSlice
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslice-8975" for this suite. 12/09/22 07:34:21.723
------------------------------
â€¢ [0.072 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:66

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:34:21.661
    Dec  9 07:34:21.661: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename endpointslice 12/09/22 07:34:21.662
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:34:21.691
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:34:21.698
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:52
    [It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
      test/e2e/network/endpointslice.go:66
    Dec  9 07:34:21.715: INFO: Endpoints addresses: [10.0.7.84] , ports: [6443]
    Dec  9 07:34:21.715: INFO: EndpointSlices addresses: [10.0.7.84] , ports: [6443]
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:34:21.716: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslice-8975" for this suite. 12/09/22 07:34:21.723
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:448
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:34:21.74
Dec  9 07:34:21.741: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename resourcequota 12/09/22 07:34:21.742
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:34:21.767
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:34:21.772
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:448
STEP: Counting existing ResourceQuota 12/09/22 07:34:21.779
STEP: Creating a ResourceQuota 12/09/22 07:34:26.783
STEP: Ensuring resource quota status is calculated 12/09/22 07:34:26.79
STEP: Creating a ReplicaSet 12/09/22 07:34:28.795
STEP: Ensuring resource quota status captures replicaset creation 12/09/22 07:34:28.808
STEP: Deleting a ReplicaSet 12/09/22 07:34:30.813
STEP: Ensuring resource quota status released usage 12/09/22 07:34:30.82
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Dec  9 07:34:32.825: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-6666" for this suite. 12/09/22 07:34:32.83
------------------------------
â€¢ [SLOW TEST] [11.096 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:448

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:34:21.74
    Dec  9 07:34:21.741: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename resourcequota 12/09/22 07:34:21.742
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:34:21.767
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:34:21.772
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
      test/e2e/apimachinery/resource_quota.go:448
    STEP: Counting existing ResourceQuota 12/09/22 07:34:21.779
    STEP: Creating a ResourceQuota 12/09/22 07:34:26.783
    STEP: Ensuring resource quota status is calculated 12/09/22 07:34:26.79
    STEP: Creating a ReplicaSet 12/09/22 07:34:28.795
    STEP: Ensuring resource quota status captures replicaset creation 12/09/22 07:34:28.808
    STEP: Deleting a ReplicaSet 12/09/22 07:34:30.813
    STEP: Ensuring resource quota status released usage 12/09/22 07:34:30.82
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:34:32.825: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-6666" for this suite. 12/09/22 07:34:32.83
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] CSIInlineVolumes
  should support CSIVolumeSource in Pod API [Conformance]
  test/e2e/storage/csi_inline.go:131
[BeforeEach] [sig-storage] CSIInlineVolumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:34:32.847
Dec  9 07:34:32.847: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename csiinlinevolumes 12/09/22 07:34:32.85
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:34:32.878
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:34:32.883
[BeforeEach] [sig-storage] CSIInlineVolumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support CSIVolumeSource in Pod API [Conformance]
  test/e2e/storage/csi_inline.go:131
STEP: creating 12/09/22 07:34:32.891
STEP: getting 12/09/22 07:34:32.914
STEP: listing in namespace 12/09/22 07:34:32.922
STEP: patching 12/09/22 07:34:32.932
STEP: deleting 12/09/22 07:34:32.943
[AfterEach] [sig-storage] CSIInlineVolumes
  test/e2e/framework/node/init/init.go:32
Dec  9 07:34:32.958: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  tear down framework | framework.go:193
STEP: Destroying namespace "csiinlinevolumes-9996" for this suite. 12/09/22 07:34:32.961
------------------------------
â€¢ [0.121 seconds]
[sig-storage] CSIInlineVolumes
test/e2e/storage/utils/framework.go:23
  should support CSIVolumeSource in Pod API [Conformance]
  test/e2e/storage/csi_inline.go:131

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] CSIInlineVolumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:34:32.847
    Dec  9 07:34:32.847: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename csiinlinevolumes 12/09/22 07:34:32.85
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:34:32.878
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:34:32.883
    [BeforeEach] [sig-storage] CSIInlineVolumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support CSIVolumeSource in Pod API [Conformance]
      test/e2e/storage/csi_inline.go:131
    STEP: creating 12/09/22 07:34:32.891
    STEP: getting 12/09/22 07:34:32.914
    STEP: listing in namespace 12/09/22 07:34:32.922
    STEP: patching 12/09/22 07:34:32.932
    STEP: deleting 12/09/22 07:34:32.943
    [AfterEach] [sig-storage] CSIInlineVolumes
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:34:32.958: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "csiinlinevolumes-9996" for this suite. 12/09/22 07:34:32.961
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:90
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:34:32.973
Dec  9 07:34:32.974: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename downward-api 12/09/22 07:34:32.975
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:34:33.007
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:34:33.015
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:90
STEP: Creating a pod to test downward api env vars 12/09/22 07:34:33.024
Dec  9 07:34:33.033: INFO: Waiting up to 5m0s for pod "downward-api-1ee614aa-91e7-433f-b5ab-3634c54caae2" in namespace "downward-api-925" to be "Succeeded or Failed"
Dec  9 07:34:33.037: INFO: Pod "downward-api-1ee614aa-91e7-433f-b5ab-3634c54caae2": Phase="Pending", Reason="", readiness=false. Elapsed: 3.999443ms
Dec  9 07:34:35.041: INFO: Pod "downward-api-1ee614aa-91e7-433f-b5ab-3634c54caae2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007882699s
Dec  9 07:34:37.041: INFO: Pod "downward-api-1ee614aa-91e7-433f-b5ab-3634c54caae2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007795125s
STEP: Saw pod success 12/09/22 07:34:37.042
Dec  9 07:34:37.043: INFO: Pod "downward-api-1ee614aa-91e7-433f-b5ab-3634c54caae2" satisfied condition "Succeeded or Failed"
Dec  9 07:34:37.046: INFO: Trying to get logs from node ip-10-0-10-179 pod downward-api-1ee614aa-91e7-433f-b5ab-3634c54caae2 container dapi-container: <nil>
STEP: delete the pod 12/09/22 07:34:37.054
Dec  9 07:34:37.064: INFO: Waiting for pod downward-api-1ee614aa-91e7-433f-b5ab-3634c54caae2 to disappear
Dec  9 07:34:37.066: INFO: Pod downward-api-1ee614aa-91e7-433f-b5ab-3634c54caae2 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Dec  9 07:34:37.066: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-925" for this suite. 12/09/22 07:34:37.071
------------------------------
â€¢ [4.102 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:90

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:34:32.973
    Dec  9 07:34:32.974: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename downward-api 12/09/22 07:34:32.975
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:34:33.007
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:34:33.015
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide host IP as an env var [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:90
    STEP: Creating a pod to test downward api env vars 12/09/22 07:34:33.024
    Dec  9 07:34:33.033: INFO: Waiting up to 5m0s for pod "downward-api-1ee614aa-91e7-433f-b5ab-3634c54caae2" in namespace "downward-api-925" to be "Succeeded or Failed"
    Dec  9 07:34:33.037: INFO: Pod "downward-api-1ee614aa-91e7-433f-b5ab-3634c54caae2": Phase="Pending", Reason="", readiness=false. Elapsed: 3.999443ms
    Dec  9 07:34:35.041: INFO: Pod "downward-api-1ee614aa-91e7-433f-b5ab-3634c54caae2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007882699s
    Dec  9 07:34:37.041: INFO: Pod "downward-api-1ee614aa-91e7-433f-b5ab-3634c54caae2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007795125s
    STEP: Saw pod success 12/09/22 07:34:37.042
    Dec  9 07:34:37.043: INFO: Pod "downward-api-1ee614aa-91e7-433f-b5ab-3634c54caae2" satisfied condition "Succeeded or Failed"
    Dec  9 07:34:37.046: INFO: Trying to get logs from node ip-10-0-10-179 pod downward-api-1ee614aa-91e7-433f-b5ab-3634c54caae2 container dapi-container: <nil>
    STEP: delete the pod 12/09/22 07:34:37.054
    Dec  9 07:34:37.064: INFO: Waiting for pod downward-api-1ee614aa-91e7-433f-b5ab-3634c54caae2 to disappear
    Dec  9 07:34:37.066: INFO: Pod downward-api-1ee614aa-91e7-433f-b5ab-3634c54caae2 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:34:37.066: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-925" for this suite. 12/09/22 07:34:37.071
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should apply a finalizer to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:394
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:34:37.076
Dec  9 07:34:37.077: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename namespaces 12/09/22 07:34:37.079
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:34:37.095
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:34:37.098
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should apply a finalizer to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:394
STEP: Creating namespace "e2e-ns-g9hn2" 12/09/22 07:34:37.102
Dec  9 07:34:37.120: INFO: Namespace "e2e-ns-g9hn2-4881" has []v1.FinalizerName{"kubernetes"}
STEP: Adding e2e finalizer to namespace "e2e-ns-g9hn2-4881" 12/09/22 07:34:37.12
Dec  9 07:34:37.128: INFO: Namespace "e2e-ns-g9hn2-4881" has []v1.FinalizerName{"kubernetes", "e2e.example.com/fakeFinalizer"}
STEP: Removing e2e finalizer from namespace "e2e-ns-g9hn2-4881" 12/09/22 07:34:37.129
Dec  9 07:34:37.136: INFO: Namespace "e2e-ns-g9hn2-4881" has []v1.FinalizerName{"kubernetes"}
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Dec  9 07:34:37.137: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-7545" for this suite. 12/09/22 07:34:37.142
STEP: Destroying namespace "e2e-ns-g9hn2-4881" for this suite. 12/09/22 07:34:37.147
------------------------------
â€¢ [0.076 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should apply a finalizer to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:394

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:34:37.076
    Dec  9 07:34:37.077: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename namespaces 12/09/22 07:34:37.079
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:34:37.095
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:34:37.098
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply a finalizer to a Namespace [Conformance]
      test/e2e/apimachinery/namespace.go:394
    STEP: Creating namespace "e2e-ns-g9hn2" 12/09/22 07:34:37.102
    Dec  9 07:34:37.120: INFO: Namespace "e2e-ns-g9hn2-4881" has []v1.FinalizerName{"kubernetes"}
    STEP: Adding e2e finalizer to namespace "e2e-ns-g9hn2-4881" 12/09/22 07:34:37.12
    Dec  9 07:34:37.128: INFO: Namespace "e2e-ns-g9hn2-4881" has []v1.FinalizerName{"kubernetes", "e2e.example.com/fakeFinalizer"}
    STEP: Removing e2e finalizer from namespace "e2e-ns-g9hn2-4881" 12/09/22 07:34:37.129
    Dec  9 07:34:37.136: INFO: Namespace "e2e-ns-g9hn2-4881" has []v1.FinalizerName{"kubernetes"}
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:34:37.137: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-7545" for this suite. 12/09/22 07:34:37.142
    STEP: Destroying namespace "e2e-ns-g9hn2-4881" for this suite. 12/09/22 07:34:37.147
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:37
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:34:37.154
Dec  9 07:34:37.155: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename sysctl 12/09/22 07:34:37.155
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:34:37.171
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:34:37.175
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:67
[It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123
STEP: Creating a pod with one valid and two invalid sysctls 12/09/22 07:34:37.179
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Dec  9 07:34:37.183: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "sysctl-298" for this suite. 12/09/22 07:34:37.186
------------------------------
â€¢ [0.038 seconds]
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
test/e2e/common/node/framework.go:23
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:37
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:34:37.154
    Dec  9 07:34:37.155: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename sysctl 12/09/22 07:34:37.155
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:34:37.171
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:34:37.175
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:67
    [It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
      test/e2e/common/node/sysctl.go:123
    STEP: Creating a pod with one valid and two invalid sysctls 12/09/22 07:34:37.179
    [AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:34:37.183: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sysctl-298" for this suite. 12/09/22 07:34:37.186
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:386
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:34:37.195
Dec  9 07:34:37.196: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename secrets 12/09/22 07:34:37.196
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:34:37.218
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:34:37.221
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:386
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Dec  9 07:34:37.255: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-2096" for this suite. 12/09/22 07:34:37.258
------------------------------
â€¢ [0.068 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:386

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:34:37.195
    Dec  9 07:34:37.196: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename secrets 12/09/22 07:34:37.196
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:34:37.218
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:34:37.221
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be immutable if `immutable` field is set [Conformance]
      test/e2e/common/storage/secrets_volume.go:386
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:34:37.255: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-2096" for this suite. 12/09/22 07:34:37.258
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:34:37.265
Dec  9 07:34:37.265: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename crd-webhook 12/09/22 07:34:37.266
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:34:37.28
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:34:37.284
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:128
STEP: Setting up server cert 12/09/22 07:34:37.288
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 12/09/22 07:34:37.801
STEP: Deploying the custom resource conversion webhook pod 12/09/22 07:34:37.809
STEP: Wait for the deployment to be ready 12/09/22 07:34:37.821
Dec  9 07:34:37.831: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
Dec  9 07:34:39.844: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.December, 9, 7, 34, 37, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 9, 7, 34, 37, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 9, 7, 34, 37, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 9, 7, 34, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-74ff66dd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 12/09/22 07:34:41.848
STEP: Verifying the service has paired with the endpoint 12/09/22 07:34:41.859
Dec  9 07:34:42.859: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184
Dec  9 07:34:42.874: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Creating a v1 custom resource 12/09/22 07:34:45.551
STEP: Create a v2 custom resource 12/09/22 07:34:45.57
STEP: List CRs in v1 12/09/22 07:34:45.643
STEP: List CRs in v2 12/09/22 07:34:45.65
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Dec  9 07:34:46.191: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:139
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-webhook-3219" for this suite. 12/09/22 07:34:46.329
------------------------------
â€¢ [SLOW TEST] [9.083 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:34:37.265
    Dec  9 07:34:37.265: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename crd-webhook 12/09/22 07:34:37.266
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:34:37.28
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:34:37.284
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:128
    STEP: Setting up server cert 12/09/22 07:34:37.288
    STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 12/09/22 07:34:37.801
    STEP: Deploying the custom resource conversion webhook pod 12/09/22 07:34:37.809
    STEP: Wait for the deployment to be ready 12/09/22 07:34:37.821
    Dec  9 07:34:37.831: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
    Dec  9 07:34:39.844: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.December, 9, 7, 34, 37, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 9, 7, 34, 37, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 9, 7, 34, 37, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 9, 7, 34, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-74ff66dd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 12/09/22 07:34:41.848
    STEP: Verifying the service has paired with the endpoint 12/09/22 07:34:41.859
    Dec  9 07:34:42.859: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
    [It] should be able to convert a non homogeneous list of CRs [Conformance]
      test/e2e/apimachinery/crd_conversion_webhook.go:184
    Dec  9 07:34:42.874: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Creating a v1 custom resource 12/09/22 07:34:45.551
    STEP: Create a v2 custom resource 12/09/22 07:34:45.57
    STEP: List CRs in v1 12/09/22 07:34:45.643
    STEP: List CRs in v2 12/09/22 07:34:45.65
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:34:46.191: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:139
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-webhook-3219" for this suite. 12/09/22 07:34:46.329
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:145
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:34:46.36
Dec  9 07:34:46.361: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename custom-resource-definition 12/09/22 07:34:46.362
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:34:46.388
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:34:46.4
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:145
Dec  9 07:34:46.415: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Dec  9 07:34:46.980: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-2295" for this suite. 12/09/22 07:34:46.986
------------------------------
â€¢ [0.635 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    getting/updating/patching custom resource definition status sub-resource works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:145

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:34:46.36
    Dec  9 07:34:46.361: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename custom-resource-definition 12/09/22 07:34:46.362
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:34:46.388
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:34:46.4
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:145
    Dec  9 07:34:46.415: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:34:46.980: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-2295" for this suite. 12/09/22 07:34:46.986
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] ConfigMap
  should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:169
[BeforeEach] [sig-node] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:34:46.996
Dec  9 07:34:46.996: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename configmap 12/09/22 07:34:46.997
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:34:47.015
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:34:47.02
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:169
STEP: creating a ConfigMap 12/09/22 07:34:47.028
STEP: fetching the ConfigMap 12/09/22 07:34:47.033
STEP: patching the ConfigMap 12/09/22 07:34:47.036
STEP: listing all ConfigMaps in all namespaces with a label selector 12/09/22 07:34:47.043
STEP: deleting the ConfigMap by collection with a label selector 12/09/22 07:34:47.046
STEP: listing all ConfigMaps in test namespace 12/09/22 07:34:47.052
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/node/init/init.go:32
Dec  9 07:34:47.055: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-1532" for this suite. 12/09/22 07:34:47.059
------------------------------
â€¢ [0.068 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:169

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:34:46.996
    Dec  9 07:34:46.996: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename configmap 12/09/22 07:34:46.997
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:34:47.015
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:34:47.02
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should run through a ConfigMap lifecycle [Conformance]
      test/e2e/common/node/configmap.go:169
    STEP: creating a ConfigMap 12/09/22 07:34:47.028
    STEP: fetching the ConfigMap 12/09/22 07:34:47.033
    STEP: patching the ConfigMap 12/09/22 07:34:47.036
    STEP: listing all ConfigMaps in all namespaces with a label selector 12/09/22 07:34:47.043
    STEP: deleting the ConfigMap by collection with a label selector 12/09/22 07:34:47.046
    STEP: listing all ConfigMaps in test namespace 12/09/22 07:34:47.052
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:34:47.055: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-1532" for this suite. 12/09/22 07:34:47.059
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:884
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:34:47.067
Dec  9 07:34:47.067: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename resourcequota 12/09/22 07:34:47.068
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:34:47.083
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:34:47.087
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:884
STEP: Creating a ResourceQuota 12/09/22 07:34:47.091
STEP: Getting a ResourceQuota 12/09/22 07:34:47.096
STEP: Updating a ResourceQuota 12/09/22 07:34:47.099
STEP: Verifying a ResourceQuota was modified 12/09/22 07:34:47.107
STEP: Deleting a ResourceQuota 12/09/22 07:34:47.109
STEP: Verifying the deleted ResourceQuota 12/09/22 07:34:47.116
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Dec  9 07:34:47.119: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-5319" for this suite. 12/09/22 07:34:47.124
------------------------------
â€¢ [0.062 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:884

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:34:47.067
    Dec  9 07:34:47.067: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename resourcequota 12/09/22 07:34:47.068
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:34:47.083
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:34:47.087
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to update and delete ResourceQuota. [Conformance]
      test/e2e/apimachinery/resource_quota.go:884
    STEP: Creating a ResourceQuota 12/09/22 07:34:47.091
    STEP: Getting a ResourceQuota 12/09/22 07:34:47.096
    STEP: Updating a ResourceQuota 12/09/22 07:34:47.099
    STEP: Verifying a ResourceQuota was modified 12/09/22 07:34:47.107
    STEP: Deleting a ResourceQuota 12/09/22 07:34:47.109
    STEP: Verifying the deleted ResourceQuota 12/09/22 07:34:47.116
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:34:47.119: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-5319" for this suite. 12/09/22 07:34:47.124
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should validate Statefulset Status endpoints [Conformance]
  test/e2e/apps/statefulset.go:977
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:34:47.133
Dec  9 07:34:47.133: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename statefulset 12/09/22 07:34:47.134
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:34:47.149
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:34:47.154
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-4742 12/09/22 07:34:47.157
[It] should validate Statefulset Status endpoints [Conformance]
  test/e2e/apps/statefulset.go:977
STEP: Creating statefulset ss in namespace statefulset-4742 12/09/22 07:34:47.167
Dec  9 07:34:47.180: INFO: Found 0 stateful pods, waiting for 1
Dec  9 07:34:57.184: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Patch Statefulset to include a label 12/09/22 07:34:57.189
STEP: Getting /status 12/09/22 07:34:57.196
Dec  9 07:34:57.204: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
STEP: updating the StatefulSet Status 12/09/22 07:34:57.204
Dec  9 07:34:57.215: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the statefulset status to be updated 12/09/22 07:34:57.215
Dec  9 07:34:57.218: INFO: Observed &StatefulSet event: ADDED
Dec  9 07:34:57.218: INFO: Found Statefulset ss in namespace statefulset-4742 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Dec  9 07:34:57.218: INFO: Statefulset ss has an updated status
STEP: patching the Statefulset Status 12/09/22 07:34:57.218
Dec  9 07:34:57.219: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Dec  9 07:34:57.225: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Statefulset status to be patched 12/09/22 07:34:57.225
Dec  9 07:34:57.228: INFO: Observed &StatefulSet event: ADDED
Dec  9 07:34:57.228: INFO: Observed Statefulset ss in namespace statefulset-4742 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Dec  9 07:34:57.228: INFO: Observed &StatefulSet event: MODIFIED
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Dec  9 07:34:57.228: INFO: Deleting all statefulset in ns statefulset-4742
Dec  9 07:34:57.230: INFO: Scaling statefulset ss to 0
Dec  9 07:35:07.247: INFO: Waiting for statefulset status.replicas updated to 0
Dec  9 07:35:07.250: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Dec  9 07:35:07.264: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-4742" for this suite. 12/09/22 07:35:07.282
------------------------------
â€¢ [SLOW TEST] [20.155 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should validate Statefulset Status endpoints [Conformance]
    test/e2e/apps/statefulset.go:977

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:34:47.133
    Dec  9 07:34:47.133: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename statefulset 12/09/22 07:34:47.134
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:34:47.149
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:34:47.154
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-4742 12/09/22 07:34:47.157
    [It] should validate Statefulset Status endpoints [Conformance]
      test/e2e/apps/statefulset.go:977
    STEP: Creating statefulset ss in namespace statefulset-4742 12/09/22 07:34:47.167
    Dec  9 07:34:47.180: INFO: Found 0 stateful pods, waiting for 1
    Dec  9 07:34:57.184: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Patch Statefulset to include a label 12/09/22 07:34:57.189
    STEP: Getting /status 12/09/22 07:34:57.196
    Dec  9 07:34:57.204: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
    STEP: updating the StatefulSet Status 12/09/22 07:34:57.204
    Dec  9 07:34:57.215: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the statefulset status to be updated 12/09/22 07:34:57.215
    Dec  9 07:34:57.218: INFO: Observed &StatefulSet event: ADDED
    Dec  9 07:34:57.218: INFO: Found Statefulset ss in namespace statefulset-4742 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Dec  9 07:34:57.218: INFO: Statefulset ss has an updated status
    STEP: patching the Statefulset Status 12/09/22 07:34:57.218
    Dec  9 07:34:57.219: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Dec  9 07:34:57.225: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Statefulset status to be patched 12/09/22 07:34:57.225
    Dec  9 07:34:57.228: INFO: Observed &StatefulSet event: ADDED
    Dec  9 07:34:57.228: INFO: Observed Statefulset ss in namespace statefulset-4742 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Dec  9 07:34:57.228: INFO: Observed &StatefulSet event: MODIFIED
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Dec  9 07:34:57.228: INFO: Deleting all statefulset in ns statefulset-4742
    Dec  9 07:34:57.230: INFO: Scaling statefulset ss to 0
    Dec  9 07:35:07.247: INFO: Waiting for statefulset status.replicas updated to 0
    Dec  9 07:35:07.250: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:35:07.264: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-4742" for this suite. 12/09/22 07:35:07.282
  << End Captured GinkgoWriter Output
------------------------------
[sig-network] Services
  should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3428
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:35:07.289
Dec  9 07:35:07.289: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename services 12/09/22 07:35:07.291
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:35:07.31
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:35:07.315
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3428
STEP: creating a Service 12/09/22 07:35:07.322
STEP: watching for the Service to be added 12/09/22 07:35:07.33
Dec  9 07:35:07.336: INFO: Found Service test-service-85tnd in namespace services-3418 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
Dec  9 07:35:07.336: INFO: Service test-service-85tnd created
STEP: Getting /status 12/09/22 07:35:07.336
Dec  9 07:35:07.340: INFO: Service test-service-85tnd has LoadBalancer: {[]}
STEP: patching the ServiceStatus 12/09/22 07:35:07.341
STEP: watching for the Service to be patched 12/09/22 07:35:07.35
Dec  9 07:35:07.353: INFO: observed Service test-service-85tnd in namespace services-3418 with annotations: map[] & LoadBalancer: {[]}
Dec  9 07:35:07.353: INFO: Found Service test-service-85tnd in namespace services-3418 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
Dec  9 07:35:07.353: INFO: Service test-service-85tnd has service status patched
STEP: updating the ServiceStatus 12/09/22 07:35:07.353
Dec  9 07:35:07.362: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Service to be updated 12/09/22 07:35:07.362
Dec  9 07:35:07.371: INFO: Observed Service test-service-85tnd in namespace services-3418 with annotations: map[] & Conditions: {[]}
Dec  9 07:35:07.371: INFO: Observed event: &Service{ObjectMeta:{test-service-85tnd  services-3418  12164ad4-da39-4a68-b30e-fe9b0dd6ede4 14265 0 2022-12-09 07:35:07 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2022-12-09 07:35:07 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2022-12-09 07:35:07 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:10.3.132.86,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[10.3.132.86],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
Dec  9 07:35:07.371: INFO: Found Service test-service-85tnd in namespace services-3418 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Dec  9 07:35:07.371: INFO: Service test-service-85tnd has service status updated
STEP: patching the service 12/09/22 07:35:07.371
STEP: watching for the Service to be patched 12/09/22 07:35:07.385
Dec  9 07:35:07.388: INFO: observed Service test-service-85tnd in namespace services-3418 with labels: map[test-service-static:true]
Dec  9 07:35:07.389: INFO: observed Service test-service-85tnd in namespace services-3418 with labels: map[test-service-static:true]
Dec  9 07:35:07.389: INFO: observed Service test-service-85tnd in namespace services-3418 with labels: map[test-service-static:true]
Dec  9 07:35:07.389: INFO: Found Service test-service-85tnd in namespace services-3418 with labels: map[test-service:patched test-service-static:true]
Dec  9 07:35:07.389: INFO: Service test-service-85tnd patched
STEP: deleting the service 12/09/22 07:35:07.39
STEP: watching for the Service to be deleted 12/09/22 07:35:07.404
Dec  9 07:35:07.407: INFO: Observed event: ADDED
Dec  9 07:35:07.407: INFO: Observed event: MODIFIED
Dec  9 07:35:07.407: INFO: Observed event: MODIFIED
Dec  9 07:35:07.408: INFO: Observed event: MODIFIED
Dec  9 07:35:07.408: INFO: Found Service test-service-85tnd in namespace services-3418 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
Dec  9 07:35:07.408: INFO: Service test-service-85tnd deleted
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Dec  9 07:35:07.408: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-3418" for this suite. 12/09/22 07:35:07.414
------------------------------
â€¢ [0.134 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3428

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:35:07.289
    Dec  9 07:35:07.289: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename services 12/09/22 07:35:07.291
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:35:07.31
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:35:07.315
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should complete a service status lifecycle [Conformance]
      test/e2e/network/service.go:3428
    STEP: creating a Service 12/09/22 07:35:07.322
    STEP: watching for the Service to be added 12/09/22 07:35:07.33
    Dec  9 07:35:07.336: INFO: Found Service test-service-85tnd in namespace services-3418 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
    Dec  9 07:35:07.336: INFO: Service test-service-85tnd created
    STEP: Getting /status 12/09/22 07:35:07.336
    Dec  9 07:35:07.340: INFO: Service test-service-85tnd has LoadBalancer: {[]}
    STEP: patching the ServiceStatus 12/09/22 07:35:07.341
    STEP: watching for the Service to be patched 12/09/22 07:35:07.35
    Dec  9 07:35:07.353: INFO: observed Service test-service-85tnd in namespace services-3418 with annotations: map[] & LoadBalancer: {[]}
    Dec  9 07:35:07.353: INFO: Found Service test-service-85tnd in namespace services-3418 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
    Dec  9 07:35:07.353: INFO: Service test-service-85tnd has service status patched
    STEP: updating the ServiceStatus 12/09/22 07:35:07.353
    Dec  9 07:35:07.362: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the Service to be updated 12/09/22 07:35:07.362
    Dec  9 07:35:07.371: INFO: Observed Service test-service-85tnd in namespace services-3418 with annotations: map[] & Conditions: {[]}
    Dec  9 07:35:07.371: INFO: Observed event: &Service{ObjectMeta:{test-service-85tnd  services-3418  12164ad4-da39-4a68-b30e-fe9b0dd6ede4 14265 0 2022-12-09 07:35:07 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2022-12-09 07:35:07 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2022-12-09 07:35:07 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:10.3.132.86,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[10.3.132.86],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
    Dec  9 07:35:07.371: INFO: Found Service test-service-85tnd in namespace services-3418 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Dec  9 07:35:07.371: INFO: Service test-service-85tnd has service status updated
    STEP: patching the service 12/09/22 07:35:07.371
    STEP: watching for the Service to be patched 12/09/22 07:35:07.385
    Dec  9 07:35:07.388: INFO: observed Service test-service-85tnd in namespace services-3418 with labels: map[test-service-static:true]
    Dec  9 07:35:07.389: INFO: observed Service test-service-85tnd in namespace services-3418 with labels: map[test-service-static:true]
    Dec  9 07:35:07.389: INFO: observed Service test-service-85tnd in namespace services-3418 with labels: map[test-service-static:true]
    Dec  9 07:35:07.389: INFO: Found Service test-service-85tnd in namespace services-3418 with labels: map[test-service:patched test-service-static:true]
    Dec  9 07:35:07.389: INFO: Service test-service-85tnd patched
    STEP: deleting the service 12/09/22 07:35:07.39
    STEP: watching for the Service to be deleted 12/09/22 07:35:07.404
    Dec  9 07:35:07.407: INFO: Observed event: ADDED
    Dec  9 07:35:07.407: INFO: Observed event: MODIFIED
    Dec  9 07:35:07.407: INFO: Observed event: MODIFIED
    Dec  9 07:35:07.408: INFO: Observed event: MODIFIED
    Dec  9 07:35:07.408: INFO: Found Service test-service-85tnd in namespace services-3418 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
    Dec  9 07:35:07.408: INFO: Service test-service-85tnd deleted
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:35:07.408: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-3418" for this suite. 12/09/22 07:35:07.414
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl logs
  should be able to retrieve and filter logs  [Conformance]
  test/e2e/kubectl/kubectl.go:1592
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:35:07.427
Dec  9 07:35:07.428: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename kubectl 12/09/22 07:35:07.429
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:35:07.446
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:35:07.452
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Kubectl logs
  test/e2e/kubectl/kubectl.go:1572
STEP: creating an pod 12/09/22 07:35:07.459
Dec  9 07:35:07.461: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=kubectl-6417 run logs-generator --image=registry.k8s.io/e2e-test-images/agnhost:2.43 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
Dec  9 07:35:07.608: INFO: stderr: ""
Dec  9 07:35:07.608: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  test/e2e/kubectl/kubectl.go:1592
STEP: Waiting for log generator to start. 12/09/22 07:35:07.608
Dec  9 07:35:07.608: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Dec  9 07:35:07.608: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-6417" to be "running and ready, or succeeded"
Dec  9 07:35:07.625: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 17.522042ms
Dec  9 07:35:07.625: INFO: Error evaluating pod condition running and ready, or succeeded: want pod 'logs-generator' on '' to be 'Running' but was 'Pending'
Dec  9 07:35:09.632: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.02379877s
Dec  9 07:35:09.632: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Dec  9 07:35:09.632: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings 12/09/22 07:35:09.632
Dec  9 07:35:09.632: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=kubectl-6417 logs logs-generator logs-generator'
Dec  9 07:35:09.796: INFO: stderr: ""
Dec  9 07:35:09.796: INFO: stdout: "I1209 07:35:08.426711       1 logs_generator.go:76] 0 GET /api/v1/namespaces/ns/pods/t2sk 535\nI1209 07:35:08.626733       1 logs_generator.go:76] 1 POST /api/v1/namespaces/default/pods/44c 528\nI1209 07:35:08.827227       1 logs_generator.go:76] 2 GET /api/v1/namespaces/default/pods/h2hh 450\nI1209 07:35:09.027542       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/default/pods/cjz9 518\nI1209 07:35:09.226839       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/ns/pods/7nj 230\nI1209 07:35:09.427182       1 logs_generator.go:76] 5 POST /api/v1/namespaces/default/pods/km9 229\nI1209 07:35:09.627700       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/kube-system/pods/ffmf 436\n"
STEP: limiting log lines 12/09/22 07:35:09.796
Dec  9 07:35:09.796: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=kubectl-6417 logs logs-generator logs-generator --tail=1'
Dec  9 07:35:09.888: INFO: stderr: ""
Dec  9 07:35:09.888: INFO: stdout: "I1209 07:35:09.827229       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/default/pods/v4bs 540\n"
Dec  9 07:35:09.888: INFO: got output "I1209 07:35:09.827229       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/default/pods/v4bs 540\n"
STEP: limiting log bytes 12/09/22 07:35:09.888
Dec  9 07:35:09.889: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=kubectl-6417 logs logs-generator logs-generator --limit-bytes=1'
Dec  9 07:35:09.977: INFO: stderr: ""
Dec  9 07:35:09.977: INFO: stdout: "I"
Dec  9 07:35:09.977: INFO: got output "I"
STEP: exposing timestamps 12/09/22 07:35:09.977
Dec  9 07:35:09.977: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=kubectl-6417 logs logs-generator logs-generator --tail=1 --timestamps'
Dec  9 07:35:10.119: INFO: stderr: ""
Dec  9 07:35:10.119: INFO: stdout: "2022-12-09T07:35:10.027660090Z I1209 07:35:10.027565       1 logs_generator.go:76] 8 PUT /api/v1/namespaces/kube-system/pods/g7q 449\n"
Dec  9 07:35:10.119: INFO: got output "2022-12-09T07:35:10.027660090Z I1209 07:35:10.027565       1 logs_generator.go:76] 8 PUT /api/v1/namespaces/kube-system/pods/g7q 449\n"
STEP: restricting to a time range 12/09/22 07:35:10.119
Dec  9 07:35:12.619: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=kubectl-6417 logs logs-generator logs-generator --since=1s'
Dec  9 07:35:12.747: INFO: stderr: ""
Dec  9 07:35:12.747: INFO: stdout: "I1209 07:35:11.826647       1 logs_generator.go:76] 17 POST /api/v1/namespaces/kube-system/pods/5zk 490\nI1209 07:35:12.027023       1 logs_generator.go:76] 18 GET /api/v1/namespaces/default/pods/ldt 258\nI1209 07:35:12.227437       1 logs_generator.go:76] 19 POST /api/v1/namespaces/ns/pods/x9w 553\nI1209 07:35:12.426681       1 logs_generator.go:76] 20 PUT /api/v1/namespaces/ns/pods/hgb8 207\nI1209 07:35:12.626998       1 logs_generator.go:76] 21 PUT /api/v1/namespaces/kube-system/pods/b44c 437\n"
Dec  9 07:35:12.747: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=kubectl-6417 logs logs-generator logs-generator --since=24h'
Dec  9 07:35:12.861: INFO: stderr: ""
Dec  9 07:35:12.861: INFO: stdout: "I1209 07:35:08.426711       1 logs_generator.go:76] 0 GET /api/v1/namespaces/ns/pods/t2sk 535\nI1209 07:35:08.626733       1 logs_generator.go:76] 1 POST /api/v1/namespaces/default/pods/44c 528\nI1209 07:35:08.827227       1 logs_generator.go:76] 2 GET /api/v1/namespaces/default/pods/h2hh 450\nI1209 07:35:09.027542       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/default/pods/cjz9 518\nI1209 07:35:09.226839       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/ns/pods/7nj 230\nI1209 07:35:09.427182       1 logs_generator.go:76] 5 POST /api/v1/namespaces/default/pods/km9 229\nI1209 07:35:09.627700       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/kube-system/pods/ffmf 436\nI1209 07:35:09.827229       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/default/pods/v4bs 540\nI1209 07:35:10.027565       1 logs_generator.go:76] 8 PUT /api/v1/namespaces/kube-system/pods/g7q 449\nI1209 07:35:10.226942       1 logs_generator.go:76] 9 PUT /api/v1/namespaces/default/pods/sgn 413\nI1209 07:35:10.427333       1 logs_generator.go:76] 10 PUT /api/v1/namespaces/ns/pods/zzq 402\nI1209 07:35:10.627588       1 logs_generator.go:76] 11 PUT /api/v1/namespaces/default/pods/twh 456\nI1209 07:35:10.827075       1 logs_generator.go:76] 12 PUT /api/v1/namespaces/kube-system/pods/6km 277\nI1209 07:35:11.027458       1 logs_generator.go:76] 13 PUT /api/v1/namespaces/kube-system/pods/qhrz 347\nI1209 07:35:11.226728       1 logs_generator.go:76] 14 POST /api/v1/namespaces/kube-system/pods/qwlr 475\nI1209 07:35:11.427107       1 logs_generator.go:76] 15 POST /api/v1/namespaces/ns/pods/xkjn 221\nI1209 07:35:11.627367       1 logs_generator.go:76] 16 GET /api/v1/namespaces/kube-system/pods/r2n 561\nI1209 07:35:11.826647       1 logs_generator.go:76] 17 POST /api/v1/namespaces/kube-system/pods/5zk 490\nI1209 07:35:12.027023       1 logs_generator.go:76] 18 GET /api/v1/namespaces/default/pods/ldt 258\nI1209 07:35:12.227437       1 logs_generator.go:76] 19 POST /api/v1/namespaces/ns/pods/x9w 553\nI1209 07:35:12.426681       1 logs_generator.go:76] 20 PUT /api/v1/namespaces/ns/pods/hgb8 207\nI1209 07:35:12.626998       1 logs_generator.go:76] 21 PUT /api/v1/namespaces/kube-system/pods/b44c 437\nI1209 07:35:12.827568       1 logs_generator.go:76] 22 POST /api/v1/namespaces/ns/pods/jxj 291\n"
[AfterEach] Kubectl logs
  test/e2e/kubectl/kubectl.go:1577
Dec  9 07:35:12.861: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=kubectl-6417 delete pod logs-generator'
Dec  9 07:35:14.390: INFO: stderr: ""
Dec  9 07:35:14.390: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Dec  9 07:35:14.390: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-6417" for this suite. 12/09/22 07:35:14.393
------------------------------
â€¢ [SLOW TEST] [6.971 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl logs
  test/e2e/kubectl/kubectl.go:1569
    should be able to retrieve and filter logs  [Conformance]
    test/e2e/kubectl/kubectl.go:1592

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:35:07.427
    Dec  9 07:35:07.428: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename kubectl 12/09/22 07:35:07.429
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:35:07.446
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:35:07.452
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Kubectl logs
      test/e2e/kubectl/kubectl.go:1572
    STEP: creating an pod 12/09/22 07:35:07.459
    Dec  9 07:35:07.461: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=kubectl-6417 run logs-generator --image=registry.k8s.io/e2e-test-images/agnhost:2.43 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
    Dec  9 07:35:07.608: INFO: stderr: ""
    Dec  9 07:35:07.608: INFO: stdout: "pod/logs-generator created\n"
    [It] should be able to retrieve and filter logs  [Conformance]
      test/e2e/kubectl/kubectl.go:1592
    STEP: Waiting for log generator to start. 12/09/22 07:35:07.608
    Dec  9 07:35:07.608: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
    Dec  9 07:35:07.608: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-6417" to be "running and ready, or succeeded"
    Dec  9 07:35:07.625: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 17.522042ms
    Dec  9 07:35:07.625: INFO: Error evaluating pod condition running and ready, or succeeded: want pod 'logs-generator' on '' to be 'Running' but was 'Pending'
    Dec  9 07:35:09.632: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.02379877s
    Dec  9 07:35:09.632: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
    Dec  9 07:35:09.632: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
    STEP: checking for a matching strings 12/09/22 07:35:09.632
    Dec  9 07:35:09.632: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=kubectl-6417 logs logs-generator logs-generator'
    Dec  9 07:35:09.796: INFO: stderr: ""
    Dec  9 07:35:09.796: INFO: stdout: "I1209 07:35:08.426711       1 logs_generator.go:76] 0 GET /api/v1/namespaces/ns/pods/t2sk 535\nI1209 07:35:08.626733       1 logs_generator.go:76] 1 POST /api/v1/namespaces/default/pods/44c 528\nI1209 07:35:08.827227       1 logs_generator.go:76] 2 GET /api/v1/namespaces/default/pods/h2hh 450\nI1209 07:35:09.027542       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/default/pods/cjz9 518\nI1209 07:35:09.226839       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/ns/pods/7nj 230\nI1209 07:35:09.427182       1 logs_generator.go:76] 5 POST /api/v1/namespaces/default/pods/km9 229\nI1209 07:35:09.627700       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/kube-system/pods/ffmf 436\n"
    STEP: limiting log lines 12/09/22 07:35:09.796
    Dec  9 07:35:09.796: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=kubectl-6417 logs logs-generator logs-generator --tail=1'
    Dec  9 07:35:09.888: INFO: stderr: ""
    Dec  9 07:35:09.888: INFO: stdout: "I1209 07:35:09.827229       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/default/pods/v4bs 540\n"
    Dec  9 07:35:09.888: INFO: got output "I1209 07:35:09.827229       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/default/pods/v4bs 540\n"
    STEP: limiting log bytes 12/09/22 07:35:09.888
    Dec  9 07:35:09.889: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=kubectl-6417 logs logs-generator logs-generator --limit-bytes=1'
    Dec  9 07:35:09.977: INFO: stderr: ""
    Dec  9 07:35:09.977: INFO: stdout: "I"
    Dec  9 07:35:09.977: INFO: got output "I"
    STEP: exposing timestamps 12/09/22 07:35:09.977
    Dec  9 07:35:09.977: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=kubectl-6417 logs logs-generator logs-generator --tail=1 --timestamps'
    Dec  9 07:35:10.119: INFO: stderr: ""
    Dec  9 07:35:10.119: INFO: stdout: "2022-12-09T07:35:10.027660090Z I1209 07:35:10.027565       1 logs_generator.go:76] 8 PUT /api/v1/namespaces/kube-system/pods/g7q 449\n"
    Dec  9 07:35:10.119: INFO: got output "2022-12-09T07:35:10.027660090Z I1209 07:35:10.027565       1 logs_generator.go:76] 8 PUT /api/v1/namespaces/kube-system/pods/g7q 449\n"
    STEP: restricting to a time range 12/09/22 07:35:10.119
    Dec  9 07:35:12.619: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=kubectl-6417 logs logs-generator logs-generator --since=1s'
    Dec  9 07:35:12.747: INFO: stderr: ""
    Dec  9 07:35:12.747: INFO: stdout: "I1209 07:35:11.826647       1 logs_generator.go:76] 17 POST /api/v1/namespaces/kube-system/pods/5zk 490\nI1209 07:35:12.027023       1 logs_generator.go:76] 18 GET /api/v1/namespaces/default/pods/ldt 258\nI1209 07:35:12.227437       1 logs_generator.go:76] 19 POST /api/v1/namespaces/ns/pods/x9w 553\nI1209 07:35:12.426681       1 logs_generator.go:76] 20 PUT /api/v1/namespaces/ns/pods/hgb8 207\nI1209 07:35:12.626998       1 logs_generator.go:76] 21 PUT /api/v1/namespaces/kube-system/pods/b44c 437\n"
    Dec  9 07:35:12.747: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=kubectl-6417 logs logs-generator logs-generator --since=24h'
    Dec  9 07:35:12.861: INFO: stderr: ""
    Dec  9 07:35:12.861: INFO: stdout: "I1209 07:35:08.426711       1 logs_generator.go:76] 0 GET /api/v1/namespaces/ns/pods/t2sk 535\nI1209 07:35:08.626733       1 logs_generator.go:76] 1 POST /api/v1/namespaces/default/pods/44c 528\nI1209 07:35:08.827227       1 logs_generator.go:76] 2 GET /api/v1/namespaces/default/pods/h2hh 450\nI1209 07:35:09.027542       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/default/pods/cjz9 518\nI1209 07:35:09.226839       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/ns/pods/7nj 230\nI1209 07:35:09.427182       1 logs_generator.go:76] 5 POST /api/v1/namespaces/default/pods/km9 229\nI1209 07:35:09.627700       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/kube-system/pods/ffmf 436\nI1209 07:35:09.827229       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/default/pods/v4bs 540\nI1209 07:35:10.027565       1 logs_generator.go:76] 8 PUT /api/v1/namespaces/kube-system/pods/g7q 449\nI1209 07:35:10.226942       1 logs_generator.go:76] 9 PUT /api/v1/namespaces/default/pods/sgn 413\nI1209 07:35:10.427333       1 logs_generator.go:76] 10 PUT /api/v1/namespaces/ns/pods/zzq 402\nI1209 07:35:10.627588       1 logs_generator.go:76] 11 PUT /api/v1/namespaces/default/pods/twh 456\nI1209 07:35:10.827075       1 logs_generator.go:76] 12 PUT /api/v1/namespaces/kube-system/pods/6km 277\nI1209 07:35:11.027458       1 logs_generator.go:76] 13 PUT /api/v1/namespaces/kube-system/pods/qhrz 347\nI1209 07:35:11.226728       1 logs_generator.go:76] 14 POST /api/v1/namespaces/kube-system/pods/qwlr 475\nI1209 07:35:11.427107       1 logs_generator.go:76] 15 POST /api/v1/namespaces/ns/pods/xkjn 221\nI1209 07:35:11.627367       1 logs_generator.go:76] 16 GET /api/v1/namespaces/kube-system/pods/r2n 561\nI1209 07:35:11.826647       1 logs_generator.go:76] 17 POST /api/v1/namespaces/kube-system/pods/5zk 490\nI1209 07:35:12.027023       1 logs_generator.go:76] 18 GET /api/v1/namespaces/default/pods/ldt 258\nI1209 07:35:12.227437       1 logs_generator.go:76] 19 POST /api/v1/namespaces/ns/pods/x9w 553\nI1209 07:35:12.426681       1 logs_generator.go:76] 20 PUT /api/v1/namespaces/ns/pods/hgb8 207\nI1209 07:35:12.626998       1 logs_generator.go:76] 21 PUT /api/v1/namespaces/kube-system/pods/b44c 437\nI1209 07:35:12.827568       1 logs_generator.go:76] 22 POST /api/v1/namespaces/ns/pods/jxj 291\n"
    [AfterEach] Kubectl logs
      test/e2e/kubectl/kubectl.go:1577
    Dec  9 07:35:12.861: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=kubectl-6417 delete pod logs-generator'
    Dec  9 07:35:14.390: INFO: stderr: ""
    Dec  9 07:35:14.390: INFO: stdout: "pod \"logs-generator\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:35:14.390: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-6417" for this suite. 12/09/22 07:35:14.393
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:112
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:35:14.399
Dec  9 07:35:14.399: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename var-expansion 12/09/22 07:35:14.4
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:35:14.416
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:35:14.42
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:112
STEP: Creating a pod to test substitution in volume subpath 12/09/22 07:35:14.424
Dec  9 07:35:14.431: INFO: Waiting up to 5m0s for pod "var-expansion-ce48b0af-e9aa-4cca-b4a9-3bcf98ac7935" in namespace "var-expansion-8572" to be "Succeeded or Failed"
Dec  9 07:35:14.437: INFO: Pod "var-expansion-ce48b0af-e9aa-4cca-b4a9-3bcf98ac7935": Phase="Pending", Reason="", readiness=false. Elapsed: 5.302174ms
Dec  9 07:35:16.451: INFO: Pod "var-expansion-ce48b0af-e9aa-4cca-b4a9-3bcf98ac7935": Phase="Running", Reason="", readiness=false. Elapsed: 2.019939399s
Dec  9 07:35:18.442: INFO: Pod "var-expansion-ce48b0af-e9aa-4cca-b4a9-3bcf98ac7935": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010721671s
STEP: Saw pod success 12/09/22 07:35:18.442
Dec  9 07:35:18.442: INFO: Pod "var-expansion-ce48b0af-e9aa-4cca-b4a9-3bcf98ac7935" satisfied condition "Succeeded or Failed"
Dec  9 07:35:18.446: INFO: Trying to get logs from node ip-10-0-10-179 pod var-expansion-ce48b0af-e9aa-4cca-b4a9-3bcf98ac7935 container dapi-container: <nil>
STEP: delete the pod 12/09/22 07:35:18.457
Dec  9 07:35:18.478: INFO: Waiting for pod var-expansion-ce48b0af-e9aa-4cca-b4a9-3bcf98ac7935 to disappear
Dec  9 07:35:18.483: INFO: Pod var-expansion-ce48b0af-e9aa-4cca-b4a9-3bcf98ac7935 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Dec  9 07:35:18.483: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-8572" for this suite. 12/09/22 07:35:18.489
------------------------------
â€¢ [4.108 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:112

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:35:14.399
    Dec  9 07:35:14.399: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename var-expansion 12/09/22 07:35:14.4
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:35:14.416
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:35:14.42
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow substituting values in a volume subpath [Conformance]
      test/e2e/common/node/expansion.go:112
    STEP: Creating a pod to test substitution in volume subpath 12/09/22 07:35:14.424
    Dec  9 07:35:14.431: INFO: Waiting up to 5m0s for pod "var-expansion-ce48b0af-e9aa-4cca-b4a9-3bcf98ac7935" in namespace "var-expansion-8572" to be "Succeeded or Failed"
    Dec  9 07:35:14.437: INFO: Pod "var-expansion-ce48b0af-e9aa-4cca-b4a9-3bcf98ac7935": Phase="Pending", Reason="", readiness=false. Elapsed: 5.302174ms
    Dec  9 07:35:16.451: INFO: Pod "var-expansion-ce48b0af-e9aa-4cca-b4a9-3bcf98ac7935": Phase="Running", Reason="", readiness=false. Elapsed: 2.019939399s
    Dec  9 07:35:18.442: INFO: Pod "var-expansion-ce48b0af-e9aa-4cca-b4a9-3bcf98ac7935": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010721671s
    STEP: Saw pod success 12/09/22 07:35:18.442
    Dec  9 07:35:18.442: INFO: Pod "var-expansion-ce48b0af-e9aa-4cca-b4a9-3bcf98ac7935" satisfied condition "Succeeded or Failed"
    Dec  9 07:35:18.446: INFO: Trying to get logs from node ip-10-0-10-179 pod var-expansion-ce48b0af-e9aa-4cca-b4a9-3bcf98ac7935 container dapi-container: <nil>
    STEP: delete the pod 12/09/22 07:35:18.457
    Dec  9 07:35:18.478: INFO: Waiting for pod var-expansion-ce48b0af-e9aa-4cca-b4a9-3bcf98ac7935 to disappear
    Dec  9 07:35:18.483: INFO: Pod var-expansion-ce48b0af-e9aa-4cca-b4a9-3bcf98ac7935 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:35:18.483: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-8572" for this suite. 12/09/22 07:35:18.489
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:334
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:35:18.507
Dec  9 07:35:18.507: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename init-container 12/09/22 07:35:18.508
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:35:18.544
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:35:18.55
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:165
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:334
STEP: creating the pod 12/09/22 07:35:18.555
Dec  9 07:35:18.556: INFO: PodSpec: initContainers in spec.initContainers
Dec  9 07:36:03.510: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-5fe3e159-2617-4f6e-af72-12b1695f10d5", GenerateName:"", Namespace:"init-container-7011", SelfLink:"", UID:"f717cbc4-bd0d-43f8-885b-92718e66b4af", ResourceVersion:"14470", Generation:0, CreationTimestamp:time.Date(2022, time.December, 9, 7, 35, 18, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"556071681"}, Annotations:map[string]string{"cni.projectcalico.org/containerID":"8b64ad79adce64546b10e57803f682a1e09c1f836ad472677453a55cc65ac2d9", "cni.projectcalico.org/podIP":"10.2.136.187/32", "cni.projectcalico.org/podIPs":"10.2.136.187/32"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2022, time.December, 9, 7, 35, 18, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc00100ec30), Subresource:""}, v1.ManagedFieldsEntry{Manager:"calico", Operation:"Update", APIVersion:"v1", Time:time.Date(2022, time.December, 9, 7, 35, 19, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc00100ec90), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2022, time.December, 9, 7, 36, 3, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc00100ed68), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-jccd4", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc0033b3d00), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-jccd4", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-jccd4", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"registry.k8s.io/pause:3.9", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-jccd4", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc0047bf6a0), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"ip-10-0-10-179", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc005ee9730), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0047bf730)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0047bf750)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc0047bf758), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc0047bf75c), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc00115b3b0), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil), HostUsers:(*bool)(nil), SchedulingGates:[]v1.PodSchedulingGate(nil), ResourceClaims:[]v1.PodResourceClaim(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2022, time.December, 9, 7, 35, 18, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2022, time.December, 9, 7, 35, 18, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2022, time.December, 9, 7, 35, 18, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2022, time.December, 9, 7, 35, 18, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.0.10.179", PodIP:"10.2.136.187", PodIPs:[]v1.PodIP{v1.PodIP{IP:"10.2.136.187"}}, StartTime:time.Date(2022, time.December, 9, 7, 35, 18, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc005ee9810)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc005ee9880)}, Ready:false, RestartCount:3, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"registry.k8s.io/e2e-test-images/busybox@sha256:2e0f836850e09b8b7cc937681d6194537a09fbd5f6b9e08f4d646a85128e8937", ContainerID:"containerd://25880f0e16c21aa4273db73cce545dcf894f70902c60963e91632ec36c1ebd36", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0033b3e00), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0033b3dc0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/pause:3.9", ImageID:"", ContainerID:"", Started:(*bool)(0xc0047bf7df)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Dec  9 07:36:03.511: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "init-container-7011" for this suite. 12/09/22 07:36:03.517
------------------------------
â€¢ [SLOW TEST] [45.016 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:334

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:35:18.507
    Dec  9 07:35:18.507: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename init-container 12/09/22 07:35:18.508
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:35:18.544
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:35:18.55
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:165
    [It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
      test/e2e/common/node/init_container.go:334
    STEP: creating the pod 12/09/22 07:35:18.555
    Dec  9 07:35:18.556: INFO: PodSpec: initContainers in spec.initContainers
    Dec  9 07:36:03.510: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-5fe3e159-2617-4f6e-af72-12b1695f10d5", GenerateName:"", Namespace:"init-container-7011", SelfLink:"", UID:"f717cbc4-bd0d-43f8-885b-92718e66b4af", ResourceVersion:"14470", Generation:0, CreationTimestamp:time.Date(2022, time.December, 9, 7, 35, 18, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"556071681"}, Annotations:map[string]string{"cni.projectcalico.org/containerID":"8b64ad79adce64546b10e57803f682a1e09c1f836ad472677453a55cc65ac2d9", "cni.projectcalico.org/podIP":"10.2.136.187/32", "cni.projectcalico.org/podIPs":"10.2.136.187/32"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2022, time.December, 9, 7, 35, 18, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc00100ec30), Subresource:""}, v1.ManagedFieldsEntry{Manager:"calico", Operation:"Update", APIVersion:"v1", Time:time.Date(2022, time.December, 9, 7, 35, 19, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc00100ec90), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2022, time.December, 9, 7, 36, 3, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc00100ed68), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-jccd4", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc0033b3d00), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-jccd4", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-jccd4", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"registry.k8s.io/pause:3.9", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-jccd4", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc0047bf6a0), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"ip-10-0-10-179", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc005ee9730), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0047bf730)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0047bf750)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc0047bf758), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc0047bf75c), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc00115b3b0), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil), HostUsers:(*bool)(nil), SchedulingGates:[]v1.PodSchedulingGate(nil), ResourceClaims:[]v1.PodResourceClaim(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2022, time.December, 9, 7, 35, 18, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2022, time.December, 9, 7, 35, 18, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2022, time.December, 9, 7, 35, 18, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2022, time.December, 9, 7, 35, 18, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.0.10.179", PodIP:"10.2.136.187", PodIPs:[]v1.PodIP{v1.PodIP{IP:"10.2.136.187"}}, StartTime:time.Date(2022, time.December, 9, 7, 35, 18, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc005ee9810)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc005ee9880)}, Ready:false, RestartCount:3, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"registry.k8s.io/e2e-test-images/busybox@sha256:2e0f836850e09b8b7cc937681d6194537a09fbd5f6b9e08f4d646a85128e8937", ContainerID:"containerd://25880f0e16c21aa4273db73cce545dcf894f70902c60963e91632ec36c1ebd36", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0033b3e00), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0033b3dc0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/pause:3.9", ImageID:"", ContainerID:"", Started:(*bool)(0xc0047bf7df)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:36:03.511: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "init-container-7011" for this suite. 12/09/22 07:36:03.517
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:455
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:36:03.53
Dec  9 07:36:03.530: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename taint-multiple-pods 12/09/22 07:36:03.531
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:36:03.564
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:36:03.577
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/node/taints.go:383
Dec  9 07:36:03.582: INFO: Waiting up to 1m0s for all nodes to be ready
Dec  9 07:37:03.612: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:455
Dec  9 07:37:03.616: INFO: Starting informer...
STEP: Starting pods... 12/09/22 07:37:03.616
Dec  9 07:37:03.837: INFO: Pod1 is running on ip-10-0-10-179. Tainting Node
Dec  9 07:37:04.054: INFO: Waiting up to 5m0s for pod "taint-eviction-b1" in namespace "taint-multiple-pods-953" to be "running"
Dec  9 07:37:04.093: INFO: Pod "taint-eviction-b1": Phase="Pending", Reason="", readiness=false. Elapsed: 38.100296ms
Dec  9 07:37:06.100: INFO: Pod "taint-eviction-b1": Phase="Running", Reason="", readiness=true. Elapsed: 2.045471965s
Dec  9 07:37:06.100: INFO: Pod "taint-eviction-b1" satisfied condition "running"
Dec  9 07:37:06.100: INFO: Waiting up to 5m0s for pod "taint-eviction-b2" in namespace "taint-multiple-pods-953" to be "running"
Dec  9 07:37:06.104: INFO: Pod "taint-eviction-b2": Phase="Running", Reason="", readiness=true. Elapsed: 3.34235ms
Dec  9 07:37:06.104: INFO: Pod "taint-eviction-b2" satisfied condition "running"
Dec  9 07:37:06.104: INFO: Pod2 is running on ip-10-0-10-179. Tainting Node
STEP: Trying to apply a taint on the Node 12/09/22 07:37:06.104
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 12/09/22 07:37:06.121
STEP: Waiting for Pod1 and Pod2 to be deleted 12/09/22 07:37:06.13
Dec  9 07:37:12.744: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
Dec  9 07:37:32.034: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 12/09/22 07:37:32.056
[AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/node/init/init.go:32
Dec  9 07:37:32.076: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "taint-multiple-pods-953" for this suite. 12/09/22 07:37:32.082
------------------------------
â€¢ [SLOW TEST] [88.562 seconds]
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
test/e2e/node/framework.go:23
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:455

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:36:03.53
    Dec  9 07:36:03.530: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename taint-multiple-pods 12/09/22 07:36:03.531
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:36:03.564
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:36:03.577
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/node/taints.go:383
    Dec  9 07:36:03.582: INFO: Waiting up to 1m0s for all nodes to be ready
    Dec  9 07:37:03.612: INFO: Waiting for terminating namespaces to be deleted...
    [It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
      test/e2e/node/taints.go:455
    Dec  9 07:37:03.616: INFO: Starting informer...
    STEP: Starting pods... 12/09/22 07:37:03.616
    Dec  9 07:37:03.837: INFO: Pod1 is running on ip-10-0-10-179. Tainting Node
    Dec  9 07:37:04.054: INFO: Waiting up to 5m0s for pod "taint-eviction-b1" in namespace "taint-multiple-pods-953" to be "running"
    Dec  9 07:37:04.093: INFO: Pod "taint-eviction-b1": Phase="Pending", Reason="", readiness=false. Elapsed: 38.100296ms
    Dec  9 07:37:06.100: INFO: Pod "taint-eviction-b1": Phase="Running", Reason="", readiness=true. Elapsed: 2.045471965s
    Dec  9 07:37:06.100: INFO: Pod "taint-eviction-b1" satisfied condition "running"
    Dec  9 07:37:06.100: INFO: Waiting up to 5m0s for pod "taint-eviction-b2" in namespace "taint-multiple-pods-953" to be "running"
    Dec  9 07:37:06.104: INFO: Pod "taint-eviction-b2": Phase="Running", Reason="", readiness=true. Elapsed: 3.34235ms
    Dec  9 07:37:06.104: INFO: Pod "taint-eviction-b2" satisfied condition "running"
    Dec  9 07:37:06.104: INFO: Pod2 is running on ip-10-0-10-179. Tainting Node
    STEP: Trying to apply a taint on the Node 12/09/22 07:37:06.104
    STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 12/09/22 07:37:06.121
    STEP: Waiting for Pod1 and Pod2 to be deleted 12/09/22 07:37:06.13
    Dec  9 07:37:12.744: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
    Dec  9 07:37:32.034: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
    STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 12/09/22 07:37:32.056
    [AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:37:32.076: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "taint-multiple-pods-953" for this suite. 12/09/22 07:37:32.082
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:46
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:37:32.093
Dec  9 07:37:32.093: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename projected 12/09/22 07:37:32.094
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:37:32.128
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:37:32.134
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:46
STEP: Creating projection with secret that has name projected-secret-test-a947acb9-7355-4b24-8a4b-bd440325f066 12/09/22 07:37:32.139
STEP: Creating a pod to test consume secrets 12/09/22 07:37:32.144
Dec  9 07:37:32.154: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-eaa111f2-00c7-4a6f-834c-759758d0f757" in namespace "projected-4186" to be "Succeeded or Failed"
Dec  9 07:37:32.168: INFO: Pod "pod-projected-secrets-eaa111f2-00c7-4a6f-834c-759758d0f757": Phase="Pending", Reason="", readiness=false. Elapsed: 14.056157ms
Dec  9 07:37:34.174: INFO: Pod "pod-projected-secrets-eaa111f2-00c7-4a6f-834c-759758d0f757": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01989177s
Dec  9 07:37:36.185: INFO: Pod "pod-projected-secrets-eaa111f2-00c7-4a6f-834c-759758d0f757": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.031522643s
STEP: Saw pod success 12/09/22 07:37:36.186
Dec  9 07:37:36.186: INFO: Pod "pod-projected-secrets-eaa111f2-00c7-4a6f-834c-759758d0f757" satisfied condition "Succeeded or Failed"
Dec  9 07:37:36.190: INFO: Trying to get logs from node ip-10-0-10-179 pod pod-projected-secrets-eaa111f2-00c7-4a6f-834c-759758d0f757 container projected-secret-volume-test: <nil>
STEP: delete the pod 12/09/22 07:37:36.223
Dec  9 07:37:36.261: INFO: Waiting for pod pod-projected-secrets-eaa111f2-00c7-4a6f-834c-759758d0f757 to disappear
Dec  9 07:37:36.277: INFO: Pod pod-projected-secrets-eaa111f2-00c7-4a6f-834c-759758d0f757 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Dec  9 07:37:36.277: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-4186" for this suite. 12/09/22 07:37:36.293
------------------------------
â€¢ [4.254 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:37:32.093
    Dec  9 07:37:32.093: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename projected 12/09/22 07:37:32.094
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:37:32.128
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:37:32.134
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:46
    STEP: Creating projection with secret that has name projected-secret-test-a947acb9-7355-4b24-8a4b-bd440325f066 12/09/22 07:37:32.139
    STEP: Creating a pod to test consume secrets 12/09/22 07:37:32.144
    Dec  9 07:37:32.154: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-eaa111f2-00c7-4a6f-834c-759758d0f757" in namespace "projected-4186" to be "Succeeded or Failed"
    Dec  9 07:37:32.168: INFO: Pod "pod-projected-secrets-eaa111f2-00c7-4a6f-834c-759758d0f757": Phase="Pending", Reason="", readiness=false. Elapsed: 14.056157ms
    Dec  9 07:37:34.174: INFO: Pod "pod-projected-secrets-eaa111f2-00c7-4a6f-834c-759758d0f757": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01989177s
    Dec  9 07:37:36.185: INFO: Pod "pod-projected-secrets-eaa111f2-00c7-4a6f-834c-759758d0f757": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.031522643s
    STEP: Saw pod success 12/09/22 07:37:36.186
    Dec  9 07:37:36.186: INFO: Pod "pod-projected-secrets-eaa111f2-00c7-4a6f-834c-759758d0f757" satisfied condition "Succeeded or Failed"
    Dec  9 07:37:36.190: INFO: Trying to get logs from node ip-10-0-10-179 pod pod-projected-secrets-eaa111f2-00c7-4a6f-834c-759758d0f757 container projected-secret-volume-test: <nil>
    STEP: delete the pod 12/09/22 07:37:36.223
    Dec  9 07:37:36.261: INFO: Waiting for pod pod-projected-secrets-eaa111f2-00c7-4a6f-834c-759758d0f757 to disappear
    Dec  9 07:37:36.277: INFO: Pod pod-projected-secrets-eaa111f2-00c7-4a6f-834c-759758d0f757 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:37:36.277: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-4186" for this suite. 12/09/22 07:37:36.293
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Pods
  should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:845
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:37:36.354
Dec  9 07:37:36.359: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename pods 12/09/22 07:37:36.372
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:37:36.413
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:37:36.419
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:845
STEP: Create set of pods 12/09/22 07:37:36.426
Dec  9 07:37:36.449: INFO: created test-pod-1
Dec  9 07:37:36.471: INFO: created test-pod-2
Dec  9 07:37:36.486: INFO: created test-pod-3
STEP: waiting for all 3 pods to be running 12/09/22 07:37:36.487
Dec  9 07:37:36.487: INFO: Waiting up to 5m0s for all pods (need at least 3) in namespace 'pods-759' to be running and ready
Dec  9 07:37:36.527: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Dec  9 07:37:36.527: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Dec  9 07:37:36.527: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Dec  9 07:37:36.527: INFO: 0 / 3 pods in namespace 'pods-759' are running and ready (0 seconds elapsed)
Dec  9 07:37:36.527: INFO: expected 0 pod replicas in namespace 'pods-759', 0 are Running and Ready.
Dec  9 07:37:36.527: INFO: POD         NODE            PHASE    GRACE  CONDITIONS
Dec  9 07:37:36.527: INFO: test-pod-1  ip-10-0-10-179  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-12-09 07:37:36 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-12-09 07:37:36 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-12-09 07:37:36 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-12-09 07:37:36 +0000 UTC  }]
Dec  9 07:37:36.527: INFO: test-pod-2  ip-10-0-10-179  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-12-09 07:37:36 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-12-09 07:37:36 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-12-09 07:37:36 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-12-09 07:37:36 +0000 UTC  }]
Dec  9 07:37:36.527: INFO: test-pod-3  ip-10-0-10-179  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-12-09 07:37:36 +0000 UTC  }]
Dec  9 07:37:36.527: INFO: 
Dec  9 07:37:38.537: INFO: 3 / 3 pods in namespace 'pods-759' are running and ready (2 seconds elapsed)
Dec  9 07:37:38.537: INFO: expected 0 pod replicas in namespace 'pods-759', 0 are Running and Ready.
STEP: waiting for all pods to be deleted 12/09/22 07:37:38.554
Dec  9 07:37:38.557: INFO: Pod quantity 3 is different from expected quantity 0
Dec  9 07:37:39.561: INFO: Pod quantity 3 is different from expected quantity 0
Dec  9 07:37:40.563: INFO: Pod quantity 3 is different from expected quantity 0
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Dec  9 07:37:41.560: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-759" for this suite. 12/09/22 07:37:41.564
------------------------------
â€¢ [SLOW TEST] [5.222 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:845

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:37:36.354
    Dec  9 07:37:36.359: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename pods 12/09/22 07:37:36.372
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:37:36.413
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:37:36.419
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should delete a collection of pods [Conformance]
      test/e2e/common/node/pods.go:845
    STEP: Create set of pods 12/09/22 07:37:36.426
    Dec  9 07:37:36.449: INFO: created test-pod-1
    Dec  9 07:37:36.471: INFO: created test-pod-2
    Dec  9 07:37:36.486: INFO: created test-pod-3
    STEP: waiting for all 3 pods to be running 12/09/22 07:37:36.487
    Dec  9 07:37:36.487: INFO: Waiting up to 5m0s for all pods (need at least 3) in namespace 'pods-759' to be running and ready
    Dec  9 07:37:36.527: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Dec  9 07:37:36.527: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Dec  9 07:37:36.527: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Dec  9 07:37:36.527: INFO: 0 / 3 pods in namespace 'pods-759' are running and ready (0 seconds elapsed)
    Dec  9 07:37:36.527: INFO: expected 0 pod replicas in namespace 'pods-759', 0 are Running and Ready.
    Dec  9 07:37:36.527: INFO: POD         NODE            PHASE    GRACE  CONDITIONS
    Dec  9 07:37:36.527: INFO: test-pod-1  ip-10-0-10-179  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-12-09 07:37:36 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-12-09 07:37:36 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-12-09 07:37:36 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-12-09 07:37:36 +0000 UTC  }]
    Dec  9 07:37:36.527: INFO: test-pod-2  ip-10-0-10-179  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-12-09 07:37:36 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-12-09 07:37:36 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-12-09 07:37:36 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-12-09 07:37:36 +0000 UTC  }]
    Dec  9 07:37:36.527: INFO: test-pod-3  ip-10-0-10-179  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-12-09 07:37:36 +0000 UTC  }]
    Dec  9 07:37:36.527: INFO: 
    Dec  9 07:37:38.537: INFO: 3 / 3 pods in namespace 'pods-759' are running and ready (2 seconds elapsed)
    Dec  9 07:37:38.537: INFO: expected 0 pod replicas in namespace 'pods-759', 0 are Running and Ready.
    STEP: waiting for all pods to be deleted 12/09/22 07:37:38.554
    Dec  9 07:37:38.557: INFO: Pod quantity 3 is different from expected quantity 0
    Dec  9 07:37:39.561: INFO: Pod quantity 3 is different from expected quantity 0
    Dec  9 07:37:40.563: INFO: Pod quantity 3 is different from expected quantity 0
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:37:41.560: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-759" for this suite. 12/09/22 07:37:41.564
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-apps] ReplicaSet
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:37:41.577
Dec  9 07:37:41.577: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename replicaset 12/09/22 07:37:41.578
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:37:41.597
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:37:41.602
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111
Dec  9 07:37:41.605: INFO: Creating ReplicaSet my-hostname-basic-f7d58773-2433-4340-a46f-352289086ecb
Dec  9 07:37:41.613: INFO: Pod name my-hostname-basic-f7d58773-2433-4340-a46f-352289086ecb: Found 0 pods out of 1
Dec  9 07:37:46.618: INFO: Pod name my-hostname-basic-f7d58773-2433-4340-a46f-352289086ecb: Found 1 pods out of 1
Dec  9 07:37:46.618: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-f7d58773-2433-4340-a46f-352289086ecb" is running
Dec  9 07:37:46.618: INFO: Waiting up to 5m0s for pod "my-hostname-basic-f7d58773-2433-4340-a46f-352289086ecb-s7mzf" in namespace "replicaset-3368" to be "running"
Dec  9 07:37:46.622: INFO: Pod "my-hostname-basic-f7d58773-2433-4340-a46f-352289086ecb-s7mzf": Phase="Running", Reason="", readiness=true. Elapsed: 3.996623ms
Dec  9 07:37:46.622: INFO: Pod "my-hostname-basic-f7d58773-2433-4340-a46f-352289086ecb-s7mzf" satisfied condition "running"
Dec  9 07:37:46.622: INFO: Pod "my-hostname-basic-f7d58773-2433-4340-a46f-352289086ecb-s7mzf" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-12-09 07:37:41 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-12-09 07:37:42 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-12-09 07:37:42 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-12-09 07:37:41 +0000 UTC Reason: Message:}])
Dec  9 07:37:46.622: INFO: Trying to dial the pod
Dec  9 07:37:51.643: INFO: Controller my-hostname-basic-f7d58773-2433-4340-a46f-352289086ecb: Got expected result from replica 1 [my-hostname-basic-f7d58773-2433-4340-a46f-352289086ecb-s7mzf]: "my-hostname-basic-f7d58773-2433-4340-a46f-352289086ecb-s7mzf", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Dec  9 07:37:51.643: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-3368" for this suite. 12/09/22 07:37:51.65
------------------------------
â€¢ [SLOW TEST] [10.089 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:37:41.577
    Dec  9 07:37:41.577: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename replicaset 12/09/22 07:37:41.578
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:37:41.597
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:37:41.602
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] should serve a basic image on each replica with a public image  [Conformance]
      test/e2e/apps/replica_set.go:111
    Dec  9 07:37:41.605: INFO: Creating ReplicaSet my-hostname-basic-f7d58773-2433-4340-a46f-352289086ecb
    Dec  9 07:37:41.613: INFO: Pod name my-hostname-basic-f7d58773-2433-4340-a46f-352289086ecb: Found 0 pods out of 1
    Dec  9 07:37:46.618: INFO: Pod name my-hostname-basic-f7d58773-2433-4340-a46f-352289086ecb: Found 1 pods out of 1
    Dec  9 07:37:46.618: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-f7d58773-2433-4340-a46f-352289086ecb" is running
    Dec  9 07:37:46.618: INFO: Waiting up to 5m0s for pod "my-hostname-basic-f7d58773-2433-4340-a46f-352289086ecb-s7mzf" in namespace "replicaset-3368" to be "running"
    Dec  9 07:37:46.622: INFO: Pod "my-hostname-basic-f7d58773-2433-4340-a46f-352289086ecb-s7mzf": Phase="Running", Reason="", readiness=true. Elapsed: 3.996623ms
    Dec  9 07:37:46.622: INFO: Pod "my-hostname-basic-f7d58773-2433-4340-a46f-352289086ecb-s7mzf" satisfied condition "running"
    Dec  9 07:37:46.622: INFO: Pod "my-hostname-basic-f7d58773-2433-4340-a46f-352289086ecb-s7mzf" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-12-09 07:37:41 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-12-09 07:37:42 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-12-09 07:37:42 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-12-09 07:37:41 +0000 UTC Reason: Message:}])
    Dec  9 07:37:46.622: INFO: Trying to dial the pod
    Dec  9 07:37:51.643: INFO: Controller my-hostname-basic-f7d58773-2433-4340-a46f-352289086ecb: Got expected result from replica 1 [my-hostname-basic-f7d58773-2433-4340-a46f-352289086ecb-s7mzf]: "my-hostname-basic-f7d58773-2433-4340-a46f-352289086ecb-s7mzf", 1 of 1 required successes so far
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:37:51.643: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-3368" for this suite. 12/09/22 07:37:51.65
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods Extended Pods Set QOS Class
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  test/e2e/node/pods.go:161
[BeforeEach] [sig-node] Pods Extended
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:37:51.668
Dec  9 07:37:51.669: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename pods 12/09/22 07:37:51.67
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:37:51.729
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:37:51.741
[BeforeEach] [sig-node] Pods Extended
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Pods Set QOS Class
  test/e2e/node/pods.go:152
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  test/e2e/node/pods.go:161
STEP: creating the pod 12/09/22 07:37:51.745
STEP: submitting the pod to kubernetes 12/09/22 07:37:51.745
STEP: verifying QOS class is set on the pod 12/09/22 07:37:51.759
[AfterEach] [sig-node] Pods Extended
  test/e2e/framework/node/init/init.go:32
Dec  9 07:37:51.773: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods Extended
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods Extended
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods Extended
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-6651" for this suite. 12/09/22 07:37:51.801
------------------------------
â€¢ [0.156 seconds]
[sig-node] Pods Extended
test/e2e/node/framework.go:23
  Pods Set QOS Class
  test/e2e/node/pods.go:150
    should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
    test/e2e/node/pods.go:161

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods Extended
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:37:51.668
    Dec  9 07:37:51.669: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename pods 12/09/22 07:37:51.67
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:37:51.729
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:37:51.741
    [BeforeEach] [sig-node] Pods Extended
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Pods Set QOS Class
      test/e2e/node/pods.go:152
    [It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
      test/e2e/node/pods.go:161
    STEP: creating the pod 12/09/22 07:37:51.745
    STEP: submitting the pod to kubernetes 12/09/22 07:37:51.745
    STEP: verifying QOS class is set on the pod 12/09/22 07:37:51.759
    [AfterEach] [sig-node] Pods Extended
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:37:51.773: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods Extended
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods Extended
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods Extended
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-6651" for this suite. 12/09/22 07:37:51.801
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:194
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:37:51.825
Dec  9 07:37:51.825: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename daemonsets 12/09/22 07:37:51.826
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:37:51.85
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:37:51.856
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:194
Dec  9 07:37:51.877: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes. 12/09/22 07:37:51.882
Dec  9 07:37:51.886: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Dec  9 07:37:51.886: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Change node label to blue, check that daemon pod is launched. 12/09/22 07:37:51.886
Dec  9 07:37:51.928: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Dec  9 07:37:51.928: INFO: Node ip-10-0-10-179 is running 0 daemon pod, expected 1
Dec  9 07:37:52.931: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Dec  9 07:37:52.931: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
STEP: Update the node label to green, and wait for daemons to be unscheduled 12/09/22 07:37:52.934
Dec  9 07:37:52.950: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Dec  9 07:37:52.950: INFO: Number of running nodes: 0, number of available pods: 1 in daemonset daemon-set
Dec  9 07:37:53.954: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Dec  9 07:37:53.954: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate 12/09/22 07:37:53.954
Dec  9 07:37:53.974: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Dec  9 07:37:53.974: INFO: Node ip-10-0-10-179 is running 0 daemon pod, expected 1
Dec  9 07:37:54.979: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Dec  9 07:37:54.979: INFO: Node ip-10-0-10-179 is running 0 daemon pod, expected 1
Dec  9 07:37:55.978: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Dec  9 07:37:55.978: INFO: Node ip-10-0-10-179 is running 0 daemon pod, expected 1
Dec  9 07:37:56.981: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Dec  9 07:37:56.981: INFO: Node ip-10-0-10-179 is running 0 daemon pod, expected 1
Dec  9 07:37:57.978: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Dec  9 07:37:57.979: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 12/09/22 07:37:57.984
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-188, will wait for the garbage collector to delete the pods 12/09/22 07:37:57.984
Dec  9 07:37:58.043: INFO: Deleting DaemonSet.extensions daemon-set took: 5.593916ms
Dec  9 07:37:58.144: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.881139ms
Dec  9 07:38:00.950: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Dec  9 07:38:00.950: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Dec  9 07:38:00.952: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"15050"},"items":null}

Dec  9 07:38:00.958: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"15050"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Dec  9 07:38:01.023: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-188" for this suite. 12/09/22 07:38:01.035
------------------------------
â€¢ [SLOW TEST] [9.225 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:194

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:37:51.825
    Dec  9 07:37:51.825: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename daemonsets 12/09/22 07:37:51.826
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:37:51.85
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:37:51.856
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should run and stop complex daemon [Conformance]
      test/e2e/apps/daemon_set.go:194
    Dec  9 07:37:51.877: INFO: Creating daemon "daemon-set" with a node selector
    STEP: Initially, daemon pods should not be running on any nodes. 12/09/22 07:37:51.882
    Dec  9 07:37:51.886: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Dec  9 07:37:51.886: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    STEP: Change node label to blue, check that daemon pod is launched. 12/09/22 07:37:51.886
    Dec  9 07:37:51.928: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Dec  9 07:37:51.928: INFO: Node ip-10-0-10-179 is running 0 daemon pod, expected 1
    Dec  9 07:37:52.931: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Dec  9 07:37:52.931: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
    STEP: Update the node label to green, and wait for daemons to be unscheduled 12/09/22 07:37:52.934
    Dec  9 07:37:52.950: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Dec  9 07:37:52.950: INFO: Number of running nodes: 0, number of available pods: 1 in daemonset daemon-set
    Dec  9 07:37:53.954: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Dec  9 07:37:53.954: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate 12/09/22 07:37:53.954
    Dec  9 07:37:53.974: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Dec  9 07:37:53.974: INFO: Node ip-10-0-10-179 is running 0 daemon pod, expected 1
    Dec  9 07:37:54.979: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Dec  9 07:37:54.979: INFO: Node ip-10-0-10-179 is running 0 daemon pod, expected 1
    Dec  9 07:37:55.978: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Dec  9 07:37:55.978: INFO: Node ip-10-0-10-179 is running 0 daemon pod, expected 1
    Dec  9 07:37:56.981: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Dec  9 07:37:56.981: INFO: Node ip-10-0-10-179 is running 0 daemon pod, expected 1
    Dec  9 07:37:57.978: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Dec  9 07:37:57.979: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 12/09/22 07:37:57.984
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-188, will wait for the garbage collector to delete the pods 12/09/22 07:37:57.984
    Dec  9 07:37:58.043: INFO: Deleting DaemonSet.extensions daemon-set took: 5.593916ms
    Dec  9 07:37:58.144: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.881139ms
    Dec  9 07:38:00.950: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Dec  9 07:38:00.950: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Dec  9 07:38:00.952: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"15050"},"items":null}

    Dec  9 07:38:00.958: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"15050"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:38:01.023: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-188" for this suite. 12/09/22 07:38:01.035
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:38:01.052
Dec  9 07:38:01.052: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename deployment 12/09/22 07:38:01.053
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:38:01.128
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:38:01.144
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132
Dec  9 07:38:01.166: INFO: Pod name rollover-pod: Found 0 pods out of 1
Dec  9 07:38:06.181: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 12/09/22 07:38:06.182
Dec  9 07:38:06.182: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Dec  9 07:38:08.186: INFO: Creating deployment "test-rollover-deployment"
Dec  9 07:38:08.198: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Dec  9 07:38:10.207: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Dec  9 07:38:10.213: INFO: Ensure that both replica sets have 1 created replica
Dec  9 07:38:10.222: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Dec  9 07:38:10.235: INFO: Updating deployment test-rollover-deployment
Dec  9 07:38:10.235: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Dec  9 07:38:12.244: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Dec  9 07:38:12.252: INFO: Make sure deployment "test-rollover-deployment" is complete
Dec  9 07:38:12.260: INFO: all replica sets need to contain the pod-template-hash label
Dec  9 07:38:12.260: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.December, 9, 7, 38, 8, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 9, 7, 38, 8, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 9, 7, 38, 11, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 9, 7, 38, 8, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  9 07:38:14.267: INFO: all replica sets need to contain the pod-template-hash label
Dec  9 07:38:14.267: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.December, 9, 7, 38, 8, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 9, 7, 38, 8, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 9, 7, 38, 11, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 9, 7, 38, 8, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  9 07:38:16.332: INFO: all replica sets need to contain the pod-template-hash label
Dec  9 07:38:16.332: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.December, 9, 7, 38, 8, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 9, 7, 38, 8, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 9, 7, 38, 11, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 9, 7, 38, 8, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  9 07:38:18.284: INFO: all replica sets need to contain the pod-template-hash label
Dec  9 07:38:18.284: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.December, 9, 7, 38, 8, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 9, 7, 38, 8, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 9, 7, 38, 11, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 9, 7, 38, 8, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  9 07:38:20.268: INFO: all replica sets need to contain the pod-template-hash label
Dec  9 07:38:20.268: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.December, 9, 7, 38, 8, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 9, 7, 38, 8, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 9, 7, 38, 11, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 9, 7, 38, 8, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  9 07:38:22.266: INFO: 
Dec  9 07:38:22.266: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Dec  9 07:38:22.277: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-4357  ba039d76-228e-40d0-af09-99df3fcef436 15193 2 2022-12-09 07:38:08 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2022-12-09 07:38:10 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-09 07:38:21 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00511ffe8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2022-12-09 07:38:08 +0000 UTC,LastTransitionTime:2022-12-09 07:38:08 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-6c6df9974f" has successfully progressed.,LastUpdateTime:2022-12-09 07:38:21 +0000 UTC,LastTransitionTime:2022-12-09 07:38:08 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Dec  9 07:38:22.281: INFO: New ReplicaSet "test-rollover-deployment-6c6df9974f" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-6c6df9974f  deployment-4357  9aacd437-0653-42ed-ae14-2abf7daac056 15183 2 2022-12-09 07:38:10 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment ba039d76-228e-40d0-af09-99df3fcef436 0xc00145c557 0xc00145c558}] [] [{kube-controller-manager Update apps/v1 2022-12-09 07:38:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ba039d76-228e-40d0-af09-99df3fcef436\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-09 07:38:21 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6c6df9974f,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00145c618 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Dec  9 07:38:22.281: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Dec  9 07:38:22.281: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-4357  0817295a-9562-4fa2-921a-e1c5ce6251f5 15192 2 2022-12-09 07:38:01 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment ba039d76-228e-40d0-af09-99df3fcef436 0xc00145c3f7 0xc00145c3f8}] [] [{e2e.test Update apps/v1 2022-12-09 07:38:01 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-09 07:38:21 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ba039d76-228e-40d0-af09-99df3fcef436\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2022-12-09 07:38:21 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc00145c4e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Dec  9 07:38:22.281: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-768dcbc65b  deployment-4357  cbc9bb33-a2a8-4585-b02c-90c92f1057a8 15142 2 2022-12-09 07:38:08 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:768dcbc65b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment ba039d76-228e-40d0-af09-99df3fcef436 0xc00145c687 0xc00145c688}] [] [{kube-controller-manager Update apps/v1 2022-12-09 07:38:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ba039d76-228e-40d0-af09-99df3fcef436\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-09 07:38:10 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 768dcbc65b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:768dcbc65b] map[] [] [] []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00145c738 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Dec  9 07:38:22.284: INFO: Pod "test-rollover-deployment-6c6df9974f-tz9g2" is available:
&Pod{ObjectMeta:{test-rollover-deployment-6c6df9974f-tz9g2 test-rollover-deployment-6c6df9974f- deployment-4357  15b03771-9410-41b8-9c88-ac6be2f94d27 15162 0 2022-12-09 07:38:10 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[cni.projectcalico.org/containerID:8f2e314b96474ed5dd9a7d118d00f73dc3ade47b7c482a9da6e54bc925279c3a cni.projectcalico.org/podIP:10.2.136.199/32 cni.projectcalico.org/podIPs:10.2.136.199/32] [{apps/v1 ReplicaSet test-rollover-deployment-6c6df9974f 9aacd437-0653-42ed-ae14-2abf7daac056 0xc000d02e67 0xc000d02e68}] [] [{calico Update v1 2022-12-09 07:38:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2022-12-09 07:38:10 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9aacd437-0653-42ed-ae14-2abf7daac056\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-09 07:38:11 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.2.136.199\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zqb8n,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zqb8n,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-10-179,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 07:38:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 07:38:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 07:38:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 07:38:10 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.10.179,PodIP:10.2.136.199,StartTime:2022-12-09 07:38:10 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-12-09 07:38:11 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://48087572da98b48bd150d2a73cf0e4fd0c988b1e34fd049882d976d2838d8ac7,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.2.136.199,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Dec  9 07:38:22.284: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-4357" for this suite. 12/09/22 07:38:22.288
------------------------------
â€¢ [SLOW TEST] [21.241 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:38:01.052
    Dec  9 07:38:01.052: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename deployment 12/09/22 07:38:01.053
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:38:01.128
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:38:01.144
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should support rollover [Conformance]
      test/e2e/apps/deployment.go:132
    Dec  9 07:38:01.166: INFO: Pod name rollover-pod: Found 0 pods out of 1
    Dec  9 07:38:06.181: INFO: Pod name rollover-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 12/09/22 07:38:06.182
    Dec  9 07:38:06.182: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
    Dec  9 07:38:08.186: INFO: Creating deployment "test-rollover-deployment"
    Dec  9 07:38:08.198: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
    Dec  9 07:38:10.207: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
    Dec  9 07:38:10.213: INFO: Ensure that both replica sets have 1 created replica
    Dec  9 07:38:10.222: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
    Dec  9 07:38:10.235: INFO: Updating deployment test-rollover-deployment
    Dec  9 07:38:10.235: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
    Dec  9 07:38:12.244: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
    Dec  9 07:38:12.252: INFO: Make sure deployment "test-rollover-deployment" is complete
    Dec  9 07:38:12.260: INFO: all replica sets need to contain the pod-template-hash label
    Dec  9 07:38:12.260: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.December, 9, 7, 38, 8, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 9, 7, 38, 8, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 9, 7, 38, 11, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 9, 7, 38, 8, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Dec  9 07:38:14.267: INFO: all replica sets need to contain the pod-template-hash label
    Dec  9 07:38:14.267: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.December, 9, 7, 38, 8, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 9, 7, 38, 8, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 9, 7, 38, 11, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 9, 7, 38, 8, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Dec  9 07:38:16.332: INFO: all replica sets need to contain the pod-template-hash label
    Dec  9 07:38:16.332: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.December, 9, 7, 38, 8, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 9, 7, 38, 8, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 9, 7, 38, 11, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 9, 7, 38, 8, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Dec  9 07:38:18.284: INFO: all replica sets need to contain the pod-template-hash label
    Dec  9 07:38:18.284: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.December, 9, 7, 38, 8, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 9, 7, 38, 8, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 9, 7, 38, 11, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 9, 7, 38, 8, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Dec  9 07:38:20.268: INFO: all replica sets need to contain the pod-template-hash label
    Dec  9 07:38:20.268: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.December, 9, 7, 38, 8, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 9, 7, 38, 8, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 9, 7, 38, 11, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 9, 7, 38, 8, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Dec  9 07:38:22.266: INFO: 
    Dec  9 07:38:22.266: INFO: Ensure that both old replica sets have no replicas
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Dec  9 07:38:22.277: INFO: Deployment "test-rollover-deployment":
    &Deployment{ObjectMeta:{test-rollover-deployment  deployment-4357  ba039d76-228e-40d0-af09-99df3fcef436 15193 2 2022-12-09 07:38:08 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2022-12-09 07:38:10 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-09 07:38:21 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00511ffe8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2022-12-09 07:38:08 +0000 UTC,LastTransitionTime:2022-12-09 07:38:08 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-6c6df9974f" has successfully progressed.,LastUpdateTime:2022-12-09 07:38:21 +0000 UTC,LastTransitionTime:2022-12-09 07:38:08 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Dec  9 07:38:22.281: INFO: New ReplicaSet "test-rollover-deployment-6c6df9974f" of Deployment "test-rollover-deployment":
    &ReplicaSet{ObjectMeta:{test-rollover-deployment-6c6df9974f  deployment-4357  9aacd437-0653-42ed-ae14-2abf7daac056 15183 2 2022-12-09 07:38:10 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment ba039d76-228e-40d0-af09-99df3fcef436 0xc00145c557 0xc00145c558}] [] [{kube-controller-manager Update apps/v1 2022-12-09 07:38:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ba039d76-228e-40d0-af09-99df3fcef436\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-09 07:38:21 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6c6df9974f,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00145c618 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Dec  9 07:38:22.281: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
    Dec  9 07:38:22.281: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-4357  0817295a-9562-4fa2-921a-e1c5ce6251f5 15192 2 2022-12-09 07:38:01 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment ba039d76-228e-40d0-af09-99df3fcef436 0xc00145c3f7 0xc00145c3f8}] [] [{e2e.test Update apps/v1 2022-12-09 07:38:01 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-09 07:38:21 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ba039d76-228e-40d0-af09-99df3fcef436\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2022-12-09 07:38:21 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc00145c4e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Dec  9 07:38:22.281: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-768dcbc65b  deployment-4357  cbc9bb33-a2a8-4585-b02c-90c92f1057a8 15142 2 2022-12-09 07:38:08 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:768dcbc65b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment ba039d76-228e-40d0-af09-99df3fcef436 0xc00145c687 0xc00145c688}] [] [{kube-controller-manager Update apps/v1 2022-12-09 07:38:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ba039d76-228e-40d0-af09-99df3fcef436\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-09 07:38:10 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 768dcbc65b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:768dcbc65b] map[] [] [] []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00145c738 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Dec  9 07:38:22.284: INFO: Pod "test-rollover-deployment-6c6df9974f-tz9g2" is available:
    &Pod{ObjectMeta:{test-rollover-deployment-6c6df9974f-tz9g2 test-rollover-deployment-6c6df9974f- deployment-4357  15b03771-9410-41b8-9c88-ac6be2f94d27 15162 0 2022-12-09 07:38:10 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[cni.projectcalico.org/containerID:8f2e314b96474ed5dd9a7d118d00f73dc3ade47b7c482a9da6e54bc925279c3a cni.projectcalico.org/podIP:10.2.136.199/32 cni.projectcalico.org/podIPs:10.2.136.199/32] [{apps/v1 ReplicaSet test-rollover-deployment-6c6df9974f 9aacd437-0653-42ed-ae14-2abf7daac056 0xc000d02e67 0xc000d02e68}] [] [{calico Update v1 2022-12-09 07:38:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2022-12-09 07:38:10 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9aacd437-0653-42ed-ae14-2abf7daac056\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-09 07:38:11 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.2.136.199\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zqb8n,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zqb8n,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-10-179,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 07:38:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 07:38:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 07:38:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 07:38:10 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.10.179,PodIP:10.2.136.199,StartTime:2022-12-09 07:38:10 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-12-09 07:38:11 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://48087572da98b48bd150d2a73cf0e4fd0c988b1e34fd049882d976d2838d8ac7,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.2.136.199,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:38:22.284: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-4357" for this suite. 12/09/22 07:38:22.288
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-apps] CronJob
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:38:22.294
Dec  9 07:38:22.295: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename cronjob 12/09/22 07:38:22.295
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:38:22.319
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:38:22.325
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124
STEP: Creating a ForbidConcurrent cronjob 12/09/22 07:38:22.331
STEP: Ensuring a job is scheduled 12/09/22 07:38:22.336
STEP: Ensuring exactly one is scheduled 12/09/22 07:39:00.341
STEP: Ensuring exactly one running job exists by listing jobs explicitly 12/09/22 07:39:00.346
STEP: Ensuring no more jobs are scheduled 12/09/22 07:39:00.35
STEP: Removing cronjob 12/09/22 07:44:00.359
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Dec  9 07:44:00.370: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-5055" for this suite. 12/09/22 07:44:00.388
------------------------------
â€¢ [SLOW TEST] [338.120 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:38:22.294
    Dec  9 07:38:22.295: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename cronjob 12/09/22 07:38:22.295
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:38:22.319
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:38:22.325
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
      test/e2e/apps/cronjob.go:124
    STEP: Creating a ForbidConcurrent cronjob 12/09/22 07:38:22.331
    STEP: Ensuring a job is scheduled 12/09/22 07:38:22.336
    STEP: Ensuring exactly one is scheduled 12/09/22 07:39:00.341
    STEP: Ensuring exactly one running job exists by listing jobs explicitly 12/09/22 07:39:00.346
    STEP: Ensuring no more jobs are scheduled 12/09/22 07:39:00.35
    STEP: Removing cronjob 12/09/22 07:44:00.359
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:44:00.370: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-5055" for this suite. 12/09/22 07:44:00.388
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:44:00.419
Dec  9 07:44:00.420: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename replicaset 12/09/22 07:44:00.421
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:44:00.485
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:44:00.496
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131
STEP: Given a Pod with a 'name' label pod-adoption-release is created 12/09/22 07:44:00.501
Dec  9 07:44:00.510: INFO: Waiting up to 5m0s for pod "pod-adoption-release" in namespace "replicaset-9280" to be "running and ready"
Dec  9 07:44:00.518: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 7.251301ms
Dec  9 07:44:00.518: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
Dec  9 07:44:02.522: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011777647s
Dec  9 07:44:02.522: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
Dec  9 07:44:04.528: INFO: Pod "pod-adoption-release": Phase="Running", Reason="", readiness=true. Elapsed: 4.018022515s
Dec  9 07:44:04.528: INFO: The phase of Pod pod-adoption-release is Running (Ready = true)
Dec  9 07:44:04.528: INFO: Pod "pod-adoption-release" satisfied condition "running and ready"
STEP: When a replicaset with a matching selector is created 12/09/22 07:44:04.537
STEP: Then the orphan pod is adopted 12/09/22 07:44:04.549
STEP: When the matched label of one of its pods change 12/09/22 07:44:05.562
Dec  9 07:44:05.571: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released 12/09/22 07:44:05.586
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Dec  9 07:44:06.596: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-9280" for this suite. 12/09/22 07:44:06.6
------------------------------
â€¢ [SLOW TEST] [6.188 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:44:00.419
    Dec  9 07:44:00.420: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename replicaset 12/09/22 07:44:00.421
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:44:00.485
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:44:00.496
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] should adopt matching pods on creation and release no longer matching pods [Conformance]
      test/e2e/apps/replica_set.go:131
    STEP: Given a Pod with a 'name' label pod-adoption-release is created 12/09/22 07:44:00.501
    Dec  9 07:44:00.510: INFO: Waiting up to 5m0s for pod "pod-adoption-release" in namespace "replicaset-9280" to be "running and ready"
    Dec  9 07:44:00.518: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 7.251301ms
    Dec  9 07:44:00.518: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
    Dec  9 07:44:02.522: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011777647s
    Dec  9 07:44:02.522: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
    Dec  9 07:44:04.528: INFO: Pod "pod-adoption-release": Phase="Running", Reason="", readiness=true. Elapsed: 4.018022515s
    Dec  9 07:44:04.528: INFO: The phase of Pod pod-adoption-release is Running (Ready = true)
    Dec  9 07:44:04.528: INFO: Pod "pod-adoption-release" satisfied condition "running and ready"
    STEP: When a replicaset with a matching selector is created 12/09/22 07:44:04.537
    STEP: Then the orphan pod is adopted 12/09/22 07:44:04.549
    STEP: When the matched label of one of its pods change 12/09/22 07:44:05.562
    Dec  9 07:44:05.571: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
    STEP: Then the pod is released 12/09/22 07:44:05.586
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:44:06.596: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-9280" for this suite. 12/09/22 07:44:06.6
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:160
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:44:06.611
Dec  9 07:44:06.611: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename resourcequota 12/09/22 07:44:06.612
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:44:06.63
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:44:06.633
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:160
STEP: Discovering how many secrets are in namespace by default 12/09/22 07:44:06.637
STEP: Counting existing ResourceQuota 12/09/22 07:44:11.649
STEP: Creating a ResourceQuota 12/09/22 07:44:16.653
STEP: Ensuring resource quota status is calculated 12/09/22 07:44:16.66
STEP: Creating a Secret 12/09/22 07:44:18.666
STEP: Ensuring resource quota status captures secret creation 12/09/22 07:44:18.678
STEP: Deleting a secret 12/09/22 07:44:20.682
STEP: Ensuring resource quota status released usage 12/09/22 07:44:20.691
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Dec  9 07:44:22.695: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-599" for this suite. 12/09/22 07:44:22.699
------------------------------
â€¢ [SLOW TEST] [16.094 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:44:06.611
    Dec  9 07:44:06.611: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename resourcequota 12/09/22 07:44:06.612
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:44:06.63
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:44:06.633
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a secret. [Conformance]
      test/e2e/apimachinery/resource_quota.go:160
    STEP: Discovering how many secrets are in namespace by default 12/09/22 07:44:06.637
    STEP: Counting existing ResourceQuota 12/09/22 07:44:11.649
    STEP: Creating a ResourceQuota 12/09/22 07:44:16.653
    STEP: Ensuring resource quota status is calculated 12/09/22 07:44:16.66
    STEP: Creating a Secret 12/09/22 07:44:18.666
    STEP: Ensuring resource quota status captures secret creation 12/09/22 07:44:18.678
    STEP: Deleting a secret 12/09/22 07:44:20.682
    STEP: Ensuring resource quota status released usage 12/09/22 07:44:20.691
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:44:22.695: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-599" for this suite. 12/09/22 07:44:22.699
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:89
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:44:22.706
Dec  9 07:44:22.707: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename secrets 12/09/22 07:44:22.707
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:44:22.73
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:44:22.734
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:89
STEP: Creating secret with name secret-test-map-17ff9b1b-c649-4cf8-892d-644074a7eaea 12/09/22 07:44:22.738
STEP: Creating a pod to test consume secrets 12/09/22 07:44:22.743
Dec  9 07:44:22.751: INFO: Waiting up to 5m0s for pod "pod-secrets-6742435a-3cd2-4084-8bd4-f2162035f6ea" in namespace "secrets-3570" to be "Succeeded or Failed"
Dec  9 07:44:22.756: INFO: Pod "pod-secrets-6742435a-3cd2-4084-8bd4-f2162035f6ea": Phase="Pending", Reason="", readiness=false. Elapsed: 4.068582ms
Dec  9 07:44:24.759: INFO: Pod "pod-secrets-6742435a-3cd2-4084-8bd4-f2162035f6ea": Phase="Running", Reason="", readiness=false. Elapsed: 2.007665325s
Dec  9 07:44:26.759: INFO: Pod "pod-secrets-6742435a-3cd2-4084-8bd4-f2162035f6ea": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00767545s
STEP: Saw pod success 12/09/22 07:44:26.76
Dec  9 07:44:26.760: INFO: Pod "pod-secrets-6742435a-3cd2-4084-8bd4-f2162035f6ea" satisfied condition "Succeeded or Failed"
Dec  9 07:44:26.762: INFO: Trying to get logs from node ip-10-0-10-179 pod pod-secrets-6742435a-3cd2-4084-8bd4-f2162035f6ea container secret-volume-test: <nil>
STEP: delete the pod 12/09/22 07:44:26.775
Dec  9 07:44:26.787: INFO: Waiting for pod pod-secrets-6742435a-3cd2-4084-8bd4-f2162035f6ea to disappear
Dec  9 07:44:26.790: INFO: Pod pod-secrets-6742435a-3cd2-4084-8bd4-f2162035f6ea no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Dec  9 07:44:26.790: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-3570" for this suite. 12/09/22 07:44:26.793
------------------------------
â€¢ [4.094 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:89

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:44:22.706
    Dec  9 07:44:22.707: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename secrets 12/09/22 07:44:22.707
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:44:22.73
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:44:22.734
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:89
    STEP: Creating secret with name secret-test-map-17ff9b1b-c649-4cf8-892d-644074a7eaea 12/09/22 07:44:22.738
    STEP: Creating a pod to test consume secrets 12/09/22 07:44:22.743
    Dec  9 07:44:22.751: INFO: Waiting up to 5m0s for pod "pod-secrets-6742435a-3cd2-4084-8bd4-f2162035f6ea" in namespace "secrets-3570" to be "Succeeded or Failed"
    Dec  9 07:44:22.756: INFO: Pod "pod-secrets-6742435a-3cd2-4084-8bd4-f2162035f6ea": Phase="Pending", Reason="", readiness=false. Elapsed: 4.068582ms
    Dec  9 07:44:24.759: INFO: Pod "pod-secrets-6742435a-3cd2-4084-8bd4-f2162035f6ea": Phase="Running", Reason="", readiness=false. Elapsed: 2.007665325s
    Dec  9 07:44:26.759: INFO: Pod "pod-secrets-6742435a-3cd2-4084-8bd4-f2162035f6ea": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00767545s
    STEP: Saw pod success 12/09/22 07:44:26.76
    Dec  9 07:44:26.760: INFO: Pod "pod-secrets-6742435a-3cd2-4084-8bd4-f2162035f6ea" satisfied condition "Succeeded or Failed"
    Dec  9 07:44:26.762: INFO: Trying to get logs from node ip-10-0-10-179 pod pod-secrets-6742435a-3cd2-4084-8bd4-f2162035f6ea container secret-volume-test: <nil>
    STEP: delete the pod 12/09/22 07:44:26.775
    Dec  9 07:44:26.787: INFO: Waiting for pod pod-secrets-6742435a-3cd2-4084-8bd4-f2162035f6ea to disappear
    Dec  9 07:44:26.790: INFO: Pod pod-secrets-6742435a-3cd2-4084-8bd4-f2162035f6ea no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:44:26.790: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-3570" for this suite. 12/09/22 07:44:26.793
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod
  should have an terminated reason [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:110
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:44:26.802
Dec  9 07:44:26.802: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename kubelet-test 12/09/22 07:44:26.803
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:44:26.82
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:44:26.826
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[BeforeEach] when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:85
[It] should have an terminated reason [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:110
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Dec  9 07:44:30.846: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-7289" for this suite. 12/09/22 07:44:30.852
------------------------------
â€¢ [4.065 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:82
    should have an terminated reason [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:110

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:44:26.802
    Dec  9 07:44:26.802: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename kubelet-test 12/09/22 07:44:26.803
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:44:26.82
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:44:26.826
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [BeforeEach] when scheduling a busybox command that always fails in a pod
      test/e2e/common/node/kubelet.go:85
    [It] should have an terminated reason [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:110
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:44:30.846: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-7289" for this suite. 12/09/22 07:44:30.852
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch
  should add annotations for pods in rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1652
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:44:30.869
Dec  9 07:44:30.869: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename kubectl 12/09/22 07:44:30.87
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:44:30.957
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:44:30.963
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should add annotations for pods in rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1652
STEP: creating Agnhost RC 12/09/22 07:44:30.972
Dec  9 07:44:30.973: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=kubectl-9662 create -f -'
Dec  9 07:44:32.130: INFO: stderr: ""
Dec  9 07:44:32.130: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 12/09/22 07:44:32.13
Dec  9 07:44:33.135: INFO: Selector matched 1 pods for map[app:agnhost]
Dec  9 07:44:33.135: INFO: Found 0 / 1
Dec  9 07:44:34.135: INFO: Selector matched 1 pods for map[app:agnhost]
Dec  9 07:44:34.135: INFO: Found 1 / 1
Dec  9 07:44:34.135: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods 12/09/22 07:44:34.135
Dec  9 07:44:34.139: INFO: Selector matched 1 pods for map[app:agnhost]
Dec  9 07:44:34.139: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Dec  9 07:44:34.139: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=kubectl-9662 patch pod agnhost-primary-nc2sh -p {"metadata":{"annotations":{"x":"y"}}}'
Dec  9 07:44:34.245: INFO: stderr: ""
Dec  9 07:44:34.245: INFO: stdout: "pod/agnhost-primary-nc2sh patched\n"
STEP: checking annotations 12/09/22 07:44:34.245
Dec  9 07:44:34.248: INFO: Selector matched 1 pods for map[app:agnhost]
Dec  9 07:44:34.248: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Dec  9 07:44:34.248: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-9662" for this suite. 12/09/22 07:44:34.251
------------------------------
â€¢ [3.387 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl patch
  test/e2e/kubectl/kubectl.go:1646
    should add annotations for pods in rc  [Conformance]
    test/e2e/kubectl/kubectl.go:1652

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:44:30.869
    Dec  9 07:44:30.869: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename kubectl 12/09/22 07:44:30.87
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:44:30.957
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:44:30.963
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should add annotations for pods in rc  [Conformance]
      test/e2e/kubectl/kubectl.go:1652
    STEP: creating Agnhost RC 12/09/22 07:44:30.972
    Dec  9 07:44:30.973: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=kubectl-9662 create -f -'
    Dec  9 07:44:32.130: INFO: stderr: ""
    Dec  9 07:44:32.130: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 12/09/22 07:44:32.13
    Dec  9 07:44:33.135: INFO: Selector matched 1 pods for map[app:agnhost]
    Dec  9 07:44:33.135: INFO: Found 0 / 1
    Dec  9 07:44:34.135: INFO: Selector matched 1 pods for map[app:agnhost]
    Dec  9 07:44:34.135: INFO: Found 1 / 1
    Dec  9 07:44:34.135: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    STEP: patching all pods 12/09/22 07:44:34.135
    Dec  9 07:44:34.139: INFO: Selector matched 1 pods for map[app:agnhost]
    Dec  9 07:44:34.139: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Dec  9 07:44:34.139: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=kubectl-9662 patch pod agnhost-primary-nc2sh -p {"metadata":{"annotations":{"x":"y"}}}'
    Dec  9 07:44:34.245: INFO: stderr: ""
    Dec  9 07:44:34.245: INFO: stdout: "pod/agnhost-primary-nc2sh patched\n"
    STEP: checking annotations 12/09/22 07:44:34.245
    Dec  9 07:44:34.248: INFO: Selector matched 1 pods for map[app:agnhost]
    Dec  9 07:44:34.248: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:44:34.248: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-9662" for this suite. 12/09/22 07:44:34.251
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-cli] Kubectl client Kubectl label
  should update the label on a resource  [Conformance]
  test/e2e/kubectl/kubectl.go:1509
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:44:34.256
Dec  9 07:44:34.256: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename kubectl 12/09/22 07:44:34.258
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:44:34.276
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:44:34.281
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Kubectl label
  test/e2e/kubectl/kubectl.go:1494
STEP: creating the pod 12/09/22 07:44:34.284
Dec  9 07:44:34.285: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=kubectl-7013 create -f -'
Dec  9 07:44:34.524: INFO: stderr: ""
Dec  9 07:44:34.524: INFO: stdout: "pod/pause created\n"
Dec  9 07:44:34.524: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Dec  9 07:44:34.524: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-7013" to be "running and ready"
Dec  9 07:44:34.535: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 10.873084ms
Dec  9 07:44:34.535: INFO: Error evaluating pod condition running and ready: want pod 'pause' on 'ip-10-0-10-179' to be 'Running' but was 'Pending'
Dec  9 07:44:36.539: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.014619272s
Dec  9 07:44:36.539: INFO: Pod "pause" satisfied condition "running and ready"
Dec  9 07:44:36.539: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  test/e2e/kubectl/kubectl.go:1509
STEP: adding the label testing-label with value testing-label-value to a pod 12/09/22 07:44:36.539
Dec  9 07:44:36.539: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=kubectl-7013 label pods pause testing-label=testing-label-value'
Dec  9 07:44:36.666: INFO: stderr: ""
Dec  9 07:44:36.666: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value 12/09/22 07:44:36.666
Dec  9 07:44:36.666: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=kubectl-7013 get pod pause -L testing-label'
Dec  9 07:44:36.761: INFO: stderr: ""
Dec  9 07:44:36.761: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
STEP: removing the label testing-label of a pod 12/09/22 07:44:36.761
Dec  9 07:44:36.761: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=kubectl-7013 label pods pause testing-label-'
Dec  9 07:44:36.854: INFO: stderr: ""
Dec  9 07:44:36.855: INFO: stdout: "pod/pause unlabeled\n"
STEP: verifying the pod doesn't have the label testing-label 12/09/22 07:44:36.855
Dec  9 07:44:36.855: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=kubectl-7013 get pod pause -L testing-label'
Dec  9 07:44:36.927: INFO: stderr: ""
Dec  9 07:44:36.927: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    \n"
[AfterEach] Kubectl label
  test/e2e/kubectl/kubectl.go:1500
STEP: using delete to clean up resources 12/09/22 07:44:36.927
Dec  9 07:44:36.927: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=kubectl-7013 delete --grace-period=0 --force -f -'
Dec  9 07:44:37.005: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec  9 07:44:37.005: INFO: stdout: "pod \"pause\" force deleted\n"
Dec  9 07:44:37.005: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=kubectl-7013 get rc,svc -l name=pause --no-headers'
Dec  9 07:44:37.082: INFO: stderr: "No resources found in kubectl-7013 namespace.\n"
Dec  9 07:44:37.082: INFO: stdout: ""
Dec  9 07:44:37.082: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=kubectl-7013 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Dec  9 07:44:37.147: INFO: stderr: ""
Dec  9 07:44:37.147: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Dec  9 07:44:37.147: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-7013" for this suite. 12/09/22 07:44:37.151
------------------------------
â€¢ [2.900 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl label
  test/e2e/kubectl/kubectl.go:1492
    should update the label on a resource  [Conformance]
    test/e2e/kubectl/kubectl.go:1509

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:44:34.256
    Dec  9 07:44:34.256: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename kubectl 12/09/22 07:44:34.258
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:44:34.276
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:44:34.281
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Kubectl label
      test/e2e/kubectl/kubectl.go:1494
    STEP: creating the pod 12/09/22 07:44:34.284
    Dec  9 07:44:34.285: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=kubectl-7013 create -f -'
    Dec  9 07:44:34.524: INFO: stderr: ""
    Dec  9 07:44:34.524: INFO: stdout: "pod/pause created\n"
    Dec  9 07:44:34.524: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
    Dec  9 07:44:34.524: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-7013" to be "running and ready"
    Dec  9 07:44:34.535: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 10.873084ms
    Dec  9 07:44:34.535: INFO: Error evaluating pod condition running and ready: want pod 'pause' on 'ip-10-0-10-179' to be 'Running' but was 'Pending'
    Dec  9 07:44:36.539: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.014619272s
    Dec  9 07:44:36.539: INFO: Pod "pause" satisfied condition "running and ready"
    Dec  9 07:44:36.539: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
    [It] should update the label on a resource  [Conformance]
      test/e2e/kubectl/kubectl.go:1509
    STEP: adding the label testing-label with value testing-label-value to a pod 12/09/22 07:44:36.539
    Dec  9 07:44:36.539: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=kubectl-7013 label pods pause testing-label=testing-label-value'
    Dec  9 07:44:36.666: INFO: stderr: ""
    Dec  9 07:44:36.666: INFO: stdout: "pod/pause labeled\n"
    STEP: verifying the pod has the label testing-label with the value testing-label-value 12/09/22 07:44:36.666
    Dec  9 07:44:36.666: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=kubectl-7013 get pod pause -L testing-label'
    Dec  9 07:44:36.761: INFO: stderr: ""
    Dec  9 07:44:36.761: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
    STEP: removing the label testing-label of a pod 12/09/22 07:44:36.761
    Dec  9 07:44:36.761: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=kubectl-7013 label pods pause testing-label-'
    Dec  9 07:44:36.854: INFO: stderr: ""
    Dec  9 07:44:36.855: INFO: stdout: "pod/pause unlabeled\n"
    STEP: verifying the pod doesn't have the label testing-label 12/09/22 07:44:36.855
    Dec  9 07:44:36.855: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=kubectl-7013 get pod pause -L testing-label'
    Dec  9 07:44:36.927: INFO: stderr: ""
    Dec  9 07:44:36.927: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    \n"
    [AfterEach] Kubectl label
      test/e2e/kubectl/kubectl.go:1500
    STEP: using delete to clean up resources 12/09/22 07:44:36.927
    Dec  9 07:44:36.927: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=kubectl-7013 delete --grace-period=0 --force -f -'
    Dec  9 07:44:37.005: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Dec  9 07:44:37.005: INFO: stdout: "pod \"pause\" force deleted\n"
    Dec  9 07:44:37.005: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=kubectl-7013 get rc,svc -l name=pause --no-headers'
    Dec  9 07:44:37.082: INFO: stderr: "No resources found in kubectl-7013 namespace.\n"
    Dec  9 07:44:37.082: INFO: stdout: ""
    Dec  9 07:44:37.082: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=kubectl-7013 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Dec  9 07:44:37.147: INFO: stderr: ""
    Dec  9 07:44:37.147: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:44:37.147: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-7013" for this suite. 12/09/22 07:44:37.151
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:137
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:44:37.158
Dec  9 07:44:37.159: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename emptydir 12/09/22 07:44:37.159
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:44:37.176
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:44:37.18
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:137
STEP: Creating a pod to test emptydir 0666 on tmpfs 12/09/22 07:44:37.184
Dec  9 07:44:37.192: INFO: Waiting up to 5m0s for pod "pod-11f2769d-7199-483f-8802-7ef5a312cea9" in namespace "emptydir-1029" to be "Succeeded or Failed"
Dec  9 07:44:37.195: INFO: Pod "pod-11f2769d-7199-483f-8802-7ef5a312cea9": Phase="Pending", Reason="", readiness=false. Elapsed: 3.485969ms
Dec  9 07:44:39.199: INFO: Pod "pod-11f2769d-7199-483f-8802-7ef5a312cea9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007863434s
Dec  9 07:44:41.211: INFO: Pod "pod-11f2769d-7199-483f-8802-7ef5a312cea9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019634534s
STEP: Saw pod success 12/09/22 07:44:41.211
Dec  9 07:44:41.211: INFO: Pod "pod-11f2769d-7199-483f-8802-7ef5a312cea9" satisfied condition "Succeeded or Failed"
Dec  9 07:44:41.230: INFO: Trying to get logs from node ip-10-0-10-179 pod pod-11f2769d-7199-483f-8802-7ef5a312cea9 container test-container: <nil>
STEP: delete the pod 12/09/22 07:44:41.253
Dec  9 07:44:41.289: INFO: Waiting for pod pod-11f2769d-7199-483f-8802-7ef5a312cea9 to disappear
Dec  9 07:44:41.298: INFO: Pod pod-11f2769d-7199-483f-8802-7ef5a312cea9 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Dec  9 07:44:41.298: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-1029" for this suite. 12/09/22 07:44:41.309
------------------------------
â€¢ [4.158 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:137

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:44:37.158
    Dec  9 07:44:37.159: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename emptydir 12/09/22 07:44:37.159
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:44:37.176
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:44:37.18
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:137
    STEP: Creating a pod to test emptydir 0666 on tmpfs 12/09/22 07:44:37.184
    Dec  9 07:44:37.192: INFO: Waiting up to 5m0s for pod "pod-11f2769d-7199-483f-8802-7ef5a312cea9" in namespace "emptydir-1029" to be "Succeeded or Failed"
    Dec  9 07:44:37.195: INFO: Pod "pod-11f2769d-7199-483f-8802-7ef5a312cea9": Phase="Pending", Reason="", readiness=false. Elapsed: 3.485969ms
    Dec  9 07:44:39.199: INFO: Pod "pod-11f2769d-7199-483f-8802-7ef5a312cea9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007863434s
    Dec  9 07:44:41.211: INFO: Pod "pod-11f2769d-7199-483f-8802-7ef5a312cea9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019634534s
    STEP: Saw pod success 12/09/22 07:44:41.211
    Dec  9 07:44:41.211: INFO: Pod "pod-11f2769d-7199-483f-8802-7ef5a312cea9" satisfied condition "Succeeded or Failed"
    Dec  9 07:44:41.230: INFO: Trying to get logs from node ip-10-0-10-179 pod pod-11f2769d-7199-483f-8802-7ef5a312cea9 container test-container: <nil>
    STEP: delete the pod 12/09/22 07:44:41.253
    Dec  9 07:44:41.289: INFO: Waiting for pod pod-11f2769d-7199-483f-8802-7ef5a312cea9 to disappear
    Dec  9 07:44:41.298: INFO: Pod pod-11f2769d-7199-483f-8802-7ef5a312cea9 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:44:41.298: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-1029" for this suite. 12/09/22 07:44:41.309
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:117
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:44:41.317
Dec  9 07:44:41.317: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename webhook 12/09/22 07:44:41.318
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:44:41.339
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:44:41.344
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 12/09/22 07:44:41.367
STEP: Create role binding to let webhook read extension-apiserver-authentication 12/09/22 07:44:41.941
STEP: Deploying the webhook pod 12/09/22 07:44:41.952
STEP: Wait for the deployment to be ready 12/09/22 07:44:41.981
Dec  9 07:44:42.016: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Dec  9 07:44:44.027: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.December, 9, 7, 44, 42, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 9, 7, 44, 42, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 9, 7, 44, 42, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 9, 7, 44, 42, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 12/09/22 07:44:46.03
STEP: Verifying the service has paired with the endpoint 12/09/22 07:44:46.04
Dec  9 07:44:47.041: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:117
STEP: fetching the /apis discovery document 12/09/22 07:44:47.045
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document 12/09/22 07:44:47.047
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document 12/09/22 07:44:47.047
STEP: fetching the /apis/admissionregistration.k8s.io discovery document 12/09/22 07:44:47.047
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document 12/09/22 07:44:47.049
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document 12/09/22 07:44:47.049
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document 12/09/22 07:44:47.051
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Dec  9 07:44:47.051: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-1286" for this suite. 12/09/22 07:44:47.099
STEP: Destroying namespace "webhook-1286-markers" for this suite. 12/09/22 07:44:47.108
------------------------------
â€¢ [SLOW TEST] [5.805 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:117

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:44:41.317
    Dec  9 07:44:41.317: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename webhook 12/09/22 07:44:41.318
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:44:41.339
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:44:41.344
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 12/09/22 07:44:41.367
    STEP: Create role binding to let webhook read extension-apiserver-authentication 12/09/22 07:44:41.941
    STEP: Deploying the webhook pod 12/09/22 07:44:41.952
    STEP: Wait for the deployment to be ready 12/09/22 07:44:41.981
    Dec  9 07:44:42.016: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    Dec  9 07:44:44.027: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.December, 9, 7, 44, 42, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 9, 7, 44, 42, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 9, 7, 44, 42, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 9, 7, 44, 42, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 12/09/22 07:44:46.03
    STEP: Verifying the service has paired with the endpoint 12/09/22 07:44:46.04
    Dec  9 07:44:47.041: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should include webhook resources in discovery documents [Conformance]
      test/e2e/apimachinery/webhook.go:117
    STEP: fetching the /apis discovery document 12/09/22 07:44:47.045
    STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document 12/09/22 07:44:47.047
    STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document 12/09/22 07:44:47.047
    STEP: fetching the /apis/admissionregistration.k8s.io discovery document 12/09/22 07:44:47.047
    STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document 12/09/22 07:44:47.049
    STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document 12/09/22 07:44:47.049
    STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document 12/09/22 07:44:47.051
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:44:47.051: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-1286" for this suite. 12/09/22 07:44:47.099
    STEP: Destroying namespace "webhook-1286-markers" for this suite. 12/09/22 07:44:47.108
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] server version
  should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39
[BeforeEach] [sig-api-machinery] server version
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:44:47.126
Dec  9 07:44:47.126: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename server-version 12/09/22 07:44:47.128
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:44:47.146
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:44:47.151
[BeforeEach] [sig-api-machinery] server version
  test/e2e/framework/metrics/init/init.go:31
[It] should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39
STEP: Request ServerVersion 12/09/22 07:44:47.155
STEP: Confirm major version 12/09/22 07:44:47.157
Dec  9 07:44:47.157: INFO: Major version: 1
STEP: Confirm minor version 12/09/22 07:44:47.157
Dec  9 07:44:47.158: INFO: cleanMinorVersion: 26
Dec  9 07:44:47.158: INFO: Minor version: 26
[AfterEach] [sig-api-machinery] server version
  test/e2e/framework/node/init/init.go:32
Dec  9 07:44:47.159: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] server version
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] server version
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] server version
  tear down framework | framework.go:193
STEP: Destroying namespace "server-version-642" for this suite. 12/09/22 07:44:47.164
------------------------------
â€¢ [0.044 seconds]
[sig-api-machinery] server version
test/e2e/apimachinery/framework.go:23
  should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] server version
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:44:47.126
    Dec  9 07:44:47.126: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename server-version 12/09/22 07:44:47.128
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:44:47.146
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:44:47.151
    [BeforeEach] [sig-api-machinery] server version
      test/e2e/framework/metrics/init/init.go:31
    [It] should find the server version [Conformance]
      test/e2e/apimachinery/server_version.go:39
    STEP: Request ServerVersion 12/09/22 07:44:47.155
    STEP: Confirm major version 12/09/22 07:44:47.157
    Dec  9 07:44:47.157: INFO: Major version: 1
    STEP: Confirm minor version 12/09/22 07:44:47.157
    Dec  9 07:44:47.158: INFO: cleanMinorVersion: 26
    Dec  9 07:44:47.158: INFO: Minor version: 26
    [AfterEach] [sig-api-machinery] server version
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:44:47.159: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] server version
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] server version
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] server version
      tear down framework | framework.go:193
    STEP: Destroying namespace "server-version-642" for this suite. 12/09/22 07:44:47.164
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:366
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:44:47.195
Dec  9 07:44:47.196: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename job 12/09/22 07:44:47.197
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:44:47.231
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:44:47.238
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:366
STEP: Creating Indexed job 12/09/22 07:44:47.242
STEP: Ensuring job reaches completions 12/09/22 07:44:47.247
STEP: Ensuring pods with index for job exist 12/09/22 07:44:55.252
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Dec  9 07:44:55.255: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-6797" for this suite. 12/09/22 07:44:55.259
------------------------------
â€¢ [SLOW TEST] [8.070 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:366

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:44:47.195
    Dec  9 07:44:47.196: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename job 12/09/22 07:44:47.197
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:44:47.231
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:44:47.238
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
      test/e2e/apps/job.go:366
    STEP: Creating Indexed job 12/09/22 07:44:47.242
    STEP: Ensuring job reaches completions 12/09/22 07:44:47.247
    STEP: Ensuring pods with index for job exist 12/09/22 07:44:55.252
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:44:55.255: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-6797" for this suite. 12/09/22 07:44:55.259
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:216
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:44:55.271
Dec  9 07:44:55.271: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename container-runtime 12/09/22 07:44:55.272
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:44:55.287
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:44:55.291
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:216
STEP: create the container 12/09/22 07:44:55.295
STEP: wait for the container to reach Failed 12/09/22 07:44:55.303
STEP: get the container status 12/09/22 07:44:59.327
STEP: the container should be terminated 12/09/22 07:44:59.33
STEP: the termination message should be set 12/09/22 07:44:59.33
Dec  9 07:44:59.330: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container 12/09/22 07:44:59.33
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Dec  9 07:44:59.341: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-855" for this suite. 12/09/22 07:44:59.345
------------------------------
â€¢ [4.081 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    on terminated container
    test/e2e/common/node/runtime.go:137
      should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:216

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:44:55.271
    Dec  9 07:44:55.271: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename container-runtime 12/09/22 07:44:55.272
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:44:55.287
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:44:55.291
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:216
    STEP: create the container 12/09/22 07:44:55.295
    STEP: wait for the container to reach Failed 12/09/22 07:44:55.303
    STEP: get the container status 12/09/22 07:44:59.327
    STEP: the container should be terminated 12/09/22 07:44:59.33
    STEP: the termination message should be set 12/09/22 07:44:59.33
    Dec  9 07:44:59.330: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
    STEP: delete the container 12/09/22 07:44:59.33
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:44:59.341: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-855" for this suite. 12/09/22 07:44:59.345
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers
  should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:59
[BeforeEach] [sig-node] Containers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:44:59.354
Dec  9 07:44:59.354: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename containers 12/09/22 07:44:59.355
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:44:59.405
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:44:59.42
[BeforeEach] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:59
STEP: Creating a pod to test override arguments 12/09/22 07:44:59.425
Dec  9 07:44:59.438: INFO: Waiting up to 5m0s for pod "client-containers-3d5f4e87-af03-4e1e-bd86-1a6dbd33fa57" in namespace "containers-2456" to be "Succeeded or Failed"
Dec  9 07:44:59.452: INFO: Pod "client-containers-3d5f4e87-af03-4e1e-bd86-1a6dbd33fa57": Phase="Pending", Reason="", readiness=false. Elapsed: 13.645112ms
Dec  9 07:45:01.457: INFO: Pod "client-containers-3d5f4e87-af03-4e1e-bd86-1a6dbd33fa57": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018912202s
Dec  9 07:45:03.456: INFO: Pod "client-containers-3d5f4e87-af03-4e1e-bd86-1a6dbd33fa57": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018263484s
STEP: Saw pod success 12/09/22 07:45:03.456
Dec  9 07:45:03.456: INFO: Pod "client-containers-3d5f4e87-af03-4e1e-bd86-1a6dbd33fa57" satisfied condition "Succeeded or Failed"
Dec  9 07:45:03.460: INFO: Trying to get logs from node ip-10-0-10-179 pod client-containers-3d5f4e87-af03-4e1e-bd86-1a6dbd33fa57 container agnhost-container: <nil>
STEP: delete the pod 12/09/22 07:45:03.485
Dec  9 07:45:03.512: INFO: Waiting for pod client-containers-3d5f4e87-af03-4e1e-bd86-1a6dbd33fa57 to disappear
Dec  9 07:45:03.518: INFO: Pod client-containers-3d5f4e87-af03-4e1e-bd86-1a6dbd33fa57 no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/node/init/init.go:32
Dec  9 07:45:03.519: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Containers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Containers
  tear down framework | framework.go:193
STEP: Destroying namespace "containers-2456" for this suite. 12/09/22 07:45:03.527
------------------------------
â€¢ [4.192 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:59

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:44:59.354
    Dec  9 07:44:59.354: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename containers 12/09/22 07:44:59.355
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:44:59.405
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:44:59.42
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:59
    STEP: Creating a pod to test override arguments 12/09/22 07:44:59.425
    Dec  9 07:44:59.438: INFO: Waiting up to 5m0s for pod "client-containers-3d5f4e87-af03-4e1e-bd86-1a6dbd33fa57" in namespace "containers-2456" to be "Succeeded or Failed"
    Dec  9 07:44:59.452: INFO: Pod "client-containers-3d5f4e87-af03-4e1e-bd86-1a6dbd33fa57": Phase="Pending", Reason="", readiness=false. Elapsed: 13.645112ms
    Dec  9 07:45:01.457: INFO: Pod "client-containers-3d5f4e87-af03-4e1e-bd86-1a6dbd33fa57": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018912202s
    Dec  9 07:45:03.456: INFO: Pod "client-containers-3d5f4e87-af03-4e1e-bd86-1a6dbd33fa57": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018263484s
    STEP: Saw pod success 12/09/22 07:45:03.456
    Dec  9 07:45:03.456: INFO: Pod "client-containers-3d5f4e87-af03-4e1e-bd86-1a6dbd33fa57" satisfied condition "Succeeded or Failed"
    Dec  9 07:45:03.460: INFO: Trying to get logs from node ip-10-0-10-179 pod client-containers-3d5f4e87-af03-4e1e-bd86-1a6dbd33fa57 container agnhost-container: <nil>
    STEP: delete the pod 12/09/22 07:45:03.485
    Dec  9 07:45:03.512: INFO: Waiting for pod client-containers-3d5f4e87-af03-4e1e-bd86-1a6dbd33fa57 to disappear
    Dec  9 07:45:03.518: INFO: Pod client-containers-3d5f4e87-af03-4e1e-bd86-1a6dbd33fa57 no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:45:03.519: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Containers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Containers
      tear down framework | framework.go:193
    STEP: Destroying namespace "containers-2456" for this suite. 12/09/22 07:45:03.527
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info
  should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  test/e2e/kubectl/kubectl.go:1250
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:45:03.547
Dec  9 07:45:03.547: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename kubectl 12/09/22 07:45:03.549
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:45:03.651
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:45:03.677
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  test/e2e/kubectl/kubectl.go:1250
STEP: validating cluster-info 12/09/22 07:45:03.683
Dec  9 07:45:03.683: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=kubectl-8195 cluster-info'
Dec  9 07:45:03.794: INFO: stderr: ""
Dec  9 07:45:03.794: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://10.3.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Dec  9 07:45:03.795: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-8195" for this suite. 12/09/22 07:45:03.799
------------------------------
â€¢ [0.266 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl cluster-info
  test/e2e/kubectl/kubectl.go:1244
    should check if Kubernetes control plane services is included in cluster-info  [Conformance]
    test/e2e/kubectl/kubectl.go:1250

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:45:03.547
    Dec  9 07:45:03.547: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename kubectl 12/09/22 07:45:03.549
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:45:03.651
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:45:03.677
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
      test/e2e/kubectl/kubectl.go:1250
    STEP: validating cluster-info 12/09/22 07:45:03.683
    Dec  9 07:45:03.683: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=kubectl-8195 cluster-info'
    Dec  9 07:45:03.794: INFO: stderr: ""
    Dec  9 07:45:03.794: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://10.3.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:45:03.795: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-8195" for this suite. 12/09/22 07:45:03.799
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] ConfigMap
  binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:175
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:45:03.814
Dec  9 07:45:03.814: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename configmap 12/09/22 07:45:03.815
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:45:03.861
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:45:03.872
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:175
STEP: Creating configMap with name configmap-test-upd-d4b36cf6-98cc-4721-a54d-d2dc624679dc 12/09/22 07:45:03.884
STEP: Creating the pod 12/09/22 07:45:03.895
Dec  9 07:45:03.921: INFO: Waiting up to 5m0s for pod "pod-configmaps-66398635-ae75-41df-a4b0-3981d632973c" in namespace "configmap-8260" to be "running"
Dec  9 07:45:03.932: INFO: Pod "pod-configmaps-66398635-ae75-41df-a4b0-3981d632973c": Phase="Pending", Reason="", readiness=false. Elapsed: 10.379421ms
Dec  9 07:45:05.936: INFO: Pod "pod-configmaps-66398635-ae75-41df-a4b0-3981d632973c": Phase="Running", Reason="", readiness=false. Elapsed: 2.014373141s
Dec  9 07:45:05.936: INFO: Pod "pod-configmaps-66398635-ae75-41df-a4b0-3981d632973c" satisfied condition "running"
STEP: Waiting for pod with text data 12/09/22 07:45:05.936
STEP: Waiting for pod with binary data 12/09/22 07:45:05.941
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Dec  9 07:45:05.946: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-8260" for this suite. 12/09/22 07:45:05.95
------------------------------
â€¢ [2.143 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:175

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:45:03.814
    Dec  9 07:45:03.814: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename configmap 12/09/22 07:45:03.815
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:45:03.861
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:45:03.872
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] binary data should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:175
    STEP: Creating configMap with name configmap-test-upd-d4b36cf6-98cc-4721-a54d-d2dc624679dc 12/09/22 07:45:03.884
    STEP: Creating the pod 12/09/22 07:45:03.895
    Dec  9 07:45:03.921: INFO: Waiting up to 5m0s for pod "pod-configmaps-66398635-ae75-41df-a4b0-3981d632973c" in namespace "configmap-8260" to be "running"
    Dec  9 07:45:03.932: INFO: Pod "pod-configmaps-66398635-ae75-41df-a4b0-3981d632973c": Phase="Pending", Reason="", readiness=false. Elapsed: 10.379421ms
    Dec  9 07:45:05.936: INFO: Pod "pod-configmaps-66398635-ae75-41df-a4b0-3981d632973c": Phase="Running", Reason="", readiness=false. Elapsed: 2.014373141s
    Dec  9 07:45:05.936: INFO: Pod "pod-configmaps-66398635-ae75-41df-a4b0-3981d632973c" satisfied condition "running"
    STEP: Waiting for pod with text data 12/09/22 07:45:05.936
    STEP: Waiting for pod with binary data 12/09/22 07:45:05.941
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:45:05.946: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-8260" for this suite. 12/09/22 07:45:05.95
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] Garbage collector
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:45:05.958
Dec  9 07:45:05.959: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename gc 12/09/22 07:45:05.961
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:45:05.981
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:45:05.996
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650
STEP: create the rc 12/09/22 07:45:06.03
STEP: delete the rc 12/09/22 07:45:11.049
STEP: wait for the rc to be deleted 12/09/22 07:45:11.066
Dec  9 07:45:12.113: INFO: 80 pods remaining
Dec  9 07:45:12.113: INFO: 80 pods has nil DeletionTimestamp
Dec  9 07:45:12.113: INFO: 
Dec  9 07:45:13.146: INFO: 72 pods remaining
Dec  9 07:45:13.155: INFO: 72 pods has nil DeletionTimestamp
Dec  9 07:45:13.155: INFO: 
Dec  9 07:45:14.109: INFO: 60 pods remaining
Dec  9 07:45:14.109: INFO: 60 pods has nil DeletionTimestamp
Dec  9 07:45:14.109: INFO: 
Dec  9 07:45:15.116: INFO: 40 pods remaining
Dec  9 07:45:15.116: INFO: 40 pods has nil DeletionTimestamp
Dec  9 07:45:15.116: INFO: 
Dec  9 07:45:16.086: INFO: 32 pods remaining
Dec  9 07:45:16.087: INFO: 32 pods has nil DeletionTimestamp
Dec  9 07:45:16.087: INFO: 
Dec  9 07:45:17.110: INFO: 20 pods remaining
Dec  9 07:45:17.110: INFO: 20 pods has nil DeletionTimestamp
Dec  9 07:45:17.110: INFO: 
STEP: Gathering metrics 12/09/22 07:45:18.125
Dec  9 07:45:18.310: INFO: Waiting up to 5m0s for pod "kube-controller-manager-ip-10-0-7-84" in namespace "kube-system" to be "running and ready"
Dec  9 07:45:18.336: INFO: Pod "kube-controller-manager-ip-10-0-7-84": Phase="Running", Reason="", readiness=true. Elapsed: 26.02938ms
Dec  9 07:45:18.336: INFO: The phase of Pod kube-controller-manager-ip-10-0-7-84 is Running (Ready = true)
Dec  9 07:45:18.336: INFO: Pod "kube-controller-manager-ip-10-0-7-84" satisfied condition "running and ready"
Dec  9 07:45:18.933: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Dec  9 07:45:18.943: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-965" for this suite. 12/09/22 07:45:18.969
------------------------------
â€¢ [SLOW TEST] [13.070 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:45:05.958
    Dec  9 07:45:05.959: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename gc 12/09/22 07:45:05.961
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:45:05.981
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:45:05.996
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
      test/e2e/apimachinery/garbage_collector.go:650
    STEP: create the rc 12/09/22 07:45:06.03
    STEP: delete the rc 12/09/22 07:45:11.049
    STEP: wait for the rc to be deleted 12/09/22 07:45:11.066
    Dec  9 07:45:12.113: INFO: 80 pods remaining
    Dec  9 07:45:12.113: INFO: 80 pods has nil DeletionTimestamp
    Dec  9 07:45:12.113: INFO: 
    Dec  9 07:45:13.146: INFO: 72 pods remaining
    Dec  9 07:45:13.155: INFO: 72 pods has nil DeletionTimestamp
    Dec  9 07:45:13.155: INFO: 
    Dec  9 07:45:14.109: INFO: 60 pods remaining
    Dec  9 07:45:14.109: INFO: 60 pods has nil DeletionTimestamp
    Dec  9 07:45:14.109: INFO: 
    Dec  9 07:45:15.116: INFO: 40 pods remaining
    Dec  9 07:45:15.116: INFO: 40 pods has nil DeletionTimestamp
    Dec  9 07:45:15.116: INFO: 
    Dec  9 07:45:16.086: INFO: 32 pods remaining
    Dec  9 07:45:16.087: INFO: 32 pods has nil DeletionTimestamp
    Dec  9 07:45:16.087: INFO: 
    Dec  9 07:45:17.110: INFO: 20 pods remaining
    Dec  9 07:45:17.110: INFO: 20 pods has nil DeletionTimestamp
    Dec  9 07:45:17.110: INFO: 
    STEP: Gathering metrics 12/09/22 07:45:18.125
    Dec  9 07:45:18.310: INFO: Waiting up to 5m0s for pod "kube-controller-manager-ip-10-0-7-84" in namespace "kube-system" to be "running and ready"
    Dec  9 07:45:18.336: INFO: Pod "kube-controller-manager-ip-10-0-7-84": Phase="Running", Reason="", readiness=true. Elapsed: 26.02938ms
    Dec  9 07:45:18.336: INFO: The phase of Pod kube-controller-manager-ip-10-0-7-84 is Running (Ready = true)
    Dec  9 07:45:18.336: INFO: Pod "kube-controller-manager-ip-10-0-7-84" satisfied condition "running and ready"
    Dec  9 07:45:18.933: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:45:18.943: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-965" for this suite. 12/09/22 07:45:18.969
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:45:19.21
Dec  9 07:45:19.210: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename dns 12/09/22 07:45:19.215
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:45:19.378
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:45:19.41
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
 12/09/22 07:45:19.42
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
 12/09/22 07:45:19.42
STEP: creating a pod to probe DNS 12/09/22 07:45:19.42
STEP: submitting the pod to kubernetes 12/09/22 07:45:19.42
Dec  9 07:45:19.440: INFO: Waiting up to 15m0s for pod "dns-test-9a575233-25ed-4ac1-a5e1-1e587fb89e00" in namespace "dns-8531" to be "running"
Dec  9 07:45:19.448: INFO: Pod "dns-test-9a575233-25ed-4ac1-a5e1-1e587fb89e00": Phase="Pending", Reason="", readiness=false. Elapsed: 8.09981ms
Dec  9 07:45:21.452: INFO: Pod "dns-test-9a575233-25ed-4ac1-a5e1-1e587fb89e00": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011969102s
Dec  9 07:45:23.452: INFO: Pod "dns-test-9a575233-25ed-4ac1-a5e1-1e587fb89e00": Phase="Pending", Reason="", readiness=false. Elapsed: 4.011683111s
Dec  9 07:45:25.463: INFO: Pod "dns-test-9a575233-25ed-4ac1-a5e1-1e587fb89e00": Phase="Pending", Reason="", readiness=false. Elapsed: 6.02277021s
Dec  9 07:45:27.452: INFO: Pod "dns-test-9a575233-25ed-4ac1-a5e1-1e587fb89e00": Phase="Pending", Reason="", readiness=false. Elapsed: 8.011751391s
Dec  9 07:45:29.453: INFO: Pod "dns-test-9a575233-25ed-4ac1-a5e1-1e587fb89e00": Phase="Pending", Reason="", readiness=false. Elapsed: 10.01340829s
Dec  9 07:45:31.452: INFO: Pod "dns-test-9a575233-25ed-4ac1-a5e1-1e587fb89e00": Phase="Pending", Reason="", readiness=false. Elapsed: 12.012120796s
Dec  9 07:45:33.454: INFO: Pod "dns-test-9a575233-25ed-4ac1-a5e1-1e587fb89e00": Phase="Pending", Reason="", readiness=false. Elapsed: 14.013543191s
Dec  9 07:45:35.452: INFO: Pod "dns-test-9a575233-25ed-4ac1-a5e1-1e587fb89e00": Phase="Pending", Reason="", readiness=false. Elapsed: 16.011712416s
Dec  9 07:45:37.454: INFO: Pod "dns-test-9a575233-25ed-4ac1-a5e1-1e587fb89e00": Phase="Running", Reason="", readiness=true. Elapsed: 18.014054259s
Dec  9 07:45:37.454: INFO: Pod "dns-test-9a575233-25ed-4ac1-a5e1-1e587fb89e00" satisfied condition "running"
STEP: retrieving the pod 12/09/22 07:45:37.454
STEP: looking for the results for each expected name from probers 12/09/22 07:45:37.468
Dec  9 07:45:37.483: INFO: DNS probes using dns-8531/dns-test-9a575233-25ed-4ac1-a5e1-1e587fb89e00 succeeded

STEP: deleting the pod 12/09/22 07:45:37.483
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Dec  9 07:45:37.503: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-8531" for this suite. 12/09/22 07:45:37.508
------------------------------
â€¢ [SLOW TEST] [18.303 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:45:19.21
    Dec  9 07:45:19.210: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename dns 12/09/22 07:45:19.215
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:45:19.378
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:45:19.41
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for the cluster  [Conformance]
      test/e2e/network/dns.go:50
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
     12/09/22 07:45:19.42
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
     12/09/22 07:45:19.42
    STEP: creating a pod to probe DNS 12/09/22 07:45:19.42
    STEP: submitting the pod to kubernetes 12/09/22 07:45:19.42
    Dec  9 07:45:19.440: INFO: Waiting up to 15m0s for pod "dns-test-9a575233-25ed-4ac1-a5e1-1e587fb89e00" in namespace "dns-8531" to be "running"
    Dec  9 07:45:19.448: INFO: Pod "dns-test-9a575233-25ed-4ac1-a5e1-1e587fb89e00": Phase="Pending", Reason="", readiness=false. Elapsed: 8.09981ms
    Dec  9 07:45:21.452: INFO: Pod "dns-test-9a575233-25ed-4ac1-a5e1-1e587fb89e00": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011969102s
    Dec  9 07:45:23.452: INFO: Pod "dns-test-9a575233-25ed-4ac1-a5e1-1e587fb89e00": Phase="Pending", Reason="", readiness=false. Elapsed: 4.011683111s
    Dec  9 07:45:25.463: INFO: Pod "dns-test-9a575233-25ed-4ac1-a5e1-1e587fb89e00": Phase="Pending", Reason="", readiness=false. Elapsed: 6.02277021s
    Dec  9 07:45:27.452: INFO: Pod "dns-test-9a575233-25ed-4ac1-a5e1-1e587fb89e00": Phase="Pending", Reason="", readiness=false. Elapsed: 8.011751391s
    Dec  9 07:45:29.453: INFO: Pod "dns-test-9a575233-25ed-4ac1-a5e1-1e587fb89e00": Phase="Pending", Reason="", readiness=false. Elapsed: 10.01340829s
    Dec  9 07:45:31.452: INFO: Pod "dns-test-9a575233-25ed-4ac1-a5e1-1e587fb89e00": Phase="Pending", Reason="", readiness=false. Elapsed: 12.012120796s
    Dec  9 07:45:33.454: INFO: Pod "dns-test-9a575233-25ed-4ac1-a5e1-1e587fb89e00": Phase="Pending", Reason="", readiness=false. Elapsed: 14.013543191s
    Dec  9 07:45:35.452: INFO: Pod "dns-test-9a575233-25ed-4ac1-a5e1-1e587fb89e00": Phase="Pending", Reason="", readiness=false. Elapsed: 16.011712416s
    Dec  9 07:45:37.454: INFO: Pod "dns-test-9a575233-25ed-4ac1-a5e1-1e587fb89e00": Phase="Running", Reason="", readiness=true. Elapsed: 18.014054259s
    Dec  9 07:45:37.454: INFO: Pod "dns-test-9a575233-25ed-4ac1-a5e1-1e587fb89e00" satisfied condition "running"
    STEP: retrieving the pod 12/09/22 07:45:37.454
    STEP: looking for the results for each expected name from probers 12/09/22 07:45:37.468
    Dec  9 07:45:37.483: INFO: DNS probes using dns-8531/dns-test-9a575233-25ed-4ac1-a5e1-1e587fb89e00 succeeded

    STEP: deleting the pod 12/09/22 07:45:37.483
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:45:37.503: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-8531" for this suite. 12/09/22 07:45:37.508
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:147
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:45:37.517
Dec  9 07:45:37.517: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename emptydir 12/09/22 07:45:37.518
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:45:37.533
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:45:37.537
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:147
STEP: Creating a pod to test emptydir 0777 on tmpfs 12/09/22 07:45:37.541
Dec  9 07:45:37.549: INFO: Waiting up to 5m0s for pod "pod-c15b5b41-60af-45c9-ac97-43d3cdd062ac" in namespace "emptydir-1326" to be "Succeeded or Failed"
Dec  9 07:45:37.553: INFO: Pod "pod-c15b5b41-60af-45c9-ac97-43d3cdd062ac": Phase="Pending", Reason="", readiness=false. Elapsed: 3.670122ms
Dec  9 07:45:39.556: INFO: Pod "pod-c15b5b41-60af-45c9-ac97-43d3cdd062ac": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007228233s
Dec  9 07:45:41.556: INFO: Pod "pod-c15b5b41-60af-45c9-ac97-43d3cdd062ac": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007213817s
STEP: Saw pod success 12/09/22 07:45:41.556
Dec  9 07:45:41.557: INFO: Pod "pod-c15b5b41-60af-45c9-ac97-43d3cdd062ac" satisfied condition "Succeeded or Failed"
Dec  9 07:45:41.559: INFO: Trying to get logs from node ip-10-0-10-179 pod pod-c15b5b41-60af-45c9-ac97-43d3cdd062ac container test-container: <nil>
STEP: delete the pod 12/09/22 07:45:41.564
Dec  9 07:45:41.574: INFO: Waiting for pod pod-c15b5b41-60af-45c9-ac97-43d3cdd062ac to disappear
Dec  9 07:45:41.576: INFO: Pod pod-c15b5b41-60af-45c9-ac97-43d3cdd062ac no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Dec  9 07:45:41.577: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-1326" for this suite. 12/09/22 07:45:41.581
------------------------------
â€¢ [4.069 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:147

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:45:37.517
    Dec  9 07:45:37.517: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename emptydir 12/09/22 07:45:37.518
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:45:37.533
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:45:37.537
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:147
    STEP: Creating a pod to test emptydir 0777 on tmpfs 12/09/22 07:45:37.541
    Dec  9 07:45:37.549: INFO: Waiting up to 5m0s for pod "pod-c15b5b41-60af-45c9-ac97-43d3cdd062ac" in namespace "emptydir-1326" to be "Succeeded or Failed"
    Dec  9 07:45:37.553: INFO: Pod "pod-c15b5b41-60af-45c9-ac97-43d3cdd062ac": Phase="Pending", Reason="", readiness=false. Elapsed: 3.670122ms
    Dec  9 07:45:39.556: INFO: Pod "pod-c15b5b41-60af-45c9-ac97-43d3cdd062ac": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007228233s
    Dec  9 07:45:41.556: INFO: Pod "pod-c15b5b41-60af-45c9-ac97-43d3cdd062ac": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007213817s
    STEP: Saw pod success 12/09/22 07:45:41.556
    Dec  9 07:45:41.557: INFO: Pod "pod-c15b5b41-60af-45c9-ac97-43d3cdd062ac" satisfied condition "Succeeded or Failed"
    Dec  9 07:45:41.559: INFO: Trying to get logs from node ip-10-0-10-179 pod pod-c15b5b41-60af-45c9-ac97-43d3cdd062ac container test-container: <nil>
    STEP: delete the pod 12/09/22 07:45:41.564
    Dec  9 07:45:41.574: INFO: Waiting for pod pod-c15b5b41-60af-45c9-ac97-43d3cdd062ac to disappear
    Dec  9 07:45:41.576: INFO: Pod pod-c15b5b41-60af-45c9-ac97-43d3cdd062ac no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:45:41.577: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-1326" for this suite. 12/09/22 07:45:41.581
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces
  should list and delete a collection of PodDisruptionBudgets [Conformance]
  test/e2e/apps/disruption.go:87
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:45:41.588
Dec  9 07:45:41.588: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename disruption 12/09/22 07:45:41.589
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:45:41.609
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:45:41.615
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[BeforeEach] Listing PodDisruptionBudgets for all namespaces
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:45:41.62
Dec  9 07:45:41.620: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename disruption-2 12/09/22 07:45:41.621
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:45:41.637
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:45:41.641
[BeforeEach] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/metrics/init/init.go:31
[It] should list and delete a collection of PodDisruptionBudgets [Conformance]
  test/e2e/apps/disruption.go:87
STEP: Waiting for the pdb to be processed 12/09/22 07:45:41.652
STEP: Waiting for the pdb to be processed 12/09/22 07:45:41.669
STEP: Waiting for the pdb to be processed 12/09/22 07:45:43.685
STEP: listing a collection of PDBs across all namespaces 12/09/22 07:45:45.696
STEP: listing a collection of PDBs in namespace disruption-9308 12/09/22 07:45:45.711
STEP: deleting a collection of PDBs 12/09/22 07:45:45.718
STEP: Waiting for the PDB collection to be deleted 12/09/22 07:45:45.732
[AfterEach] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/node/init/init.go:32
Dec  9 07:45:45.735: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Dec  9 07:45:45.738: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
  dump namespaces | framework.go:196
[DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-2-2938" for this suite. 12/09/22 07:45:45.742
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-9308" for this suite. 12/09/22 07:45:45.754
------------------------------
â€¢ [4.181 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  Listing PodDisruptionBudgets for all namespaces
  test/e2e/apps/disruption.go:78
    should list and delete a collection of PodDisruptionBudgets [Conformance]
    test/e2e/apps/disruption.go:87

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:45:41.588
    Dec  9 07:45:41.588: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename disruption 12/09/22 07:45:41.589
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:45:41.609
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:45:41.615
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [BeforeEach] Listing PodDisruptionBudgets for all namespaces
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:45:41.62
    Dec  9 07:45:41.620: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename disruption-2 12/09/22 07:45:41.621
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:45:41.637
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:45:41.641
    [BeforeEach] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/metrics/init/init.go:31
    [It] should list and delete a collection of PodDisruptionBudgets [Conformance]
      test/e2e/apps/disruption.go:87
    STEP: Waiting for the pdb to be processed 12/09/22 07:45:41.652
    STEP: Waiting for the pdb to be processed 12/09/22 07:45:41.669
    STEP: Waiting for the pdb to be processed 12/09/22 07:45:43.685
    STEP: listing a collection of PDBs across all namespaces 12/09/22 07:45:45.696
    STEP: listing a collection of PDBs in namespace disruption-9308 12/09/22 07:45:45.711
    STEP: deleting a collection of PDBs 12/09/22 07:45:45.718
    STEP: Waiting for the PDB collection to be deleted 12/09/22 07:45:45.732
    [AfterEach] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:45:45.735: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:45:45.738: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-2-2938" for this suite. 12/09/22 07:45:45.742
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-9308" for this suite. 12/09/22 07:45:45.754
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] Daemon set [Serial]
  should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:294
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:45:45.77
Dec  9 07:45:45.770: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename daemonsets 12/09/22 07:45:45.771
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:45:45.812
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:45:45.825
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:294
STEP: Creating a simple DaemonSet "daemon-set" 12/09/22 07:45:45.844
STEP: Check that daemon pods launch on every node of the cluster. 12/09/22 07:45:45.85
Dec  9 07:45:45.855: INFO: DaemonSet pods can't tolerate node ip-10-0-7-84 with taints [{Key:node-role.kubernetes.io/controller Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 07:45:45.858: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Dec  9 07:45:45.858: INFO: Node ip-10-0-10-179 is running 0 daemon pod, expected 1
Dec  9 07:45:46.862: INFO: DaemonSet pods can't tolerate node ip-10-0-7-84 with taints [{Key:node-role.kubernetes.io/controller Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 07:45:46.865: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Dec  9 07:45:46.865: INFO: Node ip-10-0-10-179 is running 0 daemon pod, expected 1
Dec  9 07:45:47.862: INFO: DaemonSet pods can't tolerate node ip-10-0-7-84 with taints [{Key:node-role.kubernetes.io/controller Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 07:45:47.865: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Dec  9 07:45:47.865: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. 12/09/22 07:45:47.867
Dec  9 07:45:47.893: INFO: DaemonSet pods can't tolerate node ip-10-0-7-84 with taints [{Key:node-role.kubernetes.io/controller Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 07:45:47.902: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Dec  9 07:45:47.902: INFO: Node ip-10-0-17-108 is running 0 daemon pod, expected 1
Dec  9 07:45:48.912: INFO: DaemonSet pods can't tolerate node ip-10-0-7-84 with taints [{Key:node-role.kubernetes.io/controller Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 07:45:48.920: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Dec  9 07:45:48.920: INFO: Node ip-10-0-17-108 is running 0 daemon pod, expected 1
Dec  9 07:45:49.906: INFO: DaemonSet pods can't tolerate node ip-10-0-7-84 with taints [{Key:node-role.kubernetes.io/controller Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 07:45:49.909: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Dec  9 07:45:49.909: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
STEP: Wait for the failed daemon pod to be completely deleted. 12/09/22 07:45:49.909
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 12/09/22 07:45:49.921
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3716, will wait for the garbage collector to delete the pods 12/09/22 07:45:49.921
Dec  9 07:45:49.981: INFO: Deleting DaemonSet.extensions daemon-set took: 6.13429ms
Dec  9 07:45:50.181: INFO: Terminating DaemonSet.extensions daemon-set pods took: 200.255486ms
Dec  9 07:45:52.986: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Dec  9 07:45:52.986: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Dec  9 07:45:52.989: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"18012"},"items":null}

Dec  9 07:45:52.993: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"18012"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Dec  9 07:45:53.004: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-3716" for this suite. 12/09/22 07:45:53.008
------------------------------
â€¢ [SLOW TEST] [7.246 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:294

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:45:45.77
    Dec  9 07:45:45.770: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename daemonsets 12/09/22 07:45:45.771
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:45:45.812
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:45:45.825
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should retry creating failed daemon pods [Conformance]
      test/e2e/apps/daemon_set.go:294
    STEP: Creating a simple DaemonSet "daemon-set" 12/09/22 07:45:45.844
    STEP: Check that daemon pods launch on every node of the cluster. 12/09/22 07:45:45.85
    Dec  9 07:45:45.855: INFO: DaemonSet pods can't tolerate node ip-10-0-7-84 with taints [{Key:node-role.kubernetes.io/controller Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Dec  9 07:45:45.858: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Dec  9 07:45:45.858: INFO: Node ip-10-0-10-179 is running 0 daemon pod, expected 1
    Dec  9 07:45:46.862: INFO: DaemonSet pods can't tolerate node ip-10-0-7-84 with taints [{Key:node-role.kubernetes.io/controller Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Dec  9 07:45:46.865: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Dec  9 07:45:46.865: INFO: Node ip-10-0-10-179 is running 0 daemon pod, expected 1
    Dec  9 07:45:47.862: INFO: DaemonSet pods can't tolerate node ip-10-0-7-84 with taints [{Key:node-role.kubernetes.io/controller Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Dec  9 07:45:47.865: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Dec  9 07:45:47.865: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. 12/09/22 07:45:47.867
    Dec  9 07:45:47.893: INFO: DaemonSet pods can't tolerate node ip-10-0-7-84 with taints [{Key:node-role.kubernetes.io/controller Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Dec  9 07:45:47.902: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Dec  9 07:45:47.902: INFO: Node ip-10-0-17-108 is running 0 daemon pod, expected 1
    Dec  9 07:45:48.912: INFO: DaemonSet pods can't tolerate node ip-10-0-7-84 with taints [{Key:node-role.kubernetes.io/controller Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Dec  9 07:45:48.920: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Dec  9 07:45:48.920: INFO: Node ip-10-0-17-108 is running 0 daemon pod, expected 1
    Dec  9 07:45:49.906: INFO: DaemonSet pods can't tolerate node ip-10-0-7-84 with taints [{Key:node-role.kubernetes.io/controller Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Dec  9 07:45:49.909: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Dec  9 07:45:49.909: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    STEP: Wait for the failed daemon pod to be completely deleted. 12/09/22 07:45:49.909
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 12/09/22 07:45:49.921
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3716, will wait for the garbage collector to delete the pods 12/09/22 07:45:49.921
    Dec  9 07:45:49.981: INFO: Deleting DaemonSet.extensions daemon-set took: 6.13429ms
    Dec  9 07:45:50.181: INFO: Terminating DaemonSet.extensions daemon-set pods took: 200.255486ms
    Dec  9 07:45:52.986: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Dec  9 07:45:52.986: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Dec  9 07:45:52.989: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"18012"},"items":null}

    Dec  9 07:45:52.993: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"18012"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:45:53.004: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-3716" for this suite. 12/09/22 07:45:53.008
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:166
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:45:53.021
Dec  9 07:45:53.021: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename daemonsets 12/09/22 07:45:53.022
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:45:53.047
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:45:53.054
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:166
STEP: Creating simple DaemonSet "daemon-set" 12/09/22 07:45:53.088
STEP: Check that daemon pods launch on every node of the cluster. 12/09/22 07:45:53.106
Dec  9 07:45:53.111: INFO: DaemonSet pods can't tolerate node ip-10-0-7-84 with taints [{Key:node-role.kubernetes.io/controller Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 07:45:53.116: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Dec  9 07:45:53.116: INFO: Node ip-10-0-10-179 is running 0 daemon pod, expected 1
Dec  9 07:45:54.121: INFO: DaemonSet pods can't tolerate node ip-10-0-7-84 with taints [{Key:node-role.kubernetes.io/controller Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 07:45:54.124: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Dec  9 07:45:54.124: INFO: Node ip-10-0-10-179 is running 0 daemon pod, expected 1
Dec  9 07:45:55.120: INFO: DaemonSet pods can't tolerate node ip-10-0-7-84 with taints [{Key:node-role.kubernetes.io/controller Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 07:45:55.124: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Dec  9 07:45:55.124: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
STEP: Stop a daemon pod, check that the daemon pod is revived. 12/09/22 07:45:55.128
Dec  9 07:45:55.173: INFO: DaemonSet pods can't tolerate node ip-10-0-7-84 with taints [{Key:node-role.kubernetes.io/controller Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 07:45:55.184: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Dec  9 07:45:55.184: INFO: Node ip-10-0-10-179 is running 0 daemon pod, expected 1
Dec  9 07:45:56.189: INFO: DaemonSet pods can't tolerate node ip-10-0-7-84 with taints [{Key:node-role.kubernetes.io/controller Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 07:45:56.212: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Dec  9 07:45:56.212: INFO: Node ip-10-0-10-179 is running 0 daemon pod, expected 1
Dec  9 07:45:57.188: INFO: DaemonSet pods can't tolerate node ip-10-0-7-84 with taints [{Key:node-role.kubernetes.io/controller Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 07:45:57.191: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Dec  9 07:45:57.192: INFO: Node ip-10-0-10-179 is running 0 daemon pod, expected 1
Dec  9 07:45:58.189: INFO: DaemonSet pods can't tolerate node ip-10-0-7-84 with taints [{Key:node-role.kubernetes.io/controller Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 07:45:58.194: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Dec  9 07:45:58.194: INFO: Node ip-10-0-10-179 is running 0 daemon pod, expected 1
Dec  9 07:45:59.188: INFO: DaemonSet pods can't tolerate node ip-10-0-7-84 with taints [{Key:node-role.kubernetes.io/controller Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 07:45:59.193: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Dec  9 07:45:59.193: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 12/09/22 07:45:59.196
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9538, will wait for the garbage collector to delete the pods 12/09/22 07:45:59.196
Dec  9 07:45:59.257: INFO: Deleting DaemonSet.extensions daemon-set took: 7.114022ms
Dec  9 07:45:59.357: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.295422ms
Dec  9 07:46:01.561: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Dec  9 07:46:01.562: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Dec  9 07:46:01.566: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"18128"},"items":null}

Dec  9 07:46:01.570: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"18128"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Dec  9 07:46:01.588: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-9538" for this suite. 12/09/22 07:46:01.602
------------------------------
â€¢ [SLOW TEST] [8.598 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:166

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:45:53.021
    Dec  9 07:45:53.021: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename daemonsets 12/09/22 07:45:53.022
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:45:53.047
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:45:53.054
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should run and stop simple daemon [Conformance]
      test/e2e/apps/daemon_set.go:166
    STEP: Creating simple DaemonSet "daemon-set" 12/09/22 07:45:53.088
    STEP: Check that daemon pods launch on every node of the cluster. 12/09/22 07:45:53.106
    Dec  9 07:45:53.111: INFO: DaemonSet pods can't tolerate node ip-10-0-7-84 with taints [{Key:node-role.kubernetes.io/controller Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Dec  9 07:45:53.116: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Dec  9 07:45:53.116: INFO: Node ip-10-0-10-179 is running 0 daemon pod, expected 1
    Dec  9 07:45:54.121: INFO: DaemonSet pods can't tolerate node ip-10-0-7-84 with taints [{Key:node-role.kubernetes.io/controller Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Dec  9 07:45:54.124: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Dec  9 07:45:54.124: INFO: Node ip-10-0-10-179 is running 0 daemon pod, expected 1
    Dec  9 07:45:55.120: INFO: DaemonSet pods can't tolerate node ip-10-0-7-84 with taints [{Key:node-role.kubernetes.io/controller Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Dec  9 07:45:55.124: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Dec  9 07:45:55.124: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    STEP: Stop a daemon pod, check that the daemon pod is revived. 12/09/22 07:45:55.128
    Dec  9 07:45:55.173: INFO: DaemonSet pods can't tolerate node ip-10-0-7-84 with taints [{Key:node-role.kubernetes.io/controller Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Dec  9 07:45:55.184: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Dec  9 07:45:55.184: INFO: Node ip-10-0-10-179 is running 0 daemon pod, expected 1
    Dec  9 07:45:56.189: INFO: DaemonSet pods can't tolerate node ip-10-0-7-84 with taints [{Key:node-role.kubernetes.io/controller Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Dec  9 07:45:56.212: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Dec  9 07:45:56.212: INFO: Node ip-10-0-10-179 is running 0 daemon pod, expected 1
    Dec  9 07:45:57.188: INFO: DaemonSet pods can't tolerate node ip-10-0-7-84 with taints [{Key:node-role.kubernetes.io/controller Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Dec  9 07:45:57.191: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Dec  9 07:45:57.192: INFO: Node ip-10-0-10-179 is running 0 daemon pod, expected 1
    Dec  9 07:45:58.189: INFO: DaemonSet pods can't tolerate node ip-10-0-7-84 with taints [{Key:node-role.kubernetes.io/controller Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Dec  9 07:45:58.194: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Dec  9 07:45:58.194: INFO: Node ip-10-0-10-179 is running 0 daemon pod, expected 1
    Dec  9 07:45:59.188: INFO: DaemonSet pods can't tolerate node ip-10-0-7-84 with taints [{Key:node-role.kubernetes.io/controller Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Dec  9 07:45:59.193: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Dec  9 07:45:59.193: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 12/09/22 07:45:59.196
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9538, will wait for the garbage collector to delete the pods 12/09/22 07:45:59.196
    Dec  9 07:45:59.257: INFO: Deleting DaemonSet.extensions daemon-set took: 7.114022ms
    Dec  9 07:45:59.357: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.295422ms
    Dec  9 07:46:01.561: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Dec  9 07:46:01.562: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Dec  9 07:46:01.566: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"18128"},"items":null}

    Dec  9 07:46:01.570: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"18128"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:46:01.588: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-9538" for this suite. 12/09/22 07:46:01.602
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:504
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:46:01.62
Dec  9 07:46:01.620: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename configmap 12/09/22 07:46:01.621
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:46:01.699
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:46:01.721
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:504
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Dec  9 07:46:01.852: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-5652" for this suite. 12/09/22 07:46:01.858
------------------------------
â€¢ [0.273 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:504

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:46:01.62
    Dec  9 07:46:01.620: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename configmap 12/09/22 07:46:01.621
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:46:01.699
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:46:01.721
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be immutable if `immutable` field is set [Conformance]
      test/e2e/common/storage/configmap_volume.go:504
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:46:01.852: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-5652" for this suite. 12/09/22 07:46:01.858
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-network] Services
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1515
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:46:01.893
Dec  9 07:46:01.893: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename services 12/09/22 07:46:01.894
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:46:01.963
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:46:01.982
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1515
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-7225 12/09/22 07:46:01.994
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 12/09/22 07:46:02.026
STEP: creating service externalsvc in namespace services-7225 12/09/22 07:46:02.026
STEP: creating replication controller externalsvc in namespace services-7225 12/09/22 07:46:02.046
I1209 07:46:02.069196      20 runners.go:193] Created replication controller with name: externalsvc, namespace: services-7225, replica count: 2
I1209 07:46:05.126029      20 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName 12/09/22 07:46:05.13
Dec  9 07:46:05.154: INFO: Creating new exec pod
Dec  9 07:46:05.171: INFO: Waiting up to 5m0s for pod "execpodrchdk" in namespace "services-7225" to be "running"
Dec  9 07:46:05.179: INFO: Pod "execpodrchdk": Phase="Pending", Reason="", readiness=false. Elapsed: 7.454263ms
Dec  9 07:46:07.183: INFO: Pod "execpodrchdk": Phase="Running", Reason="", readiness=true. Elapsed: 2.012162933s
Dec  9 07:46:07.184: INFO: Pod "execpodrchdk" satisfied condition "running"
Dec  9 07:46:07.184: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=services-7225 exec execpodrchdk -- /bin/sh -x -c nslookup clusterip-service.services-7225.svc.cluster.local'
Dec  9 07:46:07.366: INFO: stderr: "+ nslookup clusterip-service.services-7225.svc.cluster.local\n"
Dec  9 07:46:07.366: INFO: stdout: "Server:\t\t10.3.0.10\nAddress:\t10.3.0.10#53\n\nclusterip-service.services-7225.svc.cluster.local\tcanonical name = externalsvc.services-7225.svc.cluster.local.\nName:\texternalsvc.services-7225.svc.cluster.local\nAddress: 10.3.3.76\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-7225, will wait for the garbage collector to delete the pods 12/09/22 07:46:07.366
Dec  9 07:46:07.425: INFO: Deleting ReplicationController externalsvc took: 4.988231ms
Dec  9 07:46:07.527: INFO: Terminating ReplicationController externalsvc pods took: 101.953208ms
Dec  9 07:46:10.151: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Dec  9 07:46:10.166: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-7225" for this suite. 12/09/22 07:46:10.174
------------------------------
â€¢ [SLOW TEST] [8.290 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1515

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:46:01.893
    Dec  9 07:46:01.893: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename services 12/09/22 07:46:01.894
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:46:01.963
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:46:01.982
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to change the type from ClusterIP to ExternalName [Conformance]
      test/e2e/network/service.go:1515
    STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-7225 12/09/22 07:46:01.994
    STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 12/09/22 07:46:02.026
    STEP: creating service externalsvc in namespace services-7225 12/09/22 07:46:02.026
    STEP: creating replication controller externalsvc in namespace services-7225 12/09/22 07:46:02.046
    I1209 07:46:02.069196      20 runners.go:193] Created replication controller with name: externalsvc, namespace: services-7225, replica count: 2
    I1209 07:46:05.126029      20 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    STEP: changing the ClusterIP service to type=ExternalName 12/09/22 07:46:05.13
    Dec  9 07:46:05.154: INFO: Creating new exec pod
    Dec  9 07:46:05.171: INFO: Waiting up to 5m0s for pod "execpodrchdk" in namespace "services-7225" to be "running"
    Dec  9 07:46:05.179: INFO: Pod "execpodrchdk": Phase="Pending", Reason="", readiness=false. Elapsed: 7.454263ms
    Dec  9 07:46:07.183: INFO: Pod "execpodrchdk": Phase="Running", Reason="", readiness=true. Elapsed: 2.012162933s
    Dec  9 07:46:07.184: INFO: Pod "execpodrchdk" satisfied condition "running"
    Dec  9 07:46:07.184: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=services-7225 exec execpodrchdk -- /bin/sh -x -c nslookup clusterip-service.services-7225.svc.cluster.local'
    Dec  9 07:46:07.366: INFO: stderr: "+ nslookup clusterip-service.services-7225.svc.cluster.local\n"
    Dec  9 07:46:07.366: INFO: stdout: "Server:\t\t10.3.0.10\nAddress:\t10.3.0.10#53\n\nclusterip-service.services-7225.svc.cluster.local\tcanonical name = externalsvc.services-7225.svc.cluster.local.\nName:\texternalsvc.services-7225.svc.cluster.local\nAddress: 10.3.3.76\n\n"
    STEP: deleting ReplicationController externalsvc in namespace services-7225, will wait for the garbage collector to delete the pods 12/09/22 07:46:07.366
    Dec  9 07:46:07.425: INFO: Deleting ReplicationController externalsvc took: 4.988231ms
    Dec  9 07:46:07.527: INFO: Terminating ReplicationController externalsvc pods took: 101.953208ms
    Dec  9 07:46:10.151: INFO: Cleaning up the ClusterIP to ExternalName test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:46:10.166: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-7225" for this suite. 12/09/22 07:46:10.174
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:57
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:46:10.188
Dec  9 07:46:10.188: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename projected 12/09/22 07:46:10.189
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:46:10.211
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:46:10.216
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:57
STEP: Creating configMap with name projected-configmap-test-volume-5a3a3e19-ca12-41fc-930c-c98009d519e6 12/09/22 07:46:10.221
STEP: Creating a pod to test consume configMaps 12/09/22 07:46:10.225
Dec  9 07:46:10.234: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-5169af8f-c66f-437d-b658-997366972281" in namespace "projected-5689" to be "Succeeded or Failed"
Dec  9 07:46:10.241: INFO: Pod "pod-projected-configmaps-5169af8f-c66f-437d-b658-997366972281": Phase="Pending", Reason="", readiness=false. Elapsed: 6.409751ms
Dec  9 07:46:12.246: INFO: Pod "pod-projected-configmaps-5169af8f-c66f-437d-b658-997366972281": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011476412s
Dec  9 07:46:14.246: INFO: Pod "pod-projected-configmaps-5169af8f-c66f-437d-b658-997366972281": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012066432s
STEP: Saw pod success 12/09/22 07:46:14.246
Dec  9 07:46:14.247: INFO: Pod "pod-projected-configmaps-5169af8f-c66f-437d-b658-997366972281" satisfied condition "Succeeded or Failed"
Dec  9 07:46:14.252: INFO: Trying to get logs from node ip-10-0-10-179 pod pod-projected-configmaps-5169af8f-c66f-437d-b658-997366972281 container agnhost-container: <nil>
STEP: delete the pod 12/09/22 07:46:14.273
Dec  9 07:46:14.311: INFO: Waiting for pod pod-projected-configmaps-5169af8f-c66f-437d-b658-997366972281 to disappear
Dec  9 07:46:14.320: INFO: Pod pod-projected-configmaps-5169af8f-c66f-437d-b658-997366972281 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Dec  9 07:46:14.320: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-5689" for this suite. 12/09/22 07:46:14.336
------------------------------
â€¢ [4.165 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:46:10.188
    Dec  9 07:46:10.188: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename projected 12/09/22 07:46:10.189
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:46:10.211
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:46:10.216
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:57
    STEP: Creating configMap with name projected-configmap-test-volume-5a3a3e19-ca12-41fc-930c-c98009d519e6 12/09/22 07:46:10.221
    STEP: Creating a pod to test consume configMaps 12/09/22 07:46:10.225
    Dec  9 07:46:10.234: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-5169af8f-c66f-437d-b658-997366972281" in namespace "projected-5689" to be "Succeeded or Failed"
    Dec  9 07:46:10.241: INFO: Pod "pod-projected-configmaps-5169af8f-c66f-437d-b658-997366972281": Phase="Pending", Reason="", readiness=false. Elapsed: 6.409751ms
    Dec  9 07:46:12.246: INFO: Pod "pod-projected-configmaps-5169af8f-c66f-437d-b658-997366972281": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011476412s
    Dec  9 07:46:14.246: INFO: Pod "pod-projected-configmaps-5169af8f-c66f-437d-b658-997366972281": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012066432s
    STEP: Saw pod success 12/09/22 07:46:14.246
    Dec  9 07:46:14.247: INFO: Pod "pod-projected-configmaps-5169af8f-c66f-437d-b658-997366972281" satisfied condition "Succeeded or Failed"
    Dec  9 07:46:14.252: INFO: Trying to get logs from node ip-10-0-10-179 pod pod-projected-configmaps-5169af8f-c66f-437d-b658-997366972281 container agnhost-container: <nil>
    STEP: delete the pod 12/09/22 07:46:14.273
    Dec  9 07:46:14.311: INFO: Waiting for pod pod-projected-configmaps-5169af8f-c66f-437d-b658-997366972281 to disappear
    Dec  9 07:46:14.320: INFO: Pod pod-projected-configmaps-5169af8f-c66f-437d-b658-997366972281 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:46:14.320: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-5689" for this suite. 12/09/22 07:46:14.336
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:109
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:46:14.357
Dec  9 07:46:14.357: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename configmap 12/09/22 07:46:14.358
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:46:14.433
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:46:14.446
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:109
STEP: Creating configMap with name configmap-test-volume-map-69955672-403a-4eb1-98e2-8e783b715c52 12/09/22 07:46:14.458
STEP: Creating a pod to test consume configMaps 12/09/22 07:46:14.473
Dec  9 07:46:14.496: INFO: Waiting up to 5m0s for pod "pod-configmaps-0939e4c1-d8b9-46e1-8e17-06324a468d85" in namespace "configmap-7526" to be "Succeeded or Failed"
Dec  9 07:46:14.513: INFO: Pod "pod-configmaps-0939e4c1-d8b9-46e1-8e17-06324a468d85": Phase="Pending", Reason="", readiness=false. Elapsed: 17.701244ms
Dec  9 07:46:16.518: INFO: Pod "pod-configmaps-0939e4c1-d8b9-46e1-8e17-06324a468d85": Phase="Running", Reason="", readiness=false. Elapsed: 2.021937589s
Dec  9 07:46:18.517: INFO: Pod "pod-configmaps-0939e4c1-d8b9-46e1-8e17-06324a468d85": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021648154s
STEP: Saw pod success 12/09/22 07:46:18.517
Dec  9 07:46:18.518: INFO: Pod "pod-configmaps-0939e4c1-d8b9-46e1-8e17-06324a468d85" satisfied condition "Succeeded or Failed"
Dec  9 07:46:18.521: INFO: Trying to get logs from node ip-10-0-10-179 pod pod-configmaps-0939e4c1-d8b9-46e1-8e17-06324a468d85 container agnhost-container: <nil>
STEP: delete the pod 12/09/22 07:46:18.526
Dec  9 07:46:18.552: INFO: Waiting for pod pod-configmaps-0939e4c1-d8b9-46e1-8e17-06324a468d85 to disappear
Dec  9 07:46:18.556: INFO: Pod pod-configmaps-0939e4c1-d8b9-46e1-8e17-06324a468d85 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Dec  9 07:46:18.557: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-7526" for this suite. 12/09/22 07:46:18.561
------------------------------
â€¢ [4.210 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:109

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:46:14.357
    Dec  9 07:46:14.357: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename configmap 12/09/22 07:46:14.358
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:46:14.433
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:46:14.446
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:109
    STEP: Creating configMap with name configmap-test-volume-map-69955672-403a-4eb1-98e2-8e783b715c52 12/09/22 07:46:14.458
    STEP: Creating a pod to test consume configMaps 12/09/22 07:46:14.473
    Dec  9 07:46:14.496: INFO: Waiting up to 5m0s for pod "pod-configmaps-0939e4c1-d8b9-46e1-8e17-06324a468d85" in namespace "configmap-7526" to be "Succeeded or Failed"
    Dec  9 07:46:14.513: INFO: Pod "pod-configmaps-0939e4c1-d8b9-46e1-8e17-06324a468d85": Phase="Pending", Reason="", readiness=false. Elapsed: 17.701244ms
    Dec  9 07:46:16.518: INFO: Pod "pod-configmaps-0939e4c1-d8b9-46e1-8e17-06324a468d85": Phase="Running", Reason="", readiness=false. Elapsed: 2.021937589s
    Dec  9 07:46:18.517: INFO: Pod "pod-configmaps-0939e4c1-d8b9-46e1-8e17-06324a468d85": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021648154s
    STEP: Saw pod success 12/09/22 07:46:18.517
    Dec  9 07:46:18.518: INFO: Pod "pod-configmaps-0939e4c1-d8b9-46e1-8e17-06324a468d85" satisfied condition "Succeeded or Failed"
    Dec  9 07:46:18.521: INFO: Trying to get logs from node ip-10-0-10-179 pod pod-configmaps-0939e4c1-d8b9-46e1-8e17-06324a468d85 container agnhost-container: <nil>
    STEP: delete the pod 12/09/22 07:46:18.526
    Dec  9 07:46:18.552: INFO: Waiting for pod pod-configmaps-0939e4c1-d8b9-46e1-8e17-06324a468d85 to disappear
    Dec  9 07:46:18.556: INFO: Pod pod-configmaps-0939e4c1-d8b9-46e1-8e17-06324a468d85 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:46:18.557: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-7526" for this suite. 12/09/22 07:46:18.561
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] Watchers
  should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:46:18.569
Dec  9 07:46:18.569: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename watch 12/09/22 07:46:18.57
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:46:18.592
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:46:18.597
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334
STEP: getting a starting resourceVersion 12/09/22 07:46:18.601
STEP: starting a background goroutine to produce watch events 12/09/22 07:46:18.604
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order 12/09/22 07:46:18.604
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Dec  9 07:46:21.376: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-7908" for this suite. 12/09/22 07:46:21.424
------------------------------
â€¢ [2.909 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:46:18.569
    Dec  9 07:46:18.569: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename watch 12/09/22 07:46:18.57
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:46:18.592
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:46:18.597
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should receive events on concurrent watches in same order [Conformance]
      test/e2e/apimachinery/watch.go:334
    STEP: getting a starting resourceVersion 12/09/22 07:46:18.601
    STEP: starting a background goroutine to produce watch events 12/09/22 07:46:18.604
    STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order 12/09/22 07:46:18.604
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:46:21.376: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-7908" for this suite. 12/09/22 07:46:21.424
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-network] Services
  should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3244
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:46:21.48
Dec  9 07:46:21.480: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename services 12/09/22 07:46:21.481
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:46:21.504
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:46:21.51
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3244
STEP: creating an Endpoint 12/09/22 07:46:21.517
STEP: waiting for available Endpoint 12/09/22 07:46:21.521
STEP: listing all Endpoints 12/09/22 07:46:21.523
STEP: updating the Endpoint 12/09/22 07:46:21.526
STEP: fetching the Endpoint 12/09/22 07:46:21.534
STEP: patching the Endpoint 12/09/22 07:46:21.538
STEP: fetching the Endpoint 12/09/22 07:46:21.549
STEP: deleting the Endpoint by Collection 12/09/22 07:46:21.555
STEP: waiting for Endpoint deletion 12/09/22 07:46:21.562
STEP: fetching the Endpoint 12/09/22 07:46:21.564
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Dec  9 07:46:21.568: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-714" for this suite. 12/09/22 07:46:21.572
------------------------------
â€¢ [0.098 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3244

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:46:21.48
    Dec  9 07:46:21.480: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename services 12/09/22 07:46:21.481
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:46:21.504
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:46:21.51
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should test the lifecycle of an Endpoint [Conformance]
      test/e2e/network/service.go:3244
    STEP: creating an Endpoint 12/09/22 07:46:21.517
    STEP: waiting for available Endpoint 12/09/22 07:46:21.521
    STEP: listing all Endpoints 12/09/22 07:46:21.523
    STEP: updating the Endpoint 12/09/22 07:46:21.526
    STEP: fetching the Endpoint 12/09/22 07:46:21.534
    STEP: patching the Endpoint 12/09/22 07:46:21.538
    STEP: fetching the Endpoint 12/09/22 07:46:21.549
    STEP: deleting the Endpoint by Collection 12/09/22 07:46:21.555
    STEP: waiting for Endpoint deletion 12/09/22 07:46:21.562
    STEP: fetching the Endpoint 12/09/22 07:46:21.564
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:46:21.568: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-714" for this suite. 12/09/22 07:46:21.572
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] Pods
  should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:536
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:46:21.579
Dec  9 07:46:21.579: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename pods 12/09/22 07:46:21.58
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:46:21.601
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:46:21.606
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:536
Dec  9 07:46:21.610: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: creating the pod 12/09/22 07:46:21.61
STEP: submitting the pod to kubernetes 12/09/22 07:46:21.61
Dec  9 07:46:21.620: INFO: Waiting up to 5m0s for pod "pod-exec-websocket-88162ea0-bf2b-4e6a-a7a6-f1c249c1355c" in namespace "pods-9747" to be "running and ready"
Dec  9 07:46:21.625: INFO: Pod "pod-exec-websocket-88162ea0-bf2b-4e6a-a7a6-f1c249c1355c": Phase="Pending", Reason="", readiness=false. Elapsed: 5.554858ms
Dec  9 07:46:21.626: INFO: The phase of Pod pod-exec-websocket-88162ea0-bf2b-4e6a-a7a6-f1c249c1355c is Pending, waiting for it to be Running (with Ready = true)
Dec  9 07:46:23.629: INFO: Pod "pod-exec-websocket-88162ea0-bf2b-4e6a-a7a6-f1c249c1355c": Phase="Running", Reason="", readiness=true. Elapsed: 2.009108037s
Dec  9 07:46:23.629: INFO: The phase of Pod pod-exec-websocket-88162ea0-bf2b-4e6a-a7a6-f1c249c1355c is Running (Ready = true)
Dec  9 07:46:23.629: INFO: Pod "pod-exec-websocket-88162ea0-bf2b-4e6a-a7a6-f1c249c1355c" satisfied condition "running and ready"
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Dec  9 07:46:23.706: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-9747" for this suite. 12/09/22 07:46:23.71
------------------------------
â€¢ [2.137 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:536

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:46:21.579
    Dec  9 07:46:21.579: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename pods 12/09/22 07:46:21.58
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:46:21.601
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:46:21.606
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should support remote command execution over websockets [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:536
    Dec  9 07:46:21.610: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: creating the pod 12/09/22 07:46:21.61
    STEP: submitting the pod to kubernetes 12/09/22 07:46:21.61
    Dec  9 07:46:21.620: INFO: Waiting up to 5m0s for pod "pod-exec-websocket-88162ea0-bf2b-4e6a-a7a6-f1c249c1355c" in namespace "pods-9747" to be "running and ready"
    Dec  9 07:46:21.625: INFO: Pod "pod-exec-websocket-88162ea0-bf2b-4e6a-a7a6-f1c249c1355c": Phase="Pending", Reason="", readiness=false. Elapsed: 5.554858ms
    Dec  9 07:46:21.626: INFO: The phase of Pod pod-exec-websocket-88162ea0-bf2b-4e6a-a7a6-f1c249c1355c is Pending, waiting for it to be Running (with Ready = true)
    Dec  9 07:46:23.629: INFO: Pod "pod-exec-websocket-88162ea0-bf2b-4e6a-a7a6-f1c249c1355c": Phase="Running", Reason="", readiness=true. Elapsed: 2.009108037s
    Dec  9 07:46:23.629: INFO: The phase of Pod pod-exec-websocket-88162ea0-bf2b-4e6a-a7a6-f1c249c1355c is Running (Ready = true)
    Dec  9 07:46:23.629: INFO: Pod "pod-exec-websocket-88162ea0-bf2b-4e6a-a7a6-f1c249c1355c" satisfied condition "running and ready"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:46:23.706: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-9747" for this suite. 12/09/22 07:46:23.71
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Downward API
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:217
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:46:23.716
Dec  9 07:46:23.717: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename downward-api 12/09/22 07:46:23.718
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:46:23.751
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:46:23.757
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:217
STEP: Creating a pod to test downward api env vars 12/09/22 07:46:23.763
Dec  9 07:46:23.775: INFO: Waiting up to 5m0s for pod "downward-api-ee5ab358-7632-48f3-a1d9-697b84fde30a" in namespace "downward-api-7396" to be "Succeeded or Failed"
Dec  9 07:46:23.781: INFO: Pod "downward-api-ee5ab358-7632-48f3-a1d9-697b84fde30a": Phase="Pending", Reason="", readiness=false. Elapsed: 5.409972ms
Dec  9 07:46:25.785: INFO: Pod "downward-api-ee5ab358-7632-48f3-a1d9-697b84fde30a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009849813s
Dec  9 07:46:27.785: INFO: Pod "downward-api-ee5ab358-7632-48f3-a1d9-697b84fde30a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01011819s
STEP: Saw pod success 12/09/22 07:46:27.786
Dec  9 07:46:27.786: INFO: Pod "downward-api-ee5ab358-7632-48f3-a1d9-697b84fde30a" satisfied condition "Succeeded or Failed"
Dec  9 07:46:27.790: INFO: Trying to get logs from node ip-10-0-10-179 pod downward-api-ee5ab358-7632-48f3-a1d9-697b84fde30a container dapi-container: <nil>
STEP: delete the pod 12/09/22 07:46:27.796
Dec  9 07:46:27.805: INFO: Waiting for pod downward-api-ee5ab358-7632-48f3-a1d9-697b84fde30a to disappear
Dec  9 07:46:27.808: INFO: Pod downward-api-ee5ab358-7632-48f3-a1d9-697b84fde30a no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Dec  9 07:46:27.808: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-7396" for this suite. 12/09/22 07:46:27.811
------------------------------
â€¢ [4.101 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:217

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:46:23.716
    Dec  9 07:46:23.717: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename downward-api 12/09/22 07:46:23.718
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:46:23.751
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:46:23.757
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:217
    STEP: Creating a pod to test downward api env vars 12/09/22 07:46:23.763
    Dec  9 07:46:23.775: INFO: Waiting up to 5m0s for pod "downward-api-ee5ab358-7632-48f3-a1d9-697b84fde30a" in namespace "downward-api-7396" to be "Succeeded or Failed"
    Dec  9 07:46:23.781: INFO: Pod "downward-api-ee5ab358-7632-48f3-a1d9-697b84fde30a": Phase="Pending", Reason="", readiness=false. Elapsed: 5.409972ms
    Dec  9 07:46:25.785: INFO: Pod "downward-api-ee5ab358-7632-48f3-a1d9-697b84fde30a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009849813s
    Dec  9 07:46:27.785: INFO: Pod "downward-api-ee5ab358-7632-48f3-a1d9-697b84fde30a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01011819s
    STEP: Saw pod success 12/09/22 07:46:27.786
    Dec  9 07:46:27.786: INFO: Pod "downward-api-ee5ab358-7632-48f3-a1d9-697b84fde30a" satisfied condition "Succeeded or Failed"
    Dec  9 07:46:27.790: INFO: Trying to get logs from node ip-10-0-10-179 pod downward-api-ee5ab358-7632-48f3-a1d9-697b84fde30a container dapi-container: <nil>
    STEP: delete the pod 12/09/22 07:46:27.796
    Dec  9 07:46:27.805: INFO: Waiting for pod downward-api-ee5ab358-7632-48f3-a1d9-697b84fde30a to disappear
    Dec  9 07:46:27.808: INFO: Pod downward-api-ee5ab358-7632-48f3-a1d9-697b84fde30a no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:46:27.808: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-7396" for this suite. 12/09/22 07:46:27.811
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:255
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:46:27.822
Dec  9 07:46:27.822: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename init-container 12/09/22 07:46:27.824
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:46:27.843
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:46:27.848
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:165
[It] should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:255
STEP: creating the pod 12/09/22 07:46:27.851
Dec  9 07:46:27.851: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Dec  9 07:46:31.545: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "init-container-6034" for this suite. 12/09/22 07:46:31.559
------------------------------
â€¢ [3.750 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:255

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:46:27.822
    Dec  9 07:46:27.822: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename init-container 12/09/22 07:46:27.824
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:46:27.843
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:46:27.848
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:165
    [It] should invoke init containers on a RestartAlways pod [Conformance]
      test/e2e/common/node/init_container.go:255
    STEP: creating the pod 12/09/22 07:46:27.851
    Dec  9 07:46:27.851: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:46:31.545: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "init-container-6034" for this suite. 12/09/22 07:46:31.559
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:46:31.573
Dec  9 07:46:31.573: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename dns 12/09/22 07:46:31.574
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:46:31.602
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:46:31.606
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193
STEP: Creating a test headless service 12/09/22 07:46:31.609
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2761 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-2761;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2761 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-2761;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2761.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-2761.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2761.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-2761.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-2761.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-2761.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-2761.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-2761.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-2761.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-2761.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-2761.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-2761.svc;check="$$(dig +notcp +noall +answer +search 249.180.3.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.3.180.249_udp@PTR;check="$$(dig +tcp +noall +answer +search 249.180.3.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.3.180.249_tcp@PTR;sleep 1; done
 12/09/22 07:46:31.644
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2761 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-2761;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2761 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-2761;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2761.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-2761.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2761.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-2761.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-2761.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-2761.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-2761.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-2761.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-2761.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-2761.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-2761.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-2761.svc;check="$$(dig +notcp +noall +answer +search 249.180.3.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.3.180.249_udp@PTR;check="$$(dig +tcp +noall +answer +search 249.180.3.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.3.180.249_tcp@PTR;sleep 1; done
 12/09/22 07:46:31.644
STEP: creating a pod to probe DNS 12/09/22 07:46:31.644
STEP: submitting the pod to kubernetes 12/09/22 07:46:31.644
Dec  9 07:46:31.665: INFO: Waiting up to 15m0s for pod "dns-test-b238c129-a07e-454c-841c-7dce83cb65be" in namespace "dns-2761" to be "running"
Dec  9 07:46:31.672: INFO: Pod "dns-test-b238c129-a07e-454c-841c-7dce83cb65be": Phase="Pending", Reason="", readiness=false. Elapsed: 7.559576ms
Dec  9 07:46:33.677: INFO: Pod "dns-test-b238c129-a07e-454c-841c-7dce83cb65be": Phase="Running", Reason="", readiness=true. Elapsed: 2.012138849s
Dec  9 07:46:33.677: INFO: Pod "dns-test-b238c129-a07e-454c-841c-7dce83cb65be" satisfied condition "running"
STEP: retrieving the pod 12/09/22 07:46:33.677
STEP: looking for the results for each expected name from probers 12/09/22 07:46:33.681
Dec  9 07:46:33.685: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-2761/dns-test-b238c129-a07e-454c-841c-7dce83cb65be: the server could not find the requested resource (get pods dns-test-b238c129-a07e-454c-841c-7dce83cb65be)
Dec  9 07:46:33.688: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-2761/dns-test-b238c129-a07e-454c-841c-7dce83cb65be: the server could not find the requested resource (get pods dns-test-b238c129-a07e-454c-841c-7dce83cb65be)
Dec  9 07:46:33.691: INFO: Unable to read wheezy_udp@dns-test-service.dns-2761 from pod dns-2761/dns-test-b238c129-a07e-454c-841c-7dce83cb65be: the server could not find the requested resource (get pods dns-test-b238c129-a07e-454c-841c-7dce83cb65be)
Dec  9 07:46:33.695: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2761 from pod dns-2761/dns-test-b238c129-a07e-454c-841c-7dce83cb65be: the server could not find the requested resource (get pods dns-test-b238c129-a07e-454c-841c-7dce83cb65be)
Dec  9 07:46:33.698: INFO: Unable to read wheezy_udp@dns-test-service.dns-2761.svc from pod dns-2761/dns-test-b238c129-a07e-454c-841c-7dce83cb65be: the server could not find the requested resource (get pods dns-test-b238c129-a07e-454c-841c-7dce83cb65be)
Dec  9 07:46:33.701: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2761.svc from pod dns-2761/dns-test-b238c129-a07e-454c-841c-7dce83cb65be: the server could not find the requested resource (get pods dns-test-b238c129-a07e-454c-841c-7dce83cb65be)
Dec  9 07:46:33.717: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2761.svc from pod dns-2761/dns-test-b238c129-a07e-454c-841c-7dce83cb65be: the server could not find the requested resource (get pods dns-test-b238c129-a07e-454c-841c-7dce83cb65be)
Dec  9 07:46:33.721: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2761.svc from pod dns-2761/dns-test-b238c129-a07e-454c-841c-7dce83cb65be: the server could not find the requested resource (get pods dns-test-b238c129-a07e-454c-841c-7dce83cb65be)
Dec  9 07:46:33.737: INFO: Unable to read jessie_udp@dns-test-service from pod dns-2761/dns-test-b238c129-a07e-454c-841c-7dce83cb65be: the server could not find the requested resource (get pods dns-test-b238c129-a07e-454c-841c-7dce83cb65be)
Dec  9 07:46:33.741: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-2761/dns-test-b238c129-a07e-454c-841c-7dce83cb65be: the server could not find the requested resource (get pods dns-test-b238c129-a07e-454c-841c-7dce83cb65be)
Dec  9 07:46:33.744: INFO: Unable to read jessie_udp@dns-test-service.dns-2761 from pod dns-2761/dns-test-b238c129-a07e-454c-841c-7dce83cb65be: the server could not find the requested resource (get pods dns-test-b238c129-a07e-454c-841c-7dce83cb65be)
Dec  9 07:46:33.748: INFO: Unable to read jessie_tcp@dns-test-service.dns-2761 from pod dns-2761/dns-test-b238c129-a07e-454c-841c-7dce83cb65be: the server could not find the requested resource (get pods dns-test-b238c129-a07e-454c-841c-7dce83cb65be)
Dec  9 07:46:33.752: INFO: Unable to read jessie_udp@dns-test-service.dns-2761.svc from pod dns-2761/dns-test-b238c129-a07e-454c-841c-7dce83cb65be: the server could not find the requested resource (get pods dns-test-b238c129-a07e-454c-841c-7dce83cb65be)
Dec  9 07:46:33.756: INFO: Unable to read jessie_tcp@dns-test-service.dns-2761.svc from pod dns-2761/dns-test-b238c129-a07e-454c-841c-7dce83cb65be: the server could not find the requested resource (get pods dns-test-b238c129-a07e-454c-841c-7dce83cb65be)
Dec  9 07:46:33.761: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2761.svc from pod dns-2761/dns-test-b238c129-a07e-454c-841c-7dce83cb65be: the server could not find the requested resource (get pods dns-test-b238c129-a07e-454c-841c-7dce83cb65be)
Dec  9 07:46:33.767: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2761.svc from pod dns-2761/dns-test-b238c129-a07e-454c-841c-7dce83cb65be: the server could not find the requested resource (get pods dns-test-b238c129-a07e-454c-841c-7dce83cb65be)
Dec  9 07:46:33.791: INFO: Lookups using dns-2761/dns-test-b238c129-a07e-454c-841c-7dce83cb65be failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-2761 wheezy_tcp@dns-test-service.dns-2761 wheezy_udp@dns-test-service.dns-2761.svc wheezy_tcp@dns-test-service.dns-2761.svc wheezy_udp@_http._tcp.dns-test-service.dns-2761.svc wheezy_tcp@_http._tcp.dns-test-service.dns-2761.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-2761 jessie_tcp@dns-test-service.dns-2761 jessie_udp@dns-test-service.dns-2761.svc jessie_tcp@dns-test-service.dns-2761.svc jessie_udp@_http._tcp.dns-test-service.dns-2761.svc jessie_tcp@_http._tcp.dns-test-service.dns-2761.svc]

Dec  9 07:46:38.877: INFO: DNS probes using dns-2761/dns-test-b238c129-a07e-454c-841c-7dce83cb65be succeeded

STEP: deleting the pod 12/09/22 07:46:38.877
STEP: deleting the test service 12/09/22 07:46:38.901
STEP: deleting the test headless service 12/09/22 07:46:38.957
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Dec  9 07:46:38.977: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-2761" for this suite. 12/09/22 07:46:38.984
------------------------------
â€¢ [SLOW TEST] [7.422 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:46:31.573
    Dec  9 07:46:31.573: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename dns 12/09/22 07:46:31.574
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:46:31.602
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:46:31.606
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
      test/e2e/network/dns.go:193
    STEP: Creating a test headless service 12/09/22 07:46:31.609
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2761 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-2761;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2761 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-2761;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2761.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-2761.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2761.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-2761.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-2761.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-2761.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-2761.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-2761.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-2761.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-2761.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-2761.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-2761.svc;check="$$(dig +notcp +noall +answer +search 249.180.3.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.3.180.249_udp@PTR;check="$$(dig +tcp +noall +answer +search 249.180.3.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.3.180.249_tcp@PTR;sleep 1; done
     12/09/22 07:46:31.644
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2761 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-2761;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2761 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-2761;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2761.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-2761.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2761.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-2761.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-2761.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-2761.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-2761.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-2761.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-2761.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-2761.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-2761.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-2761.svc;check="$$(dig +notcp +noall +answer +search 249.180.3.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.3.180.249_udp@PTR;check="$$(dig +tcp +noall +answer +search 249.180.3.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.3.180.249_tcp@PTR;sleep 1; done
     12/09/22 07:46:31.644
    STEP: creating a pod to probe DNS 12/09/22 07:46:31.644
    STEP: submitting the pod to kubernetes 12/09/22 07:46:31.644
    Dec  9 07:46:31.665: INFO: Waiting up to 15m0s for pod "dns-test-b238c129-a07e-454c-841c-7dce83cb65be" in namespace "dns-2761" to be "running"
    Dec  9 07:46:31.672: INFO: Pod "dns-test-b238c129-a07e-454c-841c-7dce83cb65be": Phase="Pending", Reason="", readiness=false. Elapsed: 7.559576ms
    Dec  9 07:46:33.677: INFO: Pod "dns-test-b238c129-a07e-454c-841c-7dce83cb65be": Phase="Running", Reason="", readiness=true. Elapsed: 2.012138849s
    Dec  9 07:46:33.677: INFO: Pod "dns-test-b238c129-a07e-454c-841c-7dce83cb65be" satisfied condition "running"
    STEP: retrieving the pod 12/09/22 07:46:33.677
    STEP: looking for the results for each expected name from probers 12/09/22 07:46:33.681
    Dec  9 07:46:33.685: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-2761/dns-test-b238c129-a07e-454c-841c-7dce83cb65be: the server could not find the requested resource (get pods dns-test-b238c129-a07e-454c-841c-7dce83cb65be)
    Dec  9 07:46:33.688: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-2761/dns-test-b238c129-a07e-454c-841c-7dce83cb65be: the server could not find the requested resource (get pods dns-test-b238c129-a07e-454c-841c-7dce83cb65be)
    Dec  9 07:46:33.691: INFO: Unable to read wheezy_udp@dns-test-service.dns-2761 from pod dns-2761/dns-test-b238c129-a07e-454c-841c-7dce83cb65be: the server could not find the requested resource (get pods dns-test-b238c129-a07e-454c-841c-7dce83cb65be)
    Dec  9 07:46:33.695: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2761 from pod dns-2761/dns-test-b238c129-a07e-454c-841c-7dce83cb65be: the server could not find the requested resource (get pods dns-test-b238c129-a07e-454c-841c-7dce83cb65be)
    Dec  9 07:46:33.698: INFO: Unable to read wheezy_udp@dns-test-service.dns-2761.svc from pod dns-2761/dns-test-b238c129-a07e-454c-841c-7dce83cb65be: the server could not find the requested resource (get pods dns-test-b238c129-a07e-454c-841c-7dce83cb65be)
    Dec  9 07:46:33.701: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2761.svc from pod dns-2761/dns-test-b238c129-a07e-454c-841c-7dce83cb65be: the server could not find the requested resource (get pods dns-test-b238c129-a07e-454c-841c-7dce83cb65be)
    Dec  9 07:46:33.717: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2761.svc from pod dns-2761/dns-test-b238c129-a07e-454c-841c-7dce83cb65be: the server could not find the requested resource (get pods dns-test-b238c129-a07e-454c-841c-7dce83cb65be)
    Dec  9 07:46:33.721: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2761.svc from pod dns-2761/dns-test-b238c129-a07e-454c-841c-7dce83cb65be: the server could not find the requested resource (get pods dns-test-b238c129-a07e-454c-841c-7dce83cb65be)
    Dec  9 07:46:33.737: INFO: Unable to read jessie_udp@dns-test-service from pod dns-2761/dns-test-b238c129-a07e-454c-841c-7dce83cb65be: the server could not find the requested resource (get pods dns-test-b238c129-a07e-454c-841c-7dce83cb65be)
    Dec  9 07:46:33.741: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-2761/dns-test-b238c129-a07e-454c-841c-7dce83cb65be: the server could not find the requested resource (get pods dns-test-b238c129-a07e-454c-841c-7dce83cb65be)
    Dec  9 07:46:33.744: INFO: Unable to read jessie_udp@dns-test-service.dns-2761 from pod dns-2761/dns-test-b238c129-a07e-454c-841c-7dce83cb65be: the server could not find the requested resource (get pods dns-test-b238c129-a07e-454c-841c-7dce83cb65be)
    Dec  9 07:46:33.748: INFO: Unable to read jessie_tcp@dns-test-service.dns-2761 from pod dns-2761/dns-test-b238c129-a07e-454c-841c-7dce83cb65be: the server could not find the requested resource (get pods dns-test-b238c129-a07e-454c-841c-7dce83cb65be)
    Dec  9 07:46:33.752: INFO: Unable to read jessie_udp@dns-test-service.dns-2761.svc from pod dns-2761/dns-test-b238c129-a07e-454c-841c-7dce83cb65be: the server could not find the requested resource (get pods dns-test-b238c129-a07e-454c-841c-7dce83cb65be)
    Dec  9 07:46:33.756: INFO: Unable to read jessie_tcp@dns-test-service.dns-2761.svc from pod dns-2761/dns-test-b238c129-a07e-454c-841c-7dce83cb65be: the server could not find the requested resource (get pods dns-test-b238c129-a07e-454c-841c-7dce83cb65be)
    Dec  9 07:46:33.761: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2761.svc from pod dns-2761/dns-test-b238c129-a07e-454c-841c-7dce83cb65be: the server could not find the requested resource (get pods dns-test-b238c129-a07e-454c-841c-7dce83cb65be)
    Dec  9 07:46:33.767: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2761.svc from pod dns-2761/dns-test-b238c129-a07e-454c-841c-7dce83cb65be: the server could not find the requested resource (get pods dns-test-b238c129-a07e-454c-841c-7dce83cb65be)
    Dec  9 07:46:33.791: INFO: Lookups using dns-2761/dns-test-b238c129-a07e-454c-841c-7dce83cb65be failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-2761 wheezy_tcp@dns-test-service.dns-2761 wheezy_udp@dns-test-service.dns-2761.svc wheezy_tcp@dns-test-service.dns-2761.svc wheezy_udp@_http._tcp.dns-test-service.dns-2761.svc wheezy_tcp@_http._tcp.dns-test-service.dns-2761.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-2761 jessie_tcp@dns-test-service.dns-2761 jessie_udp@dns-test-service.dns-2761.svc jessie_tcp@dns-test-service.dns-2761.svc jessie_udp@_http._tcp.dns-test-service.dns-2761.svc jessie_tcp@_http._tcp.dns-test-service.dns-2761.svc]

    Dec  9 07:46:38.877: INFO: DNS probes using dns-2761/dns-test-b238c129-a07e-454c-841c-7dce83cb65be succeeded

    STEP: deleting the pod 12/09/22 07:46:38.877
    STEP: deleting the test service 12/09/22 07:46:38.901
    STEP: deleting the test headless service 12/09/22 07:46:38.957
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:46:38.977: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-2761" for this suite. 12/09/22 07:46:38.984
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints
  verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  test/e2e/scheduling/preemption.go:806
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:46:39.001
Dec  9 07:46:39.001: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename sched-preemption 12/09/22 07:46:39.002
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:46:39.029
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:46:39.036
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:96
Dec  9 07:46:39.057: INFO: Waiting up to 1m0s for all nodes to be ready
Dec  9 07:47:39.094: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PriorityClass endpoints
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:47:39.105
Dec  9 07:47:39.105: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename sched-preemption-path 12/09/22 07:47:39.107
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:47:39.136
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:47:39.14
[BeforeEach] PriorityClass endpoints
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] PriorityClass endpoints
  test/e2e/scheduling/preemption.go:763
[It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  test/e2e/scheduling/preemption.go:806
Dec  9 07:47:39.163: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
Dec  9 07:47:39.167: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
[AfterEach] PriorityClass endpoints
  test/e2e/framework/node/init/init.go:32
Dec  9 07:47:39.189: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] PriorityClass endpoints
  test/e2e/scheduling/preemption.go:779
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/node/init/init.go:32
Dec  9 07:47:39.201: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:84
[DeferCleanup (Each)] PriorityClass endpoints
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] PriorityClass endpoints
  dump namespaces | framework.go:196
[DeferCleanup (Each)] PriorityClass endpoints
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-path-7223" for this suite. 12/09/22 07:47:39.238
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-2572" for this suite. 12/09/22 07:47:39.248
------------------------------
â€¢ [SLOW TEST] [60.254 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  PriorityClass endpoints
  test/e2e/scheduling/preemption.go:756
    verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
    test/e2e/scheduling/preemption.go:806

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:46:39.001
    Dec  9 07:46:39.001: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename sched-preemption 12/09/22 07:46:39.002
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:46:39.029
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:46:39.036
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:96
    Dec  9 07:46:39.057: INFO: Waiting up to 1m0s for all nodes to be ready
    Dec  9 07:47:39.094: INFO: Waiting for terminating namespaces to be deleted...
    [BeforeEach] PriorityClass endpoints
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:47:39.105
    Dec  9 07:47:39.105: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename sched-preemption-path 12/09/22 07:47:39.107
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:47:39.136
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:47:39.14
    [BeforeEach] PriorityClass endpoints
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] PriorityClass endpoints
      test/e2e/scheduling/preemption.go:763
    [It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
      test/e2e/scheduling/preemption.go:806
    Dec  9 07:47:39.163: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
    Dec  9 07:47:39.167: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
    [AfterEach] PriorityClass endpoints
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:47:39.189: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] PriorityClass endpoints
      test/e2e/scheduling/preemption.go:779
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:47:39.201: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:84
    [DeferCleanup (Each)] PriorityClass endpoints
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] PriorityClass endpoints
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] PriorityClass endpoints
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-path-7223" for this suite. 12/09/22 07:47:39.238
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-2572" for this suite. 12/09/22 07:47:39.248
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:67
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:47:39.261
Dec  9 07:47:39.261: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename projected 12/09/22 07:47:39.262
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:47:39.283
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:47:39.288
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:67
STEP: Creating projection with secret that has name projected-secret-test-73ab379a-d9fd-481b-ae66-4df25b1cf581 12/09/22 07:47:39.293
STEP: Creating a pod to test consume secrets 12/09/22 07:47:39.297
Dec  9 07:47:39.306: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-8a0d06f3-8018-407f-9b31-bd2a4f34974d" in namespace "projected-4336" to be "Succeeded or Failed"
Dec  9 07:47:39.310: INFO: Pod "pod-projected-secrets-8a0d06f3-8018-407f-9b31-bd2a4f34974d": Phase="Pending", Reason="", readiness=false. Elapsed: 3.755437ms
Dec  9 07:47:41.314: INFO: Pod "pod-projected-secrets-8a0d06f3-8018-407f-9b31-bd2a4f34974d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008184151s
Dec  9 07:47:43.314: INFO: Pod "pod-projected-secrets-8a0d06f3-8018-407f-9b31-bd2a4f34974d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00856113s
STEP: Saw pod success 12/09/22 07:47:43.315
Dec  9 07:47:43.315: INFO: Pod "pod-projected-secrets-8a0d06f3-8018-407f-9b31-bd2a4f34974d" satisfied condition "Succeeded or Failed"
Dec  9 07:47:43.318: INFO: Trying to get logs from node ip-10-0-10-179 pod pod-projected-secrets-8a0d06f3-8018-407f-9b31-bd2a4f34974d container projected-secret-volume-test: <nil>
STEP: delete the pod 12/09/22 07:47:43.324
Dec  9 07:47:43.335: INFO: Waiting for pod pod-projected-secrets-8a0d06f3-8018-407f-9b31-bd2a4f34974d to disappear
Dec  9 07:47:43.338: INFO: Pod pod-projected-secrets-8a0d06f3-8018-407f-9b31-bd2a4f34974d no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Dec  9 07:47:43.338: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-4336" for this suite. 12/09/22 07:47:43.342
------------------------------
â€¢ [4.086 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:47:39.261
    Dec  9 07:47:39.261: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename projected 12/09/22 07:47:39.262
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:47:39.283
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:47:39.288
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:67
    STEP: Creating projection with secret that has name projected-secret-test-73ab379a-d9fd-481b-ae66-4df25b1cf581 12/09/22 07:47:39.293
    STEP: Creating a pod to test consume secrets 12/09/22 07:47:39.297
    Dec  9 07:47:39.306: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-8a0d06f3-8018-407f-9b31-bd2a4f34974d" in namespace "projected-4336" to be "Succeeded or Failed"
    Dec  9 07:47:39.310: INFO: Pod "pod-projected-secrets-8a0d06f3-8018-407f-9b31-bd2a4f34974d": Phase="Pending", Reason="", readiness=false. Elapsed: 3.755437ms
    Dec  9 07:47:41.314: INFO: Pod "pod-projected-secrets-8a0d06f3-8018-407f-9b31-bd2a4f34974d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008184151s
    Dec  9 07:47:43.314: INFO: Pod "pod-projected-secrets-8a0d06f3-8018-407f-9b31-bd2a4f34974d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00856113s
    STEP: Saw pod success 12/09/22 07:47:43.315
    Dec  9 07:47:43.315: INFO: Pod "pod-projected-secrets-8a0d06f3-8018-407f-9b31-bd2a4f34974d" satisfied condition "Succeeded or Failed"
    Dec  9 07:47:43.318: INFO: Trying to get logs from node ip-10-0-10-179 pod pod-projected-secrets-8a0d06f3-8018-407f-9b31-bd2a4f34974d container projected-secret-volume-test: <nil>
    STEP: delete the pod 12/09/22 07:47:43.324
    Dec  9 07:47:43.335: INFO: Waiting for pod pod-projected-secrets-8a0d06f3-8018-407f-9b31-bd2a4f34974d to disappear
    Dec  9 07:47:43.338: INFO: Pod pod-projected-secrets-8a0d06f3-8018-407f-9b31-bd2a4f34974d no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:47:43.338: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-4336" for this suite. 12/09/22 07:47:43.342
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:582
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:47:43.348
Dec  9 07:47:43.348: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename webhook 12/09/22 07:47:43.349
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:47:43.365
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:47:43.37
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 12/09/22 07:47:43.393
STEP: Create role binding to let webhook read extension-apiserver-authentication 12/09/22 07:47:43.89
STEP: Deploying the webhook pod 12/09/22 07:47:43.903
STEP: Wait for the deployment to be ready 12/09/22 07:47:43.928
Dec  9 07:47:43.939: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 12/09/22 07:47:45.948
STEP: Verifying the service has paired with the endpoint 12/09/22 07:47:45.957
Dec  9 07:47:46.958: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:582
STEP: Listing all of the created validation webhooks 12/09/22 07:47:47.026
STEP: Creating a configMap that does not comply to the validation webhook rules 12/09/22 07:47:47.071
STEP: Deleting the collection of validation webhooks 12/09/22 07:47:47.105
STEP: Creating a configMap that does not comply to the validation webhook rules 12/09/22 07:47:47.14
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Dec  9 07:47:47.149: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-9161" for this suite. 12/09/22 07:47:47.215
STEP: Destroying namespace "webhook-9161-markers" for this suite. 12/09/22 07:47:47.228
------------------------------
â€¢ [3.889 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:582

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:47:43.348
    Dec  9 07:47:43.348: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename webhook 12/09/22 07:47:43.349
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:47:43.365
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:47:43.37
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 12/09/22 07:47:43.393
    STEP: Create role binding to let webhook read extension-apiserver-authentication 12/09/22 07:47:43.89
    STEP: Deploying the webhook pod 12/09/22 07:47:43.903
    STEP: Wait for the deployment to be ready 12/09/22 07:47:43.928
    Dec  9 07:47:43.939: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 12/09/22 07:47:45.948
    STEP: Verifying the service has paired with the endpoint 12/09/22 07:47:45.957
    Dec  9 07:47:46.958: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] listing validating webhooks should work [Conformance]
      test/e2e/apimachinery/webhook.go:582
    STEP: Listing all of the created validation webhooks 12/09/22 07:47:47.026
    STEP: Creating a configMap that does not comply to the validation webhook rules 12/09/22 07:47:47.071
    STEP: Deleting the collection of validation webhooks 12/09/22 07:47:47.105
    STEP: Creating a configMap that does not comply to the validation webhook rules 12/09/22 07:47:47.14
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:47:47.149: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-9161" for this suite. 12/09/22 07:47:47.215
    STEP: Destroying namespace "webhook-9161-markers" for this suite. 12/09/22 07:47:47.228
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-network] Services
  should delete a collection of services [Conformance]
  test/e2e/network/service.go:3654
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:47:47.238
Dec  9 07:47:47.238: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename services 12/09/22 07:47:47.239
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:47:47.259
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:47:47.263
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should delete a collection of services [Conformance]
  test/e2e/network/service.go:3654
STEP: creating a collection of services 12/09/22 07:47:47.267
Dec  9 07:47:47.268: INFO: Creating e2e-svc-a-ttx59
Dec  9 07:47:47.277: INFO: Creating e2e-svc-b-xxl2z
Dec  9 07:47:47.288: INFO: Creating e2e-svc-c-2jcpj
STEP: deleting service collection 12/09/22 07:47:47.304
Dec  9 07:47:47.332: INFO: Collection of services has been deleted
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Dec  9 07:47:47.333: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-2377" for this suite. 12/09/22 07:47:47.336
------------------------------
â€¢ [0.103 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should delete a collection of services [Conformance]
  test/e2e/network/service.go:3654

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:47:47.238
    Dec  9 07:47:47.238: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename services 12/09/22 07:47:47.239
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:47:47.259
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:47:47.263
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should delete a collection of services [Conformance]
      test/e2e/network/service.go:3654
    STEP: creating a collection of services 12/09/22 07:47:47.267
    Dec  9 07:47:47.268: INFO: Creating e2e-svc-a-ttx59
    Dec  9 07:47:47.277: INFO: Creating e2e-svc-b-xxl2z
    Dec  9 07:47:47.288: INFO: Creating e2e-svc-c-2jcpj
    STEP: deleting service collection 12/09/22 07:47:47.304
    Dec  9 07:47:47.332: INFO: Collection of services has been deleted
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:47:47.333: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-2377" for this suite. 12/09/22 07:47:47.336
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:197
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:47:47.348
Dec  9 07:47:47.348: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename emptydir 12/09/22 07:47:47.349
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:47:47.365
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:47:47.369
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:197
STEP: Creating a pod to test emptydir 0644 on node default medium 12/09/22 07:47:47.374
Dec  9 07:47:47.381: INFO: Waiting up to 5m0s for pod "pod-d27f8c0e-d51f-4025-98f3-f26b230cb551" in namespace "emptydir-8709" to be "Succeeded or Failed"
Dec  9 07:47:47.386: INFO: Pod "pod-d27f8c0e-d51f-4025-98f3-f26b230cb551": Phase="Pending", Reason="", readiness=false. Elapsed: 4.809905ms
Dec  9 07:47:49.391: INFO: Pod "pod-d27f8c0e-d51f-4025-98f3-f26b230cb551": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009982874s
Dec  9 07:47:51.391: INFO: Pod "pod-d27f8c0e-d51f-4025-98f3-f26b230cb551": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009797963s
STEP: Saw pod success 12/09/22 07:47:51.391
Dec  9 07:47:51.391: INFO: Pod "pod-d27f8c0e-d51f-4025-98f3-f26b230cb551" satisfied condition "Succeeded or Failed"
Dec  9 07:47:51.395: INFO: Trying to get logs from node ip-10-0-10-179 pod pod-d27f8c0e-d51f-4025-98f3-f26b230cb551 container test-container: <nil>
STEP: delete the pod 12/09/22 07:47:51.4
Dec  9 07:47:51.411: INFO: Waiting for pod pod-d27f8c0e-d51f-4025-98f3-f26b230cb551 to disappear
Dec  9 07:47:51.418: INFO: Pod pod-d27f8c0e-d51f-4025-98f3-f26b230cb551 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Dec  9 07:47:51.418: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-8709" for this suite. 12/09/22 07:47:51.425
------------------------------
â€¢ [4.086 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:197

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:47:47.348
    Dec  9 07:47:47.348: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename emptydir 12/09/22 07:47:47.349
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:47:47.365
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:47:47.369
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:197
    STEP: Creating a pod to test emptydir 0644 on node default medium 12/09/22 07:47:47.374
    Dec  9 07:47:47.381: INFO: Waiting up to 5m0s for pod "pod-d27f8c0e-d51f-4025-98f3-f26b230cb551" in namespace "emptydir-8709" to be "Succeeded or Failed"
    Dec  9 07:47:47.386: INFO: Pod "pod-d27f8c0e-d51f-4025-98f3-f26b230cb551": Phase="Pending", Reason="", readiness=false. Elapsed: 4.809905ms
    Dec  9 07:47:49.391: INFO: Pod "pod-d27f8c0e-d51f-4025-98f3-f26b230cb551": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009982874s
    Dec  9 07:47:51.391: INFO: Pod "pod-d27f8c0e-d51f-4025-98f3-f26b230cb551": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009797963s
    STEP: Saw pod success 12/09/22 07:47:51.391
    Dec  9 07:47:51.391: INFO: Pod "pod-d27f8c0e-d51f-4025-98f3-f26b230cb551" satisfied condition "Succeeded or Failed"
    Dec  9 07:47:51.395: INFO: Trying to get logs from node ip-10-0-10-179 pod pod-d27f8c0e-d51f-4025-98f3-f26b230cb551 container test-container: <nil>
    STEP: delete the pod 12/09/22 07:47:51.4
    Dec  9 07:47:51.411: INFO: Waiting for pod pod-d27f8c0e-d51f-4025-98f3-f26b230cb551 to disappear
    Dec  9 07:47:51.418: INFO: Pod pod-d27f8c0e-d51f-4025-98f3-f26b230cb551 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:47:51.418: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-8709" for this suite. 12/09/22 07:47:51.425
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:47:51.436
Dec  9 07:47:51.436: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename watch 12/09/22 07:47:51.437
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:47:51.48
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:47:51.489
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257
STEP: creating a watch on configmaps with a certain label 12/09/22 07:47:51.493
STEP: creating a new configmap 12/09/22 07:47:51.495
STEP: modifying the configmap once 12/09/22 07:47:51.5
STEP: changing the label value of the configmap 12/09/22 07:47:51.508
STEP: Expecting to observe a delete notification for the watched object 12/09/22 07:47:51.521
Dec  9 07:47:51.521: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-4703  ed7eb297-d426-4be3-aa74-d194033636dc 18995 0 2022-12-09 07:47:51 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2022-12-09 07:47:51 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Dec  9 07:47:51.521: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-4703  ed7eb297-d426-4be3-aa74-d194033636dc 18996 0 2022-12-09 07:47:51 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2022-12-09 07:47:51 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Dec  9 07:47:51.521: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-4703  ed7eb297-d426-4be3-aa74-d194033636dc 18997 0 2022-12-09 07:47:51 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2022-12-09 07:47:51 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time 12/09/22 07:47:51.522
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements 12/09/22 07:47:51.536
STEP: changing the label value of the configmap back 12/09/22 07:48:01.536
STEP: modifying the configmap a third time 12/09/22 07:48:01.55
STEP: deleting the configmap 12/09/22 07:48:01.557
STEP: Expecting to observe an add notification for the watched object when the label value was restored 12/09/22 07:48:01.563
Dec  9 07:48:01.563: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-4703  ed7eb297-d426-4be3-aa74-d194033636dc 19042 0 2022-12-09 07:47:51 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2022-12-09 07:48:01 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Dec  9 07:48:01.564: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-4703  ed7eb297-d426-4be3-aa74-d194033636dc 19043 0 2022-12-09 07:47:51 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2022-12-09 07:48:01 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
Dec  9 07:48:01.564: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-4703  ed7eb297-d426-4be3-aa74-d194033636dc 19044 0 2022-12-09 07:47:51 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2022-12-09 07:48:01 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Dec  9 07:48:01.564: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-4703" for this suite. 12/09/22 07:48:01.568
------------------------------
â€¢ [SLOW TEST] [10.138 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:47:51.436
    Dec  9 07:47:51.436: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename watch 12/09/22 07:47:51.437
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:47:51.48
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:47:51.489
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
      test/e2e/apimachinery/watch.go:257
    STEP: creating a watch on configmaps with a certain label 12/09/22 07:47:51.493
    STEP: creating a new configmap 12/09/22 07:47:51.495
    STEP: modifying the configmap once 12/09/22 07:47:51.5
    STEP: changing the label value of the configmap 12/09/22 07:47:51.508
    STEP: Expecting to observe a delete notification for the watched object 12/09/22 07:47:51.521
    Dec  9 07:47:51.521: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-4703  ed7eb297-d426-4be3-aa74-d194033636dc 18995 0 2022-12-09 07:47:51 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2022-12-09 07:47:51 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Dec  9 07:47:51.521: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-4703  ed7eb297-d426-4be3-aa74-d194033636dc 18996 0 2022-12-09 07:47:51 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2022-12-09 07:47:51 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    Dec  9 07:47:51.521: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-4703  ed7eb297-d426-4be3-aa74-d194033636dc 18997 0 2022-12-09 07:47:51 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2022-12-09 07:47:51 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying the configmap a second time 12/09/22 07:47:51.522
    STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements 12/09/22 07:47:51.536
    STEP: changing the label value of the configmap back 12/09/22 07:48:01.536
    STEP: modifying the configmap a third time 12/09/22 07:48:01.55
    STEP: deleting the configmap 12/09/22 07:48:01.557
    STEP: Expecting to observe an add notification for the watched object when the label value was restored 12/09/22 07:48:01.563
    Dec  9 07:48:01.563: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-4703  ed7eb297-d426-4be3-aa74-d194033636dc 19042 0 2022-12-09 07:47:51 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2022-12-09 07:48:01 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Dec  9 07:48:01.564: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-4703  ed7eb297-d426-4be3-aa74-d194033636dc 19043 0 2022-12-09 07:47:51 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2022-12-09 07:48:01 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
    Dec  9 07:48:01.564: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-4703  ed7eb297-d426-4be3-aa74-d194033636dc 19044 0 2022-12-09 07:47:51 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2022-12-09 07:48:01 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:48:01.564: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-4703" for this suite. 12/09/22 07:48:01.568
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Ingress API
  should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552
[BeforeEach] [sig-network] Ingress API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:48:01.576
Dec  9 07:48:01.576: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename ingress 12/09/22 07:48:01.579
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:48:01.611
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:48:01.622
[BeforeEach] [sig-network] Ingress API
  test/e2e/framework/metrics/init/init.go:31
[It] should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552
STEP: getting /apis 12/09/22 07:48:01.63
STEP: getting /apis/networking.k8s.io 12/09/22 07:48:01.635
STEP: getting /apis/networking.k8s.iov1 12/09/22 07:48:01.637
STEP: creating 12/09/22 07:48:01.639
STEP: getting 12/09/22 07:48:01.724
STEP: listing 12/09/22 07:48:01.732
STEP: watching 12/09/22 07:48:01.736
Dec  9 07:48:01.736: INFO: starting watch
STEP: cluster-wide listing 12/09/22 07:48:01.752
STEP: cluster-wide watching 12/09/22 07:48:01.764
Dec  9 07:48:01.764: INFO: starting watch
STEP: patching 12/09/22 07:48:01.78
STEP: updating 12/09/22 07:48:01.803
Dec  9 07:48:01.837: INFO: waiting for watch events with expected annotations
Dec  9 07:48:01.837: INFO: saw patched and updated annotations
STEP: patching /status 12/09/22 07:48:01.837
STEP: updating /status 12/09/22 07:48:01.853
STEP: get /status 12/09/22 07:48:01.88
STEP: deleting 12/09/22 07:48:01.893
STEP: deleting a collection 12/09/22 07:48:01.958
[AfterEach] [sig-network] Ingress API
  test/e2e/framework/node/init/init.go:32
Dec  9 07:48:01.973: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Ingress API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Ingress API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Ingress API
  tear down framework | framework.go:193
STEP: Destroying namespace "ingress-3274" for this suite. 12/09/22 07:48:01.997
------------------------------
â€¢ [0.433 seconds]
[sig-network] Ingress API
test/e2e/network/common/framework.go:23
  should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Ingress API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:48:01.576
    Dec  9 07:48:01.576: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename ingress 12/09/22 07:48:01.579
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:48:01.611
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:48:01.622
    [BeforeEach] [sig-network] Ingress API
      test/e2e/framework/metrics/init/init.go:31
    [It] should support creating Ingress API operations [Conformance]
      test/e2e/network/ingress.go:552
    STEP: getting /apis 12/09/22 07:48:01.63
    STEP: getting /apis/networking.k8s.io 12/09/22 07:48:01.635
    STEP: getting /apis/networking.k8s.iov1 12/09/22 07:48:01.637
    STEP: creating 12/09/22 07:48:01.639
    STEP: getting 12/09/22 07:48:01.724
    STEP: listing 12/09/22 07:48:01.732
    STEP: watching 12/09/22 07:48:01.736
    Dec  9 07:48:01.736: INFO: starting watch
    STEP: cluster-wide listing 12/09/22 07:48:01.752
    STEP: cluster-wide watching 12/09/22 07:48:01.764
    Dec  9 07:48:01.764: INFO: starting watch
    STEP: patching 12/09/22 07:48:01.78
    STEP: updating 12/09/22 07:48:01.803
    Dec  9 07:48:01.837: INFO: waiting for watch events with expected annotations
    Dec  9 07:48:01.837: INFO: saw patched and updated annotations
    STEP: patching /status 12/09/22 07:48:01.837
    STEP: updating /status 12/09/22 07:48:01.853
    STEP: get /status 12/09/22 07:48:01.88
    STEP: deleting 12/09/22 07:48:01.893
    STEP: deleting a collection 12/09/22 07:48:01.958
    [AfterEach] [sig-network] Ingress API
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:48:01.973: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Ingress API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Ingress API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Ingress API
      tear down framework | framework.go:193
    STEP: Destroying namespace "ingress-3274" for this suite. 12/09/22 07:48:01.997
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1438
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:48:02.013
Dec  9 07:48:02.014: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename services 12/09/22 07:48:02.015
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:48:02.037
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:48:02.042
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1438
STEP: creating a service externalname-service with the type=ExternalName in namespace services-5097 12/09/22 07:48:02.046
STEP: changing the ExternalName service to type=ClusterIP 12/09/22 07:48:02.066
STEP: creating replication controller externalname-service in namespace services-5097 12/09/22 07:48:02.099
I1209 07:48:02.126851      20 runners.go:193] Created replication controller with name: externalname-service, namespace: services-5097, replica count: 2
I1209 07:48:05.179530      20 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Dec  9 07:48:05.179: INFO: Creating new exec pod
Dec  9 07:48:05.187: INFO: Waiting up to 5m0s for pod "execpodnq66j" in namespace "services-5097" to be "running"
Dec  9 07:48:05.190: INFO: Pod "execpodnq66j": Phase="Pending", Reason="", readiness=false. Elapsed: 3.294695ms
Dec  9 07:48:07.194: INFO: Pod "execpodnq66j": Phase="Running", Reason="", readiness=true. Elapsed: 2.00771975s
Dec  9 07:48:07.195: INFO: Pod "execpodnq66j" satisfied condition "running"
Dec  9 07:48:08.195: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=services-5097 exec execpodnq66j -- /bin/sh -x -c nc -v -z -w 2 externalname-service 80'
Dec  9 07:48:08.368: INFO: stderr: "+ nc -v -z -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Dec  9 07:48:08.368: INFO: stdout: ""
Dec  9 07:48:08.368: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=services-5097 exec execpodnq66j -- /bin/sh -x -c nc -v -z -w 2 10.3.45.189 80'
Dec  9 07:48:08.540: INFO: stderr: "+ nc -v -z -w 2 10.3.45.189 80\nConnection to 10.3.45.189 80 port [tcp/http] succeeded!\n"
Dec  9 07:48:08.540: INFO: stdout: ""
Dec  9 07:48:08.540: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Dec  9 07:48:08.582: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-5097" for this suite. 12/09/22 07:48:08.594
------------------------------
â€¢ [SLOW TEST] [6.587 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1438

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:48:02.013
    Dec  9 07:48:02.014: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename services 12/09/22 07:48:02.015
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:48:02.037
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:48:02.042
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to change the type from ExternalName to ClusterIP [Conformance]
      test/e2e/network/service.go:1438
    STEP: creating a service externalname-service with the type=ExternalName in namespace services-5097 12/09/22 07:48:02.046
    STEP: changing the ExternalName service to type=ClusterIP 12/09/22 07:48:02.066
    STEP: creating replication controller externalname-service in namespace services-5097 12/09/22 07:48:02.099
    I1209 07:48:02.126851      20 runners.go:193] Created replication controller with name: externalname-service, namespace: services-5097, replica count: 2
    I1209 07:48:05.179530      20 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Dec  9 07:48:05.179: INFO: Creating new exec pod
    Dec  9 07:48:05.187: INFO: Waiting up to 5m0s for pod "execpodnq66j" in namespace "services-5097" to be "running"
    Dec  9 07:48:05.190: INFO: Pod "execpodnq66j": Phase="Pending", Reason="", readiness=false. Elapsed: 3.294695ms
    Dec  9 07:48:07.194: INFO: Pod "execpodnq66j": Phase="Running", Reason="", readiness=true. Elapsed: 2.00771975s
    Dec  9 07:48:07.195: INFO: Pod "execpodnq66j" satisfied condition "running"
    Dec  9 07:48:08.195: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=services-5097 exec execpodnq66j -- /bin/sh -x -c nc -v -z -w 2 externalname-service 80'
    Dec  9 07:48:08.368: INFO: stderr: "+ nc -v -z -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Dec  9 07:48:08.368: INFO: stdout: ""
    Dec  9 07:48:08.368: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=services-5097 exec execpodnq66j -- /bin/sh -x -c nc -v -z -w 2 10.3.45.189 80'
    Dec  9 07:48:08.540: INFO: stderr: "+ nc -v -z -w 2 10.3.45.189 80\nConnection to 10.3.45.189 80 port [tcp/http] succeeded!\n"
    Dec  9 07:48:08.540: INFO: stdout: ""
    Dec  9 07:48:08.540: INFO: Cleaning up the ExternalName to ClusterIP test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:48:08.582: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-5097" for this suite. 12/09/22 07:48:08.594
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:89
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:48:08.601
Dec  9 07:48:08.601: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename configmap 12/09/22 07:48:08.602
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:48:08.624
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:48:08.635
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:89
STEP: Creating configMap with name configmap-test-volume-map-85a6b9bf-67b6-4226-aa17-4bc72f7e2a31 12/09/22 07:48:08.65
STEP: Creating a pod to test consume configMaps 12/09/22 07:48:08.667
Dec  9 07:48:08.694: INFO: Waiting up to 5m0s for pod "pod-configmaps-351ebe1e-bf10-4189-9e27-5252cacc9942" in namespace "configmap-4804" to be "Succeeded or Failed"
Dec  9 07:48:08.698: INFO: Pod "pod-configmaps-351ebe1e-bf10-4189-9e27-5252cacc9942": Phase="Pending", Reason="", readiness=false. Elapsed: 2.985136ms
Dec  9 07:48:10.711: INFO: Pod "pod-configmaps-351ebe1e-bf10-4189-9e27-5252cacc9942": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015789257s
Dec  9 07:48:12.701: INFO: Pod "pod-configmaps-351ebe1e-bf10-4189-9e27-5252cacc9942": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006110632s
Dec  9 07:48:14.703: INFO: Pod "pod-configmaps-351ebe1e-bf10-4189-9e27-5252cacc9942": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.008258732s
STEP: Saw pod success 12/09/22 07:48:14.704
Dec  9 07:48:14.704: INFO: Pod "pod-configmaps-351ebe1e-bf10-4189-9e27-5252cacc9942" satisfied condition "Succeeded or Failed"
Dec  9 07:48:14.707: INFO: Trying to get logs from node ip-10-0-10-179 pod pod-configmaps-351ebe1e-bf10-4189-9e27-5252cacc9942 container agnhost-container: <nil>
STEP: delete the pod 12/09/22 07:48:14.712
Dec  9 07:48:14.721: INFO: Waiting for pod pod-configmaps-351ebe1e-bf10-4189-9e27-5252cacc9942 to disappear
Dec  9 07:48:14.724: INFO: Pod pod-configmaps-351ebe1e-bf10-4189-9e27-5252cacc9942 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Dec  9 07:48:14.725: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-4804" for this suite. 12/09/22 07:48:14.731
------------------------------
â€¢ [SLOW TEST] [6.148 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:89

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:48:08.601
    Dec  9 07:48:08.601: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename configmap 12/09/22 07:48:08.602
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:48:08.624
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:48:08.635
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:89
    STEP: Creating configMap with name configmap-test-volume-map-85a6b9bf-67b6-4226-aa17-4bc72f7e2a31 12/09/22 07:48:08.65
    STEP: Creating a pod to test consume configMaps 12/09/22 07:48:08.667
    Dec  9 07:48:08.694: INFO: Waiting up to 5m0s for pod "pod-configmaps-351ebe1e-bf10-4189-9e27-5252cacc9942" in namespace "configmap-4804" to be "Succeeded or Failed"
    Dec  9 07:48:08.698: INFO: Pod "pod-configmaps-351ebe1e-bf10-4189-9e27-5252cacc9942": Phase="Pending", Reason="", readiness=false. Elapsed: 2.985136ms
    Dec  9 07:48:10.711: INFO: Pod "pod-configmaps-351ebe1e-bf10-4189-9e27-5252cacc9942": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015789257s
    Dec  9 07:48:12.701: INFO: Pod "pod-configmaps-351ebe1e-bf10-4189-9e27-5252cacc9942": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006110632s
    Dec  9 07:48:14.703: INFO: Pod "pod-configmaps-351ebe1e-bf10-4189-9e27-5252cacc9942": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.008258732s
    STEP: Saw pod success 12/09/22 07:48:14.704
    Dec  9 07:48:14.704: INFO: Pod "pod-configmaps-351ebe1e-bf10-4189-9e27-5252cacc9942" satisfied condition "Succeeded or Failed"
    Dec  9 07:48:14.707: INFO: Trying to get logs from node ip-10-0-10-179 pod pod-configmaps-351ebe1e-bf10-4189-9e27-5252cacc9942 container agnhost-container: <nil>
    STEP: delete the pod 12/09/22 07:48:14.712
    Dec  9 07:48:14.721: INFO: Waiting for pod pod-configmaps-351ebe1e-bf10-4189-9e27-5252cacc9942 to disappear
    Dec  9 07:48:14.724: INFO: Pod pod-configmaps-351ebe1e-bf10-4189-9e27-5252cacc9942 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:48:14.725: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-4804" for this suite. 12/09/22 07:48:14.731
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:184
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:48:14.749
Dec  9 07:48:14.749: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename container-probe 12/09/22 07:48:14.75
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:48:14.768
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:48:14.773
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:184
STEP: Creating pod liveness-0434cc7a-c1ea-4ebe-ad94-4aeafab07113 in namespace container-probe-5878 12/09/22 07:48:14.779
Dec  9 07:48:14.792: INFO: Waiting up to 5m0s for pod "liveness-0434cc7a-c1ea-4ebe-ad94-4aeafab07113" in namespace "container-probe-5878" to be "not pending"
Dec  9 07:48:14.797: INFO: Pod "liveness-0434cc7a-c1ea-4ebe-ad94-4aeafab07113": Phase="Pending", Reason="", readiness=false. Elapsed: 5.048451ms
Dec  9 07:48:16.800: INFO: Pod "liveness-0434cc7a-c1ea-4ebe-ad94-4aeafab07113": Phase="Running", Reason="", readiness=true. Elapsed: 2.008793198s
Dec  9 07:48:16.801: INFO: Pod "liveness-0434cc7a-c1ea-4ebe-ad94-4aeafab07113" satisfied condition "not pending"
Dec  9 07:48:16.801: INFO: Started pod liveness-0434cc7a-c1ea-4ebe-ad94-4aeafab07113 in namespace container-probe-5878
STEP: checking the pod's current state and verifying that restartCount is present 12/09/22 07:48:16.801
Dec  9 07:48:16.804: INFO: Initial restart count of pod liveness-0434cc7a-c1ea-4ebe-ad94-4aeafab07113 is 0
STEP: deleting the pod 12/09/22 07:52:17.389
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Dec  9 07:52:17.404: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-5878" for this suite. 12/09/22 07:52:17.408
------------------------------
â€¢ [SLOW TEST] [242.665 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:48:14.749
    Dec  9 07:48:14.749: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename container-probe 12/09/22 07:48:14.75
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:48:14.768
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:48:14.773
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:184
    STEP: Creating pod liveness-0434cc7a-c1ea-4ebe-ad94-4aeafab07113 in namespace container-probe-5878 12/09/22 07:48:14.779
    Dec  9 07:48:14.792: INFO: Waiting up to 5m0s for pod "liveness-0434cc7a-c1ea-4ebe-ad94-4aeafab07113" in namespace "container-probe-5878" to be "not pending"
    Dec  9 07:48:14.797: INFO: Pod "liveness-0434cc7a-c1ea-4ebe-ad94-4aeafab07113": Phase="Pending", Reason="", readiness=false. Elapsed: 5.048451ms
    Dec  9 07:48:16.800: INFO: Pod "liveness-0434cc7a-c1ea-4ebe-ad94-4aeafab07113": Phase="Running", Reason="", readiness=true. Elapsed: 2.008793198s
    Dec  9 07:48:16.801: INFO: Pod "liveness-0434cc7a-c1ea-4ebe-ad94-4aeafab07113" satisfied condition "not pending"
    Dec  9 07:48:16.801: INFO: Started pod liveness-0434cc7a-c1ea-4ebe-ad94-4aeafab07113 in namespace container-probe-5878
    STEP: checking the pod's current state and verifying that restartCount is present 12/09/22 07:48:16.801
    Dec  9 07:48:16.804: INFO: Initial restart count of pod liveness-0434cc7a-c1ea-4ebe-ad94-4aeafab07113 is 0
    STEP: deleting the pod 12/09/22 07:52:17.389
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:52:17.404: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-5878" for this suite. 12/09/22 07:52:17.408
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:187
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:52:17.415
Dec  9 07:52:17.415: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename emptydir 12/09/22 07:52:17.416
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:52:17.433
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:52:17.438
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:187
STEP: Creating a pod to test emptydir 0777 on node default medium 12/09/22 07:52:17.441
Dec  9 07:52:17.448: INFO: Waiting up to 5m0s for pod "pod-251ed4f5-2801-4f93-90db-bc0c44512adf" in namespace "emptydir-6824" to be "Succeeded or Failed"
Dec  9 07:52:17.455: INFO: Pod "pod-251ed4f5-2801-4f93-90db-bc0c44512adf": Phase="Pending", Reason="", readiness=false. Elapsed: 6.185424ms
Dec  9 07:52:19.474: INFO: Pod "pod-251ed4f5-2801-4f93-90db-bc0c44512adf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025809229s
Dec  9 07:52:21.459: INFO: Pod "pod-251ed4f5-2801-4f93-90db-bc0c44512adf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011008204s
STEP: Saw pod success 12/09/22 07:52:21.459
Dec  9 07:52:21.460: INFO: Pod "pod-251ed4f5-2801-4f93-90db-bc0c44512adf" satisfied condition "Succeeded or Failed"
Dec  9 07:52:21.463: INFO: Trying to get logs from node ip-10-0-10-179 pod pod-251ed4f5-2801-4f93-90db-bc0c44512adf container test-container: <nil>
STEP: delete the pod 12/09/22 07:52:21.476
Dec  9 07:52:21.487: INFO: Waiting for pod pod-251ed4f5-2801-4f93-90db-bc0c44512adf to disappear
Dec  9 07:52:21.490: INFO: Pod pod-251ed4f5-2801-4f93-90db-bc0c44512adf no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Dec  9 07:52:21.490: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-6824" for this suite. 12/09/22 07:52:21.493
------------------------------
â€¢ [4.084 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:187

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:52:17.415
    Dec  9 07:52:17.415: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename emptydir 12/09/22 07:52:17.416
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:52:17.433
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:52:17.438
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:187
    STEP: Creating a pod to test emptydir 0777 on node default medium 12/09/22 07:52:17.441
    Dec  9 07:52:17.448: INFO: Waiting up to 5m0s for pod "pod-251ed4f5-2801-4f93-90db-bc0c44512adf" in namespace "emptydir-6824" to be "Succeeded or Failed"
    Dec  9 07:52:17.455: INFO: Pod "pod-251ed4f5-2801-4f93-90db-bc0c44512adf": Phase="Pending", Reason="", readiness=false. Elapsed: 6.185424ms
    Dec  9 07:52:19.474: INFO: Pod "pod-251ed4f5-2801-4f93-90db-bc0c44512adf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025809229s
    Dec  9 07:52:21.459: INFO: Pod "pod-251ed4f5-2801-4f93-90db-bc0c44512adf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011008204s
    STEP: Saw pod success 12/09/22 07:52:21.459
    Dec  9 07:52:21.460: INFO: Pod "pod-251ed4f5-2801-4f93-90db-bc0c44512adf" satisfied condition "Succeeded or Failed"
    Dec  9 07:52:21.463: INFO: Trying to get logs from node ip-10-0-10-179 pod pod-251ed4f5-2801-4f93-90db-bc0c44512adf container test-container: <nil>
    STEP: delete the pod 12/09/22 07:52:21.476
    Dec  9 07:52:21.487: INFO: Waiting for pod pod-251ed4f5-2801-4f93-90db-bc0c44512adf to disappear
    Dec  9 07:52:21.490: INFO: Pod pod-251ed4f5-2801-4f93-90db-bc0c44512adf no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:52:21.490: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-6824" for this suite. 12/09/22 07:52:21.493
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:130
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:52:21.499
Dec  9 07:52:21.500: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename projected 12/09/22 07:52:21.501
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:52:21.527
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:52:21.531
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:130
STEP: Creating the pod 12/09/22 07:52:21.535
Dec  9 07:52:21.556: INFO: Waiting up to 5m0s for pod "labelsupdatef22b635c-a4fb-4880-97ba-7bd0bef3750e" in namespace "projected-7699" to be "running and ready"
Dec  9 07:52:21.564: INFO: Pod "labelsupdatef22b635c-a4fb-4880-97ba-7bd0bef3750e": Phase="Pending", Reason="", readiness=false. Elapsed: 7.237453ms
Dec  9 07:52:21.564: INFO: The phase of Pod labelsupdatef22b635c-a4fb-4880-97ba-7bd0bef3750e is Pending, waiting for it to be Running (with Ready = true)
Dec  9 07:52:23.568: INFO: Pod "labelsupdatef22b635c-a4fb-4880-97ba-7bd0bef3750e": Phase="Running", Reason="", readiness=true. Elapsed: 2.010954753s
Dec  9 07:52:23.568: INFO: The phase of Pod labelsupdatef22b635c-a4fb-4880-97ba-7bd0bef3750e is Running (Ready = true)
Dec  9 07:52:23.568: INFO: Pod "labelsupdatef22b635c-a4fb-4880-97ba-7bd0bef3750e" satisfied condition "running and ready"
Dec  9 07:52:24.090: INFO: Successfully updated pod "labelsupdatef22b635c-a4fb-4880-97ba-7bd0bef3750e"
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Dec  9 07:52:28.113: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-7699" for this suite. 12/09/22 07:52:28.116
------------------------------
â€¢ [SLOW TEST] [6.622 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:130

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:52:21.499
    Dec  9 07:52:21.500: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename projected 12/09/22 07:52:21.501
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:52:21.527
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:52:21.531
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should update labels on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:130
    STEP: Creating the pod 12/09/22 07:52:21.535
    Dec  9 07:52:21.556: INFO: Waiting up to 5m0s for pod "labelsupdatef22b635c-a4fb-4880-97ba-7bd0bef3750e" in namespace "projected-7699" to be "running and ready"
    Dec  9 07:52:21.564: INFO: Pod "labelsupdatef22b635c-a4fb-4880-97ba-7bd0bef3750e": Phase="Pending", Reason="", readiness=false. Elapsed: 7.237453ms
    Dec  9 07:52:21.564: INFO: The phase of Pod labelsupdatef22b635c-a4fb-4880-97ba-7bd0bef3750e is Pending, waiting for it to be Running (with Ready = true)
    Dec  9 07:52:23.568: INFO: Pod "labelsupdatef22b635c-a4fb-4880-97ba-7bd0bef3750e": Phase="Running", Reason="", readiness=true. Elapsed: 2.010954753s
    Dec  9 07:52:23.568: INFO: The phase of Pod labelsupdatef22b635c-a4fb-4880-97ba-7bd0bef3750e is Running (Ready = true)
    Dec  9 07:52:23.568: INFO: Pod "labelsupdatef22b635c-a4fb-4880-97ba-7bd0bef3750e" satisfied condition "running and ready"
    Dec  9 07:52:24.090: INFO: Successfully updated pod "labelsupdatef22b635c-a4fb-4880-97ba-7bd0bef3750e"
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:52:28.113: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-7699" for this suite. 12/09/22 07:52:28.116
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:127
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:52:28.122
Dec  9 07:52:28.122: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename emptydir 12/09/22 07:52:28.123
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:52:28.143
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:52:28.148
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:127
STEP: Creating a pod to test emptydir 0644 on tmpfs 12/09/22 07:52:28.153
Dec  9 07:52:28.161: INFO: Waiting up to 5m0s for pod "pod-4a7b82d4-a7b6-4a9e-afea-f14632f8e6d3" in namespace "emptydir-5554" to be "Succeeded or Failed"
Dec  9 07:52:28.168: INFO: Pod "pod-4a7b82d4-a7b6-4a9e-afea-f14632f8e6d3": Phase="Pending", Reason="", readiness=false. Elapsed: 6.844729ms
Dec  9 07:52:30.173: INFO: Pod "pod-4a7b82d4-a7b6-4a9e-afea-f14632f8e6d3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011435996s
Dec  9 07:52:32.174: INFO: Pod "pod-4a7b82d4-a7b6-4a9e-afea-f14632f8e6d3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012281654s
STEP: Saw pod success 12/09/22 07:52:32.174
Dec  9 07:52:32.174: INFO: Pod "pod-4a7b82d4-a7b6-4a9e-afea-f14632f8e6d3" satisfied condition "Succeeded or Failed"
Dec  9 07:52:32.177: INFO: Trying to get logs from node ip-10-0-10-179 pod pod-4a7b82d4-a7b6-4a9e-afea-f14632f8e6d3 container test-container: <nil>
STEP: delete the pod 12/09/22 07:52:32.181
Dec  9 07:52:32.192: INFO: Waiting for pod pod-4a7b82d4-a7b6-4a9e-afea-f14632f8e6d3 to disappear
Dec  9 07:52:32.195: INFO: Pod pod-4a7b82d4-a7b6-4a9e-afea-f14632f8e6d3 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Dec  9 07:52:32.195: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-5554" for this suite. 12/09/22 07:52:32.199
------------------------------
â€¢ [4.083 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:127

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:52:28.122
    Dec  9 07:52:28.122: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename emptydir 12/09/22 07:52:28.123
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:52:28.143
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:52:28.148
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:127
    STEP: Creating a pod to test emptydir 0644 on tmpfs 12/09/22 07:52:28.153
    Dec  9 07:52:28.161: INFO: Waiting up to 5m0s for pod "pod-4a7b82d4-a7b6-4a9e-afea-f14632f8e6d3" in namespace "emptydir-5554" to be "Succeeded or Failed"
    Dec  9 07:52:28.168: INFO: Pod "pod-4a7b82d4-a7b6-4a9e-afea-f14632f8e6d3": Phase="Pending", Reason="", readiness=false. Elapsed: 6.844729ms
    Dec  9 07:52:30.173: INFO: Pod "pod-4a7b82d4-a7b6-4a9e-afea-f14632f8e6d3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011435996s
    Dec  9 07:52:32.174: INFO: Pod "pod-4a7b82d4-a7b6-4a9e-afea-f14632f8e6d3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012281654s
    STEP: Saw pod success 12/09/22 07:52:32.174
    Dec  9 07:52:32.174: INFO: Pod "pod-4a7b82d4-a7b6-4a9e-afea-f14632f8e6d3" satisfied condition "Succeeded or Failed"
    Dec  9 07:52:32.177: INFO: Trying to get logs from node ip-10-0-10-179 pod pod-4a7b82d4-a7b6-4a9e-afea-f14632f8e6d3 container test-container: <nil>
    STEP: delete the pod 12/09/22 07:52:32.181
    Dec  9 07:52:32.192: INFO: Waiting for pod pod-4a7b82d4-a7b6-4a9e-afea-f14632f8e6d3 to disappear
    Dec  9 07:52:32.195: INFO: Pod pod-4a7b82d4-a7b6-4a9e-afea-f14632f8e6d3 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:52:32.195: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-5554" for this suite. 12/09/22 07:52:32.199
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:117
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:52:32.208
Dec  9 07:52:32.208: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename emptydir 12/09/22 07:52:32.21
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:52:32.235
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:52:32.241
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:117
STEP: Creating a pod to test emptydir 0777 on tmpfs 12/09/22 07:52:32.248
Dec  9 07:52:32.262: INFO: Waiting up to 5m0s for pod "pod-7e71eb7f-c264-4f78-8ffb-3167efa954c3" in namespace "emptydir-8098" to be "Succeeded or Failed"
Dec  9 07:52:32.271: INFO: Pod "pod-7e71eb7f-c264-4f78-8ffb-3167efa954c3": Phase="Pending", Reason="", readiness=false. Elapsed: 8.36612ms
Dec  9 07:52:34.274: INFO: Pod "pod-7e71eb7f-c264-4f78-8ffb-3167efa954c3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011976384s
Dec  9 07:52:36.277: INFO: Pod "pod-7e71eb7f-c264-4f78-8ffb-3167efa954c3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014141417s
STEP: Saw pod success 12/09/22 07:52:36.277
Dec  9 07:52:36.277: INFO: Pod "pod-7e71eb7f-c264-4f78-8ffb-3167efa954c3" satisfied condition "Succeeded or Failed"
Dec  9 07:52:36.282: INFO: Trying to get logs from node ip-10-0-10-179 pod pod-7e71eb7f-c264-4f78-8ffb-3167efa954c3 container test-container: <nil>
STEP: delete the pod 12/09/22 07:52:36.29
Dec  9 07:52:36.318: INFO: Waiting for pod pod-7e71eb7f-c264-4f78-8ffb-3167efa954c3 to disappear
Dec  9 07:52:36.325: INFO: Pod pod-7e71eb7f-c264-4f78-8ffb-3167efa954c3 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Dec  9 07:52:36.326: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-8098" for this suite. 12/09/22 07:52:36.339
------------------------------
â€¢ [4.142 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:117

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:52:32.208
    Dec  9 07:52:32.208: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename emptydir 12/09/22 07:52:32.21
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:52:32.235
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:52:32.241
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:117
    STEP: Creating a pod to test emptydir 0777 on tmpfs 12/09/22 07:52:32.248
    Dec  9 07:52:32.262: INFO: Waiting up to 5m0s for pod "pod-7e71eb7f-c264-4f78-8ffb-3167efa954c3" in namespace "emptydir-8098" to be "Succeeded or Failed"
    Dec  9 07:52:32.271: INFO: Pod "pod-7e71eb7f-c264-4f78-8ffb-3167efa954c3": Phase="Pending", Reason="", readiness=false. Elapsed: 8.36612ms
    Dec  9 07:52:34.274: INFO: Pod "pod-7e71eb7f-c264-4f78-8ffb-3167efa954c3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011976384s
    Dec  9 07:52:36.277: INFO: Pod "pod-7e71eb7f-c264-4f78-8ffb-3167efa954c3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014141417s
    STEP: Saw pod success 12/09/22 07:52:36.277
    Dec  9 07:52:36.277: INFO: Pod "pod-7e71eb7f-c264-4f78-8ffb-3167efa954c3" satisfied condition "Succeeded or Failed"
    Dec  9 07:52:36.282: INFO: Trying to get logs from node ip-10-0-10-179 pod pod-7e71eb7f-c264-4f78-8ffb-3167efa954c3 container test-container: <nil>
    STEP: delete the pod 12/09/22 07:52:36.29
    Dec  9 07:52:36.318: INFO: Waiting for pod pod-7e71eb7f-c264-4f78-8ffb-3167efa954c3 to disappear
    Dec  9 07:52:36.325: INFO: Pod pod-7e71eb7f-c264-4f78-8ffb-3167efa954c3 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:52:36.326: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-8098" for this suite. 12/09/22 07:52:36.339
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:107
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:52:36.369
Dec  9 07:52:36.369: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename emptydir 12/09/22 07:52:36.381
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:52:36.417
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:52:36.425
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:107
STEP: Creating a pod to test emptydir 0666 on tmpfs 12/09/22 07:52:36.434
Dec  9 07:52:36.449: INFO: Waiting up to 5m0s for pod "pod-da76f7e8-dd51-46d3-956a-4795cf82ce8a" in namespace "emptydir-6690" to be "Succeeded or Failed"
Dec  9 07:52:36.465: INFO: Pod "pod-da76f7e8-dd51-46d3-956a-4795cf82ce8a": Phase="Pending", Reason="", readiness=false. Elapsed: 15.990042ms
Dec  9 07:52:38.473: INFO: Pod "pod-da76f7e8-dd51-46d3-956a-4795cf82ce8a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024064536s
Dec  9 07:52:40.469: INFO: Pod "pod-da76f7e8-dd51-46d3-956a-4795cf82ce8a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019859132s
STEP: Saw pod success 12/09/22 07:52:40.469
Dec  9 07:52:40.469: INFO: Pod "pod-da76f7e8-dd51-46d3-956a-4795cf82ce8a" satisfied condition "Succeeded or Failed"
Dec  9 07:52:40.472: INFO: Trying to get logs from node ip-10-0-10-179 pod pod-da76f7e8-dd51-46d3-956a-4795cf82ce8a container test-container: <nil>
STEP: delete the pod 12/09/22 07:52:40.479
Dec  9 07:52:40.490: INFO: Waiting for pod pod-da76f7e8-dd51-46d3-956a-4795cf82ce8a to disappear
Dec  9 07:52:40.493: INFO: Pod pod-da76f7e8-dd51-46d3-956a-4795cf82ce8a no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Dec  9 07:52:40.493: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-6690" for this suite. 12/09/22 07:52:40.496
------------------------------
â€¢ [4.132 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:107

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:52:36.369
    Dec  9 07:52:36.369: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename emptydir 12/09/22 07:52:36.381
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:52:36.417
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:52:36.425
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:107
    STEP: Creating a pod to test emptydir 0666 on tmpfs 12/09/22 07:52:36.434
    Dec  9 07:52:36.449: INFO: Waiting up to 5m0s for pod "pod-da76f7e8-dd51-46d3-956a-4795cf82ce8a" in namespace "emptydir-6690" to be "Succeeded or Failed"
    Dec  9 07:52:36.465: INFO: Pod "pod-da76f7e8-dd51-46d3-956a-4795cf82ce8a": Phase="Pending", Reason="", readiness=false. Elapsed: 15.990042ms
    Dec  9 07:52:38.473: INFO: Pod "pod-da76f7e8-dd51-46d3-956a-4795cf82ce8a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024064536s
    Dec  9 07:52:40.469: INFO: Pod "pod-da76f7e8-dd51-46d3-956a-4795cf82ce8a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019859132s
    STEP: Saw pod success 12/09/22 07:52:40.469
    Dec  9 07:52:40.469: INFO: Pod "pod-da76f7e8-dd51-46d3-956a-4795cf82ce8a" satisfied condition "Succeeded or Failed"
    Dec  9 07:52:40.472: INFO: Trying to get logs from node ip-10-0-10-179 pod pod-da76f7e8-dd51-46d3-956a-4795cf82ce8a container test-container: <nil>
    STEP: delete the pod 12/09/22 07:52:40.479
    Dec  9 07:52:40.490: INFO: Waiting for pod pod-da76f7e8-dd51-46d3-956a-4795cf82ce8a to disappear
    Dec  9 07:52:40.493: INFO: Pod pod-da76f7e8-dd51-46d3-956a-4795cf82ce8a no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:52:40.493: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-6690" for this suite. 12/09/22 07:52:40.496
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-apps] CronJob
  should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:52:40.503
Dec  9 07:52:40.503: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename cronjob 12/09/22 07:52:40.505
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:52:40.523
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:52:40.528
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319
STEP: Creating a cronjob 12/09/22 07:52:40.531
STEP: creating 12/09/22 07:52:40.531
STEP: getting 12/09/22 07:52:40.538
STEP: listing 12/09/22 07:52:40.54
STEP: watching 12/09/22 07:52:40.543
Dec  9 07:52:40.543: INFO: starting watch
STEP: cluster-wide listing 12/09/22 07:52:40.545
STEP: cluster-wide watching 12/09/22 07:52:40.548
Dec  9 07:52:40.548: INFO: starting watch
STEP: patching 12/09/22 07:52:40.549
STEP: updating 12/09/22 07:52:40.556
Dec  9 07:52:40.565: INFO: waiting for watch events with expected annotations
Dec  9 07:52:40.565: INFO: saw patched and updated annotations
STEP: patching /status 12/09/22 07:52:40.565
STEP: updating /status 12/09/22 07:52:40.571
STEP: get /status 12/09/22 07:52:40.578
STEP: deleting 12/09/22 07:52:40.581
STEP: deleting a collection 12/09/22 07:52:40.595
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Dec  9 07:52:40.603: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-113" for this suite. 12/09/22 07:52:40.607
------------------------------
â€¢ [0.109 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:52:40.503
    Dec  9 07:52:40.503: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename cronjob 12/09/22 07:52:40.505
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:52:40.523
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:52:40.528
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should support CronJob API operations [Conformance]
      test/e2e/apps/cronjob.go:319
    STEP: Creating a cronjob 12/09/22 07:52:40.531
    STEP: creating 12/09/22 07:52:40.531
    STEP: getting 12/09/22 07:52:40.538
    STEP: listing 12/09/22 07:52:40.54
    STEP: watching 12/09/22 07:52:40.543
    Dec  9 07:52:40.543: INFO: starting watch
    STEP: cluster-wide listing 12/09/22 07:52:40.545
    STEP: cluster-wide watching 12/09/22 07:52:40.548
    Dec  9 07:52:40.548: INFO: starting watch
    STEP: patching 12/09/22 07:52:40.549
    STEP: updating 12/09/22 07:52:40.556
    Dec  9 07:52:40.565: INFO: waiting for watch events with expected annotations
    Dec  9 07:52:40.565: INFO: saw patched and updated annotations
    STEP: patching /status 12/09/22 07:52:40.565
    STEP: updating /status 12/09/22 07:52:40.571
    STEP: get /status 12/09/22 07:52:40.578
    STEP: deleting 12/09/22 07:52:40.581
    STEP: deleting a collection 12/09/22 07:52:40.595
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:52:40.603: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-113" for this suite. 12/09/22 07:52:40.607
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Pods
  should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:896
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:52:40.614
Dec  9 07:52:40.614: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename pods 12/09/22 07:52:40.617
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:52:40.635
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:52:40.64
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:896
STEP: creating a Pod with a static label 12/09/22 07:52:40.651
STEP: watching for Pod to be ready 12/09/22 07:52:40.66
Dec  9 07:52:40.664: INFO: observed Pod pod-test in namespace pods-761 in phase Pending with labels: map[test-pod-static:true] & conditions []
Dec  9 07:52:40.669: INFO: observed Pod pod-test in namespace pods-761 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-12-09 07:52:40 +0000 UTC  }]
Dec  9 07:52:40.683: INFO: observed Pod pod-test in namespace pods-761 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-12-09 07:52:40 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-12-09 07:52:40 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-12-09 07:52:40 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-12-09 07:52:40 +0000 UTC  }]
Dec  9 07:52:41.190: INFO: observed Pod pod-test in namespace pods-761 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-12-09 07:52:40 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-12-09 07:52:40 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-12-09 07:52:40 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-12-09 07:52:40 +0000 UTC  }]
Dec  9 07:52:42.368: INFO: Found Pod pod-test in namespace pods-761 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-12-09 07:52:40 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2022-12-09 07:52:42 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2022-12-09 07:52:42 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-12-09 07:52:40 +0000 UTC  }]
STEP: patching the Pod with a new Label and updated data 12/09/22 07:52:42.373
STEP: getting the Pod and ensuring that it's patched 12/09/22 07:52:42.385
STEP: replacing the Pod's status Ready condition to False 12/09/22 07:52:42.387
STEP: check the Pod again to ensure its Ready conditions are False 12/09/22 07:52:42.397
STEP: deleting the Pod via a Collection with a LabelSelector 12/09/22 07:52:42.397
STEP: watching for the Pod to be deleted 12/09/22 07:52:42.404
Dec  9 07:52:42.407: INFO: observed event type MODIFIED
Dec  9 07:52:44.654: INFO: observed event type MODIFIED
Dec  9 07:52:44.711: INFO: observed event type MODIFIED
Dec  9 07:52:45.375: INFO: observed event type MODIFIED
Dec  9 07:52:45.383: INFO: observed event type MODIFIED
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Dec  9 07:52:45.389: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-761" for this suite. 12/09/22 07:52:45.392
------------------------------
â€¢ [4.784 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:896

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:52:40.614
    Dec  9 07:52:40.614: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename pods 12/09/22 07:52:40.617
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:52:40.635
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:52:40.64
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should run through the lifecycle of Pods and PodStatus [Conformance]
      test/e2e/common/node/pods.go:896
    STEP: creating a Pod with a static label 12/09/22 07:52:40.651
    STEP: watching for Pod to be ready 12/09/22 07:52:40.66
    Dec  9 07:52:40.664: INFO: observed Pod pod-test in namespace pods-761 in phase Pending with labels: map[test-pod-static:true] & conditions []
    Dec  9 07:52:40.669: INFO: observed Pod pod-test in namespace pods-761 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-12-09 07:52:40 +0000 UTC  }]
    Dec  9 07:52:40.683: INFO: observed Pod pod-test in namespace pods-761 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-12-09 07:52:40 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-12-09 07:52:40 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-12-09 07:52:40 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-12-09 07:52:40 +0000 UTC  }]
    Dec  9 07:52:41.190: INFO: observed Pod pod-test in namespace pods-761 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-12-09 07:52:40 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-12-09 07:52:40 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-12-09 07:52:40 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-12-09 07:52:40 +0000 UTC  }]
    Dec  9 07:52:42.368: INFO: Found Pod pod-test in namespace pods-761 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-12-09 07:52:40 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2022-12-09 07:52:42 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2022-12-09 07:52:42 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-12-09 07:52:40 +0000 UTC  }]
    STEP: patching the Pod with a new Label and updated data 12/09/22 07:52:42.373
    STEP: getting the Pod and ensuring that it's patched 12/09/22 07:52:42.385
    STEP: replacing the Pod's status Ready condition to False 12/09/22 07:52:42.387
    STEP: check the Pod again to ensure its Ready conditions are False 12/09/22 07:52:42.397
    STEP: deleting the Pod via a Collection with a LabelSelector 12/09/22 07:52:42.397
    STEP: watching for the Pod to be deleted 12/09/22 07:52:42.404
    Dec  9 07:52:42.407: INFO: observed event type MODIFIED
    Dec  9 07:52:44.654: INFO: observed event type MODIFIED
    Dec  9 07:52:44.711: INFO: observed event type MODIFIED
    Dec  9 07:52:45.375: INFO: observed event type MODIFIED
    Dec  9 07:52:45.383: INFO: observed event type MODIFIED
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:52:45.389: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-761" for this suite. 12/09/22 07:52:45.392
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2228
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:52:45.4
Dec  9 07:52:45.400: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename services 12/09/22 07:52:45.401
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:52:45.416
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:52:45.422
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2228
STEP: creating service in namespace services-2299 12/09/22 07:52:45.427
STEP: creating service affinity-nodeport in namespace services-2299 12/09/22 07:52:45.427
STEP: creating replication controller affinity-nodeport in namespace services-2299 12/09/22 07:52:45.442
I1209 07:52:45.455193      20 runners.go:193] Created replication controller with name: affinity-nodeport, namespace: services-2299, replica count: 3
I1209 07:52:48.506898      20 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Dec  9 07:52:48.518: INFO: Creating new exec pod
Dec  9 07:52:48.524: INFO: Waiting up to 5m0s for pod "execpod-affinity7xppn" in namespace "services-2299" to be "running"
Dec  9 07:52:48.530: INFO: Pod "execpod-affinity7xppn": Phase="Pending", Reason="", readiness=false. Elapsed: 5.538575ms
Dec  9 07:52:50.534: INFO: Pod "execpod-affinity7xppn": Phase="Running", Reason="", readiness=true. Elapsed: 2.009006117s
Dec  9 07:52:50.534: INFO: Pod "execpod-affinity7xppn" satisfied condition "running"
Dec  9 07:52:51.539: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=services-2299 exec execpod-affinity7xppn -- /bin/sh -x -c nc -v -z -w 2 affinity-nodeport 80'
Dec  9 07:52:51.696: INFO: stderr: "+ nc -v -z -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
Dec  9 07:52:51.696: INFO: stdout: ""
Dec  9 07:52:51.697: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=services-2299 exec execpod-affinity7xppn -- /bin/sh -x -c nc -v -z -w 2 10.3.47.197 80'
Dec  9 07:52:51.853: INFO: stderr: "+ nc -v -z -w 2 10.3.47.197 80\nConnection to 10.3.47.197 80 port [tcp/http] succeeded!\n"
Dec  9 07:52:51.853: INFO: stdout: ""
Dec  9 07:52:51.853: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=services-2299 exec execpod-affinity7xppn -- /bin/sh -x -c nc -v -z -w 2 10.0.10.179 30183'
Dec  9 07:52:51.993: INFO: stderr: "+ nc -v -z -w 2 10.0.10.179 30183\nConnection to 10.0.10.179 30183 port [tcp/*] succeeded!\n"
Dec  9 07:52:51.993: INFO: stdout: ""
Dec  9 07:52:51.993: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=services-2299 exec execpod-affinity7xppn -- /bin/sh -x -c nc -v -z -w 2 10.0.17.108 30183'
Dec  9 07:52:52.228: INFO: stderr: "+ nc -v -z -w 2 10.0.17.108 30183\nConnection to 10.0.17.108 30183 port [tcp/*] succeeded!\n"
Dec  9 07:52:52.228: INFO: stdout: ""
Dec  9 07:52:52.228: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=services-2299 exec execpod-affinity7xppn -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.0.10.179:30183/ ; done'
Dec  9 07:52:52.469: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.179:30183/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.179:30183/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.179:30183/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.179:30183/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.179:30183/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.179:30183/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.179:30183/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.179:30183/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.179:30183/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.179:30183/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.179:30183/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.179:30183/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.179:30183/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.179:30183/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.179:30183/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.179:30183/\n"
Dec  9 07:52:52.469: INFO: stdout: "\naffinity-nodeport-f5xx9\naffinity-nodeport-f5xx9\naffinity-nodeport-f5xx9\naffinity-nodeport-f5xx9\naffinity-nodeport-f5xx9\naffinity-nodeport-f5xx9\naffinity-nodeport-f5xx9\naffinity-nodeport-f5xx9\naffinity-nodeport-f5xx9\naffinity-nodeport-f5xx9\naffinity-nodeport-f5xx9\naffinity-nodeport-f5xx9\naffinity-nodeport-f5xx9\naffinity-nodeport-f5xx9\naffinity-nodeport-f5xx9\naffinity-nodeport-f5xx9"
Dec  9 07:52:52.469: INFO: Received response from host: affinity-nodeport-f5xx9
Dec  9 07:52:52.469: INFO: Received response from host: affinity-nodeport-f5xx9
Dec  9 07:52:52.469: INFO: Received response from host: affinity-nodeport-f5xx9
Dec  9 07:52:52.469: INFO: Received response from host: affinity-nodeport-f5xx9
Dec  9 07:52:52.469: INFO: Received response from host: affinity-nodeport-f5xx9
Dec  9 07:52:52.469: INFO: Received response from host: affinity-nodeport-f5xx9
Dec  9 07:52:52.469: INFO: Received response from host: affinity-nodeport-f5xx9
Dec  9 07:52:52.469: INFO: Received response from host: affinity-nodeport-f5xx9
Dec  9 07:52:52.469: INFO: Received response from host: affinity-nodeport-f5xx9
Dec  9 07:52:52.469: INFO: Received response from host: affinity-nodeport-f5xx9
Dec  9 07:52:52.469: INFO: Received response from host: affinity-nodeport-f5xx9
Dec  9 07:52:52.469: INFO: Received response from host: affinity-nodeport-f5xx9
Dec  9 07:52:52.469: INFO: Received response from host: affinity-nodeport-f5xx9
Dec  9 07:52:52.469: INFO: Received response from host: affinity-nodeport-f5xx9
Dec  9 07:52:52.469: INFO: Received response from host: affinity-nodeport-f5xx9
Dec  9 07:52:52.469: INFO: Received response from host: affinity-nodeport-f5xx9
Dec  9 07:52:52.469: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport in namespace services-2299, will wait for the garbage collector to delete the pods 12/09/22 07:52:52.482
Dec  9 07:52:52.553: INFO: Deleting ReplicationController affinity-nodeport took: 13.825646ms
Dec  9 07:52:52.654: INFO: Terminating ReplicationController affinity-nodeport pods took: 100.438581ms
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Dec  9 07:52:55.079: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-2299" for this suite. 12/09/22 07:52:55.082
------------------------------
â€¢ [SLOW TEST] [9.688 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2228

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:52:45.4
    Dec  9 07:52:45.400: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename services 12/09/22 07:52:45.401
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:52:45.416
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:52:45.422
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2228
    STEP: creating service in namespace services-2299 12/09/22 07:52:45.427
    STEP: creating service affinity-nodeport in namespace services-2299 12/09/22 07:52:45.427
    STEP: creating replication controller affinity-nodeport in namespace services-2299 12/09/22 07:52:45.442
    I1209 07:52:45.455193      20 runners.go:193] Created replication controller with name: affinity-nodeport, namespace: services-2299, replica count: 3
    I1209 07:52:48.506898      20 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Dec  9 07:52:48.518: INFO: Creating new exec pod
    Dec  9 07:52:48.524: INFO: Waiting up to 5m0s for pod "execpod-affinity7xppn" in namespace "services-2299" to be "running"
    Dec  9 07:52:48.530: INFO: Pod "execpod-affinity7xppn": Phase="Pending", Reason="", readiness=false. Elapsed: 5.538575ms
    Dec  9 07:52:50.534: INFO: Pod "execpod-affinity7xppn": Phase="Running", Reason="", readiness=true. Elapsed: 2.009006117s
    Dec  9 07:52:50.534: INFO: Pod "execpod-affinity7xppn" satisfied condition "running"
    Dec  9 07:52:51.539: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=services-2299 exec execpod-affinity7xppn -- /bin/sh -x -c nc -v -z -w 2 affinity-nodeport 80'
    Dec  9 07:52:51.696: INFO: stderr: "+ nc -v -z -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
    Dec  9 07:52:51.696: INFO: stdout: ""
    Dec  9 07:52:51.697: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=services-2299 exec execpod-affinity7xppn -- /bin/sh -x -c nc -v -z -w 2 10.3.47.197 80'
    Dec  9 07:52:51.853: INFO: stderr: "+ nc -v -z -w 2 10.3.47.197 80\nConnection to 10.3.47.197 80 port [tcp/http] succeeded!\n"
    Dec  9 07:52:51.853: INFO: stdout: ""
    Dec  9 07:52:51.853: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=services-2299 exec execpod-affinity7xppn -- /bin/sh -x -c nc -v -z -w 2 10.0.10.179 30183'
    Dec  9 07:52:51.993: INFO: stderr: "+ nc -v -z -w 2 10.0.10.179 30183\nConnection to 10.0.10.179 30183 port [tcp/*] succeeded!\n"
    Dec  9 07:52:51.993: INFO: stdout: ""
    Dec  9 07:52:51.993: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=services-2299 exec execpod-affinity7xppn -- /bin/sh -x -c nc -v -z -w 2 10.0.17.108 30183'
    Dec  9 07:52:52.228: INFO: stderr: "+ nc -v -z -w 2 10.0.17.108 30183\nConnection to 10.0.17.108 30183 port [tcp/*] succeeded!\n"
    Dec  9 07:52:52.228: INFO: stdout: ""
    Dec  9 07:52:52.228: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=services-2299 exec execpod-affinity7xppn -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.0.10.179:30183/ ; done'
    Dec  9 07:52:52.469: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.179:30183/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.179:30183/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.179:30183/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.179:30183/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.179:30183/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.179:30183/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.179:30183/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.179:30183/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.179:30183/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.179:30183/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.179:30183/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.179:30183/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.179:30183/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.179:30183/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.179:30183/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.179:30183/\n"
    Dec  9 07:52:52.469: INFO: stdout: "\naffinity-nodeport-f5xx9\naffinity-nodeport-f5xx9\naffinity-nodeport-f5xx9\naffinity-nodeport-f5xx9\naffinity-nodeport-f5xx9\naffinity-nodeport-f5xx9\naffinity-nodeport-f5xx9\naffinity-nodeport-f5xx9\naffinity-nodeport-f5xx9\naffinity-nodeport-f5xx9\naffinity-nodeport-f5xx9\naffinity-nodeport-f5xx9\naffinity-nodeport-f5xx9\naffinity-nodeport-f5xx9\naffinity-nodeport-f5xx9\naffinity-nodeport-f5xx9"
    Dec  9 07:52:52.469: INFO: Received response from host: affinity-nodeport-f5xx9
    Dec  9 07:52:52.469: INFO: Received response from host: affinity-nodeport-f5xx9
    Dec  9 07:52:52.469: INFO: Received response from host: affinity-nodeport-f5xx9
    Dec  9 07:52:52.469: INFO: Received response from host: affinity-nodeport-f5xx9
    Dec  9 07:52:52.469: INFO: Received response from host: affinity-nodeport-f5xx9
    Dec  9 07:52:52.469: INFO: Received response from host: affinity-nodeport-f5xx9
    Dec  9 07:52:52.469: INFO: Received response from host: affinity-nodeport-f5xx9
    Dec  9 07:52:52.469: INFO: Received response from host: affinity-nodeport-f5xx9
    Dec  9 07:52:52.469: INFO: Received response from host: affinity-nodeport-f5xx9
    Dec  9 07:52:52.469: INFO: Received response from host: affinity-nodeport-f5xx9
    Dec  9 07:52:52.469: INFO: Received response from host: affinity-nodeport-f5xx9
    Dec  9 07:52:52.469: INFO: Received response from host: affinity-nodeport-f5xx9
    Dec  9 07:52:52.469: INFO: Received response from host: affinity-nodeport-f5xx9
    Dec  9 07:52:52.469: INFO: Received response from host: affinity-nodeport-f5xx9
    Dec  9 07:52:52.469: INFO: Received response from host: affinity-nodeport-f5xx9
    Dec  9 07:52:52.469: INFO: Received response from host: affinity-nodeport-f5xx9
    Dec  9 07:52:52.469: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-nodeport in namespace services-2299, will wait for the garbage collector to delete the pods 12/09/22 07:52:52.482
    Dec  9 07:52:52.553: INFO: Deleting ReplicationController affinity-nodeport took: 13.825646ms
    Dec  9 07:52:52.654: INFO: Terminating ReplicationController affinity-nodeport pods took: 100.438581ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:52:55.079: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-2299" for this suite. 12/09/22 07:52:55.082
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:391
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:52:55.089
Dec  9 07:52:55.089: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename crd-publish-openapi 12/09/22 07:52:55.092
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:52:55.107
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:52:55.114
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:391
STEP: set up a multi version CRD 12/09/22 07:52:55.119
Dec  9 07:52:55.120: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: rename a version 12/09/22 07:52:59.089
STEP: check the new version name is served 12/09/22 07:52:59.105
STEP: check the old version name is removed 12/09/22 07:53:00.643
STEP: check the other version is not changed 12/09/22 07:53:01.566
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Dec  9 07:53:05.753: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-3217" for this suite. 12/09/22 07:53:05.766
------------------------------
â€¢ [SLOW TEST] [10.683 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:391

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:52:55.089
    Dec  9 07:52:55.089: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename crd-publish-openapi 12/09/22 07:52:55.092
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:52:55.107
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:52:55.114
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] updates the published spec when one version gets renamed [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:391
    STEP: set up a multi version CRD 12/09/22 07:52:55.119
    Dec  9 07:52:55.120: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: rename a version 12/09/22 07:52:59.089
    STEP: check the new version name is served 12/09/22 07:52:59.105
    STEP: check the old version name is removed 12/09/22 07:53:00.643
    STEP: check the other version is not changed 12/09/22 07:53:01.566
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:53:05.753: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-3217" for this suite. 12/09/22 07:53:05.766
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:235
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:53:05.772
Dec  9 07:53:05.772: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename projected 12/09/22 07:53:05.773
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:53:05.79
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:53:05.794
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:235
STEP: Creating a pod to test downward API volume plugin 12/09/22 07:53:05.797
Dec  9 07:53:05.807: INFO: Waiting up to 5m0s for pod "downwardapi-volume-8fa2c9ee-b5b2-4af7-b5e5-1ab573e4ac5e" in namespace "projected-5077" to be "Succeeded or Failed"
Dec  9 07:53:05.810: INFO: Pod "downwardapi-volume-8fa2c9ee-b5b2-4af7-b5e5-1ab573e4ac5e": Phase="Pending", Reason="", readiness=false. Elapsed: 3.486825ms
Dec  9 07:53:07.815: INFO: Pod "downwardapi-volume-8fa2c9ee-b5b2-4af7-b5e5-1ab573e4ac5e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00791862s
Dec  9 07:53:09.814: INFO: Pod "downwardapi-volume-8fa2c9ee-b5b2-4af7-b5e5-1ab573e4ac5e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007521207s
STEP: Saw pod success 12/09/22 07:53:09.814
Dec  9 07:53:09.815: INFO: Pod "downwardapi-volume-8fa2c9ee-b5b2-4af7-b5e5-1ab573e4ac5e" satisfied condition "Succeeded or Failed"
Dec  9 07:53:09.817: INFO: Trying to get logs from node ip-10-0-10-179 pod downwardapi-volume-8fa2c9ee-b5b2-4af7-b5e5-1ab573e4ac5e container client-container: <nil>
STEP: delete the pod 12/09/22 07:53:09.822
Dec  9 07:53:09.833: INFO: Waiting for pod downwardapi-volume-8fa2c9ee-b5b2-4af7-b5e5-1ab573e4ac5e to disappear
Dec  9 07:53:09.835: INFO: Pod downwardapi-volume-8fa2c9ee-b5b2-4af7-b5e5-1ab573e4ac5e no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Dec  9 07:53:09.836: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-5077" for this suite. 12/09/22 07:53:09.839
------------------------------
â€¢ [4.074 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:235

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:53:05.772
    Dec  9 07:53:05.772: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename projected 12/09/22 07:53:05.773
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:53:05.79
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:53:05.794
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide container's memory request [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:235
    STEP: Creating a pod to test downward API volume plugin 12/09/22 07:53:05.797
    Dec  9 07:53:05.807: INFO: Waiting up to 5m0s for pod "downwardapi-volume-8fa2c9ee-b5b2-4af7-b5e5-1ab573e4ac5e" in namespace "projected-5077" to be "Succeeded or Failed"
    Dec  9 07:53:05.810: INFO: Pod "downwardapi-volume-8fa2c9ee-b5b2-4af7-b5e5-1ab573e4ac5e": Phase="Pending", Reason="", readiness=false. Elapsed: 3.486825ms
    Dec  9 07:53:07.815: INFO: Pod "downwardapi-volume-8fa2c9ee-b5b2-4af7-b5e5-1ab573e4ac5e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00791862s
    Dec  9 07:53:09.814: INFO: Pod "downwardapi-volume-8fa2c9ee-b5b2-4af7-b5e5-1ab573e4ac5e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007521207s
    STEP: Saw pod success 12/09/22 07:53:09.814
    Dec  9 07:53:09.815: INFO: Pod "downwardapi-volume-8fa2c9ee-b5b2-4af7-b5e5-1ab573e4ac5e" satisfied condition "Succeeded or Failed"
    Dec  9 07:53:09.817: INFO: Trying to get logs from node ip-10-0-10-179 pod downwardapi-volume-8fa2c9ee-b5b2-4af7-b5e5-1ab573e4ac5e container client-container: <nil>
    STEP: delete the pod 12/09/22 07:53:09.822
    Dec  9 07:53:09.833: INFO: Waiting for pod downwardapi-volume-8fa2c9ee-b5b2-4af7-b5e5-1ab573e4ac5e to disappear
    Dec  9 07:53:09.835: INFO: Pod downwardapi-volume-8fa2c9ee-b5b2-4af7-b5e5-1ab573e4ac5e no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:53:09.836: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-5077" for this suite. 12/09/22 07:53:09.839
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:78
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:53:09.848
Dec  9 07:53:09.848: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename projected 12/09/22 07:53:09.849
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:53:09.865
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:53:09.868
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:78
STEP: Creating projection with secret that has name projected-secret-test-map-d92f8c4d-6bc0-4841-be9b-9c275573ebba 12/09/22 07:53:09.873
STEP: Creating a pod to test consume secrets 12/09/22 07:53:09.878
Dec  9 07:53:09.884: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-7b2badb2-4023-49a0-87b7-6646c90f2098" in namespace "projected-3866" to be "Succeeded or Failed"
Dec  9 07:53:09.888: INFO: Pod "pod-projected-secrets-7b2badb2-4023-49a0-87b7-6646c90f2098": Phase="Pending", Reason="", readiness=false. Elapsed: 3.66617ms
Dec  9 07:53:11.892: INFO: Pod "pod-projected-secrets-7b2badb2-4023-49a0-87b7-6646c90f2098": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007385787s
Dec  9 07:53:13.893: INFO: Pod "pod-projected-secrets-7b2badb2-4023-49a0-87b7-6646c90f2098": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008344163s
STEP: Saw pod success 12/09/22 07:53:13.893
Dec  9 07:53:13.893: INFO: Pod "pod-projected-secrets-7b2badb2-4023-49a0-87b7-6646c90f2098" satisfied condition "Succeeded or Failed"
Dec  9 07:53:13.896: INFO: Trying to get logs from node ip-10-0-10-179 pod pod-projected-secrets-7b2badb2-4023-49a0-87b7-6646c90f2098 container projected-secret-volume-test: <nil>
STEP: delete the pod 12/09/22 07:53:13.901
Dec  9 07:53:13.911: INFO: Waiting for pod pod-projected-secrets-7b2badb2-4023-49a0-87b7-6646c90f2098 to disappear
Dec  9 07:53:13.914: INFO: Pod pod-projected-secrets-7b2badb2-4023-49a0-87b7-6646c90f2098 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Dec  9 07:53:13.914: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-3866" for this suite. 12/09/22 07:53:13.917
------------------------------
â€¢ [4.075 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:78

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:53:09.848
    Dec  9 07:53:09.848: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename projected 12/09/22 07:53:09.849
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:53:09.865
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:53:09.868
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:78
    STEP: Creating projection with secret that has name projected-secret-test-map-d92f8c4d-6bc0-4841-be9b-9c275573ebba 12/09/22 07:53:09.873
    STEP: Creating a pod to test consume secrets 12/09/22 07:53:09.878
    Dec  9 07:53:09.884: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-7b2badb2-4023-49a0-87b7-6646c90f2098" in namespace "projected-3866" to be "Succeeded or Failed"
    Dec  9 07:53:09.888: INFO: Pod "pod-projected-secrets-7b2badb2-4023-49a0-87b7-6646c90f2098": Phase="Pending", Reason="", readiness=false. Elapsed: 3.66617ms
    Dec  9 07:53:11.892: INFO: Pod "pod-projected-secrets-7b2badb2-4023-49a0-87b7-6646c90f2098": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007385787s
    Dec  9 07:53:13.893: INFO: Pod "pod-projected-secrets-7b2badb2-4023-49a0-87b7-6646c90f2098": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008344163s
    STEP: Saw pod success 12/09/22 07:53:13.893
    Dec  9 07:53:13.893: INFO: Pod "pod-projected-secrets-7b2badb2-4023-49a0-87b7-6646c90f2098" satisfied condition "Succeeded or Failed"
    Dec  9 07:53:13.896: INFO: Trying to get logs from node ip-10-0-10-179 pod pod-projected-secrets-7b2badb2-4023-49a0-87b7-6646c90f2098 container projected-secret-volume-test: <nil>
    STEP: delete the pod 12/09/22 07:53:13.901
    Dec  9 07:53:13.911: INFO: Waiting for pod pod-projected-secrets-7b2badb2-4023-49a0-87b7-6646c90f2098 to disappear
    Dec  9 07:53:13.914: INFO: Pod pod-projected-secrets-7b2badb2-4023-49a0-87b7-6646c90f2098 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:53:13.914: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-3866" for this suite. 12/09/22 07:53:13.917
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:53:13.923
Dec  9 07:53:13.923: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename dns 12/09/22 07:53:13.925
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:53:13.939
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:53:13.944
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333
STEP: Creating a test externalName service 12/09/22 07:53:13.95
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8358.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-8358.svc.cluster.local; sleep 1; done
 12/09/22 07:53:13.956
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8358.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-8358.svc.cluster.local; sleep 1; done
 12/09/22 07:53:13.957
STEP: creating a pod to probe DNS 12/09/22 07:53:13.958
STEP: submitting the pod to kubernetes 12/09/22 07:53:13.959
Dec  9 07:53:13.970: INFO: Waiting up to 15m0s for pod "dns-test-29d3cdec-263d-4c8e-85f1-b21ee7773421" in namespace "dns-8358" to be "running"
Dec  9 07:53:13.974: INFO: Pod "dns-test-29d3cdec-263d-4c8e-85f1-b21ee7773421": Phase="Pending", Reason="", readiness=false. Elapsed: 4.171677ms
Dec  9 07:53:15.978: INFO: Pod "dns-test-29d3cdec-263d-4c8e-85f1-b21ee7773421": Phase="Running", Reason="", readiness=true. Elapsed: 2.00840643s
Dec  9 07:53:15.978: INFO: Pod "dns-test-29d3cdec-263d-4c8e-85f1-b21ee7773421" satisfied condition "running"
STEP: retrieving the pod 12/09/22 07:53:15.979
STEP: looking for the results for each expected name from probers 12/09/22 07:53:15.981
Dec  9 07:53:15.988: INFO: DNS probes using dns-test-29d3cdec-263d-4c8e-85f1-b21ee7773421 succeeded

STEP: deleting the pod 12/09/22 07:53:15.988
STEP: changing the externalName to bar.example.com 12/09/22 07:53:15.999
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8358.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-8358.svc.cluster.local; sleep 1; done
 12/09/22 07:53:16.011
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8358.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-8358.svc.cluster.local; sleep 1; done
 12/09/22 07:53:16.011
STEP: creating a second pod to probe DNS 12/09/22 07:53:16.012
STEP: submitting the pod to kubernetes 12/09/22 07:53:16.012
Dec  9 07:53:16.019: INFO: Waiting up to 15m0s for pod "dns-test-ba4ec543-2f69-4569-a0dd-6e4685e1b094" in namespace "dns-8358" to be "running"
Dec  9 07:53:16.022: INFO: Pod "dns-test-ba4ec543-2f69-4569-a0dd-6e4685e1b094": Phase="Pending", Reason="", readiness=false. Elapsed: 2.84524ms
Dec  9 07:53:18.026: INFO: Pod "dns-test-ba4ec543-2f69-4569-a0dd-6e4685e1b094": Phase="Running", Reason="", readiness=true. Elapsed: 2.006981049s
Dec  9 07:53:18.026: INFO: Pod "dns-test-ba4ec543-2f69-4569-a0dd-6e4685e1b094" satisfied condition "running"
STEP: retrieving the pod 12/09/22 07:53:18.026
STEP: looking for the results for each expected name from probers 12/09/22 07:53:18.029
Dec  9 07:53:18.033: INFO: File wheezy_udp@dns-test-service-3.dns-8358.svc.cluster.local from pod  dns-8358/dns-test-ba4ec543-2f69-4569-a0dd-6e4685e1b094 contains 'foo.example.com.
' instead of 'bar.example.com.'
Dec  9 07:53:18.037: INFO: File jessie_udp@dns-test-service-3.dns-8358.svc.cluster.local from pod  dns-8358/dns-test-ba4ec543-2f69-4569-a0dd-6e4685e1b094 contains 'foo.example.com.
' instead of 'bar.example.com.'
Dec  9 07:53:18.037: INFO: Lookups using dns-8358/dns-test-ba4ec543-2f69-4569-a0dd-6e4685e1b094 failed for: [wheezy_udp@dns-test-service-3.dns-8358.svc.cluster.local jessie_udp@dns-test-service-3.dns-8358.svc.cluster.local]

Dec  9 07:53:23.047: INFO: DNS probes using dns-test-ba4ec543-2f69-4569-a0dd-6e4685e1b094 succeeded

STEP: deleting the pod 12/09/22 07:53:23.047
STEP: changing the service to type=ClusterIP 12/09/22 07:53:23.071
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8358.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-8358.svc.cluster.local; sleep 1; done
 12/09/22 07:53:23.107
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8358.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-8358.svc.cluster.local; sleep 1; done
 12/09/22 07:53:23.107
STEP: creating a third pod to probe DNS 12/09/22 07:53:23.107
STEP: submitting the pod to kubernetes 12/09/22 07:53:23.11
Dec  9 07:53:23.121: INFO: Waiting up to 15m0s for pod "dns-test-5e278a0f-98c4-4bb4-a1d4-e7031b386f51" in namespace "dns-8358" to be "running"
Dec  9 07:53:23.131: INFO: Pod "dns-test-5e278a0f-98c4-4bb4-a1d4-e7031b386f51": Phase="Pending", Reason="", readiness=false. Elapsed: 8.58021ms
Dec  9 07:53:25.135: INFO: Pod "dns-test-5e278a0f-98c4-4bb4-a1d4-e7031b386f51": Phase="Running", Reason="", readiness=true. Elapsed: 2.012672117s
Dec  9 07:53:25.135: INFO: Pod "dns-test-5e278a0f-98c4-4bb4-a1d4-e7031b386f51" satisfied condition "running"
STEP: retrieving the pod 12/09/22 07:53:25.135
STEP: looking for the results for each expected name from probers 12/09/22 07:53:25.138
Dec  9 07:53:25.145: INFO: DNS probes using dns-test-5e278a0f-98c4-4bb4-a1d4-e7031b386f51 succeeded

STEP: deleting the pod 12/09/22 07:53:25.146
STEP: deleting the test externalName service 12/09/22 07:53:25.157
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Dec  9 07:53:25.174: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-8358" for this suite. 12/09/22 07:53:25.178
------------------------------
â€¢ [SLOW TEST] [11.262 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:53:13.923
    Dec  9 07:53:13.923: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename dns 12/09/22 07:53:13.925
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:53:13.939
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:53:13.944
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for ExternalName services [Conformance]
      test/e2e/network/dns.go:333
    STEP: Creating a test externalName service 12/09/22 07:53:13.95
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8358.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-8358.svc.cluster.local; sleep 1; done
     12/09/22 07:53:13.956
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8358.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-8358.svc.cluster.local; sleep 1; done
     12/09/22 07:53:13.957
    STEP: creating a pod to probe DNS 12/09/22 07:53:13.958
    STEP: submitting the pod to kubernetes 12/09/22 07:53:13.959
    Dec  9 07:53:13.970: INFO: Waiting up to 15m0s for pod "dns-test-29d3cdec-263d-4c8e-85f1-b21ee7773421" in namespace "dns-8358" to be "running"
    Dec  9 07:53:13.974: INFO: Pod "dns-test-29d3cdec-263d-4c8e-85f1-b21ee7773421": Phase="Pending", Reason="", readiness=false. Elapsed: 4.171677ms
    Dec  9 07:53:15.978: INFO: Pod "dns-test-29d3cdec-263d-4c8e-85f1-b21ee7773421": Phase="Running", Reason="", readiness=true. Elapsed: 2.00840643s
    Dec  9 07:53:15.978: INFO: Pod "dns-test-29d3cdec-263d-4c8e-85f1-b21ee7773421" satisfied condition "running"
    STEP: retrieving the pod 12/09/22 07:53:15.979
    STEP: looking for the results for each expected name from probers 12/09/22 07:53:15.981
    Dec  9 07:53:15.988: INFO: DNS probes using dns-test-29d3cdec-263d-4c8e-85f1-b21ee7773421 succeeded

    STEP: deleting the pod 12/09/22 07:53:15.988
    STEP: changing the externalName to bar.example.com 12/09/22 07:53:15.999
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8358.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-8358.svc.cluster.local; sleep 1; done
     12/09/22 07:53:16.011
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8358.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-8358.svc.cluster.local; sleep 1; done
     12/09/22 07:53:16.011
    STEP: creating a second pod to probe DNS 12/09/22 07:53:16.012
    STEP: submitting the pod to kubernetes 12/09/22 07:53:16.012
    Dec  9 07:53:16.019: INFO: Waiting up to 15m0s for pod "dns-test-ba4ec543-2f69-4569-a0dd-6e4685e1b094" in namespace "dns-8358" to be "running"
    Dec  9 07:53:16.022: INFO: Pod "dns-test-ba4ec543-2f69-4569-a0dd-6e4685e1b094": Phase="Pending", Reason="", readiness=false. Elapsed: 2.84524ms
    Dec  9 07:53:18.026: INFO: Pod "dns-test-ba4ec543-2f69-4569-a0dd-6e4685e1b094": Phase="Running", Reason="", readiness=true. Elapsed: 2.006981049s
    Dec  9 07:53:18.026: INFO: Pod "dns-test-ba4ec543-2f69-4569-a0dd-6e4685e1b094" satisfied condition "running"
    STEP: retrieving the pod 12/09/22 07:53:18.026
    STEP: looking for the results for each expected name from probers 12/09/22 07:53:18.029
    Dec  9 07:53:18.033: INFO: File wheezy_udp@dns-test-service-3.dns-8358.svc.cluster.local from pod  dns-8358/dns-test-ba4ec543-2f69-4569-a0dd-6e4685e1b094 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Dec  9 07:53:18.037: INFO: File jessie_udp@dns-test-service-3.dns-8358.svc.cluster.local from pod  dns-8358/dns-test-ba4ec543-2f69-4569-a0dd-6e4685e1b094 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Dec  9 07:53:18.037: INFO: Lookups using dns-8358/dns-test-ba4ec543-2f69-4569-a0dd-6e4685e1b094 failed for: [wheezy_udp@dns-test-service-3.dns-8358.svc.cluster.local jessie_udp@dns-test-service-3.dns-8358.svc.cluster.local]

    Dec  9 07:53:23.047: INFO: DNS probes using dns-test-ba4ec543-2f69-4569-a0dd-6e4685e1b094 succeeded

    STEP: deleting the pod 12/09/22 07:53:23.047
    STEP: changing the service to type=ClusterIP 12/09/22 07:53:23.071
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8358.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-8358.svc.cluster.local; sleep 1; done
     12/09/22 07:53:23.107
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8358.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-8358.svc.cluster.local; sleep 1; done
     12/09/22 07:53:23.107
    STEP: creating a third pod to probe DNS 12/09/22 07:53:23.107
    STEP: submitting the pod to kubernetes 12/09/22 07:53:23.11
    Dec  9 07:53:23.121: INFO: Waiting up to 15m0s for pod "dns-test-5e278a0f-98c4-4bb4-a1d4-e7031b386f51" in namespace "dns-8358" to be "running"
    Dec  9 07:53:23.131: INFO: Pod "dns-test-5e278a0f-98c4-4bb4-a1d4-e7031b386f51": Phase="Pending", Reason="", readiness=false. Elapsed: 8.58021ms
    Dec  9 07:53:25.135: INFO: Pod "dns-test-5e278a0f-98c4-4bb4-a1d4-e7031b386f51": Phase="Running", Reason="", readiness=true. Elapsed: 2.012672117s
    Dec  9 07:53:25.135: INFO: Pod "dns-test-5e278a0f-98c4-4bb4-a1d4-e7031b386f51" satisfied condition "running"
    STEP: retrieving the pod 12/09/22 07:53:25.135
    STEP: looking for the results for each expected name from probers 12/09/22 07:53:25.138
    Dec  9 07:53:25.145: INFO: DNS probes using dns-test-5e278a0f-98c4-4bb4-a1d4-e7031b386f51 succeeded

    STEP: deleting the pod 12/09/22 07:53:25.146
    STEP: deleting the test externalName service 12/09/22 07:53:25.157
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:53:25.174: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-8358" for this suite. 12/09/22 07:53:25.178
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] ReplicationController
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:83
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:53:25.187
Dec  9 07:53:25.187: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename replication-controller 12/09/22 07:53:25.188
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:53:25.215
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:53:25.222
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:83
Dec  9 07:53:25.229: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota 12/09/22 07:53:26.248
STEP: Checking rc "condition-test" has the desired failure condition set 12/09/22 07:53:26.261
STEP: Scaling down rc "condition-test" to satisfy pod quota 12/09/22 07:53:27.269
Dec  9 07:53:27.276: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set 12/09/22 07:53:27.277
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Dec  9 07:53:28.285: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-8423" for this suite. 12/09/22 07:53:28.288
------------------------------
â€¢ [3.106 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:83

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:53:25.187
    Dec  9 07:53:25.187: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename replication-controller 12/09/22 07:53:25.188
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:53:25.215
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:53:25.222
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should surface a failure condition on a common issue like exceeded quota [Conformance]
      test/e2e/apps/rc.go:83
    Dec  9 07:53:25.229: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
    STEP: Creating rc "condition-test" that asks for more than the allowed pod quota 12/09/22 07:53:26.248
    STEP: Checking rc "condition-test" has the desired failure condition set 12/09/22 07:53:26.261
    STEP: Scaling down rc "condition-test" to satisfy pod quota 12/09/22 07:53:27.269
    Dec  9 07:53:27.276: INFO: Updating replication controller "condition-test"
    STEP: Checking rc "condition-test" has no failure condition set 12/09/22 07:53:27.277
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:53:28.285: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-8423" for this suite. 12/09/22 07:53:28.288
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with projected pod [Conformance]
  test/e2e/storage/subpath.go:106
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:53:28.297
Dec  9 07:53:28.297: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename subpath 12/09/22 07:53:28.298
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:53:28.328
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:53:28.34
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 12/09/22 07:53:28.345
[It] should support subpaths with projected pod [Conformance]
  test/e2e/storage/subpath.go:106
STEP: Creating pod pod-subpath-test-projected-6qkp 12/09/22 07:53:28.356
STEP: Creating a pod to test atomic-volume-subpath 12/09/22 07:53:28.357
Dec  9 07:53:28.372: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-6qkp" in namespace "subpath-9229" to be "Succeeded or Failed"
Dec  9 07:53:28.379: INFO: Pod "pod-subpath-test-projected-6qkp": Phase="Pending", Reason="", readiness=false. Elapsed: 7.439569ms
Dec  9 07:53:30.386: INFO: Pod "pod-subpath-test-projected-6qkp": Phase="Running", Reason="", readiness=true. Elapsed: 2.013776778s
Dec  9 07:53:32.383: INFO: Pod "pod-subpath-test-projected-6qkp": Phase="Running", Reason="", readiness=true. Elapsed: 4.010929307s
Dec  9 07:53:34.384: INFO: Pod "pod-subpath-test-projected-6qkp": Phase="Running", Reason="", readiness=true. Elapsed: 6.012010476s
Dec  9 07:53:36.417: INFO: Pod "pod-subpath-test-projected-6qkp": Phase="Running", Reason="", readiness=true. Elapsed: 8.045132598s
Dec  9 07:53:38.383: INFO: Pod "pod-subpath-test-projected-6qkp": Phase="Running", Reason="", readiness=true. Elapsed: 10.010771396s
Dec  9 07:53:40.383: INFO: Pod "pod-subpath-test-projected-6qkp": Phase="Running", Reason="", readiness=true. Elapsed: 12.011142934s
Dec  9 07:53:42.384: INFO: Pod "pod-subpath-test-projected-6qkp": Phase="Running", Reason="", readiness=true. Elapsed: 14.011851922s
Dec  9 07:53:44.384: INFO: Pod "pod-subpath-test-projected-6qkp": Phase="Running", Reason="", readiness=true. Elapsed: 16.011601333s
Dec  9 07:53:46.387: INFO: Pod "pod-subpath-test-projected-6qkp": Phase="Running", Reason="", readiness=true. Elapsed: 18.014935023s
Dec  9 07:53:48.383: INFO: Pod "pod-subpath-test-projected-6qkp": Phase="Running", Reason="", readiness=true. Elapsed: 20.01139244s
Dec  9 07:53:50.384: INFO: Pod "pod-subpath-test-projected-6qkp": Phase="Running", Reason="", readiness=false. Elapsed: 22.012352454s
Dec  9 07:53:52.384: INFO: Pod "pod-subpath-test-projected-6qkp": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.01241484s
STEP: Saw pod success 12/09/22 07:53:52.384
Dec  9 07:53:52.385: INFO: Pod "pod-subpath-test-projected-6qkp" satisfied condition "Succeeded or Failed"
Dec  9 07:53:52.388: INFO: Trying to get logs from node ip-10-0-10-179 pod pod-subpath-test-projected-6qkp container test-container-subpath-projected-6qkp: <nil>
STEP: delete the pod 12/09/22 07:53:52.396
Dec  9 07:53:52.406: INFO: Waiting for pod pod-subpath-test-projected-6qkp to disappear
Dec  9 07:53:52.410: INFO: Pod pod-subpath-test-projected-6qkp no longer exists
STEP: Deleting pod pod-subpath-test-projected-6qkp 12/09/22 07:53:52.41
Dec  9 07:53:52.410: INFO: Deleting pod "pod-subpath-test-projected-6qkp" in namespace "subpath-9229"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Dec  9 07:53:52.413: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-9229" for this suite. 12/09/22 07:53:52.416
------------------------------
â€¢ [SLOW TEST] [24.127 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with projected pod [Conformance]
    test/e2e/storage/subpath.go:106

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:53:28.297
    Dec  9 07:53:28.297: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename subpath 12/09/22 07:53:28.298
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:53:28.328
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:53:28.34
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 12/09/22 07:53:28.345
    [It] should support subpaths with projected pod [Conformance]
      test/e2e/storage/subpath.go:106
    STEP: Creating pod pod-subpath-test-projected-6qkp 12/09/22 07:53:28.356
    STEP: Creating a pod to test atomic-volume-subpath 12/09/22 07:53:28.357
    Dec  9 07:53:28.372: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-6qkp" in namespace "subpath-9229" to be "Succeeded or Failed"
    Dec  9 07:53:28.379: INFO: Pod "pod-subpath-test-projected-6qkp": Phase="Pending", Reason="", readiness=false. Elapsed: 7.439569ms
    Dec  9 07:53:30.386: INFO: Pod "pod-subpath-test-projected-6qkp": Phase="Running", Reason="", readiness=true. Elapsed: 2.013776778s
    Dec  9 07:53:32.383: INFO: Pod "pod-subpath-test-projected-6qkp": Phase="Running", Reason="", readiness=true. Elapsed: 4.010929307s
    Dec  9 07:53:34.384: INFO: Pod "pod-subpath-test-projected-6qkp": Phase="Running", Reason="", readiness=true. Elapsed: 6.012010476s
    Dec  9 07:53:36.417: INFO: Pod "pod-subpath-test-projected-6qkp": Phase="Running", Reason="", readiness=true. Elapsed: 8.045132598s
    Dec  9 07:53:38.383: INFO: Pod "pod-subpath-test-projected-6qkp": Phase="Running", Reason="", readiness=true. Elapsed: 10.010771396s
    Dec  9 07:53:40.383: INFO: Pod "pod-subpath-test-projected-6qkp": Phase="Running", Reason="", readiness=true. Elapsed: 12.011142934s
    Dec  9 07:53:42.384: INFO: Pod "pod-subpath-test-projected-6qkp": Phase="Running", Reason="", readiness=true. Elapsed: 14.011851922s
    Dec  9 07:53:44.384: INFO: Pod "pod-subpath-test-projected-6qkp": Phase="Running", Reason="", readiness=true. Elapsed: 16.011601333s
    Dec  9 07:53:46.387: INFO: Pod "pod-subpath-test-projected-6qkp": Phase="Running", Reason="", readiness=true. Elapsed: 18.014935023s
    Dec  9 07:53:48.383: INFO: Pod "pod-subpath-test-projected-6qkp": Phase="Running", Reason="", readiness=true. Elapsed: 20.01139244s
    Dec  9 07:53:50.384: INFO: Pod "pod-subpath-test-projected-6qkp": Phase="Running", Reason="", readiness=false. Elapsed: 22.012352454s
    Dec  9 07:53:52.384: INFO: Pod "pod-subpath-test-projected-6qkp": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.01241484s
    STEP: Saw pod success 12/09/22 07:53:52.384
    Dec  9 07:53:52.385: INFO: Pod "pod-subpath-test-projected-6qkp" satisfied condition "Succeeded or Failed"
    Dec  9 07:53:52.388: INFO: Trying to get logs from node ip-10-0-10-179 pod pod-subpath-test-projected-6qkp container test-container-subpath-projected-6qkp: <nil>
    STEP: delete the pod 12/09/22 07:53:52.396
    Dec  9 07:53:52.406: INFO: Waiting for pod pod-subpath-test-projected-6qkp to disappear
    Dec  9 07:53:52.410: INFO: Pod pod-subpath-test-projected-6qkp no longer exists
    STEP: Deleting pod pod-subpath-test-projected-6qkp 12/09/22 07:53:52.41
    Dec  9 07:53:52.410: INFO: Deleting pod "pod-subpath-test-projected-6qkp" in namespace "subpath-9229"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:53:52.413: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-9229" for this suite. 12/09/22 07:53:52.416
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-instrumentation] Events
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175
[BeforeEach] [sig-instrumentation] Events
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:53:52.425
Dec  9 07:53:52.425: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename events 12/09/22 07:53:52.426
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:53:52.449
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:53:52.457
[BeforeEach] [sig-instrumentation] Events
  test/e2e/framework/metrics/init/init.go:31
[It] should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175
STEP: Create set of events 12/09/22 07:53:52.461
Dec  9 07:53:52.466: INFO: created test-event-1
Dec  9 07:53:52.470: INFO: created test-event-2
Dec  9 07:53:52.475: INFO: created test-event-3
STEP: get a list of Events with a label in the current namespace 12/09/22 07:53:52.475
STEP: delete collection of events 12/09/22 07:53:52.477
Dec  9 07:53:52.477: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity 12/09/22 07:53:52.489
Dec  9 07:53:52.489: INFO: requesting list of events to confirm quantity
[AfterEach] [sig-instrumentation] Events
  test/e2e/framework/node/init/init.go:32
Dec  9 07:53:52.493: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-instrumentation] Events
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-instrumentation] Events
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-instrumentation] Events
  tear down framework | framework.go:193
STEP: Destroying namespace "events-255" for this suite. 12/09/22 07:53:52.497
------------------------------
â€¢ [0.079 seconds]
[sig-instrumentation] Events
test/e2e/instrumentation/common/framework.go:23
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:53:52.425
    Dec  9 07:53:52.425: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename events 12/09/22 07:53:52.426
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:53:52.449
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:53:52.457
    [BeforeEach] [sig-instrumentation] Events
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete a collection of events [Conformance]
      test/e2e/instrumentation/core_events.go:175
    STEP: Create set of events 12/09/22 07:53:52.461
    Dec  9 07:53:52.466: INFO: created test-event-1
    Dec  9 07:53:52.470: INFO: created test-event-2
    Dec  9 07:53:52.475: INFO: created test-event-3
    STEP: get a list of Events with a label in the current namespace 12/09/22 07:53:52.475
    STEP: delete collection of events 12/09/22 07:53:52.477
    Dec  9 07:53:52.477: INFO: requesting DeleteCollection of events
    STEP: check that the list of events matches the requested quantity 12/09/22 07:53:52.489
    Dec  9 07:53:52.489: INFO: requesting list of events to confirm quantity
    [AfterEach] [sig-instrumentation] Events
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:53:52.493: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-instrumentation] Events
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-instrumentation] Events
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-instrumentation] Events
      tear down framework | framework.go:193
    STEP: Destroying namespace "events-255" for this suite. 12/09/22 07:53:52.497
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-network] Services
  should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1557
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:53:52.505
Dec  9 07:53:52.505: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename services 12/09/22 07:53:52.506
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:53:52.521
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:53:52.526
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1557
STEP: creating a service nodeport-service with the type=NodePort in namespace services-9222 12/09/22 07:53:52.529
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 12/09/22 07:53:52.55
STEP: creating service externalsvc in namespace services-9222 12/09/22 07:53:52.551
STEP: creating replication controller externalsvc in namespace services-9222 12/09/22 07:53:52.568
I1209 07:53:52.577452      20 runners.go:193] Created replication controller with name: externalsvc, namespace: services-9222, replica count: 2
I1209 07:53:55.630592      20 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName 12/09/22 07:53:55.634
Dec  9 07:53:55.649: INFO: Creating new exec pod
Dec  9 07:53:55.658: INFO: Waiting up to 5m0s for pod "execpodb68qt" in namespace "services-9222" to be "running"
Dec  9 07:53:55.665: INFO: Pod "execpodb68qt": Phase="Pending", Reason="", readiness=false. Elapsed: 6.75056ms
Dec  9 07:53:57.671: INFO: Pod "execpodb68qt": Phase="Running", Reason="", readiness=true. Elapsed: 2.012886606s
Dec  9 07:53:57.671: INFO: Pod "execpodb68qt" satisfied condition "running"
Dec  9 07:53:57.672: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=services-9222 exec execpodb68qt -- /bin/sh -x -c nslookup nodeport-service.services-9222.svc.cluster.local'
Dec  9 07:53:58.014: INFO: stderr: "+ nslookup nodeport-service.services-9222.svc.cluster.local\n"
Dec  9 07:53:58.015: INFO: stdout: "Server:\t\t10.3.0.10\nAddress:\t10.3.0.10#53\n\nnodeport-service.services-9222.svc.cluster.local\tcanonical name = externalsvc.services-9222.svc.cluster.local.\nName:\texternalsvc.services-9222.svc.cluster.local\nAddress: 10.3.222.231\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-9222, will wait for the garbage collector to delete the pods 12/09/22 07:53:58.015
Dec  9 07:53:58.075: INFO: Deleting ReplicationController externalsvc took: 6.006152ms
Dec  9 07:53:58.176: INFO: Terminating ReplicationController externalsvc pods took: 101.582478ms
Dec  9 07:54:00.197: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Dec  9 07:54:00.218: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-9222" for this suite. 12/09/22 07:54:00.224
------------------------------
â€¢ [SLOW TEST] [7.733 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1557

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:53:52.505
    Dec  9 07:53:52.505: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename services 12/09/22 07:53:52.506
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:53:52.521
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:53:52.526
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to change the type from NodePort to ExternalName [Conformance]
      test/e2e/network/service.go:1557
    STEP: creating a service nodeport-service with the type=NodePort in namespace services-9222 12/09/22 07:53:52.529
    STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 12/09/22 07:53:52.55
    STEP: creating service externalsvc in namespace services-9222 12/09/22 07:53:52.551
    STEP: creating replication controller externalsvc in namespace services-9222 12/09/22 07:53:52.568
    I1209 07:53:52.577452      20 runners.go:193] Created replication controller with name: externalsvc, namespace: services-9222, replica count: 2
    I1209 07:53:55.630592      20 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    STEP: changing the NodePort service to type=ExternalName 12/09/22 07:53:55.634
    Dec  9 07:53:55.649: INFO: Creating new exec pod
    Dec  9 07:53:55.658: INFO: Waiting up to 5m0s for pod "execpodb68qt" in namespace "services-9222" to be "running"
    Dec  9 07:53:55.665: INFO: Pod "execpodb68qt": Phase="Pending", Reason="", readiness=false. Elapsed: 6.75056ms
    Dec  9 07:53:57.671: INFO: Pod "execpodb68qt": Phase="Running", Reason="", readiness=true. Elapsed: 2.012886606s
    Dec  9 07:53:57.671: INFO: Pod "execpodb68qt" satisfied condition "running"
    Dec  9 07:53:57.672: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=services-9222 exec execpodb68qt -- /bin/sh -x -c nslookup nodeport-service.services-9222.svc.cluster.local'
    Dec  9 07:53:58.014: INFO: stderr: "+ nslookup nodeport-service.services-9222.svc.cluster.local\n"
    Dec  9 07:53:58.015: INFO: stdout: "Server:\t\t10.3.0.10\nAddress:\t10.3.0.10#53\n\nnodeport-service.services-9222.svc.cluster.local\tcanonical name = externalsvc.services-9222.svc.cluster.local.\nName:\texternalsvc.services-9222.svc.cluster.local\nAddress: 10.3.222.231\n\n"
    STEP: deleting ReplicationController externalsvc in namespace services-9222, will wait for the garbage collector to delete the pods 12/09/22 07:53:58.015
    Dec  9 07:53:58.075: INFO: Deleting ReplicationController externalsvc took: 6.006152ms
    Dec  9 07:53:58.176: INFO: Terminating ReplicationController externalsvc pods took: 101.582478ms
    Dec  9 07:54:00.197: INFO: Cleaning up the NodePort to ExternalName test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:54:00.218: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-9222" for this suite. 12/09/22 07:54:00.224
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Ephemeral Containers [NodeConformance]
  will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:54:00.24
Dec  9 07:54:00.240: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename ephemeral-containers-test 12/09/22 07:54:00.243
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:54:00.27
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:54:00.283
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/common/node/ephemeral_containers.go:38
[It] will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45
STEP: creating a target pod 12/09/22 07:54:00.292
Dec  9 07:54:00.302: INFO: Waiting up to 5m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-2401" to be "running and ready"
Dec  9 07:54:00.308: INFO: Pod "ephemeral-containers-target-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 5.182125ms
Dec  9 07:54:00.308: INFO: The phase of Pod ephemeral-containers-target-pod is Pending, waiting for it to be Running (with Ready = true)
Dec  9 07:54:02.311: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.008721743s
Dec  9 07:54:02.311: INFO: The phase of Pod ephemeral-containers-target-pod is Running (Ready = true)
Dec  9 07:54:02.311: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "running and ready"
STEP: adding an ephemeral container 12/09/22 07:54:02.316
Dec  9 07:54:02.374: INFO: Waiting up to 1m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-2401" to be "container debugger running"
Dec  9 07:54:02.377: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 3.00883ms
Dec  9 07:54:04.382: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.007619194s
Dec  9 07:54:06.383: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.009006108s
Dec  9 07:54:06.383: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "container debugger running"
STEP: checking pod container endpoints 12/09/22 07:54:06.383
Dec  9 07:54:06.383: INFO: ExecWithOptions {Command:[/bin/echo marco] Namespace:ephemeral-containers-test-2401 PodName:ephemeral-containers-target-pod ContainerName:debugger Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec  9 07:54:06.383: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
Dec  9 07:54:06.386: INFO: ExecWithOptions: Clientset creation
Dec  9 07:54:06.386: INFO: ExecWithOptions: execute(POST https://10.3.0.1:443/api/v1/namespaces/ephemeral-containers-test-2401/pods/ephemeral-containers-target-pod/exec?command=%2Fbin%2Fecho&command=marco&container=debugger&container=debugger&stderr=true&stdout=true)
Dec  9 07:54:06.468: INFO: Exec stderr: ""
[AfterEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Dec  9 07:54:06.480: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "ephemeral-containers-test-2401" for this suite. 12/09/22 07:54:06.499
------------------------------
â€¢ [SLOW TEST] [6.266 seconds]
[sig-node] Ephemeral Containers [NodeConformance]
test/e2e/common/node/framework.go:23
  will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:54:00.24
    Dec  9 07:54:00.240: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename ephemeral-containers-test 12/09/22 07:54:00.243
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:54:00.27
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:54:00.283
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/common/node/ephemeral_containers.go:38
    [It] will start an ephemeral container in an existing pod [Conformance]
      test/e2e/common/node/ephemeral_containers.go:45
    STEP: creating a target pod 12/09/22 07:54:00.292
    Dec  9 07:54:00.302: INFO: Waiting up to 5m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-2401" to be "running and ready"
    Dec  9 07:54:00.308: INFO: Pod "ephemeral-containers-target-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 5.182125ms
    Dec  9 07:54:00.308: INFO: The phase of Pod ephemeral-containers-target-pod is Pending, waiting for it to be Running (with Ready = true)
    Dec  9 07:54:02.311: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.008721743s
    Dec  9 07:54:02.311: INFO: The phase of Pod ephemeral-containers-target-pod is Running (Ready = true)
    Dec  9 07:54:02.311: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "running and ready"
    STEP: adding an ephemeral container 12/09/22 07:54:02.316
    Dec  9 07:54:02.374: INFO: Waiting up to 1m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-2401" to be "container debugger running"
    Dec  9 07:54:02.377: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 3.00883ms
    Dec  9 07:54:04.382: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.007619194s
    Dec  9 07:54:06.383: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.009006108s
    Dec  9 07:54:06.383: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "container debugger running"
    STEP: checking pod container endpoints 12/09/22 07:54:06.383
    Dec  9 07:54:06.383: INFO: ExecWithOptions {Command:[/bin/echo marco] Namespace:ephemeral-containers-test-2401 PodName:ephemeral-containers-target-pod ContainerName:debugger Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Dec  9 07:54:06.383: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    Dec  9 07:54:06.386: INFO: ExecWithOptions: Clientset creation
    Dec  9 07:54:06.386: INFO: ExecWithOptions: execute(POST https://10.3.0.1:443/api/v1/namespaces/ephemeral-containers-test-2401/pods/ephemeral-containers-target-pod/exec?command=%2Fbin%2Fecho&command=marco&container=debugger&container=debugger&stderr=true&stdout=true)
    Dec  9 07:54:06.468: INFO: Exec stderr: ""
    [AfterEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:54:06.480: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "ephemeral-containers-test-2401" for this suite. 12/09/22 07:54:06.499
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:243
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:54:06.539
Dec  9 07:54:06.539: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename namespaces 12/09/22 07:54:06.551
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:54:06.577
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:54:06.591
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:243
STEP: Creating a test namespace 12/09/22 07:54:06.603
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:54:06.646
STEP: Creating a pod in the namespace 12/09/22 07:54:06.665
STEP: Waiting for the pod to have running status 12/09/22 07:54:06.677
Dec  9 07:54:06.678: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "nsdeletetest-6402" to be "running"
Dec  9 07:54:06.708: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 29.392061ms
Dec  9 07:54:08.714: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.035721665s
Dec  9 07:54:08.714: INFO: Pod "test-pod" satisfied condition "running"
STEP: Deleting the namespace 12/09/22 07:54:08.714
STEP: Waiting for the namespace to be removed. 12/09/22 07:54:08.725
STEP: Recreating the namespace 12/09/22 07:54:19.729
STEP: Verifying there are no pods in the namespace 12/09/22 07:54:19.753
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Dec  9 07:54:19.757: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-8683" for this suite. 12/09/22 07:54:19.762
STEP: Destroying namespace "nsdeletetest-6402" for this suite. 12/09/22 07:54:19.768
Dec  9 07:54:19.776: INFO: Namespace nsdeletetest-6402 was already deleted
STEP: Destroying namespace "nsdeletetest-4963" for this suite. 12/09/22 07:54:19.776
------------------------------
â€¢ [SLOW TEST] [13.245 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:243

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:54:06.539
    Dec  9 07:54:06.539: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename namespaces 12/09/22 07:54:06.551
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:54:06.577
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:54:06.591
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should ensure that all pods are removed when a namespace is deleted [Conformance]
      test/e2e/apimachinery/namespace.go:243
    STEP: Creating a test namespace 12/09/22 07:54:06.603
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:54:06.646
    STEP: Creating a pod in the namespace 12/09/22 07:54:06.665
    STEP: Waiting for the pod to have running status 12/09/22 07:54:06.677
    Dec  9 07:54:06.678: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "nsdeletetest-6402" to be "running"
    Dec  9 07:54:06.708: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 29.392061ms
    Dec  9 07:54:08.714: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.035721665s
    Dec  9 07:54:08.714: INFO: Pod "test-pod" satisfied condition "running"
    STEP: Deleting the namespace 12/09/22 07:54:08.714
    STEP: Waiting for the namespace to be removed. 12/09/22 07:54:08.725
    STEP: Recreating the namespace 12/09/22 07:54:19.729
    STEP: Verifying there are no pods in the namespace 12/09/22 07:54:19.753
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:54:19.757: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-8683" for this suite. 12/09/22 07:54:19.762
    STEP: Destroying namespace "nsdeletetest-6402" for this suite. 12/09/22 07:54:19.768
    Dec  9 07:54:19.776: INFO: Namespace nsdeletetest-6402 was already deleted
    STEP: Destroying namespace "nsdeletetest-4963" for this suite. 12/09/22 07:54:19.776
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1
  should proxy through a service and a pod  [Conformance]
  test/e2e/network/proxy.go:101
[BeforeEach] version v1
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:54:19.786
Dec  9 07:54:19.786: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename proxy 12/09/22 07:54:19.787
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:54:19.802
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:54:19.806
[BeforeEach] version v1
  test/e2e/framework/metrics/init/init.go:31
[It] should proxy through a service and a pod  [Conformance]
  test/e2e/network/proxy.go:101
STEP: starting an echo server on multiple ports 12/09/22 07:54:19.821
STEP: creating replication controller proxy-service-8nbp9 in namespace proxy-5690 12/09/22 07:54:19.821
I1209 07:54:19.832450      20 runners.go:193] Created replication controller with name: proxy-service-8nbp9, namespace: proxy-5690, replica count: 1
I1209 07:54:20.884386      20 runners.go:193] proxy-service-8nbp9 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1209 07:54:21.886121      20 runners.go:193] proxy-service-8nbp9 Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Dec  9 07:54:21.890: INFO: setup took 2.080601931s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts 12/09/22 07:54:21.891
Dec  9 07:54:21.923: INFO: (0) /api/v1/namespaces/proxy-5690/services/http:proxy-service-8nbp9:portname2/proxy/: bar (200; 30.621394ms)
Dec  9 07:54:21.924: INFO: (0) /api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc:160/proxy/: foo (200; 31.889888ms)
Dec  9 07:54:21.924: INFO: (0) /api/v1/namespaces/proxy-5690/pods/http:proxy-service-8nbp9-49zcc:1080/proxy/: <a href="/api/v1/namespaces/proxy-5690/pods/http:proxy-service-8nbp9-49zcc:1080/proxy/rewriteme">... (200; 31.282597ms)
Dec  9 07:54:21.924: INFO: (0) /api/v1/namespaces/proxy-5690/services/proxy-service-8nbp9:portname2/proxy/: bar (200; 31.448464ms)
Dec  9 07:54:21.924: INFO: (0) /api/v1/namespaces/proxy-5690/services/http:proxy-service-8nbp9:portname1/proxy/: foo (200; 32.488863ms)
Dec  9 07:54:21.924: INFO: (0) /api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc/proxy/: <a href="/api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc/proxy/rewriteme">test</a> (200; 31.907985ms)
Dec  9 07:54:21.924: INFO: (0) /api/v1/namespaces/proxy-5690/services/proxy-service-8nbp9:portname1/proxy/: foo (200; 31.757593ms)
Dec  9 07:54:21.924: INFO: (0) /api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc:162/proxy/: bar (200; 33.418294ms)
Dec  9 07:54:21.925: INFO: (0) /api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc:1080/proxy/: <a href="/api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc:1080/proxy/rewriteme">test<... (200; 33.209799ms)
Dec  9 07:54:21.925: INFO: (0) /api/v1/namespaces/proxy-5690/pods/http:proxy-service-8nbp9-49zcc:160/proxy/: foo (200; 33.109807ms)
Dec  9 07:54:21.925: INFO: (0) /api/v1/namespaces/proxy-5690/pods/http:proxy-service-8nbp9-49zcc:162/proxy/: bar (200; 32.375837ms)
Dec  9 07:54:21.930: INFO: (0) /api/v1/namespaces/proxy-5690/pods/https:proxy-service-8nbp9-49zcc:462/proxy/: tls qux (200; 38.71434ms)
Dec  9 07:54:21.930: INFO: (0) /api/v1/namespaces/proxy-5690/services/https:proxy-service-8nbp9:tlsportname2/proxy/: tls qux (200; 38.146369ms)
Dec  9 07:54:21.930: INFO: (0) /api/v1/namespaces/proxy-5690/pods/https:proxy-service-8nbp9-49zcc:443/proxy/: <a href="/api/v1/namespaces/proxy-5690/pods/https:proxy-service-8nbp9-49zcc:443/proxy/tlsrewritem... (200; 37.823372ms)
Dec  9 07:54:21.930: INFO: (0) /api/v1/namespaces/proxy-5690/pods/https:proxy-service-8nbp9-49zcc:460/proxy/: tls baz (200; 38.345244ms)
Dec  9 07:54:21.930: INFO: (0) /api/v1/namespaces/proxy-5690/services/https:proxy-service-8nbp9:tlsportname1/proxy/: tls baz (200; 37.471009ms)
Dec  9 07:54:21.945: INFO: (1) /api/v1/namespaces/proxy-5690/pods/https:proxy-service-8nbp9-49zcc:443/proxy/: <a href="/api/v1/namespaces/proxy-5690/pods/https:proxy-service-8nbp9-49zcc:443/proxy/tlsrewritem... (200; 14.569257ms)
Dec  9 07:54:21.945: INFO: (1) /api/v1/namespaces/proxy-5690/pods/http:proxy-service-8nbp9-49zcc:160/proxy/: foo (200; 14.910756ms)
Dec  9 07:54:21.945: INFO: (1) /api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc:160/proxy/: foo (200; 14.791105ms)
Dec  9 07:54:21.946: INFO: (1) /api/v1/namespaces/proxy-5690/pods/http:proxy-service-8nbp9-49zcc:162/proxy/: bar (200; 15.248608ms)
Dec  9 07:54:21.946: INFO: (1) /api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc:162/proxy/: bar (200; 14.896982ms)
Dec  9 07:54:21.946: INFO: (1) /api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc:1080/proxy/: <a href="/api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc:1080/proxy/rewriteme">test<... (200; 14.66783ms)
Dec  9 07:54:21.946: INFO: (1) /api/v1/namespaces/proxy-5690/services/http:proxy-service-8nbp9:portname1/proxy/: foo (200; 15.642846ms)
Dec  9 07:54:21.946: INFO: (1) /api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc/proxy/: <a href="/api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc/proxy/rewriteme">test</a> (200; 15.388269ms)
Dec  9 07:54:21.946: INFO: (1) /api/v1/namespaces/proxy-5690/pods/http:proxy-service-8nbp9-49zcc:1080/proxy/: <a href="/api/v1/namespaces/proxy-5690/pods/http:proxy-service-8nbp9-49zcc:1080/proxy/rewriteme">... (200; 15.001247ms)
Dec  9 07:54:21.946: INFO: (1) /api/v1/namespaces/proxy-5690/pods/https:proxy-service-8nbp9-49zcc:460/proxy/: tls baz (200; 14.795129ms)
Dec  9 07:54:21.946: INFO: (1) /api/v1/namespaces/proxy-5690/services/https:proxy-service-8nbp9:tlsportname2/proxy/: tls qux (200; 15.583345ms)
Dec  9 07:54:21.946: INFO: (1) /api/v1/namespaces/proxy-5690/pods/https:proxy-service-8nbp9-49zcc:462/proxy/: tls qux (200; 15.311023ms)
Dec  9 07:54:21.951: INFO: (1) /api/v1/namespaces/proxy-5690/services/proxy-service-8nbp9:portname1/proxy/: foo (200; 19.827953ms)
Dec  9 07:54:21.951: INFO: (1) /api/v1/namespaces/proxy-5690/services/proxy-service-8nbp9:portname2/proxy/: bar (200; 19.981318ms)
Dec  9 07:54:21.951: INFO: (1) /api/v1/namespaces/proxy-5690/services/https:proxy-service-8nbp9:tlsportname1/proxy/: tls baz (200; 20.342273ms)
Dec  9 07:54:21.951: INFO: (1) /api/v1/namespaces/proxy-5690/services/http:proxy-service-8nbp9:portname2/proxy/: bar (200; 20.541141ms)
Dec  9 07:54:21.966: INFO: (2) /api/v1/namespaces/proxy-5690/pods/http:proxy-service-8nbp9-49zcc:160/proxy/: foo (200; 14.40727ms)
Dec  9 07:54:21.966: INFO: (2) /api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc:1080/proxy/: <a href="/api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc:1080/proxy/rewriteme">test<... (200; 14.411651ms)
Dec  9 07:54:21.966: INFO: (2) /api/v1/namespaces/proxy-5690/pods/https:proxy-service-8nbp9-49zcc:460/proxy/: tls baz (200; 14.697613ms)
Dec  9 07:54:21.973: INFO: (2) /api/v1/namespaces/proxy-5690/pods/http:proxy-service-8nbp9-49zcc:162/proxy/: bar (200; 20.218059ms)
Dec  9 07:54:21.973: INFO: (2) /api/v1/namespaces/proxy-5690/pods/http:proxy-service-8nbp9-49zcc:1080/proxy/: <a href="/api/v1/namespaces/proxy-5690/pods/http:proxy-service-8nbp9-49zcc:1080/proxy/rewriteme">... (200; 20.134538ms)
Dec  9 07:54:21.977: INFO: (2) /api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc:160/proxy/: foo (200; 24.727348ms)
Dec  9 07:54:21.978: INFO: (2) /api/v1/namespaces/proxy-5690/services/proxy-service-8nbp9:portname1/proxy/: foo (200; 26.640943ms)
Dec  9 07:54:21.978: INFO: (2) /api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc:162/proxy/: bar (200; 25.334381ms)
Dec  9 07:54:21.979: INFO: (2) /api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc/proxy/: <a href="/api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc/proxy/rewriteme">test</a> (200; 25.855286ms)
Dec  9 07:54:21.979: INFO: (2) /api/v1/namespaces/proxy-5690/pods/https:proxy-service-8nbp9-49zcc:462/proxy/: tls qux (200; 25.83878ms)
Dec  9 07:54:21.979: INFO: (2) /api/v1/namespaces/proxy-5690/pods/https:proxy-service-8nbp9-49zcc:443/proxy/: <a href="/api/v1/namespaces/proxy-5690/pods/https:proxy-service-8nbp9-49zcc:443/proxy/tlsrewritem... (200; 26.531176ms)
Dec  9 07:54:21.979: INFO: (2) /api/v1/namespaces/proxy-5690/services/proxy-service-8nbp9:portname2/proxy/: bar (200; 26.021178ms)
Dec  9 07:54:21.979: INFO: (2) /api/v1/namespaces/proxy-5690/services/http:proxy-service-8nbp9:portname2/proxy/: bar (200; 26.226444ms)
Dec  9 07:54:21.979: INFO: (2) /api/v1/namespaces/proxy-5690/services/http:proxy-service-8nbp9:portname1/proxy/: foo (200; 26.945059ms)
Dec  9 07:54:21.979: INFO: (2) /api/v1/namespaces/proxy-5690/services/https:proxy-service-8nbp9:tlsportname1/proxy/: tls baz (200; 26.97436ms)
Dec  9 07:54:21.980: INFO: (2) /api/v1/namespaces/proxy-5690/services/https:proxy-service-8nbp9:tlsportname2/proxy/: tls qux (200; 27.783989ms)
Dec  9 07:54:22.005: INFO: (3) /api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc/proxy/: <a href="/api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc/proxy/rewriteme">test</a> (200; 24.356695ms)
Dec  9 07:54:22.005: INFO: (3) /api/v1/namespaces/proxy-5690/pods/http:proxy-service-8nbp9-49zcc:162/proxy/: bar (200; 24.288837ms)
Dec  9 07:54:22.005: INFO: (3) /api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc:1080/proxy/: <a href="/api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc:1080/proxy/rewriteme">test<... (200; 25.189437ms)
Dec  9 07:54:22.006: INFO: (3) /api/v1/namespaces/proxy-5690/pods/https:proxy-service-8nbp9-49zcc:460/proxy/: tls baz (200; 26.056011ms)
Dec  9 07:54:22.006: INFO: (3) /api/v1/namespaces/proxy-5690/pods/https:proxy-service-8nbp9-49zcc:462/proxy/: tls qux (200; 25.373003ms)
Dec  9 07:54:22.008: INFO: (3) /api/v1/namespaces/proxy-5690/pods/http:proxy-service-8nbp9-49zcc:1080/proxy/: <a href="/api/v1/namespaces/proxy-5690/pods/http:proxy-service-8nbp9-49zcc:1080/proxy/rewriteme">... (200; 26.283213ms)
Dec  9 07:54:22.008: INFO: (3) /api/v1/namespaces/proxy-5690/services/http:proxy-service-8nbp9:portname2/proxy/: bar (200; 26.857758ms)
Dec  9 07:54:22.008: INFO: (3) /api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc:160/proxy/: foo (200; 27.193554ms)
Dec  9 07:54:22.008: INFO: (3) /api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc:162/proxy/: bar (200; 27.926198ms)
Dec  9 07:54:22.008: INFO: (3) /api/v1/namespaces/proxy-5690/services/https:proxy-service-8nbp9:tlsportname2/proxy/: tls qux (200; 27.491704ms)
Dec  9 07:54:22.008: INFO: (3) /api/v1/namespaces/proxy-5690/services/proxy-service-8nbp9:portname2/proxy/: bar (200; 26.915824ms)
Dec  9 07:54:22.008: INFO: (3) /api/v1/namespaces/proxy-5690/services/https:proxy-service-8nbp9:tlsportname1/proxy/: tls baz (200; 27.348639ms)
Dec  9 07:54:22.008: INFO: (3) /api/v1/namespaces/proxy-5690/pods/http:proxy-service-8nbp9-49zcc:160/proxy/: foo (200; 27.950394ms)
Dec  9 07:54:22.008: INFO: (3) /api/v1/namespaces/proxy-5690/pods/https:proxy-service-8nbp9-49zcc:443/proxy/: <a href="/api/v1/namespaces/proxy-5690/pods/https:proxy-service-8nbp9-49zcc:443/proxy/tlsrewritem... (200; 27.54644ms)
Dec  9 07:54:22.008: INFO: (3) /api/v1/namespaces/proxy-5690/services/proxy-service-8nbp9:portname1/proxy/: foo (200; 26.977331ms)
Dec  9 07:54:22.008: INFO: (3) /api/v1/namespaces/proxy-5690/services/http:proxy-service-8nbp9:portname1/proxy/: foo (200; 27.684815ms)
Dec  9 07:54:22.025: INFO: (4) /api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc/proxy/: <a href="/api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc/proxy/rewriteme">test</a> (200; 16.388466ms)
Dec  9 07:54:22.025: INFO: (4) /api/v1/namespaces/proxy-5690/services/http:proxy-service-8nbp9:portname1/proxy/: foo (200; 16.813527ms)
Dec  9 07:54:22.025: INFO: (4) /api/v1/namespaces/proxy-5690/services/proxy-service-8nbp9:portname2/proxy/: bar (200; 16.725109ms)
Dec  9 07:54:22.025: INFO: (4) /api/v1/namespaces/proxy-5690/services/https:proxy-service-8nbp9:tlsportname1/proxy/: tls baz (200; 16.546209ms)
Dec  9 07:54:22.025: INFO: (4) /api/v1/namespaces/proxy-5690/services/http:proxy-service-8nbp9:portname2/proxy/: bar (200; 16.582575ms)
Dec  9 07:54:22.025: INFO: (4) /api/v1/namespaces/proxy-5690/services/proxy-service-8nbp9:portname1/proxy/: foo (200; 16.804388ms)
Dec  9 07:54:22.025: INFO: (4) /api/v1/namespaces/proxy-5690/services/https:proxy-service-8nbp9:tlsportname2/proxy/: tls qux (200; 16.702963ms)
Dec  9 07:54:22.025: INFO: (4) /api/v1/namespaces/proxy-5690/pods/https:proxy-service-8nbp9-49zcc:462/proxy/: tls qux (200; 16.995414ms)
Dec  9 07:54:22.025: INFO: (4) /api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc:162/proxy/: bar (200; 16.568623ms)
Dec  9 07:54:22.025: INFO: (4) /api/v1/namespaces/proxy-5690/pods/http:proxy-service-8nbp9-49zcc:162/proxy/: bar (200; 16.560869ms)
Dec  9 07:54:22.025: INFO: (4) /api/v1/namespaces/proxy-5690/pods/http:proxy-service-8nbp9-49zcc:160/proxy/: foo (200; 16.540027ms)
Dec  9 07:54:22.025: INFO: (4) /api/v1/namespaces/proxy-5690/pods/https:proxy-service-8nbp9-49zcc:460/proxy/: tls baz (200; 16.521626ms)
Dec  9 07:54:22.025: INFO: (4) /api/v1/namespaces/proxy-5690/pods/http:proxy-service-8nbp9-49zcc:1080/proxy/: <a href="/api/v1/namespaces/proxy-5690/pods/http:proxy-service-8nbp9-49zcc:1080/proxy/rewriteme">... (200; 16.852668ms)
Dec  9 07:54:22.025: INFO: (4) /api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc:1080/proxy/: <a href="/api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc:1080/proxy/rewriteme">test<... (200; 16.512423ms)
Dec  9 07:54:22.025: INFO: (4) /api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc:160/proxy/: foo (200; 16.822224ms)
Dec  9 07:54:22.026: INFO: (4) /api/v1/namespaces/proxy-5690/pods/https:proxy-service-8nbp9-49zcc:443/proxy/: <a href="/api/v1/namespaces/proxy-5690/pods/https:proxy-service-8nbp9-49zcc:443/proxy/tlsrewritem... (200; 17.708528ms)
Dec  9 07:54:22.046: INFO: (5) /api/v1/namespaces/proxy-5690/pods/http:proxy-service-8nbp9-49zcc:1080/proxy/: <a href="/api/v1/namespaces/proxy-5690/pods/http:proxy-service-8nbp9-49zcc:1080/proxy/rewriteme">... (200; 17.98764ms)
Dec  9 07:54:22.046: INFO: (5) /api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc:1080/proxy/: <a href="/api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc:1080/proxy/rewriteme">test<... (200; 17.873872ms)
Dec  9 07:54:22.047: INFO: (5) /api/v1/namespaces/proxy-5690/services/https:proxy-service-8nbp9:tlsportname1/proxy/: tls baz (200; 18.584304ms)
Dec  9 07:54:22.047: INFO: (5) /api/v1/namespaces/proxy-5690/pods/http:proxy-service-8nbp9-49zcc:160/proxy/: foo (200; 18.518399ms)
Dec  9 07:54:22.047: INFO: (5) /api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc/proxy/: <a href="/api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc/proxy/rewriteme">test</a> (200; 19.118364ms)
Dec  9 07:54:22.047: INFO: (5) /api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc:160/proxy/: foo (200; 19.060134ms)
Dec  9 07:54:22.047: INFO: (5) /api/v1/namespaces/proxy-5690/services/http:proxy-service-8nbp9:portname2/proxy/: bar (200; 19.047372ms)
Dec  9 07:54:22.047: INFO: (5) /api/v1/namespaces/proxy-5690/pods/https:proxy-service-8nbp9-49zcc:462/proxy/: tls qux (200; 19.27751ms)
Dec  9 07:54:22.047: INFO: (5) /api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc:162/proxy/: bar (200; 19.065175ms)
Dec  9 07:54:22.048: INFO: (5) /api/v1/namespaces/proxy-5690/services/https:proxy-service-8nbp9:tlsportname2/proxy/: tls qux (200; 19.573746ms)
Dec  9 07:54:22.048: INFO: (5) /api/v1/namespaces/proxy-5690/pods/https:proxy-service-8nbp9-49zcc:460/proxy/: tls baz (200; 19.971744ms)
Dec  9 07:54:22.048: INFO: (5) /api/v1/namespaces/proxy-5690/pods/https:proxy-service-8nbp9-49zcc:443/proxy/: <a href="/api/v1/namespaces/proxy-5690/pods/https:proxy-service-8nbp9-49zcc:443/proxy/tlsrewritem... (200; 19.522034ms)
Dec  9 07:54:22.049: INFO: (5) /api/v1/namespaces/proxy-5690/pods/http:proxy-service-8nbp9-49zcc:162/proxy/: bar (200; 20.499475ms)
Dec  9 07:54:22.049: INFO: (5) /api/v1/namespaces/proxy-5690/services/proxy-service-8nbp9:portname2/proxy/: bar (200; 20.522771ms)
Dec  9 07:54:22.049: INFO: (5) /api/v1/namespaces/proxy-5690/services/proxy-service-8nbp9:portname1/proxy/: foo (200; 20.67426ms)
Dec  9 07:54:22.049: INFO: (5) /api/v1/namespaces/proxy-5690/services/http:proxy-service-8nbp9:portname1/proxy/: foo (200; 21.076322ms)
Dec  9 07:54:22.065: INFO: (6) /api/v1/namespaces/proxy-5690/pods/https:proxy-service-8nbp9-49zcc:462/proxy/: tls qux (200; 15.856886ms)
Dec  9 07:54:22.066: INFO: (6) /api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc:160/proxy/: foo (200; 16.548755ms)
Dec  9 07:54:22.066: INFO: (6) /api/v1/namespaces/proxy-5690/pods/https:proxy-service-8nbp9-49zcc:443/proxy/: <a href="/api/v1/namespaces/proxy-5690/pods/https:proxy-service-8nbp9-49zcc:443/proxy/tlsrewritem... (200; 16.510033ms)
Dec  9 07:54:22.066: INFO: (6) /api/v1/namespaces/proxy-5690/pods/https:proxy-service-8nbp9-49zcc:460/proxy/: tls baz (200; 16.824793ms)
Dec  9 07:54:22.067: INFO: (6) /api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc:1080/proxy/: <a href="/api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc:1080/proxy/rewriteme">test<... (200; 17.163448ms)
Dec  9 07:54:22.067: INFO: (6) /api/v1/namespaces/proxy-5690/pods/http:proxy-service-8nbp9-49zcc:1080/proxy/: <a href="/api/v1/namespaces/proxy-5690/pods/http:proxy-service-8nbp9-49zcc:1080/proxy/rewriteme">... (200; 17.344614ms)
Dec  9 07:54:22.067: INFO: (6) /api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc/proxy/: <a href="/api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc/proxy/rewriteme">test</a> (200; 17.566819ms)
Dec  9 07:54:22.068: INFO: (6) /api/v1/namespaces/proxy-5690/pods/http:proxy-service-8nbp9-49zcc:160/proxy/: foo (200; 17.8898ms)
Dec  9 07:54:22.068: INFO: (6) /api/v1/namespaces/proxy-5690/services/http:proxy-service-8nbp9:portname1/proxy/: foo (200; 18.82543ms)
Dec  9 07:54:22.068: INFO: (6) /api/v1/namespaces/proxy-5690/services/http:proxy-service-8nbp9:portname2/proxy/: bar (200; 18.585562ms)
Dec  9 07:54:22.068: INFO: (6) /api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc:162/proxy/: bar (200; 18.289274ms)
Dec  9 07:54:22.068: INFO: (6) /api/v1/namespaces/proxy-5690/services/https:proxy-service-8nbp9:tlsportname1/proxy/: tls baz (200; 18.834438ms)
Dec  9 07:54:22.068: INFO: (6) /api/v1/namespaces/proxy-5690/services/https:proxy-service-8nbp9:tlsportname2/proxy/: tls qux (200; 19.094213ms)
Dec  9 07:54:22.069: INFO: (6) /api/v1/namespaces/proxy-5690/pods/http:proxy-service-8nbp9-49zcc:162/proxy/: bar (200; 19.712621ms)
Dec  9 07:54:22.070: INFO: (6) /api/v1/namespaces/proxy-5690/services/proxy-service-8nbp9:portname2/proxy/: bar (200; 19.904652ms)
Dec  9 07:54:22.070: INFO: (6) /api/v1/namespaces/proxy-5690/services/proxy-service-8nbp9:portname1/proxy/: foo (200; 20.133986ms)
Dec  9 07:54:22.081: INFO: (7) /api/v1/namespaces/proxy-5690/pods/https:proxy-service-8nbp9-49zcc:462/proxy/: tls qux (200; 11.374461ms)
Dec  9 07:54:22.083: INFO: (7) /api/v1/namespaces/proxy-5690/pods/http:proxy-service-8nbp9-49zcc:162/proxy/: bar (200; 12.388099ms)
Dec  9 07:54:22.083: INFO: (7) /api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc:1080/proxy/: <a href="/api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc:1080/proxy/rewriteme">test<... (200; 13.034159ms)
Dec  9 07:54:22.083: INFO: (7) /api/v1/namespaces/proxy-5690/pods/https:proxy-service-8nbp9-49zcc:460/proxy/: tls baz (200; 12.961961ms)
Dec  9 07:54:22.083: INFO: (7) /api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc:160/proxy/: foo (200; 12.843607ms)
Dec  9 07:54:22.083: INFO: (7) /api/v1/namespaces/proxy-5690/pods/https:proxy-service-8nbp9-49zcc:443/proxy/: <a href="/api/v1/namespaces/proxy-5690/pods/https:proxy-service-8nbp9-49zcc:443/proxy/tlsrewritem... (200; 12.81496ms)
Dec  9 07:54:22.083: INFO: (7) /api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc/proxy/: <a href="/api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc/proxy/rewriteme">test</a> (200; 12.708102ms)
Dec  9 07:54:22.083: INFO: (7) /api/v1/namespaces/proxy-5690/pods/http:proxy-service-8nbp9-49zcc:1080/proxy/: <a href="/api/v1/namespaces/proxy-5690/pods/http:proxy-service-8nbp9-49zcc:1080/proxy/rewriteme">... (200; 13.211882ms)
Dec  9 07:54:22.083: INFO: (7) /api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc:162/proxy/: bar (200; 13.174725ms)
Dec  9 07:54:22.083: INFO: (7) /api/v1/namespaces/proxy-5690/services/proxy-service-8nbp9:portname1/proxy/: foo (200; 13.357996ms)
Dec  9 07:54:22.085: INFO: (7) /api/v1/namespaces/proxy-5690/pods/http:proxy-service-8nbp9-49zcc:160/proxy/: foo (200; 15.075443ms)
Dec  9 07:54:22.086: INFO: (7) /api/v1/namespaces/proxy-5690/services/http:proxy-service-8nbp9:portname2/proxy/: bar (200; 15.245762ms)
Dec  9 07:54:22.086: INFO: (7) /api/v1/namespaces/proxy-5690/services/https:proxy-service-8nbp9:tlsportname2/proxy/: tls qux (200; 15.45631ms)
Dec  9 07:54:22.086: INFO: (7) /api/v1/namespaces/proxy-5690/services/https:proxy-service-8nbp9:tlsportname1/proxy/: tls baz (200; 15.356111ms)
Dec  9 07:54:22.086: INFO: (7) /api/v1/namespaces/proxy-5690/services/http:proxy-service-8nbp9:portname1/proxy/: foo (200; 15.585665ms)
Dec  9 07:54:22.086: INFO: (7) /api/v1/namespaces/proxy-5690/services/proxy-service-8nbp9:portname2/proxy/: bar (200; 16.094292ms)
Dec  9 07:54:22.100: INFO: (8) /api/v1/namespaces/proxy-5690/pods/https:proxy-service-8nbp9-49zcc:462/proxy/: tls qux (200; 14.038037ms)
Dec  9 07:54:22.101: INFO: (8) /api/v1/namespaces/proxy-5690/pods/http:proxy-service-8nbp9-49zcc:1080/proxy/: <a href="/api/v1/namespaces/proxy-5690/pods/http:proxy-service-8nbp9-49zcc:1080/proxy/rewriteme">... (200; 14.0764ms)
Dec  9 07:54:22.101: INFO: (8) /api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc/proxy/: <a href="/api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc/proxy/rewriteme">test</a> (200; 14.174166ms)
Dec  9 07:54:22.101: INFO: (8) /api/v1/namespaces/proxy-5690/services/proxy-service-8nbp9:portname1/proxy/: foo (200; 14.862883ms)
Dec  9 07:54:22.101: INFO: (8) /api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc:1080/proxy/: <a href="/api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc:1080/proxy/rewriteme">test<... (200; 14.6861ms)
Dec  9 07:54:22.101: INFO: (8) /api/v1/namespaces/proxy-5690/pods/http:proxy-service-8nbp9-49zcc:162/proxy/: bar (200; 14.346808ms)
Dec  9 07:54:22.102: INFO: (8) /api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc:162/proxy/: bar (200; 14.936627ms)
Dec  9 07:54:22.102: INFO: (8) /api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc:160/proxy/: foo (200; 15.033372ms)
Dec  9 07:54:22.102: INFO: (8) /api/v1/namespaces/proxy-5690/pods/https:proxy-service-8nbp9-49zcc:443/proxy/: <a href="/api/v1/namespaces/proxy-5690/pods/https:proxy-service-8nbp9-49zcc:443/proxy/tlsrewritem... (200; 15.035352ms)
Dec  9 07:54:22.103: INFO: (8) /api/v1/namespaces/proxy-5690/services/http:proxy-service-8nbp9:portname1/proxy/: foo (200; 15.822916ms)
Dec  9 07:54:22.103: INFO: (8) /api/v1/namespaces/proxy-5690/services/http:proxy-service-8nbp9:portname2/proxy/: bar (200; 15.690512ms)
Dec  9 07:54:22.103: INFO: (8) /api/v1/namespaces/proxy-5690/pods/http:proxy-service-8nbp9-49zcc:160/proxy/: foo (200; 15.978145ms)
Dec  9 07:54:22.103: INFO: (8) /api/v1/namespaces/proxy-5690/services/https:proxy-service-8nbp9:tlsportname1/proxy/: tls baz (200; 15.817285ms)
Dec  9 07:54:22.103: INFO: (8) /api/v1/namespaces/proxy-5690/services/https:proxy-service-8nbp9:tlsportname2/proxy/: tls qux (200; 16.01455ms)
Dec  9 07:54:22.103: INFO: (8) /api/v1/namespaces/proxy-5690/services/proxy-service-8nbp9:portname2/proxy/: bar (200; 16.35329ms)
Dec  9 07:54:22.103: INFO: (8) /api/v1/namespaces/proxy-5690/pods/https:proxy-service-8nbp9-49zcc:460/proxy/: tls baz (200; 16.122221ms)
Dec  9 07:54:22.119: INFO: (9) /api/v1/namespaces/proxy-5690/pods/https:proxy-service-8nbp9-49zcc:462/proxy/: tls qux (200; 16.197518ms)
Dec  9 07:54:22.119: INFO: (9) /api/v1/namespaces/proxy-5690/pods/http:proxy-service-8nbp9-49zcc:1080/proxy/: <a href="/api/v1/namespaces/proxy-5690/pods/http:proxy-service-8nbp9-49zcc:1080/proxy/rewriteme">... (200; 15.659931ms)
Dec  9 07:54:22.120: INFO: (9) /api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc:160/proxy/: foo (200; 16.499866ms)
Dec  9 07:54:22.121: INFO: (9) /api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc/proxy/: <a href="/api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc/proxy/rewriteme">test</a> (200; 16.687654ms)
Dec  9 07:54:22.121: INFO: (9) /api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc:162/proxy/: bar (200; 17.645775ms)
Dec  9 07:54:22.121: INFO: (9) /api/v1/namespaces/proxy-5690/services/proxy-service-8nbp9:portname1/proxy/: foo (200; 17.843302ms)
Dec  9 07:54:22.121: INFO: (9) /api/v1/namespaces/proxy-5690/pods/http:proxy-service-8nbp9-49zcc:162/proxy/: bar (200; 16.781301ms)
Dec  9 07:54:22.121: INFO: (9) /api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc:1080/proxy/: <a href="/api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc:1080/proxy/rewriteme">test<... (200; 17.696595ms)
Dec  9 07:54:22.121: INFO: (9) /api/v1/namespaces/proxy-5690/pods/https:proxy-service-8nbp9-49zcc:460/proxy/: tls baz (200; 17.627419ms)
Dec  9 07:54:22.122: INFO: (9) /api/v1/namespaces/proxy-5690/pods/https:proxy-service-8nbp9-49zcc:443/proxy/: <a href="/api/v1/namespaces/proxy-5690/pods/https:proxy-service-8nbp9-49zcc:443/proxy/tlsrewritem... (200; 17.578856ms)
Dec  9 07:54:22.122: INFO: (9) /api/v1/namespaces/proxy-5690/services/http:proxy-service-8nbp9:portname2/proxy/: bar (200; 17.851118ms)
Dec  9 07:54:22.122: INFO: (9) /api/v1/namespaces/proxy-5690/services/http:proxy-service-8nbp9:portname1/proxy/: foo (200; 18.194718ms)
Dec  9 07:54:22.122: INFO: (9) /api/v1/namespaces/proxy-5690/services/https:proxy-service-8nbp9:tlsportname2/proxy/: tls qux (200; 18.285254ms)
Dec  9 07:54:22.122: INFO: (9) /api/v1/namespaces/proxy-5690/services/https:proxy-service-8nbp9:tlsportname1/proxy/: tls baz (200; 18.094017ms)
Dec  9 07:54:22.122: INFO: (9) /api/v1/namespaces/proxy-5690/services/proxy-service-8nbp9:portname2/proxy/: bar (200; 18.758712ms)
Dec  9 07:54:22.122: INFO: (9) /api/v1/namespaces/proxy-5690/pods/http:proxy-service-8nbp9-49zcc:160/proxy/: foo (200; 18.582181ms)
Dec  9 07:54:22.136: INFO: (10) /api/v1/namespaces/proxy-5690/pods/http:proxy-service-8nbp9-49zcc:162/proxy/: bar (200; 13.724254ms)
Dec  9 07:54:22.137: INFO: (10) /api/v1/namespaces/proxy-5690/pods/https:proxy-service-8nbp9-49zcc:462/proxy/: tls qux (200; 14.077513ms)
Dec  9 07:54:22.137: INFO: (10) /api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc/proxy/: <a href="/api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc/proxy/rewriteme">test</a> (200; 14.199423ms)
Dec  9 07:54:22.137: INFO: (10) /api/v1/namespaces/proxy-5690/pods/https:proxy-service-8nbp9-49zcc:443/proxy/: <a href="/api/v1/namespaces/proxy-5690/pods/https:proxy-service-8nbp9-49zcc:443/proxy/tlsrewritem... (200; 14.438546ms)
Dec  9 07:54:22.138: INFO: (10) /api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc:162/proxy/: bar (200; 14.809623ms)
Dec  9 07:54:22.138: INFO: (10) /api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc:1080/proxy/: <a href="/api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc:1080/proxy/rewriteme">test<... (200; 14.379683ms)
Dec  9 07:54:22.138: INFO: (10) /api/v1/namespaces/proxy-5690/services/proxy-service-8nbp9:portname1/proxy/: foo (200; 15.293205ms)
Dec  9 07:54:22.138: INFO: (10) /api/v1/namespaces/proxy-5690/services/http:proxy-service-8nbp9:portname2/proxy/: bar (200; 15.724919ms)
Dec  9 07:54:22.138: INFO: (10) /api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc:160/proxy/: foo (200; 14.525728ms)
Dec  9 07:54:22.138: INFO: (10) /api/v1/namespaces/proxy-5690/pods/http:proxy-service-8nbp9-49zcc:1080/proxy/: <a href="/api/v1/namespaces/proxy-5690/pods/http:proxy-service-8nbp9-49zcc:1080/proxy/rewriteme">... (200; 15.490264ms)
Dec  9 07:54:22.138: INFO: (10) /api/v1/namespaces/proxy-5690/services/https:proxy-service-8nbp9:tlsportname1/proxy/: tls baz (200; 15.813741ms)
Dec  9 07:54:22.138: INFO: (10) /api/v1/namespaces/proxy-5690/pods/https:proxy-service-8nbp9-49zcc:460/proxy/: tls baz (200; 15.198928ms)
Dec  9 07:54:22.140: INFO: (10) /api/v1/namespaces/proxy-5690/pods/http:proxy-service-8nbp9-49zcc:160/proxy/: foo (200; 16.433556ms)
Dec  9 07:54:22.140: INFO: (10) /api/v1/namespaces/proxy-5690/services/http:proxy-service-8nbp9:portname1/proxy/: foo (200; 16.734507ms)
Dec  9 07:54:22.140: INFO: (10) /api/v1/namespaces/proxy-5690/services/proxy-service-8nbp9:portname2/proxy/: bar (200; 17.595006ms)
Dec  9 07:54:22.141: INFO: (10) /api/v1/namespaces/proxy-5690/services/https:proxy-service-8nbp9:tlsportname2/proxy/: tls qux (200; 17.087225ms)
Dec  9 07:54:22.153: INFO: (11) /api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc:162/proxy/: bar (200; 11.558149ms)
Dec  9 07:54:22.153: INFO: (11) /api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc/proxy/: <a href="/api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc/proxy/rewriteme">test</a> (200; 11.634012ms)
Dec  9 07:54:22.153: INFO: (11) /api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc:1080/proxy/: <a href="/api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc:1080/proxy/rewriteme">test<... (200; 11.70453ms)
Dec  9 07:54:22.154: INFO: (11) /api/v1/namespaces/proxy-5690/pods/http:proxy-service-8nbp9-49zcc:162/proxy/: bar (200; 12.469013ms)
Dec  9 07:54:22.154: INFO: (11) /api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc:160/proxy/: foo (200; 12.645763ms)
Dec  9 07:54:22.154: INFO: (11) /api/v1/namespaces/proxy-5690/services/http:proxy-service-8nbp9:portname1/proxy/: foo (200; 13.000877ms)
Dec  9 07:54:22.154: INFO: (11) /api/v1/namespaces/proxy-5690/pods/https:proxy-service-8nbp9-49zcc:460/proxy/: tls baz (200; 12.639398ms)
Dec  9 07:54:22.154: INFO: (11) /api/v1/namespaces/proxy-5690/pods/http:proxy-service-8nbp9-49zcc:1080/proxy/: <a href="/api/v1/namespaces/proxy-5690/pods/http:proxy-service-8nbp9-49zcc:1080/proxy/rewriteme">... (200; 12.808442ms)
Dec  9 07:54:22.154: INFO: (11) /api/v1/namespaces/proxy-5690/pods/https:proxy-service-8nbp9-49zcc:462/proxy/: tls qux (200; 13.162377ms)
Dec  9 07:54:22.156: INFO: (11) /api/v1/namespaces/proxy-5690/pods/http:proxy-service-8nbp9-49zcc:160/proxy/: foo (200; 14.647377ms)
Dec  9 07:54:22.156: INFO: (11) /api/v1/namespaces/proxy-5690/pods/https:proxy-service-8nbp9-49zcc:443/proxy/: <a href="/api/v1/namespaces/proxy-5690/pods/https:proxy-service-8nbp9-49zcc:443/proxy/tlsrewritem... (200; 14.905168ms)
Dec  9 07:54:22.157: INFO: (11) /api/v1/namespaces/proxy-5690/services/proxy-service-8nbp9:portname2/proxy/: bar (200; 15.463417ms)
Dec  9 07:54:22.157: INFO: (11) /api/v1/namespaces/proxy-5690/services/http:proxy-service-8nbp9:portname2/proxy/: bar (200; 15.568577ms)
Dec  9 07:54:22.157: INFO: (11) /api/v1/namespaces/proxy-5690/services/proxy-service-8nbp9:portname1/proxy/: foo (200; 15.852818ms)
Dec  9 07:54:22.158: INFO: (11) /api/v1/namespaces/proxy-5690/services/https:proxy-service-8nbp9:tlsportname1/proxy/: tls baz (200; 16.302603ms)
Dec  9 07:54:22.158: INFO: (11) /api/v1/namespaces/proxy-5690/services/https:proxy-service-8nbp9:tlsportname2/proxy/: tls qux (200; 16.492278ms)
Dec  9 07:54:22.170: INFO: (12) /api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc:1080/proxy/: <a href="/api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc:1080/proxy/rewriteme">test<... (200; 12.293203ms)
Dec  9 07:54:22.171: INFO: (12) /api/v1/namespaces/proxy-5690/pods/https:proxy-service-8nbp9-49zcc:462/proxy/: tls qux (200; 12.917546ms)
Dec  9 07:54:22.171: INFO: (12) /api/v1/namespaces/proxy-5690/pods/http:proxy-service-8nbp9-49zcc:1080/proxy/: <a href="/api/v1/namespaces/proxy-5690/pods/http:proxy-service-8nbp9-49zcc:1080/proxy/rewriteme">... (200; 12.769225ms)
Dec  9 07:54:22.171: INFO: (12) /api/v1/namespaces/proxy-5690/pods/http:proxy-service-8nbp9-49zcc:160/proxy/: foo (200; 12.676468ms)
Dec  9 07:54:22.171: INFO: (12) /api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc:162/proxy/: bar (200; 12.803727ms)
Dec  9 07:54:22.173: INFO: (12) /api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc/proxy/: <a href="/api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc/proxy/rewriteme">test</a> (200; 14.678528ms)
Dec  9 07:54:22.173: INFO: (12) /api/v1/namespaces/proxy-5690/services/proxy-service-8nbp9:portname2/proxy/: bar (200; 15.431712ms)
Dec  9 07:54:22.174: INFO: (12) /api/v1/namespaces/proxy-5690/services/https:proxy-service-8nbp9:tlsportname1/proxy/: tls baz (200; 15.115954ms)
Dec  9 07:54:22.174: INFO: (12) /api/v1/namespaces/proxy-5690/services/proxy-service-8nbp9:portname1/proxy/: foo (200; 15.720488ms)
Dec  9 07:54:22.174: INFO: (12) /api/v1/namespaces/proxy-5690/pods/http:proxy-service-8nbp9-49zcc:162/proxy/: bar (200; 15.104511ms)
Dec  9 07:54:22.174: INFO: (12) /api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc:160/proxy/: foo (200; 15.402097ms)
Dec  9 07:54:22.174: INFO: (12) /api/v1/namespaces/proxy-5690/pods/https:proxy-service-8nbp9-49zcc:443/proxy/: <a href="/api/v1/namespaces/proxy-5690/pods/https:proxy-service-8nbp9-49zcc:443/proxy/tlsrewritem... (200; 15.433581ms)
Dec  9 07:54:22.174: INFO: (12) /api/v1/namespaces/proxy-5690/pods/https:proxy-service-8nbp9-49zcc:460/proxy/: tls baz (200; 15.696034ms)
Dec  9 07:54:22.174: INFO: (12) /api/v1/namespaces/proxy-5690/services/http:proxy-service-8nbp9:portname1/proxy/: foo (200; 15.779374ms)
Dec  9 07:54:22.174: INFO: (12) /api/v1/namespaces/proxy-5690/services/https:proxy-service-8nbp9:tlsportname2/proxy/: tls qux (200; 15.817049ms)
Dec  9 07:54:22.174: INFO: (12) /api/v1/namespaces/proxy-5690/services/http:proxy-service-8nbp9:portname2/proxy/: bar (200; 15.693512ms)
Dec  9 07:54:22.187: INFO: (13) /api/v1/namespaces/proxy-5690/pods/https:proxy-service-8nbp9-49zcc:462/proxy/: tls qux (200; 12.880317ms)
Dec  9 07:54:22.187: INFO: (13) /api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc:160/proxy/: foo (200; 13.160787ms)
Dec  9 07:54:22.187: INFO: (13) /api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc/proxy/: <a href="/api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc/proxy/rewriteme">test</a> (200; 12.800964ms)
Dec  9 07:54:22.187: INFO: (13) /api/v1/namespaces/proxy-5690/pods/http:proxy-service-8nbp9-49zcc:162/proxy/: bar (200; 13.142247ms)
Dec  9 07:54:22.189: INFO: (13) /api/v1/namespaces/proxy-5690/pods/http:proxy-service-8nbp9-49zcc:1080/proxy/: <a href="/api/v1/namespaces/proxy-5690/pods/http:proxy-service-8nbp9-49zcc:1080/proxy/rewriteme">... (200; 14.634801ms)
Dec  9 07:54:22.190: INFO: (13) /api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc:1080/proxy/: <a href="/api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc:1080/proxy/rewriteme">test<... (200; 15.070721ms)
Dec  9 07:54:22.190: INFO: (13) /api/v1/namespaces/proxy-5690/pods/http:proxy-service-8nbp9-49zcc:160/proxy/: foo (200; 15.046056ms)
Dec  9 07:54:22.190: INFO: (13) /api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc:162/proxy/: bar (200; 15.419493ms)
Dec  9 07:54:22.190: INFO: (13) /api/v1/namespaces/proxy-5690/services/http:proxy-service-8nbp9:portname2/proxy/: bar (200; 15.560992ms)
Dec  9 07:54:22.190: INFO: (13) /api/v1/namespaces/proxy-5690/services/https:proxy-service-8nbp9:tlsportname1/proxy/: tls baz (200; 15.797821ms)
Dec  9 07:54:22.190: INFO: (13) /api/v1/namespaces/proxy-5690/pods/https:proxy-service-8nbp9-49zcc:460/proxy/: tls baz (200; 15.522254ms)
Dec  9 07:54:22.190: INFO: (13) /api/v1/namespaces/proxy-5690/pods/https:proxy-service-8nbp9-49zcc:443/proxy/: <a href="/api/v1/namespaces/proxy-5690/pods/https:proxy-service-8nbp9-49zcc:443/proxy/tlsrewritem... (200; 15.595095ms)
Dec  9 07:54:22.191: INFO: (13) /api/v1/namespaces/proxy-5690/services/proxy-service-8nbp9:portname2/proxy/: bar (200; 16.004369ms)
Dec  9 07:54:22.191: INFO: (13) /api/v1/namespaces/proxy-5690/services/http:proxy-service-8nbp9:portname1/proxy/: foo (200; 15.805586ms)
Dec  9 07:54:22.191: INFO: (13) /api/v1/namespaces/proxy-5690/services/proxy-service-8nbp9:portname1/proxy/: foo (200; 16.205617ms)
Dec  9 07:54:22.191: INFO: (13) /api/v1/namespaces/proxy-5690/services/https:proxy-service-8nbp9:tlsportname2/proxy/: tls qux (200; 15.846907ms)
Dec  9 07:54:22.203: INFO: (14) /api/v1/namespaces/proxy-5690/pods/http:proxy-service-8nbp9-49zcc:162/proxy/: bar (200; 11.798413ms)
Dec  9 07:54:22.203: INFO: (14) /api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc:1080/proxy/: <a href="/api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc:1080/proxy/rewriteme">test<... (200; 11.892084ms)
Dec  9 07:54:22.203: INFO: (14) /api/v1/namespaces/proxy-5690/pods/http:proxy-service-8nbp9-49zcc:160/proxy/: foo (200; 11.881344ms)
Dec  9 07:54:22.204: INFO: (14) /api/v1/namespaces/proxy-5690/pods/http:proxy-service-8nbp9-49zcc:1080/proxy/: <a href="/api/v1/namespaces/proxy-5690/pods/http:proxy-service-8nbp9-49zcc:1080/proxy/rewriteme">... (200; 12.618677ms)
Dec  9 07:54:22.204: INFO: (14) /api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc:160/proxy/: foo (200; 12.824671ms)
Dec  9 07:54:22.204: INFO: (14) /api/v1/namespaces/proxy-5690/pods/https:proxy-service-8nbp9-49zcc:462/proxy/: tls qux (200; 12.397949ms)
Dec  9 07:54:22.204: INFO: (14) /api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc:162/proxy/: bar (200; 12.745543ms)
Dec  9 07:54:22.204: INFO: (14) /api/v1/namespaces/proxy-5690/pods/https:proxy-service-8nbp9-49zcc:443/proxy/: <a href="/api/v1/namespaces/proxy-5690/pods/https:proxy-service-8nbp9-49zcc:443/proxy/tlsrewritem... (200; 12.920754ms)
Dec  9 07:54:22.204: INFO: (14) /api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc/proxy/: <a href="/api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc/proxy/rewriteme">test</a> (200; 12.630422ms)
Dec  9 07:54:22.206: INFO: (14) /api/v1/namespaces/proxy-5690/pods/https:proxy-service-8nbp9-49zcc:460/proxy/: tls baz (200; 14.982588ms)
Dec  9 07:54:22.207: INFO: (14) /api/v1/namespaces/proxy-5690/services/http:proxy-service-8nbp9:portname1/proxy/: foo (200; 15.740264ms)
Dec  9 07:54:22.207: INFO: (14) /api/v1/namespaces/proxy-5690/services/http:proxy-service-8nbp9:portname2/proxy/: bar (200; 15.36317ms)
Dec  9 07:54:22.207: INFO: (14) /api/v1/namespaces/proxy-5690/services/proxy-service-8nbp9:portname2/proxy/: bar (200; 15.705982ms)
Dec  9 07:54:22.207: INFO: (14) /api/v1/namespaces/proxy-5690/services/proxy-service-8nbp9:portname1/proxy/: foo (200; 15.752912ms)
Dec  9 07:54:22.207: INFO: (14) /api/v1/namespaces/proxy-5690/services/https:proxy-service-8nbp9:tlsportname1/proxy/: tls baz (200; 16.227043ms)
Dec  9 07:54:22.207: INFO: (14) /api/v1/namespaces/proxy-5690/services/https:proxy-service-8nbp9:tlsportname2/proxy/: tls qux (200; 16.095519ms)
Dec  9 07:54:22.225: INFO: (15) /api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc:1080/proxy/: <a href="/api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc:1080/proxy/rewriteme">test<... (200; 16.711078ms)
Dec  9 07:54:22.225: INFO: (15) /api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc:160/proxy/: foo (200; 16.625035ms)
Dec  9 07:54:22.225: INFO: (15) /api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc:162/proxy/: bar (200; 17.339985ms)
Dec  9 07:54:22.225: INFO: (15) /api/v1/namespaces/proxy-5690/pods/http:proxy-service-8nbp9-49zcc:160/proxy/: foo (200; 17.415797ms)
Dec  9 07:54:22.225: INFO: (15) /api/v1/namespaces/proxy-5690/pods/http:proxy-service-8nbp9-49zcc:1080/proxy/: <a href="/api/v1/namespaces/proxy-5690/pods/http:proxy-service-8nbp9-49zcc:1080/proxy/rewriteme">... (200; 17.771654ms)
Dec  9 07:54:22.226: INFO: (15) /api/v1/namespaces/proxy-5690/pods/http:proxy-service-8nbp9-49zcc:162/proxy/: bar (200; 17.463288ms)
Dec  9 07:54:22.226: INFO: (15) /api/v1/namespaces/proxy-5690/pods/https:proxy-service-8nbp9-49zcc:443/proxy/: <a href="/api/v1/namespaces/proxy-5690/pods/https:proxy-service-8nbp9-49zcc:443/proxy/tlsrewritem... (200; 17.736517ms)
Dec  9 07:54:22.226: INFO: (15) /api/v1/namespaces/proxy-5690/pods/https:proxy-service-8nbp9-49zcc:460/proxy/: tls baz (200; 17.945885ms)
Dec  9 07:54:22.228: INFO: (15) /api/v1/namespaces/proxy-5690/services/proxy-service-8nbp9:portname1/proxy/: foo (200; 20.023125ms)
Dec  9 07:54:22.228: INFO: (15) /api/v1/namespaces/proxy-5690/services/proxy-service-8nbp9:portname2/proxy/: bar (200; 20.281108ms)
Dec  9 07:54:22.228: INFO: (15) /api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc/proxy/: <a href="/api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc/proxy/rewriteme">test</a> (200; 19.751745ms)
Dec  9 07:54:22.228: INFO: (15) /api/v1/namespaces/proxy-5690/services/http:proxy-service-8nbp9:portname1/proxy/: foo (200; 19.896287ms)
Dec  9 07:54:22.228: INFO: (15) /api/v1/namespaces/proxy-5690/services/https:proxy-service-8nbp9:tlsportname2/proxy/: tls qux (200; 21.033479ms)
Dec  9 07:54:22.228: INFO: (15) /api/v1/namespaces/proxy-5690/services/https:proxy-service-8nbp9:tlsportname1/proxy/: tls baz (200; 20.466102ms)
Dec  9 07:54:22.229: INFO: (15) /api/v1/namespaces/proxy-5690/pods/https:proxy-service-8nbp9-49zcc:462/proxy/: tls qux (200; 20.226521ms)
Dec  9 07:54:22.229: INFO: (15) /api/v1/namespaces/proxy-5690/services/http:proxy-service-8nbp9:portname2/proxy/: bar (200; 22.165428ms)
Dec  9 07:54:22.242: INFO: (16) /api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc:162/proxy/: bar (200; 12.777695ms)
Dec  9 07:54:22.246: INFO: (16) /api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc/proxy/: <a href="/api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc/proxy/rewriteme">test</a> (200; 15.389952ms)
Dec  9 07:54:22.246: INFO: (16) /api/v1/namespaces/proxy-5690/pods/http:proxy-service-8nbp9-49zcc:162/proxy/: bar (200; 15.040297ms)
Dec  9 07:54:22.247: INFO: (16) /api/v1/namespaces/proxy-5690/pods/http:proxy-service-8nbp9-49zcc:160/proxy/: foo (200; 16.241873ms)
Dec  9 07:54:22.247: INFO: (16) /api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc:160/proxy/: foo (200; 16.250623ms)
Dec  9 07:54:22.247: INFO: (16) /api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc:1080/proxy/: <a href="/api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc:1080/proxy/rewriteme">test<... (200; 16.545819ms)
Dec  9 07:54:22.247: INFO: (16) /api/v1/namespaces/proxy-5690/services/https:proxy-service-8nbp9:tlsportname1/proxy/: tls baz (200; 16.832068ms)
Dec  9 07:54:22.247: INFO: (16) /api/v1/namespaces/proxy-5690/pods/https:proxy-service-8nbp9-49zcc:460/proxy/: tls baz (200; 16.608971ms)
Dec  9 07:54:22.247: INFO: (16) /api/v1/namespaces/proxy-5690/pods/https:proxy-service-8nbp9-49zcc:462/proxy/: tls qux (200; 15.538211ms)
Dec  9 07:54:22.247: INFO: (16) /api/v1/namespaces/proxy-5690/pods/http:proxy-service-8nbp9-49zcc:1080/proxy/: <a href="/api/v1/namespaces/proxy-5690/pods/http:proxy-service-8nbp9-49zcc:1080/proxy/rewriteme">... (200; 16.250501ms)
Dec  9 07:54:22.247: INFO: (16) /api/v1/namespaces/proxy-5690/pods/https:proxy-service-8nbp9-49zcc:443/proxy/: <a href="/api/v1/namespaces/proxy-5690/pods/https:proxy-service-8nbp9-49zcc:443/proxy/tlsrewritem... (200; 15.816269ms)
Dec  9 07:54:22.247: INFO: (16) /api/v1/namespaces/proxy-5690/services/http:proxy-service-8nbp9:portname1/proxy/: foo (200; 15.807892ms)
Dec  9 07:54:22.247: INFO: (16) /api/v1/namespaces/proxy-5690/services/proxy-service-8nbp9:portname2/proxy/: bar (200; 16.841434ms)
Dec  9 07:54:22.247: INFO: (16) /api/v1/namespaces/proxy-5690/services/http:proxy-service-8nbp9:portname2/proxy/: bar (200; 15.500454ms)
Dec  9 07:54:22.247: INFO: (16) /api/v1/namespaces/proxy-5690/services/proxy-service-8nbp9:portname1/proxy/: foo (200; 17.207417ms)
Dec  9 07:54:22.247: INFO: (16) /api/v1/namespaces/proxy-5690/services/https:proxy-service-8nbp9:tlsportname2/proxy/: tls qux (200; 16.071829ms)
Dec  9 07:54:22.265: INFO: (17) /api/v1/namespaces/proxy-5690/pods/https:proxy-service-8nbp9-49zcc:462/proxy/: tls qux (200; 17.714886ms)
Dec  9 07:54:22.268: INFO: (17) /api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc/proxy/: <a href="/api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc/proxy/rewriteme">test</a> (200; 19.618867ms)
Dec  9 07:54:22.268: INFO: (17) /api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc:1080/proxy/: <a href="/api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc:1080/proxy/rewriteme">test<... (200; 20.547353ms)
Dec  9 07:54:22.268: INFO: (17) /api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc:162/proxy/: bar (200; 20.643691ms)
Dec  9 07:54:22.268: INFO: (17) /api/v1/namespaces/proxy-5690/services/proxy-service-8nbp9:portname2/proxy/: bar (200; 21.040373ms)
Dec  9 07:54:22.269: INFO: (17) /api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc:160/proxy/: foo (200; 20.338067ms)
Dec  9 07:54:22.269: INFO: (17) /api/v1/namespaces/proxy-5690/services/proxy-service-8nbp9:portname1/proxy/: foo (200; 21.330049ms)
Dec  9 07:54:22.269: INFO: (17) /api/v1/namespaces/proxy-5690/pods/http:proxy-service-8nbp9-49zcc:1080/proxy/: <a href="/api/v1/namespaces/proxy-5690/pods/http:proxy-service-8nbp9-49zcc:1080/proxy/rewriteme">... (200; 21.424522ms)
Dec  9 07:54:22.269: INFO: (17) /api/v1/namespaces/proxy-5690/pods/https:proxy-service-8nbp9-49zcc:460/proxy/: tls baz (200; 21.081339ms)
Dec  9 07:54:22.269: INFO: (17) /api/v1/namespaces/proxy-5690/pods/https:proxy-service-8nbp9-49zcc:443/proxy/: <a href="/api/v1/namespaces/proxy-5690/pods/https:proxy-service-8nbp9-49zcc:443/proxy/tlsrewritem... (200; 20.755803ms)
Dec  9 07:54:22.269: INFO: (17) /api/v1/namespaces/proxy-5690/services/http:proxy-service-8nbp9:portname1/proxy/: foo (200; 21.409536ms)
Dec  9 07:54:22.270: INFO: (17) /api/v1/namespaces/proxy-5690/services/http:proxy-service-8nbp9:portname2/proxy/: bar (200; 21.372598ms)
Dec  9 07:54:22.270: INFO: (17) /api/v1/namespaces/proxy-5690/services/https:proxy-service-8nbp9:tlsportname2/proxy/: tls qux (200; 21.874377ms)
Dec  9 07:54:22.270: INFO: (17) /api/v1/namespaces/proxy-5690/pods/http:proxy-service-8nbp9-49zcc:162/proxy/: bar (200; 21.416558ms)
Dec  9 07:54:22.270: INFO: (17) /api/v1/namespaces/proxy-5690/pods/http:proxy-service-8nbp9-49zcc:160/proxy/: foo (200; 22.03081ms)
Dec  9 07:54:22.270: INFO: (17) /api/v1/namespaces/proxy-5690/services/https:proxy-service-8nbp9:tlsportname1/proxy/: tls baz (200; 21.577757ms)
Dec  9 07:54:22.284: INFO: (18) /api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc/proxy/: <a href="/api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc/proxy/rewriteme">test</a> (200; 13.580335ms)
Dec  9 07:54:22.284: INFO: (18) /api/v1/namespaces/proxy-5690/pods/http:proxy-service-8nbp9-49zcc:162/proxy/: bar (200; 13.863556ms)
Dec  9 07:54:22.284: INFO: (18) /api/v1/namespaces/proxy-5690/pods/https:proxy-service-8nbp9-49zcc:462/proxy/: tls qux (200; 13.824786ms)
Dec  9 07:54:22.286: INFO: (18) /api/v1/namespaces/proxy-5690/pods/http:proxy-service-8nbp9-49zcc:160/proxy/: foo (200; 15.118325ms)
Dec  9 07:54:22.287: INFO: (18) /api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc:1080/proxy/: <a href="/api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc:1080/proxy/rewriteme">test<... (200; 15.39043ms)
Dec  9 07:54:22.287: INFO: (18) /api/v1/namespaces/proxy-5690/services/http:proxy-service-8nbp9:portname2/proxy/: bar (200; 16.223477ms)
Dec  9 07:54:22.287: INFO: (18) /api/v1/namespaces/proxy-5690/pods/http:proxy-service-8nbp9-49zcc:1080/proxy/: <a href="/api/v1/namespaces/proxy-5690/pods/http:proxy-service-8nbp9-49zcc:1080/proxy/rewriteme">... (200; 16.466643ms)
Dec  9 07:54:22.287: INFO: (18) /api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc:162/proxy/: bar (200; 16.469118ms)
Dec  9 07:54:22.288: INFO: (18) /api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc:160/proxy/: foo (200; 16.27738ms)
Dec  9 07:54:22.288: INFO: (18) /api/v1/namespaces/proxy-5690/pods/https:proxy-service-8nbp9-49zcc:460/proxy/: tls baz (200; 16.841024ms)
Dec  9 07:54:22.288: INFO: (18) /api/v1/namespaces/proxy-5690/pods/https:proxy-service-8nbp9-49zcc:443/proxy/: <a href="/api/v1/namespaces/proxy-5690/pods/https:proxy-service-8nbp9-49zcc:443/proxy/tlsrewritem... (200; 16.600246ms)
Dec  9 07:54:22.288: INFO: (18) /api/v1/namespaces/proxy-5690/services/https:proxy-service-8nbp9:tlsportname2/proxy/: tls qux (200; 17.917935ms)
Dec  9 07:54:22.288: INFO: (18) /api/v1/namespaces/proxy-5690/services/https:proxy-service-8nbp9:tlsportname1/proxy/: tls baz (200; 17.717584ms)
Dec  9 07:54:22.288: INFO: (18) /api/v1/namespaces/proxy-5690/services/proxy-service-8nbp9:portname1/proxy/: foo (200; 17.475429ms)
Dec  9 07:54:22.288: INFO: (18) /api/v1/namespaces/proxy-5690/services/proxy-service-8nbp9:portname2/proxy/: bar (200; 17.419703ms)
Dec  9 07:54:22.288: INFO: (18) /api/v1/namespaces/proxy-5690/services/http:proxy-service-8nbp9:portname1/proxy/: foo (200; 16.959773ms)
Dec  9 07:54:22.303: INFO: (19) /api/v1/namespaces/proxy-5690/pods/https:proxy-service-8nbp9-49zcc:462/proxy/: tls qux (200; 13.89032ms)
Dec  9 07:54:22.303: INFO: (19) /api/v1/namespaces/proxy-5690/pods/http:proxy-service-8nbp9-49zcc:1080/proxy/: <a href="/api/v1/namespaces/proxy-5690/pods/http:proxy-service-8nbp9-49zcc:1080/proxy/rewriteme">... (200; 14.347044ms)
Dec  9 07:54:22.303: INFO: (19) /api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc:162/proxy/: bar (200; 14.320302ms)
Dec  9 07:54:22.303: INFO: (19) /api/v1/namespaces/proxy-5690/pods/http:proxy-service-8nbp9-49zcc:162/proxy/: bar (200; 14.593026ms)
Dec  9 07:54:22.303: INFO: (19) /api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc/proxy/: <a href="/api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc/proxy/rewriteme">test</a> (200; 13.85106ms)
Dec  9 07:54:22.303: INFO: (19) /api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc:1080/proxy/: <a href="/api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc:1080/proxy/rewriteme">test<... (200; 14.301751ms)
Dec  9 07:54:22.303: INFO: (19) /api/v1/namespaces/proxy-5690/pods/https:proxy-service-8nbp9-49zcc:443/proxy/: <a href="/api/v1/namespaces/proxy-5690/pods/https:proxy-service-8nbp9-49zcc:443/proxy/tlsrewritem... (200; 14.063028ms)
Dec  9 07:54:22.304: INFO: (19) /api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc:160/proxy/: foo (200; 15.441804ms)
Dec  9 07:54:22.305: INFO: (19) /api/v1/namespaces/proxy-5690/pods/http:proxy-service-8nbp9-49zcc:160/proxy/: foo (200; 15.827693ms)
Dec  9 07:54:22.305: INFO: (19) /api/v1/namespaces/proxy-5690/pods/https:proxy-service-8nbp9-49zcc:460/proxy/: tls baz (200; 15.782124ms)
Dec  9 07:54:22.306: INFO: (19) /api/v1/namespaces/proxy-5690/services/proxy-service-8nbp9:portname2/proxy/: bar (200; 17.451411ms)
Dec  9 07:54:22.306: INFO: (19) /api/v1/namespaces/proxy-5690/services/proxy-service-8nbp9:portname1/proxy/: foo (200; 17.657794ms)
Dec  9 07:54:22.306: INFO: (19) /api/v1/namespaces/proxy-5690/services/http:proxy-service-8nbp9:portname2/proxy/: bar (200; 17.274795ms)
Dec  9 07:54:22.306: INFO: (19) /api/v1/namespaces/proxy-5690/services/http:proxy-service-8nbp9:portname1/proxy/: foo (200; 17.506534ms)
Dec  9 07:54:22.306: INFO: (19) /api/v1/namespaces/proxy-5690/services/https:proxy-service-8nbp9:tlsportname1/proxy/: tls baz (200; 17.267147ms)
Dec  9 07:54:22.306: INFO: (19) /api/v1/namespaces/proxy-5690/services/https:proxy-service-8nbp9:tlsportname2/proxy/: tls qux (200; 17.508095ms)
STEP: deleting ReplicationController proxy-service-8nbp9 in namespace proxy-5690, will wait for the garbage collector to delete the pods 12/09/22 07:54:22.307
Dec  9 07:54:22.365: INFO: Deleting ReplicationController proxy-service-8nbp9 took: 4.878293ms
Dec  9 07:54:22.466: INFO: Terminating ReplicationController proxy-service-8nbp9 pods took: 100.997223ms
[AfterEach] version v1
  test/e2e/framework/node/init/init.go:32
Dec  9 07:54:24.771: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] version v1
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] version v1
  dump namespaces | framework.go:196
[DeferCleanup (Each)] version v1
  tear down framework | framework.go:193
STEP: Destroying namespace "proxy-5690" for this suite. 12/09/22 07:54:24.785
------------------------------
â€¢ [SLOW TEST] [5.016 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    should proxy through a service and a pod  [Conformance]
    test/e2e/network/proxy.go:101

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:54:19.786
    Dec  9 07:54:19.786: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename proxy 12/09/22 07:54:19.787
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:54:19.802
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:54:19.806
    [BeforeEach] version v1
      test/e2e/framework/metrics/init/init.go:31
    [It] should proxy through a service and a pod  [Conformance]
      test/e2e/network/proxy.go:101
    STEP: starting an echo server on multiple ports 12/09/22 07:54:19.821
    STEP: creating replication controller proxy-service-8nbp9 in namespace proxy-5690 12/09/22 07:54:19.821
    I1209 07:54:19.832450      20 runners.go:193] Created replication controller with name: proxy-service-8nbp9, namespace: proxy-5690, replica count: 1
    I1209 07:54:20.884386      20 runners.go:193] proxy-service-8nbp9 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I1209 07:54:21.886121      20 runners.go:193] proxy-service-8nbp9 Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Dec  9 07:54:21.890: INFO: setup took 2.080601931s, starting test cases
    STEP: running 16 cases, 20 attempts per case, 320 total attempts 12/09/22 07:54:21.891
    Dec  9 07:54:21.923: INFO: (0) /api/v1/namespaces/proxy-5690/services/http:proxy-service-8nbp9:portname2/proxy/: bar (200; 30.621394ms)
    Dec  9 07:54:21.924: INFO: (0) /api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc:160/proxy/: foo (200; 31.889888ms)
    Dec  9 07:54:21.924: INFO: (0) /api/v1/namespaces/proxy-5690/pods/http:proxy-service-8nbp9-49zcc:1080/proxy/: <a href="/api/v1/namespaces/proxy-5690/pods/http:proxy-service-8nbp9-49zcc:1080/proxy/rewriteme">... (200; 31.282597ms)
    Dec  9 07:54:21.924: INFO: (0) /api/v1/namespaces/proxy-5690/services/proxy-service-8nbp9:portname2/proxy/: bar (200; 31.448464ms)
    Dec  9 07:54:21.924: INFO: (0) /api/v1/namespaces/proxy-5690/services/http:proxy-service-8nbp9:portname1/proxy/: foo (200; 32.488863ms)
    Dec  9 07:54:21.924: INFO: (0) /api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc/proxy/: <a href="/api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc/proxy/rewriteme">test</a> (200; 31.907985ms)
    Dec  9 07:54:21.924: INFO: (0) /api/v1/namespaces/proxy-5690/services/proxy-service-8nbp9:portname1/proxy/: foo (200; 31.757593ms)
    Dec  9 07:54:21.924: INFO: (0) /api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc:162/proxy/: bar (200; 33.418294ms)
    Dec  9 07:54:21.925: INFO: (0) /api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc:1080/proxy/: <a href="/api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc:1080/proxy/rewriteme">test<... (200; 33.209799ms)
    Dec  9 07:54:21.925: INFO: (0) /api/v1/namespaces/proxy-5690/pods/http:proxy-service-8nbp9-49zcc:160/proxy/: foo (200; 33.109807ms)
    Dec  9 07:54:21.925: INFO: (0) /api/v1/namespaces/proxy-5690/pods/http:proxy-service-8nbp9-49zcc:162/proxy/: bar (200; 32.375837ms)
    Dec  9 07:54:21.930: INFO: (0) /api/v1/namespaces/proxy-5690/pods/https:proxy-service-8nbp9-49zcc:462/proxy/: tls qux (200; 38.71434ms)
    Dec  9 07:54:21.930: INFO: (0) /api/v1/namespaces/proxy-5690/services/https:proxy-service-8nbp9:tlsportname2/proxy/: tls qux (200; 38.146369ms)
    Dec  9 07:54:21.930: INFO: (0) /api/v1/namespaces/proxy-5690/pods/https:proxy-service-8nbp9-49zcc:443/proxy/: <a href="/api/v1/namespaces/proxy-5690/pods/https:proxy-service-8nbp9-49zcc:443/proxy/tlsrewritem... (200; 37.823372ms)
    Dec  9 07:54:21.930: INFO: (0) /api/v1/namespaces/proxy-5690/pods/https:proxy-service-8nbp9-49zcc:460/proxy/: tls baz (200; 38.345244ms)
    Dec  9 07:54:21.930: INFO: (0) /api/v1/namespaces/proxy-5690/services/https:proxy-service-8nbp9:tlsportname1/proxy/: tls baz (200; 37.471009ms)
    Dec  9 07:54:21.945: INFO: (1) /api/v1/namespaces/proxy-5690/pods/https:proxy-service-8nbp9-49zcc:443/proxy/: <a href="/api/v1/namespaces/proxy-5690/pods/https:proxy-service-8nbp9-49zcc:443/proxy/tlsrewritem... (200; 14.569257ms)
    Dec  9 07:54:21.945: INFO: (1) /api/v1/namespaces/proxy-5690/pods/http:proxy-service-8nbp9-49zcc:160/proxy/: foo (200; 14.910756ms)
    Dec  9 07:54:21.945: INFO: (1) /api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc:160/proxy/: foo (200; 14.791105ms)
    Dec  9 07:54:21.946: INFO: (1) /api/v1/namespaces/proxy-5690/pods/http:proxy-service-8nbp9-49zcc:162/proxy/: bar (200; 15.248608ms)
    Dec  9 07:54:21.946: INFO: (1) /api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc:162/proxy/: bar (200; 14.896982ms)
    Dec  9 07:54:21.946: INFO: (1) /api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc:1080/proxy/: <a href="/api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc:1080/proxy/rewriteme">test<... (200; 14.66783ms)
    Dec  9 07:54:21.946: INFO: (1) /api/v1/namespaces/proxy-5690/services/http:proxy-service-8nbp9:portname1/proxy/: foo (200; 15.642846ms)
    Dec  9 07:54:21.946: INFO: (1) /api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc/proxy/: <a href="/api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc/proxy/rewriteme">test</a> (200; 15.388269ms)
    Dec  9 07:54:21.946: INFO: (1) /api/v1/namespaces/proxy-5690/pods/http:proxy-service-8nbp9-49zcc:1080/proxy/: <a href="/api/v1/namespaces/proxy-5690/pods/http:proxy-service-8nbp9-49zcc:1080/proxy/rewriteme">... (200; 15.001247ms)
    Dec  9 07:54:21.946: INFO: (1) /api/v1/namespaces/proxy-5690/pods/https:proxy-service-8nbp9-49zcc:460/proxy/: tls baz (200; 14.795129ms)
    Dec  9 07:54:21.946: INFO: (1) /api/v1/namespaces/proxy-5690/services/https:proxy-service-8nbp9:tlsportname2/proxy/: tls qux (200; 15.583345ms)
    Dec  9 07:54:21.946: INFO: (1) /api/v1/namespaces/proxy-5690/pods/https:proxy-service-8nbp9-49zcc:462/proxy/: tls qux (200; 15.311023ms)
    Dec  9 07:54:21.951: INFO: (1) /api/v1/namespaces/proxy-5690/services/proxy-service-8nbp9:portname1/proxy/: foo (200; 19.827953ms)
    Dec  9 07:54:21.951: INFO: (1) /api/v1/namespaces/proxy-5690/services/proxy-service-8nbp9:portname2/proxy/: bar (200; 19.981318ms)
    Dec  9 07:54:21.951: INFO: (1) /api/v1/namespaces/proxy-5690/services/https:proxy-service-8nbp9:tlsportname1/proxy/: tls baz (200; 20.342273ms)
    Dec  9 07:54:21.951: INFO: (1) /api/v1/namespaces/proxy-5690/services/http:proxy-service-8nbp9:portname2/proxy/: bar (200; 20.541141ms)
    Dec  9 07:54:21.966: INFO: (2) /api/v1/namespaces/proxy-5690/pods/http:proxy-service-8nbp9-49zcc:160/proxy/: foo (200; 14.40727ms)
    Dec  9 07:54:21.966: INFO: (2) /api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc:1080/proxy/: <a href="/api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc:1080/proxy/rewriteme">test<... (200; 14.411651ms)
    Dec  9 07:54:21.966: INFO: (2) /api/v1/namespaces/proxy-5690/pods/https:proxy-service-8nbp9-49zcc:460/proxy/: tls baz (200; 14.697613ms)
    Dec  9 07:54:21.973: INFO: (2) /api/v1/namespaces/proxy-5690/pods/http:proxy-service-8nbp9-49zcc:162/proxy/: bar (200; 20.218059ms)
    Dec  9 07:54:21.973: INFO: (2) /api/v1/namespaces/proxy-5690/pods/http:proxy-service-8nbp9-49zcc:1080/proxy/: <a href="/api/v1/namespaces/proxy-5690/pods/http:proxy-service-8nbp9-49zcc:1080/proxy/rewriteme">... (200; 20.134538ms)
    Dec  9 07:54:21.977: INFO: (2) /api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc:160/proxy/: foo (200; 24.727348ms)
    Dec  9 07:54:21.978: INFO: (2) /api/v1/namespaces/proxy-5690/services/proxy-service-8nbp9:portname1/proxy/: foo (200; 26.640943ms)
    Dec  9 07:54:21.978: INFO: (2) /api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc:162/proxy/: bar (200; 25.334381ms)
    Dec  9 07:54:21.979: INFO: (2) /api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc/proxy/: <a href="/api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc/proxy/rewriteme">test</a> (200; 25.855286ms)
    Dec  9 07:54:21.979: INFO: (2) /api/v1/namespaces/proxy-5690/pods/https:proxy-service-8nbp9-49zcc:462/proxy/: tls qux (200; 25.83878ms)
    Dec  9 07:54:21.979: INFO: (2) /api/v1/namespaces/proxy-5690/pods/https:proxy-service-8nbp9-49zcc:443/proxy/: <a href="/api/v1/namespaces/proxy-5690/pods/https:proxy-service-8nbp9-49zcc:443/proxy/tlsrewritem... (200; 26.531176ms)
    Dec  9 07:54:21.979: INFO: (2) /api/v1/namespaces/proxy-5690/services/proxy-service-8nbp9:portname2/proxy/: bar (200; 26.021178ms)
    Dec  9 07:54:21.979: INFO: (2) /api/v1/namespaces/proxy-5690/services/http:proxy-service-8nbp9:portname2/proxy/: bar (200; 26.226444ms)
    Dec  9 07:54:21.979: INFO: (2) /api/v1/namespaces/proxy-5690/services/http:proxy-service-8nbp9:portname1/proxy/: foo (200; 26.945059ms)
    Dec  9 07:54:21.979: INFO: (2) /api/v1/namespaces/proxy-5690/services/https:proxy-service-8nbp9:tlsportname1/proxy/: tls baz (200; 26.97436ms)
    Dec  9 07:54:21.980: INFO: (2) /api/v1/namespaces/proxy-5690/services/https:proxy-service-8nbp9:tlsportname2/proxy/: tls qux (200; 27.783989ms)
    Dec  9 07:54:22.005: INFO: (3) /api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc/proxy/: <a href="/api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc/proxy/rewriteme">test</a> (200; 24.356695ms)
    Dec  9 07:54:22.005: INFO: (3) /api/v1/namespaces/proxy-5690/pods/http:proxy-service-8nbp9-49zcc:162/proxy/: bar (200; 24.288837ms)
    Dec  9 07:54:22.005: INFO: (3) /api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc:1080/proxy/: <a href="/api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc:1080/proxy/rewriteme">test<... (200; 25.189437ms)
    Dec  9 07:54:22.006: INFO: (3) /api/v1/namespaces/proxy-5690/pods/https:proxy-service-8nbp9-49zcc:460/proxy/: tls baz (200; 26.056011ms)
    Dec  9 07:54:22.006: INFO: (3) /api/v1/namespaces/proxy-5690/pods/https:proxy-service-8nbp9-49zcc:462/proxy/: tls qux (200; 25.373003ms)
    Dec  9 07:54:22.008: INFO: (3) /api/v1/namespaces/proxy-5690/pods/http:proxy-service-8nbp9-49zcc:1080/proxy/: <a href="/api/v1/namespaces/proxy-5690/pods/http:proxy-service-8nbp9-49zcc:1080/proxy/rewriteme">... (200; 26.283213ms)
    Dec  9 07:54:22.008: INFO: (3) /api/v1/namespaces/proxy-5690/services/http:proxy-service-8nbp9:portname2/proxy/: bar (200; 26.857758ms)
    Dec  9 07:54:22.008: INFO: (3) /api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc:160/proxy/: foo (200; 27.193554ms)
    Dec  9 07:54:22.008: INFO: (3) /api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc:162/proxy/: bar (200; 27.926198ms)
    Dec  9 07:54:22.008: INFO: (3) /api/v1/namespaces/proxy-5690/services/https:proxy-service-8nbp9:tlsportname2/proxy/: tls qux (200; 27.491704ms)
    Dec  9 07:54:22.008: INFO: (3) /api/v1/namespaces/proxy-5690/services/proxy-service-8nbp9:portname2/proxy/: bar (200; 26.915824ms)
    Dec  9 07:54:22.008: INFO: (3) /api/v1/namespaces/proxy-5690/services/https:proxy-service-8nbp9:tlsportname1/proxy/: tls baz (200; 27.348639ms)
    Dec  9 07:54:22.008: INFO: (3) /api/v1/namespaces/proxy-5690/pods/http:proxy-service-8nbp9-49zcc:160/proxy/: foo (200; 27.950394ms)
    Dec  9 07:54:22.008: INFO: (3) /api/v1/namespaces/proxy-5690/pods/https:proxy-service-8nbp9-49zcc:443/proxy/: <a href="/api/v1/namespaces/proxy-5690/pods/https:proxy-service-8nbp9-49zcc:443/proxy/tlsrewritem... (200; 27.54644ms)
    Dec  9 07:54:22.008: INFO: (3) /api/v1/namespaces/proxy-5690/services/proxy-service-8nbp9:portname1/proxy/: foo (200; 26.977331ms)
    Dec  9 07:54:22.008: INFO: (3) /api/v1/namespaces/proxy-5690/services/http:proxy-service-8nbp9:portname1/proxy/: foo (200; 27.684815ms)
    Dec  9 07:54:22.025: INFO: (4) /api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc/proxy/: <a href="/api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc/proxy/rewriteme">test</a> (200; 16.388466ms)
    Dec  9 07:54:22.025: INFO: (4) /api/v1/namespaces/proxy-5690/services/http:proxy-service-8nbp9:portname1/proxy/: foo (200; 16.813527ms)
    Dec  9 07:54:22.025: INFO: (4) /api/v1/namespaces/proxy-5690/services/proxy-service-8nbp9:portname2/proxy/: bar (200; 16.725109ms)
    Dec  9 07:54:22.025: INFO: (4) /api/v1/namespaces/proxy-5690/services/https:proxy-service-8nbp9:tlsportname1/proxy/: tls baz (200; 16.546209ms)
    Dec  9 07:54:22.025: INFO: (4) /api/v1/namespaces/proxy-5690/services/http:proxy-service-8nbp9:portname2/proxy/: bar (200; 16.582575ms)
    Dec  9 07:54:22.025: INFO: (4) /api/v1/namespaces/proxy-5690/services/proxy-service-8nbp9:portname1/proxy/: foo (200; 16.804388ms)
    Dec  9 07:54:22.025: INFO: (4) /api/v1/namespaces/proxy-5690/services/https:proxy-service-8nbp9:tlsportname2/proxy/: tls qux (200; 16.702963ms)
    Dec  9 07:54:22.025: INFO: (4) /api/v1/namespaces/proxy-5690/pods/https:proxy-service-8nbp9-49zcc:462/proxy/: tls qux (200; 16.995414ms)
    Dec  9 07:54:22.025: INFO: (4) /api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc:162/proxy/: bar (200; 16.568623ms)
    Dec  9 07:54:22.025: INFO: (4) /api/v1/namespaces/proxy-5690/pods/http:proxy-service-8nbp9-49zcc:162/proxy/: bar (200; 16.560869ms)
    Dec  9 07:54:22.025: INFO: (4) /api/v1/namespaces/proxy-5690/pods/http:proxy-service-8nbp9-49zcc:160/proxy/: foo (200; 16.540027ms)
    Dec  9 07:54:22.025: INFO: (4) /api/v1/namespaces/proxy-5690/pods/https:proxy-service-8nbp9-49zcc:460/proxy/: tls baz (200; 16.521626ms)
    Dec  9 07:54:22.025: INFO: (4) /api/v1/namespaces/proxy-5690/pods/http:proxy-service-8nbp9-49zcc:1080/proxy/: <a href="/api/v1/namespaces/proxy-5690/pods/http:proxy-service-8nbp9-49zcc:1080/proxy/rewriteme">... (200; 16.852668ms)
    Dec  9 07:54:22.025: INFO: (4) /api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc:1080/proxy/: <a href="/api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc:1080/proxy/rewriteme">test<... (200; 16.512423ms)
    Dec  9 07:54:22.025: INFO: (4) /api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc:160/proxy/: foo (200; 16.822224ms)
    Dec  9 07:54:22.026: INFO: (4) /api/v1/namespaces/proxy-5690/pods/https:proxy-service-8nbp9-49zcc:443/proxy/: <a href="/api/v1/namespaces/proxy-5690/pods/https:proxy-service-8nbp9-49zcc:443/proxy/tlsrewritem... (200; 17.708528ms)
    Dec  9 07:54:22.046: INFO: (5) /api/v1/namespaces/proxy-5690/pods/http:proxy-service-8nbp9-49zcc:1080/proxy/: <a href="/api/v1/namespaces/proxy-5690/pods/http:proxy-service-8nbp9-49zcc:1080/proxy/rewriteme">... (200; 17.98764ms)
    Dec  9 07:54:22.046: INFO: (5) /api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc:1080/proxy/: <a href="/api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc:1080/proxy/rewriteme">test<... (200; 17.873872ms)
    Dec  9 07:54:22.047: INFO: (5) /api/v1/namespaces/proxy-5690/services/https:proxy-service-8nbp9:tlsportname1/proxy/: tls baz (200; 18.584304ms)
    Dec  9 07:54:22.047: INFO: (5) /api/v1/namespaces/proxy-5690/pods/http:proxy-service-8nbp9-49zcc:160/proxy/: foo (200; 18.518399ms)
    Dec  9 07:54:22.047: INFO: (5) /api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc/proxy/: <a href="/api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc/proxy/rewriteme">test</a> (200; 19.118364ms)
    Dec  9 07:54:22.047: INFO: (5) /api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc:160/proxy/: foo (200; 19.060134ms)
    Dec  9 07:54:22.047: INFO: (5) /api/v1/namespaces/proxy-5690/services/http:proxy-service-8nbp9:portname2/proxy/: bar (200; 19.047372ms)
    Dec  9 07:54:22.047: INFO: (5) /api/v1/namespaces/proxy-5690/pods/https:proxy-service-8nbp9-49zcc:462/proxy/: tls qux (200; 19.27751ms)
    Dec  9 07:54:22.047: INFO: (5) /api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc:162/proxy/: bar (200; 19.065175ms)
    Dec  9 07:54:22.048: INFO: (5) /api/v1/namespaces/proxy-5690/services/https:proxy-service-8nbp9:tlsportname2/proxy/: tls qux (200; 19.573746ms)
    Dec  9 07:54:22.048: INFO: (5) /api/v1/namespaces/proxy-5690/pods/https:proxy-service-8nbp9-49zcc:460/proxy/: tls baz (200; 19.971744ms)
    Dec  9 07:54:22.048: INFO: (5) /api/v1/namespaces/proxy-5690/pods/https:proxy-service-8nbp9-49zcc:443/proxy/: <a href="/api/v1/namespaces/proxy-5690/pods/https:proxy-service-8nbp9-49zcc:443/proxy/tlsrewritem... (200; 19.522034ms)
    Dec  9 07:54:22.049: INFO: (5) /api/v1/namespaces/proxy-5690/pods/http:proxy-service-8nbp9-49zcc:162/proxy/: bar (200; 20.499475ms)
    Dec  9 07:54:22.049: INFO: (5) /api/v1/namespaces/proxy-5690/services/proxy-service-8nbp9:portname2/proxy/: bar (200; 20.522771ms)
    Dec  9 07:54:22.049: INFO: (5) /api/v1/namespaces/proxy-5690/services/proxy-service-8nbp9:portname1/proxy/: foo (200; 20.67426ms)
    Dec  9 07:54:22.049: INFO: (5) /api/v1/namespaces/proxy-5690/services/http:proxy-service-8nbp9:portname1/proxy/: foo (200; 21.076322ms)
    Dec  9 07:54:22.065: INFO: (6) /api/v1/namespaces/proxy-5690/pods/https:proxy-service-8nbp9-49zcc:462/proxy/: tls qux (200; 15.856886ms)
    Dec  9 07:54:22.066: INFO: (6) /api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc:160/proxy/: foo (200; 16.548755ms)
    Dec  9 07:54:22.066: INFO: (6) /api/v1/namespaces/proxy-5690/pods/https:proxy-service-8nbp9-49zcc:443/proxy/: <a href="/api/v1/namespaces/proxy-5690/pods/https:proxy-service-8nbp9-49zcc:443/proxy/tlsrewritem... (200; 16.510033ms)
    Dec  9 07:54:22.066: INFO: (6) /api/v1/namespaces/proxy-5690/pods/https:proxy-service-8nbp9-49zcc:460/proxy/: tls baz (200; 16.824793ms)
    Dec  9 07:54:22.067: INFO: (6) /api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc:1080/proxy/: <a href="/api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc:1080/proxy/rewriteme">test<... (200; 17.163448ms)
    Dec  9 07:54:22.067: INFO: (6) /api/v1/namespaces/proxy-5690/pods/http:proxy-service-8nbp9-49zcc:1080/proxy/: <a href="/api/v1/namespaces/proxy-5690/pods/http:proxy-service-8nbp9-49zcc:1080/proxy/rewriteme">... (200; 17.344614ms)
    Dec  9 07:54:22.067: INFO: (6) /api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc/proxy/: <a href="/api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc/proxy/rewriteme">test</a> (200; 17.566819ms)
    Dec  9 07:54:22.068: INFO: (6) /api/v1/namespaces/proxy-5690/pods/http:proxy-service-8nbp9-49zcc:160/proxy/: foo (200; 17.8898ms)
    Dec  9 07:54:22.068: INFO: (6) /api/v1/namespaces/proxy-5690/services/http:proxy-service-8nbp9:portname1/proxy/: foo (200; 18.82543ms)
    Dec  9 07:54:22.068: INFO: (6) /api/v1/namespaces/proxy-5690/services/http:proxy-service-8nbp9:portname2/proxy/: bar (200; 18.585562ms)
    Dec  9 07:54:22.068: INFO: (6) /api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc:162/proxy/: bar (200; 18.289274ms)
    Dec  9 07:54:22.068: INFO: (6) /api/v1/namespaces/proxy-5690/services/https:proxy-service-8nbp9:tlsportname1/proxy/: tls baz (200; 18.834438ms)
    Dec  9 07:54:22.068: INFO: (6) /api/v1/namespaces/proxy-5690/services/https:proxy-service-8nbp9:tlsportname2/proxy/: tls qux (200; 19.094213ms)
    Dec  9 07:54:22.069: INFO: (6) /api/v1/namespaces/proxy-5690/pods/http:proxy-service-8nbp9-49zcc:162/proxy/: bar (200; 19.712621ms)
    Dec  9 07:54:22.070: INFO: (6) /api/v1/namespaces/proxy-5690/services/proxy-service-8nbp9:portname2/proxy/: bar (200; 19.904652ms)
    Dec  9 07:54:22.070: INFO: (6) /api/v1/namespaces/proxy-5690/services/proxy-service-8nbp9:portname1/proxy/: foo (200; 20.133986ms)
    Dec  9 07:54:22.081: INFO: (7) /api/v1/namespaces/proxy-5690/pods/https:proxy-service-8nbp9-49zcc:462/proxy/: tls qux (200; 11.374461ms)
    Dec  9 07:54:22.083: INFO: (7) /api/v1/namespaces/proxy-5690/pods/http:proxy-service-8nbp9-49zcc:162/proxy/: bar (200; 12.388099ms)
    Dec  9 07:54:22.083: INFO: (7) /api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc:1080/proxy/: <a href="/api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc:1080/proxy/rewriteme">test<... (200; 13.034159ms)
    Dec  9 07:54:22.083: INFO: (7) /api/v1/namespaces/proxy-5690/pods/https:proxy-service-8nbp9-49zcc:460/proxy/: tls baz (200; 12.961961ms)
    Dec  9 07:54:22.083: INFO: (7) /api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc:160/proxy/: foo (200; 12.843607ms)
    Dec  9 07:54:22.083: INFO: (7) /api/v1/namespaces/proxy-5690/pods/https:proxy-service-8nbp9-49zcc:443/proxy/: <a href="/api/v1/namespaces/proxy-5690/pods/https:proxy-service-8nbp9-49zcc:443/proxy/tlsrewritem... (200; 12.81496ms)
    Dec  9 07:54:22.083: INFO: (7) /api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc/proxy/: <a href="/api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc/proxy/rewriteme">test</a> (200; 12.708102ms)
    Dec  9 07:54:22.083: INFO: (7) /api/v1/namespaces/proxy-5690/pods/http:proxy-service-8nbp9-49zcc:1080/proxy/: <a href="/api/v1/namespaces/proxy-5690/pods/http:proxy-service-8nbp9-49zcc:1080/proxy/rewriteme">... (200; 13.211882ms)
    Dec  9 07:54:22.083: INFO: (7) /api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc:162/proxy/: bar (200; 13.174725ms)
    Dec  9 07:54:22.083: INFO: (7) /api/v1/namespaces/proxy-5690/services/proxy-service-8nbp9:portname1/proxy/: foo (200; 13.357996ms)
    Dec  9 07:54:22.085: INFO: (7) /api/v1/namespaces/proxy-5690/pods/http:proxy-service-8nbp9-49zcc:160/proxy/: foo (200; 15.075443ms)
    Dec  9 07:54:22.086: INFO: (7) /api/v1/namespaces/proxy-5690/services/http:proxy-service-8nbp9:portname2/proxy/: bar (200; 15.245762ms)
    Dec  9 07:54:22.086: INFO: (7) /api/v1/namespaces/proxy-5690/services/https:proxy-service-8nbp9:tlsportname2/proxy/: tls qux (200; 15.45631ms)
    Dec  9 07:54:22.086: INFO: (7) /api/v1/namespaces/proxy-5690/services/https:proxy-service-8nbp9:tlsportname1/proxy/: tls baz (200; 15.356111ms)
    Dec  9 07:54:22.086: INFO: (7) /api/v1/namespaces/proxy-5690/services/http:proxy-service-8nbp9:portname1/proxy/: foo (200; 15.585665ms)
    Dec  9 07:54:22.086: INFO: (7) /api/v1/namespaces/proxy-5690/services/proxy-service-8nbp9:portname2/proxy/: bar (200; 16.094292ms)
    Dec  9 07:54:22.100: INFO: (8) /api/v1/namespaces/proxy-5690/pods/https:proxy-service-8nbp9-49zcc:462/proxy/: tls qux (200; 14.038037ms)
    Dec  9 07:54:22.101: INFO: (8) /api/v1/namespaces/proxy-5690/pods/http:proxy-service-8nbp9-49zcc:1080/proxy/: <a href="/api/v1/namespaces/proxy-5690/pods/http:proxy-service-8nbp9-49zcc:1080/proxy/rewriteme">... (200; 14.0764ms)
    Dec  9 07:54:22.101: INFO: (8) /api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc/proxy/: <a href="/api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc/proxy/rewriteme">test</a> (200; 14.174166ms)
    Dec  9 07:54:22.101: INFO: (8) /api/v1/namespaces/proxy-5690/services/proxy-service-8nbp9:portname1/proxy/: foo (200; 14.862883ms)
    Dec  9 07:54:22.101: INFO: (8) /api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc:1080/proxy/: <a href="/api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc:1080/proxy/rewriteme">test<... (200; 14.6861ms)
    Dec  9 07:54:22.101: INFO: (8) /api/v1/namespaces/proxy-5690/pods/http:proxy-service-8nbp9-49zcc:162/proxy/: bar (200; 14.346808ms)
    Dec  9 07:54:22.102: INFO: (8) /api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc:162/proxy/: bar (200; 14.936627ms)
    Dec  9 07:54:22.102: INFO: (8) /api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc:160/proxy/: foo (200; 15.033372ms)
    Dec  9 07:54:22.102: INFO: (8) /api/v1/namespaces/proxy-5690/pods/https:proxy-service-8nbp9-49zcc:443/proxy/: <a href="/api/v1/namespaces/proxy-5690/pods/https:proxy-service-8nbp9-49zcc:443/proxy/tlsrewritem... (200; 15.035352ms)
    Dec  9 07:54:22.103: INFO: (8) /api/v1/namespaces/proxy-5690/services/http:proxy-service-8nbp9:portname1/proxy/: foo (200; 15.822916ms)
    Dec  9 07:54:22.103: INFO: (8) /api/v1/namespaces/proxy-5690/services/http:proxy-service-8nbp9:portname2/proxy/: bar (200; 15.690512ms)
    Dec  9 07:54:22.103: INFO: (8) /api/v1/namespaces/proxy-5690/pods/http:proxy-service-8nbp9-49zcc:160/proxy/: foo (200; 15.978145ms)
    Dec  9 07:54:22.103: INFO: (8) /api/v1/namespaces/proxy-5690/services/https:proxy-service-8nbp9:tlsportname1/proxy/: tls baz (200; 15.817285ms)
    Dec  9 07:54:22.103: INFO: (8) /api/v1/namespaces/proxy-5690/services/https:proxy-service-8nbp9:tlsportname2/proxy/: tls qux (200; 16.01455ms)
    Dec  9 07:54:22.103: INFO: (8) /api/v1/namespaces/proxy-5690/services/proxy-service-8nbp9:portname2/proxy/: bar (200; 16.35329ms)
    Dec  9 07:54:22.103: INFO: (8) /api/v1/namespaces/proxy-5690/pods/https:proxy-service-8nbp9-49zcc:460/proxy/: tls baz (200; 16.122221ms)
    Dec  9 07:54:22.119: INFO: (9) /api/v1/namespaces/proxy-5690/pods/https:proxy-service-8nbp9-49zcc:462/proxy/: tls qux (200; 16.197518ms)
    Dec  9 07:54:22.119: INFO: (9) /api/v1/namespaces/proxy-5690/pods/http:proxy-service-8nbp9-49zcc:1080/proxy/: <a href="/api/v1/namespaces/proxy-5690/pods/http:proxy-service-8nbp9-49zcc:1080/proxy/rewriteme">... (200; 15.659931ms)
    Dec  9 07:54:22.120: INFO: (9) /api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc:160/proxy/: foo (200; 16.499866ms)
    Dec  9 07:54:22.121: INFO: (9) /api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc/proxy/: <a href="/api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc/proxy/rewriteme">test</a> (200; 16.687654ms)
    Dec  9 07:54:22.121: INFO: (9) /api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc:162/proxy/: bar (200; 17.645775ms)
    Dec  9 07:54:22.121: INFO: (9) /api/v1/namespaces/proxy-5690/services/proxy-service-8nbp9:portname1/proxy/: foo (200; 17.843302ms)
    Dec  9 07:54:22.121: INFO: (9) /api/v1/namespaces/proxy-5690/pods/http:proxy-service-8nbp9-49zcc:162/proxy/: bar (200; 16.781301ms)
    Dec  9 07:54:22.121: INFO: (9) /api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc:1080/proxy/: <a href="/api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc:1080/proxy/rewriteme">test<... (200; 17.696595ms)
    Dec  9 07:54:22.121: INFO: (9) /api/v1/namespaces/proxy-5690/pods/https:proxy-service-8nbp9-49zcc:460/proxy/: tls baz (200; 17.627419ms)
    Dec  9 07:54:22.122: INFO: (9) /api/v1/namespaces/proxy-5690/pods/https:proxy-service-8nbp9-49zcc:443/proxy/: <a href="/api/v1/namespaces/proxy-5690/pods/https:proxy-service-8nbp9-49zcc:443/proxy/tlsrewritem... (200; 17.578856ms)
    Dec  9 07:54:22.122: INFO: (9) /api/v1/namespaces/proxy-5690/services/http:proxy-service-8nbp9:portname2/proxy/: bar (200; 17.851118ms)
    Dec  9 07:54:22.122: INFO: (9) /api/v1/namespaces/proxy-5690/services/http:proxy-service-8nbp9:portname1/proxy/: foo (200; 18.194718ms)
    Dec  9 07:54:22.122: INFO: (9) /api/v1/namespaces/proxy-5690/services/https:proxy-service-8nbp9:tlsportname2/proxy/: tls qux (200; 18.285254ms)
    Dec  9 07:54:22.122: INFO: (9) /api/v1/namespaces/proxy-5690/services/https:proxy-service-8nbp9:tlsportname1/proxy/: tls baz (200; 18.094017ms)
    Dec  9 07:54:22.122: INFO: (9) /api/v1/namespaces/proxy-5690/services/proxy-service-8nbp9:portname2/proxy/: bar (200; 18.758712ms)
    Dec  9 07:54:22.122: INFO: (9) /api/v1/namespaces/proxy-5690/pods/http:proxy-service-8nbp9-49zcc:160/proxy/: foo (200; 18.582181ms)
    Dec  9 07:54:22.136: INFO: (10) /api/v1/namespaces/proxy-5690/pods/http:proxy-service-8nbp9-49zcc:162/proxy/: bar (200; 13.724254ms)
    Dec  9 07:54:22.137: INFO: (10) /api/v1/namespaces/proxy-5690/pods/https:proxy-service-8nbp9-49zcc:462/proxy/: tls qux (200; 14.077513ms)
    Dec  9 07:54:22.137: INFO: (10) /api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc/proxy/: <a href="/api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc/proxy/rewriteme">test</a> (200; 14.199423ms)
    Dec  9 07:54:22.137: INFO: (10) /api/v1/namespaces/proxy-5690/pods/https:proxy-service-8nbp9-49zcc:443/proxy/: <a href="/api/v1/namespaces/proxy-5690/pods/https:proxy-service-8nbp9-49zcc:443/proxy/tlsrewritem... (200; 14.438546ms)
    Dec  9 07:54:22.138: INFO: (10) /api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc:162/proxy/: bar (200; 14.809623ms)
    Dec  9 07:54:22.138: INFO: (10) /api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc:1080/proxy/: <a href="/api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc:1080/proxy/rewriteme">test<... (200; 14.379683ms)
    Dec  9 07:54:22.138: INFO: (10) /api/v1/namespaces/proxy-5690/services/proxy-service-8nbp9:portname1/proxy/: foo (200; 15.293205ms)
    Dec  9 07:54:22.138: INFO: (10) /api/v1/namespaces/proxy-5690/services/http:proxy-service-8nbp9:portname2/proxy/: bar (200; 15.724919ms)
    Dec  9 07:54:22.138: INFO: (10) /api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc:160/proxy/: foo (200; 14.525728ms)
    Dec  9 07:54:22.138: INFO: (10) /api/v1/namespaces/proxy-5690/pods/http:proxy-service-8nbp9-49zcc:1080/proxy/: <a href="/api/v1/namespaces/proxy-5690/pods/http:proxy-service-8nbp9-49zcc:1080/proxy/rewriteme">... (200; 15.490264ms)
    Dec  9 07:54:22.138: INFO: (10) /api/v1/namespaces/proxy-5690/services/https:proxy-service-8nbp9:tlsportname1/proxy/: tls baz (200; 15.813741ms)
    Dec  9 07:54:22.138: INFO: (10) /api/v1/namespaces/proxy-5690/pods/https:proxy-service-8nbp9-49zcc:460/proxy/: tls baz (200; 15.198928ms)
    Dec  9 07:54:22.140: INFO: (10) /api/v1/namespaces/proxy-5690/pods/http:proxy-service-8nbp9-49zcc:160/proxy/: foo (200; 16.433556ms)
    Dec  9 07:54:22.140: INFO: (10) /api/v1/namespaces/proxy-5690/services/http:proxy-service-8nbp9:portname1/proxy/: foo (200; 16.734507ms)
    Dec  9 07:54:22.140: INFO: (10) /api/v1/namespaces/proxy-5690/services/proxy-service-8nbp9:portname2/proxy/: bar (200; 17.595006ms)
    Dec  9 07:54:22.141: INFO: (10) /api/v1/namespaces/proxy-5690/services/https:proxy-service-8nbp9:tlsportname2/proxy/: tls qux (200; 17.087225ms)
    Dec  9 07:54:22.153: INFO: (11) /api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc:162/proxy/: bar (200; 11.558149ms)
    Dec  9 07:54:22.153: INFO: (11) /api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc/proxy/: <a href="/api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc/proxy/rewriteme">test</a> (200; 11.634012ms)
    Dec  9 07:54:22.153: INFO: (11) /api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc:1080/proxy/: <a href="/api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc:1080/proxy/rewriteme">test<... (200; 11.70453ms)
    Dec  9 07:54:22.154: INFO: (11) /api/v1/namespaces/proxy-5690/pods/http:proxy-service-8nbp9-49zcc:162/proxy/: bar (200; 12.469013ms)
    Dec  9 07:54:22.154: INFO: (11) /api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc:160/proxy/: foo (200; 12.645763ms)
    Dec  9 07:54:22.154: INFO: (11) /api/v1/namespaces/proxy-5690/services/http:proxy-service-8nbp9:portname1/proxy/: foo (200; 13.000877ms)
    Dec  9 07:54:22.154: INFO: (11) /api/v1/namespaces/proxy-5690/pods/https:proxy-service-8nbp9-49zcc:460/proxy/: tls baz (200; 12.639398ms)
    Dec  9 07:54:22.154: INFO: (11) /api/v1/namespaces/proxy-5690/pods/http:proxy-service-8nbp9-49zcc:1080/proxy/: <a href="/api/v1/namespaces/proxy-5690/pods/http:proxy-service-8nbp9-49zcc:1080/proxy/rewriteme">... (200; 12.808442ms)
    Dec  9 07:54:22.154: INFO: (11) /api/v1/namespaces/proxy-5690/pods/https:proxy-service-8nbp9-49zcc:462/proxy/: tls qux (200; 13.162377ms)
    Dec  9 07:54:22.156: INFO: (11) /api/v1/namespaces/proxy-5690/pods/http:proxy-service-8nbp9-49zcc:160/proxy/: foo (200; 14.647377ms)
    Dec  9 07:54:22.156: INFO: (11) /api/v1/namespaces/proxy-5690/pods/https:proxy-service-8nbp9-49zcc:443/proxy/: <a href="/api/v1/namespaces/proxy-5690/pods/https:proxy-service-8nbp9-49zcc:443/proxy/tlsrewritem... (200; 14.905168ms)
    Dec  9 07:54:22.157: INFO: (11) /api/v1/namespaces/proxy-5690/services/proxy-service-8nbp9:portname2/proxy/: bar (200; 15.463417ms)
    Dec  9 07:54:22.157: INFO: (11) /api/v1/namespaces/proxy-5690/services/http:proxy-service-8nbp9:portname2/proxy/: bar (200; 15.568577ms)
    Dec  9 07:54:22.157: INFO: (11) /api/v1/namespaces/proxy-5690/services/proxy-service-8nbp9:portname1/proxy/: foo (200; 15.852818ms)
    Dec  9 07:54:22.158: INFO: (11) /api/v1/namespaces/proxy-5690/services/https:proxy-service-8nbp9:tlsportname1/proxy/: tls baz (200; 16.302603ms)
    Dec  9 07:54:22.158: INFO: (11) /api/v1/namespaces/proxy-5690/services/https:proxy-service-8nbp9:tlsportname2/proxy/: tls qux (200; 16.492278ms)
    Dec  9 07:54:22.170: INFO: (12) /api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc:1080/proxy/: <a href="/api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc:1080/proxy/rewriteme">test<... (200; 12.293203ms)
    Dec  9 07:54:22.171: INFO: (12) /api/v1/namespaces/proxy-5690/pods/https:proxy-service-8nbp9-49zcc:462/proxy/: tls qux (200; 12.917546ms)
    Dec  9 07:54:22.171: INFO: (12) /api/v1/namespaces/proxy-5690/pods/http:proxy-service-8nbp9-49zcc:1080/proxy/: <a href="/api/v1/namespaces/proxy-5690/pods/http:proxy-service-8nbp9-49zcc:1080/proxy/rewriteme">... (200; 12.769225ms)
    Dec  9 07:54:22.171: INFO: (12) /api/v1/namespaces/proxy-5690/pods/http:proxy-service-8nbp9-49zcc:160/proxy/: foo (200; 12.676468ms)
    Dec  9 07:54:22.171: INFO: (12) /api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc:162/proxy/: bar (200; 12.803727ms)
    Dec  9 07:54:22.173: INFO: (12) /api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc/proxy/: <a href="/api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc/proxy/rewriteme">test</a> (200; 14.678528ms)
    Dec  9 07:54:22.173: INFO: (12) /api/v1/namespaces/proxy-5690/services/proxy-service-8nbp9:portname2/proxy/: bar (200; 15.431712ms)
    Dec  9 07:54:22.174: INFO: (12) /api/v1/namespaces/proxy-5690/services/https:proxy-service-8nbp9:tlsportname1/proxy/: tls baz (200; 15.115954ms)
    Dec  9 07:54:22.174: INFO: (12) /api/v1/namespaces/proxy-5690/services/proxy-service-8nbp9:portname1/proxy/: foo (200; 15.720488ms)
    Dec  9 07:54:22.174: INFO: (12) /api/v1/namespaces/proxy-5690/pods/http:proxy-service-8nbp9-49zcc:162/proxy/: bar (200; 15.104511ms)
    Dec  9 07:54:22.174: INFO: (12) /api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc:160/proxy/: foo (200; 15.402097ms)
    Dec  9 07:54:22.174: INFO: (12) /api/v1/namespaces/proxy-5690/pods/https:proxy-service-8nbp9-49zcc:443/proxy/: <a href="/api/v1/namespaces/proxy-5690/pods/https:proxy-service-8nbp9-49zcc:443/proxy/tlsrewritem... (200; 15.433581ms)
    Dec  9 07:54:22.174: INFO: (12) /api/v1/namespaces/proxy-5690/pods/https:proxy-service-8nbp9-49zcc:460/proxy/: tls baz (200; 15.696034ms)
    Dec  9 07:54:22.174: INFO: (12) /api/v1/namespaces/proxy-5690/services/http:proxy-service-8nbp9:portname1/proxy/: foo (200; 15.779374ms)
    Dec  9 07:54:22.174: INFO: (12) /api/v1/namespaces/proxy-5690/services/https:proxy-service-8nbp9:tlsportname2/proxy/: tls qux (200; 15.817049ms)
    Dec  9 07:54:22.174: INFO: (12) /api/v1/namespaces/proxy-5690/services/http:proxy-service-8nbp9:portname2/proxy/: bar (200; 15.693512ms)
    Dec  9 07:54:22.187: INFO: (13) /api/v1/namespaces/proxy-5690/pods/https:proxy-service-8nbp9-49zcc:462/proxy/: tls qux (200; 12.880317ms)
    Dec  9 07:54:22.187: INFO: (13) /api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc:160/proxy/: foo (200; 13.160787ms)
    Dec  9 07:54:22.187: INFO: (13) /api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc/proxy/: <a href="/api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc/proxy/rewriteme">test</a> (200; 12.800964ms)
    Dec  9 07:54:22.187: INFO: (13) /api/v1/namespaces/proxy-5690/pods/http:proxy-service-8nbp9-49zcc:162/proxy/: bar (200; 13.142247ms)
    Dec  9 07:54:22.189: INFO: (13) /api/v1/namespaces/proxy-5690/pods/http:proxy-service-8nbp9-49zcc:1080/proxy/: <a href="/api/v1/namespaces/proxy-5690/pods/http:proxy-service-8nbp9-49zcc:1080/proxy/rewriteme">... (200; 14.634801ms)
    Dec  9 07:54:22.190: INFO: (13) /api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc:1080/proxy/: <a href="/api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc:1080/proxy/rewriteme">test<... (200; 15.070721ms)
    Dec  9 07:54:22.190: INFO: (13) /api/v1/namespaces/proxy-5690/pods/http:proxy-service-8nbp9-49zcc:160/proxy/: foo (200; 15.046056ms)
    Dec  9 07:54:22.190: INFO: (13) /api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc:162/proxy/: bar (200; 15.419493ms)
    Dec  9 07:54:22.190: INFO: (13) /api/v1/namespaces/proxy-5690/services/http:proxy-service-8nbp9:portname2/proxy/: bar (200; 15.560992ms)
    Dec  9 07:54:22.190: INFO: (13) /api/v1/namespaces/proxy-5690/services/https:proxy-service-8nbp9:tlsportname1/proxy/: tls baz (200; 15.797821ms)
    Dec  9 07:54:22.190: INFO: (13) /api/v1/namespaces/proxy-5690/pods/https:proxy-service-8nbp9-49zcc:460/proxy/: tls baz (200; 15.522254ms)
    Dec  9 07:54:22.190: INFO: (13) /api/v1/namespaces/proxy-5690/pods/https:proxy-service-8nbp9-49zcc:443/proxy/: <a href="/api/v1/namespaces/proxy-5690/pods/https:proxy-service-8nbp9-49zcc:443/proxy/tlsrewritem... (200; 15.595095ms)
    Dec  9 07:54:22.191: INFO: (13) /api/v1/namespaces/proxy-5690/services/proxy-service-8nbp9:portname2/proxy/: bar (200; 16.004369ms)
    Dec  9 07:54:22.191: INFO: (13) /api/v1/namespaces/proxy-5690/services/http:proxy-service-8nbp9:portname1/proxy/: foo (200; 15.805586ms)
    Dec  9 07:54:22.191: INFO: (13) /api/v1/namespaces/proxy-5690/services/proxy-service-8nbp9:portname1/proxy/: foo (200; 16.205617ms)
    Dec  9 07:54:22.191: INFO: (13) /api/v1/namespaces/proxy-5690/services/https:proxy-service-8nbp9:tlsportname2/proxy/: tls qux (200; 15.846907ms)
    Dec  9 07:54:22.203: INFO: (14) /api/v1/namespaces/proxy-5690/pods/http:proxy-service-8nbp9-49zcc:162/proxy/: bar (200; 11.798413ms)
    Dec  9 07:54:22.203: INFO: (14) /api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc:1080/proxy/: <a href="/api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc:1080/proxy/rewriteme">test<... (200; 11.892084ms)
    Dec  9 07:54:22.203: INFO: (14) /api/v1/namespaces/proxy-5690/pods/http:proxy-service-8nbp9-49zcc:160/proxy/: foo (200; 11.881344ms)
    Dec  9 07:54:22.204: INFO: (14) /api/v1/namespaces/proxy-5690/pods/http:proxy-service-8nbp9-49zcc:1080/proxy/: <a href="/api/v1/namespaces/proxy-5690/pods/http:proxy-service-8nbp9-49zcc:1080/proxy/rewriteme">... (200; 12.618677ms)
    Dec  9 07:54:22.204: INFO: (14) /api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc:160/proxy/: foo (200; 12.824671ms)
    Dec  9 07:54:22.204: INFO: (14) /api/v1/namespaces/proxy-5690/pods/https:proxy-service-8nbp9-49zcc:462/proxy/: tls qux (200; 12.397949ms)
    Dec  9 07:54:22.204: INFO: (14) /api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc:162/proxy/: bar (200; 12.745543ms)
    Dec  9 07:54:22.204: INFO: (14) /api/v1/namespaces/proxy-5690/pods/https:proxy-service-8nbp9-49zcc:443/proxy/: <a href="/api/v1/namespaces/proxy-5690/pods/https:proxy-service-8nbp9-49zcc:443/proxy/tlsrewritem... (200; 12.920754ms)
    Dec  9 07:54:22.204: INFO: (14) /api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc/proxy/: <a href="/api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc/proxy/rewriteme">test</a> (200; 12.630422ms)
    Dec  9 07:54:22.206: INFO: (14) /api/v1/namespaces/proxy-5690/pods/https:proxy-service-8nbp9-49zcc:460/proxy/: tls baz (200; 14.982588ms)
    Dec  9 07:54:22.207: INFO: (14) /api/v1/namespaces/proxy-5690/services/http:proxy-service-8nbp9:portname1/proxy/: foo (200; 15.740264ms)
    Dec  9 07:54:22.207: INFO: (14) /api/v1/namespaces/proxy-5690/services/http:proxy-service-8nbp9:portname2/proxy/: bar (200; 15.36317ms)
    Dec  9 07:54:22.207: INFO: (14) /api/v1/namespaces/proxy-5690/services/proxy-service-8nbp9:portname2/proxy/: bar (200; 15.705982ms)
    Dec  9 07:54:22.207: INFO: (14) /api/v1/namespaces/proxy-5690/services/proxy-service-8nbp9:portname1/proxy/: foo (200; 15.752912ms)
    Dec  9 07:54:22.207: INFO: (14) /api/v1/namespaces/proxy-5690/services/https:proxy-service-8nbp9:tlsportname1/proxy/: tls baz (200; 16.227043ms)
    Dec  9 07:54:22.207: INFO: (14) /api/v1/namespaces/proxy-5690/services/https:proxy-service-8nbp9:tlsportname2/proxy/: tls qux (200; 16.095519ms)
    Dec  9 07:54:22.225: INFO: (15) /api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc:1080/proxy/: <a href="/api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc:1080/proxy/rewriteme">test<... (200; 16.711078ms)
    Dec  9 07:54:22.225: INFO: (15) /api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc:160/proxy/: foo (200; 16.625035ms)
    Dec  9 07:54:22.225: INFO: (15) /api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc:162/proxy/: bar (200; 17.339985ms)
    Dec  9 07:54:22.225: INFO: (15) /api/v1/namespaces/proxy-5690/pods/http:proxy-service-8nbp9-49zcc:160/proxy/: foo (200; 17.415797ms)
    Dec  9 07:54:22.225: INFO: (15) /api/v1/namespaces/proxy-5690/pods/http:proxy-service-8nbp9-49zcc:1080/proxy/: <a href="/api/v1/namespaces/proxy-5690/pods/http:proxy-service-8nbp9-49zcc:1080/proxy/rewriteme">... (200; 17.771654ms)
    Dec  9 07:54:22.226: INFO: (15) /api/v1/namespaces/proxy-5690/pods/http:proxy-service-8nbp9-49zcc:162/proxy/: bar (200; 17.463288ms)
    Dec  9 07:54:22.226: INFO: (15) /api/v1/namespaces/proxy-5690/pods/https:proxy-service-8nbp9-49zcc:443/proxy/: <a href="/api/v1/namespaces/proxy-5690/pods/https:proxy-service-8nbp9-49zcc:443/proxy/tlsrewritem... (200; 17.736517ms)
    Dec  9 07:54:22.226: INFO: (15) /api/v1/namespaces/proxy-5690/pods/https:proxy-service-8nbp9-49zcc:460/proxy/: tls baz (200; 17.945885ms)
    Dec  9 07:54:22.228: INFO: (15) /api/v1/namespaces/proxy-5690/services/proxy-service-8nbp9:portname1/proxy/: foo (200; 20.023125ms)
    Dec  9 07:54:22.228: INFO: (15) /api/v1/namespaces/proxy-5690/services/proxy-service-8nbp9:portname2/proxy/: bar (200; 20.281108ms)
    Dec  9 07:54:22.228: INFO: (15) /api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc/proxy/: <a href="/api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc/proxy/rewriteme">test</a> (200; 19.751745ms)
    Dec  9 07:54:22.228: INFO: (15) /api/v1/namespaces/proxy-5690/services/http:proxy-service-8nbp9:portname1/proxy/: foo (200; 19.896287ms)
    Dec  9 07:54:22.228: INFO: (15) /api/v1/namespaces/proxy-5690/services/https:proxy-service-8nbp9:tlsportname2/proxy/: tls qux (200; 21.033479ms)
    Dec  9 07:54:22.228: INFO: (15) /api/v1/namespaces/proxy-5690/services/https:proxy-service-8nbp9:tlsportname1/proxy/: tls baz (200; 20.466102ms)
    Dec  9 07:54:22.229: INFO: (15) /api/v1/namespaces/proxy-5690/pods/https:proxy-service-8nbp9-49zcc:462/proxy/: tls qux (200; 20.226521ms)
    Dec  9 07:54:22.229: INFO: (15) /api/v1/namespaces/proxy-5690/services/http:proxy-service-8nbp9:portname2/proxy/: bar (200; 22.165428ms)
    Dec  9 07:54:22.242: INFO: (16) /api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc:162/proxy/: bar (200; 12.777695ms)
    Dec  9 07:54:22.246: INFO: (16) /api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc/proxy/: <a href="/api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc/proxy/rewriteme">test</a> (200; 15.389952ms)
    Dec  9 07:54:22.246: INFO: (16) /api/v1/namespaces/proxy-5690/pods/http:proxy-service-8nbp9-49zcc:162/proxy/: bar (200; 15.040297ms)
    Dec  9 07:54:22.247: INFO: (16) /api/v1/namespaces/proxy-5690/pods/http:proxy-service-8nbp9-49zcc:160/proxy/: foo (200; 16.241873ms)
    Dec  9 07:54:22.247: INFO: (16) /api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc:160/proxy/: foo (200; 16.250623ms)
    Dec  9 07:54:22.247: INFO: (16) /api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc:1080/proxy/: <a href="/api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc:1080/proxy/rewriteme">test<... (200; 16.545819ms)
    Dec  9 07:54:22.247: INFO: (16) /api/v1/namespaces/proxy-5690/services/https:proxy-service-8nbp9:tlsportname1/proxy/: tls baz (200; 16.832068ms)
    Dec  9 07:54:22.247: INFO: (16) /api/v1/namespaces/proxy-5690/pods/https:proxy-service-8nbp9-49zcc:460/proxy/: tls baz (200; 16.608971ms)
    Dec  9 07:54:22.247: INFO: (16) /api/v1/namespaces/proxy-5690/pods/https:proxy-service-8nbp9-49zcc:462/proxy/: tls qux (200; 15.538211ms)
    Dec  9 07:54:22.247: INFO: (16) /api/v1/namespaces/proxy-5690/pods/http:proxy-service-8nbp9-49zcc:1080/proxy/: <a href="/api/v1/namespaces/proxy-5690/pods/http:proxy-service-8nbp9-49zcc:1080/proxy/rewriteme">... (200; 16.250501ms)
    Dec  9 07:54:22.247: INFO: (16) /api/v1/namespaces/proxy-5690/pods/https:proxy-service-8nbp9-49zcc:443/proxy/: <a href="/api/v1/namespaces/proxy-5690/pods/https:proxy-service-8nbp9-49zcc:443/proxy/tlsrewritem... (200; 15.816269ms)
    Dec  9 07:54:22.247: INFO: (16) /api/v1/namespaces/proxy-5690/services/http:proxy-service-8nbp9:portname1/proxy/: foo (200; 15.807892ms)
    Dec  9 07:54:22.247: INFO: (16) /api/v1/namespaces/proxy-5690/services/proxy-service-8nbp9:portname2/proxy/: bar (200; 16.841434ms)
    Dec  9 07:54:22.247: INFO: (16) /api/v1/namespaces/proxy-5690/services/http:proxy-service-8nbp9:portname2/proxy/: bar (200; 15.500454ms)
    Dec  9 07:54:22.247: INFO: (16) /api/v1/namespaces/proxy-5690/services/proxy-service-8nbp9:portname1/proxy/: foo (200; 17.207417ms)
    Dec  9 07:54:22.247: INFO: (16) /api/v1/namespaces/proxy-5690/services/https:proxy-service-8nbp9:tlsportname2/proxy/: tls qux (200; 16.071829ms)
    Dec  9 07:54:22.265: INFO: (17) /api/v1/namespaces/proxy-5690/pods/https:proxy-service-8nbp9-49zcc:462/proxy/: tls qux (200; 17.714886ms)
    Dec  9 07:54:22.268: INFO: (17) /api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc/proxy/: <a href="/api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc/proxy/rewriteme">test</a> (200; 19.618867ms)
    Dec  9 07:54:22.268: INFO: (17) /api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc:1080/proxy/: <a href="/api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc:1080/proxy/rewriteme">test<... (200; 20.547353ms)
    Dec  9 07:54:22.268: INFO: (17) /api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc:162/proxy/: bar (200; 20.643691ms)
    Dec  9 07:54:22.268: INFO: (17) /api/v1/namespaces/proxy-5690/services/proxy-service-8nbp9:portname2/proxy/: bar (200; 21.040373ms)
    Dec  9 07:54:22.269: INFO: (17) /api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc:160/proxy/: foo (200; 20.338067ms)
    Dec  9 07:54:22.269: INFO: (17) /api/v1/namespaces/proxy-5690/services/proxy-service-8nbp9:portname1/proxy/: foo (200; 21.330049ms)
    Dec  9 07:54:22.269: INFO: (17) /api/v1/namespaces/proxy-5690/pods/http:proxy-service-8nbp9-49zcc:1080/proxy/: <a href="/api/v1/namespaces/proxy-5690/pods/http:proxy-service-8nbp9-49zcc:1080/proxy/rewriteme">... (200; 21.424522ms)
    Dec  9 07:54:22.269: INFO: (17) /api/v1/namespaces/proxy-5690/pods/https:proxy-service-8nbp9-49zcc:460/proxy/: tls baz (200; 21.081339ms)
    Dec  9 07:54:22.269: INFO: (17) /api/v1/namespaces/proxy-5690/pods/https:proxy-service-8nbp9-49zcc:443/proxy/: <a href="/api/v1/namespaces/proxy-5690/pods/https:proxy-service-8nbp9-49zcc:443/proxy/tlsrewritem... (200; 20.755803ms)
    Dec  9 07:54:22.269: INFO: (17) /api/v1/namespaces/proxy-5690/services/http:proxy-service-8nbp9:portname1/proxy/: foo (200; 21.409536ms)
    Dec  9 07:54:22.270: INFO: (17) /api/v1/namespaces/proxy-5690/services/http:proxy-service-8nbp9:portname2/proxy/: bar (200; 21.372598ms)
    Dec  9 07:54:22.270: INFO: (17) /api/v1/namespaces/proxy-5690/services/https:proxy-service-8nbp9:tlsportname2/proxy/: tls qux (200; 21.874377ms)
    Dec  9 07:54:22.270: INFO: (17) /api/v1/namespaces/proxy-5690/pods/http:proxy-service-8nbp9-49zcc:162/proxy/: bar (200; 21.416558ms)
    Dec  9 07:54:22.270: INFO: (17) /api/v1/namespaces/proxy-5690/pods/http:proxy-service-8nbp9-49zcc:160/proxy/: foo (200; 22.03081ms)
    Dec  9 07:54:22.270: INFO: (17) /api/v1/namespaces/proxy-5690/services/https:proxy-service-8nbp9:tlsportname1/proxy/: tls baz (200; 21.577757ms)
    Dec  9 07:54:22.284: INFO: (18) /api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc/proxy/: <a href="/api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc/proxy/rewriteme">test</a> (200; 13.580335ms)
    Dec  9 07:54:22.284: INFO: (18) /api/v1/namespaces/proxy-5690/pods/http:proxy-service-8nbp9-49zcc:162/proxy/: bar (200; 13.863556ms)
    Dec  9 07:54:22.284: INFO: (18) /api/v1/namespaces/proxy-5690/pods/https:proxy-service-8nbp9-49zcc:462/proxy/: tls qux (200; 13.824786ms)
    Dec  9 07:54:22.286: INFO: (18) /api/v1/namespaces/proxy-5690/pods/http:proxy-service-8nbp9-49zcc:160/proxy/: foo (200; 15.118325ms)
    Dec  9 07:54:22.287: INFO: (18) /api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc:1080/proxy/: <a href="/api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc:1080/proxy/rewriteme">test<... (200; 15.39043ms)
    Dec  9 07:54:22.287: INFO: (18) /api/v1/namespaces/proxy-5690/services/http:proxy-service-8nbp9:portname2/proxy/: bar (200; 16.223477ms)
    Dec  9 07:54:22.287: INFO: (18) /api/v1/namespaces/proxy-5690/pods/http:proxy-service-8nbp9-49zcc:1080/proxy/: <a href="/api/v1/namespaces/proxy-5690/pods/http:proxy-service-8nbp9-49zcc:1080/proxy/rewriteme">... (200; 16.466643ms)
    Dec  9 07:54:22.287: INFO: (18) /api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc:162/proxy/: bar (200; 16.469118ms)
    Dec  9 07:54:22.288: INFO: (18) /api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc:160/proxy/: foo (200; 16.27738ms)
    Dec  9 07:54:22.288: INFO: (18) /api/v1/namespaces/proxy-5690/pods/https:proxy-service-8nbp9-49zcc:460/proxy/: tls baz (200; 16.841024ms)
    Dec  9 07:54:22.288: INFO: (18) /api/v1/namespaces/proxy-5690/pods/https:proxy-service-8nbp9-49zcc:443/proxy/: <a href="/api/v1/namespaces/proxy-5690/pods/https:proxy-service-8nbp9-49zcc:443/proxy/tlsrewritem... (200; 16.600246ms)
    Dec  9 07:54:22.288: INFO: (18) /api/v1/namespaces/proxy-5690/services/https:proxy-service-8nbp9:tlsportname2/proxy/: tls qux (200; 17.917935ms)
    Dec  9 07:54:22.288: INFO: (18) /api/v1/namespaces/proxy-5690/services/https:proxy-service-8nbp9:tlsportname1/proxy/: tls baz (200; 17.717584ms)
    Dec  9 07:54:22.288: INFO: (18) /api/v1/namespaces/proxy-5690/services/proxy-service-8nbp9:portname1/proxy/: foo (200; 17.475429ms)
    Dec  9 07:54:22.288: INFO: (18) /api/v1/namespaces/proxy-5690/services/proxy-service-8nbp9:portname2/proxy/: bar (200; 17.419703ms)
    Dec  9 07:54:22.288: INFO: (18) /api/v1/namespaces/proxy-5690/services/http:proxy-service-8nbp9:portname1/proxy/: foo (200; 16.959773ms)
    Dec  9 07:54:22.303: INFO: (19) /api/v1/namespaces/proxy-5690/pods/https:proxy-service-8nbp9-49zcc:462/proxy/: tls qux (200; 13.89032ms)
    Dec  9 07:54:22.303: INFO: (19) /api/v1/namespaces/proxy-5690/pods/http:proxy-service-8nbp9-49zcc:1080/proxy/: <a href="/api/v1/namespaces/proxy-5690/pods/http:proxy-service-8nbp9-49zcc:1080/proxy/rewriteme">... (200; 14.347044ms)
    Dec  9 07:54:22.303: INFO: (19) /api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc:162/proxy/: bar (200; 14.320302ms)
    Dec  9 07:54:22.303: INFO: (19) /api/v1/namespaces/proxy-5690/pods/http:proxy-service-8nbp9-49zcc:162/proxy/: bar (200; 14.593026ms)
    Dec  9 07:54:22.303: INFO: (19) /api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc/proxy/: <a href="/api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc/proxy/rewriteme">test</a> (200; 13.85106ms)
    Dec  9 07:54:22.303: INFO: (19) /api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc:1080/proxy/: <a href="/api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc:1080/proxy/rewriteme">test<... (200; 14.301751ms)
    Dec  9 07:54:22.303: INFO: (19) /api/v1/namespaces/proxy-5690/pods/https:proxy-service-8nbp9-49zcc:443/proxy/: <a href="/api/v1/namespaces/proxy-5690/pods/https:proxy-service-8nbp9-49zcc:443/proxy/tlsrewritem... (200; 14.063028ms)
    Dec  9 07:54:22.304: INFO: (19) /api/v1/namespaces/proxy-5690/pods/proxy-service-8nbp9-49zcc:160/proxy/: foo (200; 15.441804ms)
    Dec  9 07:54:22.305: INFO: (19) /api/v1/namespaces/proxy-5690/pods/http:proxy-service-8nbp9-49zcc:160/proxy/: foo (200; 15.827693ms)
    Dec  9 07:54:22.305: INFO: (19) /api/v1/namespaces/proxy-5690/pods/https:proxy-service-8nbp9-49zcc:460/proxy/: tls baz (200; 15.782124ms)
    Dec  9 07:54:22.306: INFO: (19) /api/v1/namespaces/proxy-5690/services/proxy-service-8nbp9:portname2/proxy/: bar (200; 17.451411ms)
    Dec  9 07:54:22.306: INFO: (19) /api/v1/namespaces/proxy-5690/services/proxy-service-8nbp9:portname1/proxy/: foo (200; 17.657794ms)
    Dec  9 07:54:22.306: INFO: (19) /api/v1/namespaces/proxy-5690/services/http:proxy-service-8nbp9:portname2/proxy/: bar (200; 17.274795ms)
    Dec  9 07:54:22.306: INFO: (19) /api/v1/namespaces/proxy-5690/services/http:proxy-service-8nbp9:portname1/proxy/: foo (200; 17.506534ms)
    Dec  9 07:54:22.306: INFO: (19) /api/v1/namespaces/proxy-5690/services/https:proxy-service-8nbp9:tlsportname1/proxy/: tls baz (200; 17.267147ms)
    Dec  9 07:54:22.306: INFO: (19) /api/v1/namespaces/proxy-5690/services/https:proxy-service-8nbp9:tlsportname2/proxy/: tls qux (200; 17.508095ms)
    STEP: deleting ReplicationController proxy-service-8nbp9 in namespace proxy-5690, will wait for the garbage collector to delete the pods 12/09/22 07:54:22.307
    Dec  9 07:54:22.365: INFO: Deleting ReplicationController proxy-service-8nbp9 took: 4.878293ms
    Dec  9 07:54:22.466: INFO: Terminating ReplicationController proxy-service-8nbp9 pods took: 100.997223ms
    [AfterEach] version v1
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:54:24.771: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] version v1
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] version v1
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] version v1
      tear down framework | framework.go:193
    STEP: Destroying namespace "proxy-5690" for this suite. 12/09/22 07:54:24.785
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:195
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:54:24.803
Dec  9 07:54:24.803: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename container-runtime 12/09/22 07:54:24.805
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:54:24.852
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:54:24.861
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:195
STEP: create the container 12/09/22 07:54:24.869
STEP: wait for the container to reach Succeeded 12/09/22 07:54:24.881
STEP: get the container status 12/09/22 07:54:28.946
STEP: the container should be terminated 12/09/22 07:54:28.948
STEP: the termination message should be set 12/09/22 07:54:28.949
Dec  9 07:54:28.949: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container 12/09/22 07:54:28.949
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Dec  9 07:54:28.970: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-8731" for this suite. 12/09/22 07:54:28.975
------------------------------
â€¢ [4.180 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    on terminated container
    test/e2e/common/node/runtime.go:137
      should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:195

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:54:24.803
    Dec  9 07:54:24.803: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename container-runtime 12/09/22 07:54:24.805
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:54:24.852
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:54:24.861
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:195
    STEP: create the container 12/09/22 07:54:24.869
    STEP: wait for the container to reach Succeeded 12/09/22 07:54:24.881
    STEP: get the container status 12/09/22 07:54:28.946
    STEP: the container should be terminated 12/09/22 07:54:28.948
    STEP: the termination message should be set 12/09/22 07:54:28.949
    Dec  9 07:54:28.949: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
    STEP: delete the container 12/09/22 07:54:28.949
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:54:28.970: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-8731" for this suite. 12/09/22 07:54:28.975
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:308
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:54:28.984
Dec  9 07:54:28.984: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename webhook 12/09/22 07:54:28.985
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:54:29.004
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:54:29.008
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 12/09/22 07:54:29.025
STEP: Create role binding to let webhook read extension-apiserver-authentication 12/09/22 07:54:29.86
STEP: Deploying the webhook pod 12/09/22 07:54:29.873
STEP: Wait for the deployment to be ready 12/09/22 07:54:29.89
Dec  9 07:54:29.902: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 12/09/22 07:54:31.912
STEP: Verifying the service has paired with the endpoint 12/09/22 07:54:31.923
Dec  9 07:54:32.923: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:308
STEP: Registering the crd webhook via the AdmissionRegistration API 12/09/22 07:54:32.927
STEP: Creating a custom resource definition that should be denied by the webhook 12/09/22 07:54:32.941
Dec  9 07:54:32.941: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Dec  9 07:54:32.960: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-5633" for this suite. 12/09/22 07:54:33.029
STEP: Destroying namespace "webhook-5633-markers" for this suite. 12/09/22 07:54:33.041
------------------------------
â€¢ [4.072 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:308

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:54:28.984
    Dec  9 07:54:28.984: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename webhook 12/09/22 07:54:28.985
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:54:29.004
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:54:29.008
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 12/09/22 07:54:29.025
    STEP: Create role binding to let webhook read extension-apiserver-authentication 12/09/22 07:54:29.86
    STEP: Deploying the webhook pod 12/09/22 07:54:29.873
    STEP: Wait for the deployment to be ready 12/09/22 07:54:29.89
    Dec  9 07:54:29.902: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 12/09/22 07:54:31.912
    STEP: Verifying the service has paired with the endpoint 12/09/22 07:54:31.923
    Dec  9 07:54:32.923: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should deny crd creation [Conformance]
      test/e2e/apimachinery/webhook.go:308
    STEP: Registering the crd webhook via the AdmissionRegistration API 12/09/22 07:54:32.927
    STEP: Creating a custom resource definition that should be denied by the webhook 12/09/22 07:54:32.941
    Dec  9 07:54:32.941: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:54:32.960: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-5633" for this suite. 12/09/22 07:54:33.029
    STEP: Destroying namespace "webhook-5633-markers" for this suite. 12/09/22 07:54:33.041
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] Pods
  should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1083
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:54:33.057
Dec  9 07:54:33.057: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename pods 12/09/22 07:54:33.058
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:54:33.091
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:54:33.097
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1083
STEP: Create a pod 12/09/22 07:54:33.103
Dec  9 07:54:33.113: INFO: Waiting up to 5m0s for pod "pod-9lkrp" in namespace "pods-4685" to be "running"
Dec  9 07:54:33.122: INFO: Pod "pod-9lkrp": Phase="Pending", Reason="", readiness=false. Elapsed: 8.305324ms
Dec  9 07:54:35.126: INFO: Pod "pod-9lkrp": Phase="Running", Reason="", readiness=true. Elapsed: 2.012387786s
Dec  9 07:54:35.126: INFO: Pod "pod-9lkrp" satisfied condition "running"
STEP: patching /status 12/09/22 07:54:35.126
Dec  9 07:54:35.135: INFO: Status Message: "Patched by e2e test" and Reason: "E2E"
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Dec  9 07:54:35.135: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-4685" for this suite. 12/09/22 07:54:35.138
------------------------------
â€¢ [2.087 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1083

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:54:33.057
    Dec  9 07:54:33.057: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename pods 12/09/22 07:54:33.058
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:54:33.091
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:54:33.097
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should patch a pod status [Conformance]
      test/e2e/common/node/pods.go:1083
    STEP: Create a pod 12/09/22 07:54:33.103
    Dec  9 07:54:33.113: INFO: Waiting up to 5m0s for pod "pod-9lkrp" in namespace "pods-4685" to be "running"
    Dec  9 07:54:33.122: INFO: Pod "pod-9lkrp": Phase="Pending", Reason="", readiness=false. Elapsed: 8.305324ms
    Dec  9 07:54:35.126: INFO: Pod "pod-9lkrp": Phase="Running", Reason="", readiness=true. Elapsed: 2.012387786s
    Dec  9 07:54:35.126: INFO: Pod "pod-9lkrp" satisfied condition "running"
    STEP: patching /status 12/09/22 07:54:35.126
    Dec  9 07:54:35.135: INFO: Status Message: "Patched by e2e test" and Reason: "E2E"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:54:35.135: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-4685" for this suite. 12/09/22 07:54:35.138
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:56
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:54:35.146
Dec  9 07:54:35.146: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename projected 12/09/22 07:54:35.147
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:54:35.163
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:54:35.168
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:56
STEP: Creating projection with secret that has name projected-secret-test-689cd0db-2d4b-4564-83a2-9ffa05dd19f3 12/09/22 07:54:35.171
STEP: Creating a pod to test consume secrets 12/09/22 07:54:35.175
Dec  9 07:54:35.184: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-a896c22f-b820-471f-8d00-d37985c7cd04" in namespace "projected-5612" to be "Succeeded or Failed"
Dec  9 07:54:35.187: INFO: Pod "pod-projected-secrets-a896c22f-b820-471f-8d00-d37985c7cd04": Phase="Pending", Reason="", readiness=false. Elapsed: 3.065999ms
Dec  9 07:54:37.191: INFO: Pod "pod-projected-secrets-a896c22f-b820-471f-8d00-d37985c7cd04": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007261364s
Dec  9 07:54:39.190: INFO: Pod "pod-projected-secrets-a896c22f-b820-471f-8d00-d37985c7cd04": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006266922s
STEP: Saw pod success 12/09/22 07:54:39.19
Dec  9 07:54:39.190: INFO: Pod "pod-projected-secrets-a896c22f-b820-471f-8d00-d37985c7cd04" satisfied condition "Succeeded or Failed"
Dec  9 07:54:39.193: INFO: Trying to get logs from node ip-10-0-10-179 pod pod-projected-secrets-a896c22f-b820-471f-8d00-d37985c7cd04 container projected-secret-volume-test: <nil>
STEP: delete the pod 12/09/22 07:54:39.198
Dec  9 07:54:39.207: INFO: Waiting for pod pod-projected-secrets-a896c22f-b820-471f-8d00-d37985c7cd04 to disappear
Dec  9 07:54:39.209: INFO: Pod pod-projected-secrets-a896c22f-b820-471f-8d00-d37985c7cd04 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Dec  9 07:54:39.210: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-5612" for this suite. 12/09/22 07:54:39.213
------------------------------
â€¢ [4.072 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:56

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:54:35.146
    Dec  9 07:54:35.146: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename projected 12/09/22 07:54:35.147
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:54:35.163
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:54:35.168
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:56
    STEP: Creating projection with secret that has name projected-secret-test-689cd0db-2d4b-4564-83a2-9ffa05dd19f3 12/09/22 07:54:35.171
    STEP: Creating a pod to test consume secrets 12/09/22 07:54:35.175
    Dec  9 07:54:35.184: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-a896c22f-b820-471f-8d00-d37985c7cd04" in namespace "projected-5612" to be "Succeeded or Failed"
    Dec  9 07:54:35.187: INFO: Pod "pod-projected-secrets-a896c22f-b820-471f-8d00-d37985c7cd04": Phase="Pending", Reason="", readiness=false. Elapsed: 3.065999ms
    Dec  9 07:54:37.191: INFO: Pod "pod-projected-secrets-a896c22f-b820-471f-8d00-d37985c7cd04": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007261364s
    Dec  9 07:54:39.190: INFO: Pod "pod-projected-secrets-a896c22f-b820-471f-8d00-d37985c7cd04": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006266922s
    STEP: Saw pod success 12/09/22 07:54:39.19
    Dec  9 07:54:39.190: INFO: Pod "pod-projected-secrets-a896c22f-b820-471f-8d00-d37985c7cd04" satisfied condition "Succeeded or Failed"
    Dec  9 07:54:39.193: INFO: Trying to get logs from node ip-10-0-10-179 pod pod-projected-secrets-a896c22f-b820-471f-8d00-d37985c7cd04 container projected-secret-volume-test: <nil>
    STEP: delete the pod 12/09/22 07:54:39.198
    Dec  9 07:54:39.207: INFO: Waiting for pod pod-projected-secrets-a896c22f-b820-471f-8d00-d37985c7cd04 to disappear
    Dec  9 07:54:39.209: INFO: Pod pod-projected-secrets-a896c22f-b820-471f-8d00-d37985c7cd04 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:54:39.210: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-5612" for this suite. 12/09/22 07:54:39.213
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] ResourceQuota
  should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:943
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:54:39.218
Dec  9 07:54:39.218: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename resourcequota 12/09/22 07:54:39.22
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:54:39.237
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:54:39.241
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:943
STEP: Creating a ResourceQuota 12/09/22 07:54:39.246
STEP: Getting a ResourceQuota 12/09/22 07:54:39.25
STEP: Listing all ResourceQuotas with LabelSelector 12/09/22 07:54:39.253
STEP: Patching the ResourceQuota 12/09/22 07:54:39.257
STEP: Deleting a Collection of ResourceQuotas 12/09/22 07:54:39.263
STEP: Verifying the deleted ResourceQuota 12/09/22 07:54:39.272
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Dec  9 07:54:39.275: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-4895" for this suite. 12/09/22 07:54:39.279
------------------------------
â€¢ [0.066 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:943

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:54:39.218
    Dec  9 07:54:39.218: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename resourcequota 12/09/22 07:54:39.22
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:54:39.237
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:54:39.241
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should manage the lifecycle of a ResourceQuota [Conformance]
      test/e2e/apimachinery/resource_quota.go:943
    STEP: Creating a ResourceQuota 12/09/22 07:54:39.246
    STEP: Getting a ResourceQuota 12/09/22 07:54:39.25
    STEP: Listing all ResourceQuotas with LabelSelector 12/09/22 07:54:39.253
    STEP: Patching the ResourceQuota 12/09/22 07:54:39.257
    STEP: Deleting a Collection of ResourceQuotas 12/09/22 07:54:39.263
    STEP: Verifying the deleted ResourceQuota 12/09/22 07:54:39.272
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:54:39.275: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-4895" for this suite. 12/09/22 07:54:39.279
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:426
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:54:39.293
Dec  9 07:54:39.293: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename job 12/09/22 07:54:39.294
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:54:39.308
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:54:39.313
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:426
STEP: Creating a job 12/09/22 07:54:39.316
STEP: Ensuring job reaches completions 12/09/22 07:54:39.326
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Dec  9 07:54:51.330: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-5617" for this suite. 12/09/22 07:54:51.334
------------------------------
â€¢ [SLOW TEST] [12.047 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:426

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:54:39.293
    Dec  9 07:54:39.293: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename job 12/09/22 07:54:39.294
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:54:39.308
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:54:39.313
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
      test/e2e/apps/job.go:426
    STEP: Creating a job 12/09/22 07:54:39.316
    STEP: Ensuring job reaches completions 12/09/22 07:54:39.326
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:54:51.330: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-5617" for this suite. 12/09/22 07:54:51.334
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  test/e2e/apps/statefulset.go:697
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:54:51.34
Dec  9 07:54:51.340: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename statefulset 12/09/22 07:54:51.342
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:54:51.375
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:54:51.388
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-958 12/09/22 07:54:51.398
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  test/e2e/apps/statefulset.go:697
STEP: Creating stateful set ss in namespace statefulset-958 12/09/22 07:54:51.408
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-958 12/09/22 07:54:51.423
Dec  9 07:54:51.427: INFO: Found 0 stateful pods, waiting for 1
Dec  9 07:55:01.431: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod 12/09/22 07:55:01.431
Dec  9 07:55:01.435: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=statefulset-958 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Dec  9 07:55:01.715: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Dec  9 07:55:01.715: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Dec  9 07:55:01.715: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Dec  9 07:55:01.720: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Dec  9 07:55:11.724: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Dec  9 07:55:11.724: INFO: Waiting for statefulset status.replicas updated to 0
Dec  9 07:55:11.749: INFO: POD   NODE            PHASE    GRACE  CONDITIONS
Dec  9 07:55:11.749: INFO: ss-0  ip-10-0-10-179  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-12-09 07:54:51 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-12-09 07:55:01 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-12-09 07:55:01 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-12-09 07:54:51 +0000 UTC  }]
Dec  9 07:55:11.749: INFO: 
Dec  9 07:55:11.749: INFO: StatefulSet ss has not reached scale 3, at 1
Dec  9 07:55:12.756: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.988050177s
Dec  9 07:55:13.762: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.982759782s
Dec  9 07:55:14.767: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.976753015s
Dec  9 07:55:15.771: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.971913709s
Dec  9 07:55:16.776: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.967707724s
Dec  9 07:55:17.780: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.963375482s
Dec  9 07:55:18.787: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.958862998s
Dec  9 07:55:19.792: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.952420962s
Dec  9 07:55:20.796: INFO: Verifying statefulset ss doesn't scale past 3 for another 947.19074ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-958 12/09/22 07:55:21.797
Dec  9 07:55:21.802: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=statefulset-958 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec  9 07:55:22.008: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Dec  9 07:55:22.008: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Dec  9 07:55:22.008: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Dec  9 07:55:22.008: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=statefulset-958 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec  9 07:55:22.194: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Dec  9 07:55:22.194: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Dec  9 07:55:22.194: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Dec  9 07:55:22.194: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=statefulset-958 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec  9 07:55:22.418: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Dec  9 07:55:22.418: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Dec  9 07:55:22.418: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Dec  9 07:55:22.422: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
Dec  9 07:55:32.426: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Dec  9 07:55:32.426: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Dec  9 07:55:32.426: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod 12/09/22 07:55:32.426
Dec  9 07:55:32.429: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=statefulset-958 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Dec  9 07:55:32.623: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Dec  9 07:55:32.623: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Dec  9 07:55:32.623: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Dec  9 07:55:32.623: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=statefulset-958 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Dec  9 07:55:32.817: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Dec  9 07:55:32.817: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Dec  9 07:55:32.817: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Dec  9 07:55:32.817: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=statefulset-958 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Dec  9 07:55:33.006: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Dec  9 07:55:33.006: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Dec  9 07:55:33.006: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Dec  9 07:55:33.006: INFO: Waiting for statefulset status.replicas updated to 0
Dec  9 07:55:33.009: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Dec  9 07:55:43.017: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Dec  9 07:55:43.017: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Dec  9 07:55:43.017: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Dec  9 07:55:43.033: INFO: POD   NODE            PHASE    GRACE  CONDITIONS
Dec  9 07:55:43.033: INFO: ss-0  ip-10-0-10-179  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-12-09 07:54:51 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-12-09 07:55:32 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-12-09 07:55:32 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-12-09 07:54:51 +0000 UTC  }]
Dec  9 07:55:43.033: INFO: ss-1  ip-10-0-17-108  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-12-09 07:55:11 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-12-09 07:55:33 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-12-09 07:55:33 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-12-09 07:55:11 +0000 UTC  }]
Dec  9 07:55:43.033: INFO: ss-2  ip-10-0-10-179  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-12-09 07:55:11 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-12-09 07:55:33 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-12-09 07:55:33 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-12-09 07:55:11 +0000 UTC  }]
Dec  9 07:55:43.033: INFO: 
Dec  9 07:55:43.033: INFO: StatefulSet ss has not reached scale 0, at 3
Dec  9 07:55:44.045: INFO: POD   NODE            PHASE    GRACE  CONDITIONS
Dec  9 07:55:44.045: INFO: ss-1  ip-10-0-17-108  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-12-09 07:55:11 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-12-09 07:55:33 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-12-09 07:55:33 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-12-09 07:55:11 +0000 UTC  }]
Dec  9 07:55:44.045: INFO: ss-2  ip-10-0-10-179  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-12-09 07:55:11 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-12-09 07:55:33 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-12-09 07:55:33 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-12-09 07:55:11 +0000 UTC  }]
Dec  9 07:55:44.045: INFO: 
Dec  9 07:55:44.045: INFO: StatefulSet ss has not reached scale 0, at 2
Dec  9 07:55:45.049: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.981968546s
Dec  9 07:55:46.056: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.978537815s
Dec  9 07:55:47.059: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.971609867s
Dec  9 07:55:48.063: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.968016589s
Dec  9 07:55:49.067: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.963686532s
Dec  9 07:55:50.070: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.960237151s
Dec  9 07:55:51.075: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.956186525s
Dec  9 07:55:52.078: INFO: Verifying statefulset ss doesn't scale past 0 for another 952.777623ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-958 12/09/22 07:55:53.078
Dec  9 07:55:53.081: INFO: Scaling statefulset ss to 0
Dec  9 07:55:53.093: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Dec  9 07:55:53.096: INFO: Deleting all statefulset in ns statefulset-958
Dec  9 07:55:53.099: INFO: Scaling statefulset ss to 0
Dec  9 07:55:53.108: INFO: Waiting for statefulset status.replicas updated to 0
Dec  9 07:55:53.110: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Dec  9 07:55:53.132: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-958" for this suite. 12/09/22 07:55:53.149
------------------------------
â€¢ [SLOW TEST] [61.815 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    test/e2e/apps/statefulset.go:697

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:54:51.34
    Dec  9 07:54:51.340: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename statefulset 12/09/22 07:54:51.342
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:54:51.375
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:54:51.388
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-958 12/09/22 07:54:51.398
    [It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
      test/e2e/apps/statefulset.go:697
    STEP: Creating stateful set ss in namespace statefulset-958 12/09/22 07:54:51.408
    STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-958 12/09/22 07:54:51.423
    Dec  9 07:54:51.427: INFO: Found 0 stateful pods, waiting for 1
    Dec  9 07:55:01.431: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod 12/09/22 07:55:01.431
    Dec  9 07:55:01.435: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=statefulset-958 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Dec  9 07:55:01.715: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Dec  9 07:55:01.715: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Dec  9 07:55:01.715: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Dec  9 07:55:01.720: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
    Dec  9 07:55:11.724: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Dec  9 07:55:11.724: INFO: Waiting for statefulset status.replicas updated to 0
    Dec  9 07:55:11.749: INFO: POD   NODE            PHASE    GRACE  CONDITIONS
    Dec  9 07:55:11.749: INFO: ss-0  ip-10-0-10-179  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-12-09 07:54:51 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-12-09 07:55:01 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-12-09 07:55:01 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-12-09 07:54:51 +0000 UTC  }]
    Dec  9 07:55:11.749: INFO: 
    Dec  9 07:55:11.749: INFO: StatefulSet ss has not reached scale 3, at 1
    Dec  9 07:55:12.756: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.988050177s
    Dec  9 07:55:13.762: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.982759782s
    Dec  9 07:55:14.767: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.976753015s
    Dec  9 07:55:15.771: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.971913709s
    Dec  9 07:55:16.776: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.967707724s
    Dec  9 07:55:17.780: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.963375482s
    Dec  9 07:55:18.787: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.958862998s
    Dec  9 07:55:19.792: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.952420962s
    Dec  9 07:55:20.796: INFO: Verifying statefulset ss doesn't scale past 3 for another 947.19074ms
    STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-958 12/09/22 07:55:21.797
    Dec  9 07:55:21.802: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=statefulset-958 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Dec  9 07:55:22.008: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Dec  9 07:55:22.008: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Dec  9 07:55:22.008: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Dec  9 07:55:22.008: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=statefulset-958 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Dec  9 07:55:22.194: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
    Dec  9 07:55:22.194: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Dec  9 07:55:22.194: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Dec  9 07:55:22.194: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=statefulset-958 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Dec  9 07:55:22.418: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
    Dec  9 07:55:22.418: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Dec  9 07:55:22.418: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Dec  9 07:55:22.422: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
    Dec  9 07:55:32.426: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Dec  9 07:55:32.426: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
    Dec  9 07:55:32.426: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Scale down will not halt with unhealthy stateful pod 12/09/22 07:55:32.426
    Dec  9 07:55:32.429: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=statefulset-958 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Dec  9 07:55:32.623: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Dec  9 07:55:32.623: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Dec  9 07:55:32.623: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Dec  9 07:55:32.623: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=statefulset-958 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Dec  9 07:55:32.817: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Dec  9 07:55:32.817: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Dec  9 07:55:32.817: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Dec  9 07:55:32.817: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=statefulset-958 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Dec  9 07:55:33.006: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Dec  9 07:55:33.006: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Dec  9 07:55:33.006: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Dec  9 07:55:33.006: INFO: Waiting for statefulset status.replicas updated to 0
    Dec  9 07:55:33.009: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
    Dec  9 07:55:43.017: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Dec  9 07:55:43.017: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
    Dec  9 07:55:43.017: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
    Dec  9 07:55:43.033: INFO: POD   NODE            PHASE    GRACE  CONDITIONS
    Dec  9 07:55:43.033: INFO: ss-0  ip-10-0-10-179  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-12-09 07:54:51 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-12-09 07:55:32 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-12-09 07:55:32 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-12-09 07:54:51 +0000 UTC  }]
    Dec  9 07:55:43.033: INFO: ss-1  ip-10-0-17-108  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-12-09 07:55:11 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-12-09 07:55:33 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-12-09 07:55:33 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-12-09 07:55:11 +0000 UTC  }]
    Dec  9 07:55:43.033: INFO: ss-2  ip-10-0-10-179  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-12-09 07:55:11 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-12-09 07:55:33 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-12-09 07:55:33 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-12-09 07:55:11 +0000 UTC  }]
    Dec  9 07:55:43.033: INFO: 
    Dec  9 07:55:43.033: INFO: StatefulSet ss has not reached scale 0, at 3
    Dec  9 07:55:44.045: INFO: POD   NODE            PHASE    GRACE  CONDITIONS
    Dec  9 07:55:44.045: INFO: ss-1  ip-10-0-17-108  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-12-09 07:55:11 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-12-09 07:55:33 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-12-09 07:55:33 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-12-09 07:55:11 +0000 UTC  }]
    Dec  9 07:55:44.045: INFO: ss-2  ip-10-0-10-179  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-12-09 07:55:11 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-12-09 07:55:33 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-12-09 07:55:33 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-12-09 07:55:11 +0000 UTC  }]
    Dec  9 07:55:44.045: INFO: 
    Dec  9 07:55:44.045: INFO: StatefulSet ss has not reached scale 0, at 2
    Dec  9 07:55:45.049: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.981968546s
    Dec  9 07:55:46.056: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.978537815s
    Dec  9 07:55:47.059: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.971609867s
    Dec  9 07:55:48.063: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.968016589s
    Dec  9 07:55:49.067: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.963686532s
    Dec  9 07:55:50.070: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.960237151s
    Dec  9 07:55:51.075: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.956186525s
    Dec  9 07:55:52.078: INFO: Verifying statefulset ss doesn't scale past 0 for another 952.777623ms
    STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-958 12/09/22 07:55:53.078
    Dec  9 07:55:53.081: INFO: Scaling statefulset ss to 0
    Dec  9 07:55:53.093: INFO: Waiting for statefulset status.replicas updated to 0
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Dec  9 07:55:53.096: INFO: Deleting all statefulset in ns statefulset-958
    Dec  9 07:55:53.099: INFO: Scaling statefulset ss to 0
    Dec  9 07:55:53.108: INFO: Waiting for statefulset status.replicas updated to 0
    Dec  9 07:55:53.110: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:55:53.132: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-958" for this suite. 12/09/22 07:55:53.149
  << End Captured GinkgoWriter Output
------------------------------
[sig-network] Service endpoints latency
  should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59
[BeforeEach] [sig-network] Service endpoints latency
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:55:53.157
Dec  9 07:55:53.157: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename svc-latency 12/09/22 07:55:53.158
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:55:53.178
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:55:53.182
[BeforeEach] [sig-network] Service endpoints latency
  test/e2e/framework/metrics/init/init.go:31
[It] should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59
Dec  9 07:55:53.186: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: creating replication controller svc-latency-rc in namespace svc-latency-6592 12/09/22 07:55:53.187
I1209 07:55:53.194575      20 runners.go:193] Created replication controller with name: svc-latency-rc, namespace: svc-latency-6592, replica count: 1
I1209 07:55:54.246318      20 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1209 07:55:55.246601      20 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Dec  9 07:55:55.361: INFO: Created: latency-svc-fgfpr
Dec  9 07:55:55.369: INFO: Got endpoints: latency-svc-fgfpr [21.760851ms]
Dec  9 07:55:55.399: INFO: Created: latency-svc-mm42v
Dec  9 07:55:55.415: INFO: Created: latency-svc-559q2
Dec  9 07:55:55.416: INFO: Got endpoints: latency-svc-mm42v [46.138904ms]
Dec  9 07:55:55.428: INFO: Created: latency-svc-kqf4p
Dec  9 07:55:55.435: INFO: Created: latency-svc-2wjjm
Dec  9 07:55:55.437: INFO: Got endpoints: latency-svc-559q2 [66.953004ms]
Dec  9 07:55:55.463: INFO: Created: latency-svc-kdvvc
Dec  9 07:55:55.465: INFO: Got endpoints: latency-svc-kqf4p [94.289999ms]
Dec  9 07:55:55.467: INFO: Got endpoints: latency-svc-2wjjm [96.419308ms]
Dec  9 07:55:55.468: INFO: Got endpoints: latency-svc-kdvvc [97.934357ms]
Dec  9 07:55:55.469: INFO: Created: latency-svc-9kr8q
Dec  9 07:55:55.495: INFO: Got endpoints: latency-svc-9kr8q [124.459971ms]
Dec  9 07:55:55.498: INFO: Created: latency-svc-lx22l
Dec  9 07:55:55.514: INFO: Got endpoints: latency-svc-lx22l [142.95537ms]
Dec  9 07:55:55.519: INFO: Created: latency-svc-4qzbg
Dec  9 07:55:55.529: INFO: Got endpoints: latency-svc-4qzbg [157.964142ms]
Dec  9 07:55:55.594: INFO: Created: latency-svc-j9gnm
Dec  9 07:55:55.594: INFO: Created: latency-svc-pdjrv
Dec  9 07:55:55.595: INFO: Created: latency-svc-cj968
Dec  9 07:55:55.596: INFO: Created: latency-svc-wh2cf
Dec  9 07:55:55.597: INFO: Created: latency-svc-lj7cf
Dec  9 07:55:55.598: INFO: Created: latency-svc-d54kb
Dec  9 07:55:55.598: INFO: Created: latency-svc-qwsv5
Dec  9 07:55:55.598: INFO: Created: latency-svc-zlnvk
Dec  9 07:55:55.596: INFO: Created: latency-svc-p9qvh
Dec  9 07:55:55.598: INFO: Created: latency-svc-bfzrv
Dec  9 07:55:55.598: INFO: Created: latency-svc-7pq6g
Dec  9 07:55:55.599: INFO: Created: latency-svc-sr7gl
Dec  9 07:55:55.599: INFO: Created: latency-svc-89ss9
Dec  9 07:55:55.599: INFO: Created: latency-svc-5xps6
Dec  9 07:55:55.605: INFO: Created: latency-svc-k7zdg
Dec  9 07:55:55.671: INFO: Got endpoints: latency-svc-7pq6g [299.79813ms]
Dec  9 07:55:55.672: INFO: Got endpoints: latency-svc-pdjrv [176.733843ms]
Dec  9 07:55:55.673: INFO: Got endpoints: latency-svc-k7zdg [143.610124ms]
Dec  9 07:55:55.674: INFO: Got endpoints: latency-svc-bfzrv [302.752433ms]
Dec  9 07:55:55.674: INFO: Got endpoints: latency-svc-sr7gl [160.247763ms]
Dec  9 07:55:55.719: INFO: Got endpoints: latency-svc-p9qvh [346.822172ms]
Dec  9 07:55:55.720: INFO: Got endpoints: latency-svc-wh2cf [346.82052ms]
Dec  9 07:55:55.720: INFO: Got endpoints: latency-svc-89ss9 [348.496075ms]
Dec  9 07:55:55.720: INFO: Got endpoints: latency-svc-lj7cf [346.716412ms]
Dec  9 07:55:55.721: INFO: Got endpoints: latency-svc-5xps6 [348.259669ms]
Dec  9 07:55:55.743: INFO: Created: latency-svc-7v24p
Dec  9 07:55:55.744: INFO: Got endpoints: latency-svc-j9gnm [275.350312ms]
Dec  9 07:55:55.746: INFO: Got endpoints: latency-svc-d54kb [325.65433ms]
Dec  9 07:55:55.753: INFO: Got endpoints: latency-svc-cj968 [285.461579ms]
Dec  9 07:55:55.754: INFO: Got endpoints: latency-svc-zlnvk [286.363705ms]
Dec  9 07:55:55.754: INFO: Got endpoints: latency-svc-qwsv5 [317.071819ms]
Dec  9 07:55:55.761: INFO: Got endpoints: latency-svc-7v24p [88.712216ms]
Dec  9 07:55:55.766: INFO: Created: latency-svc-smpk2
Dec  9 07:55:55.781: INFO: Created: latency-svc-j8k6p
Dec  9 07:55:55.784: INFO: Got endpoints: latency-svc-smpk2 [111.743976ms]
Dec  9 07:55:55.796: INFO: Got endpoints: latency-svc-j8k6p [122.356498ms]
Dec  9 07:55:55.863: INFO: Created: latency-svc-crjqx
Dec  9 07:55:55.875: INFO: Created: latency-svc-cttr2
Dec  9 07:55:55.881: INFO: Created: latency-svc-r7ltb
Dec  9 07:55:55.882: INFO: Created: latency-svc-nqdhk
Dec  9 07:55:55.884: INFO: Created: latency-svc-xq6jm
Dec  9 07:55:55.886: INFO: Created: latency-svc-882l9
Dec  9 07:55:55.894: INFO: Created: latency-svc-gxz6r
Dec  9 07:55:55.898: INFO: Created: latency-svc-gh8w8
Dec  9 07:55:55.919: INFO: Created: latency-svc-p2nbc
Dec  9 07:55:55.920: INFO: Created: latency-svc-5pndz
Dec  9 07:55:55.922: INFO: Created: latency-svc-z6m82
Dec  9 07:55:55.922: INFO: Created: latency-svc-rmt69
Dec  9 07:55:55.923: INFO: Created: latency-svc-95ncs
Dec  9 07:55:55.924: INFO: Created: latency-svc-jr54z
Dec  9 07:55:55.929: INFO: Created: latency-svc-w26kc
Dec  9 07:55:55.971: INFO: Got endpoints: latency-svc-882l9 [173.539278ms]
Dec  9 07:55:55.974: INFO: Got endpoints: latency-svc-xq6jm [300.272008ms]
Dec  9 07:55:55.973: INFO: Got endpoints: latency-svc-crjqx [299.621352ms]
Dec  9 07:55:55.973: INFO: Got endpoints: latency-svc-gh8w8 [251.082509ms]
Dec  9 07:55:55.973: INFO: Got endpoints: latency-svc-nqdhk [187.914002ms]
Dec  9 07:55:56.010: INFO: Got endpoints: latency-svc-p2nbc [289.160819ms]
Dec  9 07:55:56.011: INFO: Got endpoints: latency-svc-gxz6r [280.180449ms]
Dec  9 07:55:56.012: INFO: Got endpoints: latency-svc-jr54z [267.781804ms]
Dec  9 07:55:56.012: INFO: Got endpoints: latency-svc-5pndz [277.862975ms]
Dec  9 07:55:56.013: INFO: Got endpoints: latency-svc-z6m82 [278.023147ms]
Dec  9 07:55:56.050: INFO: Got endpoints: latency-svc-95ncs [299.446823ms]
Dec  9 07:55:56.055: INFO: Created: latency-svc-zh798
Dec  9 07:55:56.058: INFO: Got endpoints: latency-svc-cttr2 [296.54554ms]
Dec  9 07:55:56.058: INFO: Got endpoints: latency-svc-w26kc [304.077474ms]
Dec  9 07:55:56.058: INFO: Got endpoints: latency-svc-r7ltb [305.015448ms]
Dec  9 07:55:56.058: INFO: Got endpoints: latency-svc-rmt69 [303.859011ms]
Dec  9 07:55:56.072: INFO: Got endpoints: latency-svc-zh798 [99.211574ms]
Dec  9 07:55:56.072: INFO: Created: latency-svc-ccrsx
Dec  9 07:55:56.079: INFO: Created: latency-svc-574nl
Dec  9 07:55:56.086: INFO: Got endpoints: latency-svc-ccrsx [110.036477ms]
Dec  9 07:55:56.122: INFO: Got endpoints: latency-svc-574nl [147.610136ms]
Dec  9 07:55:56.152: INFO: Created: latency-svc-mx7wz
Dec  9 07:55:56.158: INFO: Created: latency-svc-cjwx2
Dec  9 07:55:56.161: INFO: Created: latency-svc-pp7qh
Dec  9 07:55:56.162: INFO: Created: latency-svc-z4zjk
Dec  9 07:55:56.172: INFO: Created: latency-svc-jbqfk
Dec  9 07:55:56.165: INFO: Created: latency-svc-tq86f
Dec  9 07:55:56.166: INFO: Created: latency-svc-5jkzj
Dec  9 07:55:56.167: INFO: Created: latency-svc-qx4xm
Dec  9 07:55:56.167: INFO: Created: latency-svc-t6tcw
Dec  9 07:55:56.168: INFO: Created: latency-svc-d84ph
Dec  9 07:55:56.167: INFO: Created: latency-svc-r9pp4
Dec  9 07:55:56.170: INFO: Created: latency-svc-68ql7
Dec  9 07:55:56.170: INFO: Created: latency-svc-7b9sp
Dec  9 07:55:56.172: INFO: Created: latency-svc-98gpd
Dec  9 07:55:56.173: INFO: Created: latency-svc-lvt4f
Dec  9 07:55:56.232: INFO: Got endpoints: latency-svc-z4zjk [219.24754ms]
Dec  9 07:55:56.233: INFO: Got endpoints: latency-svc-mx7wz [174.727806ms]
Dec  9 07:55:56.282: INFO: Got endpoints: latency-svc-t6tcw [158.411838ms]
Dec  9 07:55:56.286: INFO: Created: latency-svc-9984m
Dec  9 07:55:56.301: INFO: Created: latency-svc-2vx8x
Dec  9 07:55:56.306: INFO: Created: latency-svc-pfjlj
Dec  9 07:55:56.328: INFO: Got endpoints: latency-svc-r9pp4 [267.410847ms]
Dec  9 07:55:56.343: INFO: Created: latency-svc-9jcv4
Dec  9 07:55:56.399: INFO: Got endpoints: latency-svc-68ql7 [339.966146ms]
Dec  9 07:55:56.414: INFO: Created: latency-svc-ghvsm
Dec  9 07:55:56.424: INFO: Got endpoints: latency-svc-jbqfk [364.757525ms]
Dec  9 07:55:56.439: INFO: Created: latency-svc-5ppvq
Dec  9 07:55:56.467: INFO: Got endpoints: latency-svc-lvt4f [411.669877ms]
Dec  9 07:55:56.478: INFO: Created: latency-svc-dz2j2
Dec  9 07:55:56.516: INFO: Got endpoints: latency-svc-tq86f [443.253604ms]
Dec  9 07:55:56.528: INFO: Created: latency-svc-jgfzm
Dec  9 07:55:56.569: INFO: Got endpoints: latency-svc-5jkzj [481.962572ms]
Dec  9 07:55:56.578: INFO: Created: latency-svc-mh92g
Dec  9 07:55:56.619: INFO: Got endpoints: latency-svc-qx4xm [643.984563ms]
Dec  9 07:55:56.628: INFO: Created: latency-svc-fps2f
Dec  9 07:55:56.668: INFO: Got endpoints: latency-svc-d84ph [689.765216ms]
Dec  9 07:55:56.681: INFO: Created: latency-svc-kf8g9
Dec  9 07:55:56.718: INFO: Got endpoints: latency-svc-7b9sp [706.867601ms]
Dec  9 07:55:56.730: INFO: Created: latency-svc-bw46h
Dec  9 07:55:56.769: INFO: Got endpoints: latency-svc-98gpd [755.058974ms]
Dec  9 07:55:56.781: INFO: Created: latency-svc-kmlnj
Dec  9 07:55:56.821: INFO: Got endpoints: latency-svc-cjwx2 [808.034065ms]
Dec  9 07:55:56.834: INFO: Created: latency-svc-bsx4w
Dec  9 07:55:56.869: INFO: Got endpoints: latency-svc-pp7qh [855.490999ms]
Dec  9 07:55:56.888: INFO: Created: latency-svc-s8p68
Dec  9 07:55:56.920: INFO: Got endpoints: latency-svc-9984m [687.893239ms]
Dec  9 07:55:56.932: INFO: Created: latency-svc-ns84s
Dec  9 07:55:56.970: INFO: Got endpoints: latency-svc-2vx8x [737.016869ms]
Dec  9 07:55:56.983: INFO: Created: latency-svc-jn9tv
Dec  9 07:55:57.017: INFO: Got endpoints: latency-svc-pfjlj [735.657349ms]
Dec  9 07:55:57.031: INFO: Created: latency-svc-ktpcl
Dec  9 07:55:57.069: INFO: Got endpoints: latency-svc-9jcv4 [741.38271ms]
Dec  9 07:55:57.081: INFO: Created: latency-svc-sjknm
Dec  9 07:55:57.121: INFO: Got endpoints: latency-svc-ghvsm [721.40526ms]
Dec  9 07:55:57.130: INFO: Created: latency-svc-j22s9
Dec  9 07:55:57.167: INFO: Got endpoints: latency-svc-5ppvq [739.768402ms]
Dec  9 07:55:57.182: INFO: Created: latency-svc-fdnlk
Dec  9 07:55:57.219: INFO: Got endpoints: latency-svc-dz2j2 [751.708233ms]
Dec  9 07:55:57.231: INFO: Created: latency-svc-9c6rd
Dec  9 07:55:57.268: INFO: Got endpoints: latency-svc-jgfzm [752.409695ms]
Dec  9 07:55:57.281: INFO: Created: latency-svc-7krhs
Dec  9 07:55:57.319: INFO: Got endpoints: latency-svc-mh92g [750.646064ms]
Dec  9 07:55:57.329: INFO: Created: latency-svc-mcdv6
Dec  9 07:55:57.369: INFO: Got endpoints: latency-svc-fps2f [749.353064ms]
Dec  9 07:55:57.379: INFO: Created: latency-svc-dqd9q
Dec  9 07:55:57.418: INFO: Got endpoints: latency-svc-kf8g9 [749.76015ms]
Dec  9 07:55:57.428: INFO: Created: latency-svc-pfr47
Dec  9 07:55:57.468: INFO: Got endpoints: latency-svc-bw46h [749.463644ms]
Dec  9 07:55:57.478: INFO: Created: latency-svc-xnbrz
Dec  9 07:55:57.518: INFO: Got endpoints: latency-svc-kmlnj [748.412051ms]
Dec  9 07:55:57.528: INFO: Created: latency-svc-48p8q
Dec  9 07:55:57.568: INFO: Got endpoints: latency-svc-bsx4w [744.788148ms]
Dec  9 07:55:57.577: INFO: Created: latency-svc-22984
Dec  9 07:55:57.621: INFO: Got endpoints: latency-svc-s8p68 [751.583516ms]
Dec  9 07:55:57.632: INFO: Created: latency-svc-4bcck
Dec  9 07:55:57.668: INFO: Got endpoints: latency-svc-ns84s [747.164902ms]
Dec  9 07:55:57.682: INFO: Created: latency-svc-4fzjh
Dec  9 07:55:57.718: INFO: Got endpoints: latency-svc-jn9tv [747.505951ms]
Dec  9 07:55:57.729: INFO: Created: latency-svc-gsqm5
Dec  9 07:55:57.772: INFO: Got endpoints: latency-svc-ktpcl [754.927078ms]
Dec  9 07:55:57.782: INFO: Created: latency-svc-n94kz
Dec  9 07:55:57.819: INFO: Got endpoints: latency-svc-sjknm [749.202601ms]
Dec  9 07:55:57.828: INFO: Created: latency-svc-8pjrm
Dec  9 07:55:57.868: INFO: Got endpoints: latency-svc-j22s9 [746.76878ms]
Dec  9 07:55:57.880: INFO: Created: latency-svc-mnkxg
Dec  9 07:55:57.920: INFO: Got endpoints: latency-svc-fdnlk [746.937086ms]
Dec  9 07:55:57.930: INFO: Created: latency-svc-8bf9z
Dec  9 07:55:57.970: INFO: Got endpoints: latency-svc-9c6rd [749.963416ms]
Dec  9 07:55:57.981: INFO: Created: latency-svc-7pbf2
Dec  9 07:55:58.017: INFO: Got endpoints: latency-svc-7krhs [748.293884ms]
Dec  9 07:55:58.029: INFO: Created: latency-svc-mkzbm
Dec  9 07:55:58.067: INFO: Got endpoints: latency-svc-mcdv6 [747.149154ms]
Dec  9 07:55:58.083: INFO: Created: latency-svc-lhkv2
Dec  9 07:55:58.122: INFO: Got endpoints: latency-svc-dqd9q [752.740521ms]
Dec  9 07:55:58.132: INFO: Created: latency-svc-sl52x
Dec  9 07:55:58.168: INFO: Got endpoints: latency-svc-pfr47 [749.673739ms]
Dec  9 07:55:58.190: INFO: Created: latency-svc-vzfdd
Dec  9 07:55:58.224: INFO: Got endpoints: latency-svc-xnbrz [756.420699ms]
Dec  9 07:55:58.250: INFO: Created: latency-svc-lztzx
Dec  9 07:55:58.275: INFO: Got endpoints: latency-svc-48p8q [756.965623ms]
Dec  9 07:55:58.295: INFO: Created: latency-svc-4pbr4
Dec  9 07:55:58.322: INFO: Got endpoints: latency-svc-22984 [753.850757ms]
Dec  9 07:55:58.333: INFO: Created: latency-svc-zgd6z
Dec  9 07:55:58.366: INFO: Got endpoints: latency-svc-4bcck [744.540288ms]
Dec  9 07:55:58.382: INFO: Created: latency-svc-9jvfr
Dec  9 07:55:58.419: INFO: Got endpoints: latency-svc-4fzjh [750.532897ms]
Dec  9 07:55:58.431: INFO: Created: latency-svc-pjv9b
Dec  9 07:55:58.470: INFO: Got endpoints: latency-svc-gsqm5 [751.887724ms]
Dec  9 07:55:58.481: INFO: Created: latency-svc-hzjwh
Dec  9 07:55:58.520: INFO: Got endpoints: latency-svc-n94kz [747.068502ms]
Dec  9 07:55:58.535: INFO: Created: latency-svc-szhbf
Dec  9 07:55:58.568: INFO: Got endpoints: latency-svc-8pjrm [749.754731ms]
Dec  9 07:55:58.583: INFO: Created: latency-svc-5gzs9
Dec  9 07:55:58.620: INFO: Got endpoints: latency-svc-mnkxg [752.542842ms]
Dec  9 07:55:58.635: INFO: Created: latency-svc-8x75c
Dec  9 07:55:58.668: INFO: Got endpoints: latency-svc-8bf9z [747.157171ms]
Dec  9 07:55:58.679: INFO: Created: latency-svc-jjzqz
Dec  9 07:55:58.719: INFO: Got endpoints: latency-svc-7pbf2 [749.399376ms]
Dec  9 07:55:58.730: INFO: Created: latency-svc-g829v
Dec  9 07:55:58.775: INFO: Got endpoints: latency-svc-mkzbm [757.20379ms]
Dec  9 07:55:58.785: INFO: Created: latency-svc-wznqq
Dec  9 07:55:58.821: INFO: Got endpoints: latency-svc-lhkv2 [750.42932ms]
Dec  9 07:55:58.832: INFO: Created: latency-svc-x2zqb
Dec  9 07:55:58.866: INFO: Got endpoints: latency-svc-sl52x [744.50891ms]
Dec  9 07:55:58.882: INFO: Created: latency-svc-z8t4p
Dec  9 07:55:58.918: INFO: Got endpoints: latency-svc-vzfdd [749.532163ms]
Dec  9 07:55:58.931: INFO: Created: latency-svc-sr2qj
Dec  9 07:55:58.975: INFO: Got endpoints: latency-svc-lztzx [751.148131ms]
Dec  9 07:55:58.989: INFO: Created: latency-svc-tz9gw
Dec  9 07:55:59.068: INFO: Got endpoints: latency-svc-4pbr4 [792.761681ms]
Dec  9 07:55:59.081: INFO: Created: latency-svc-gtw5w
Dec  9 07:55:59.118: INFO: Got endpoints: latency-svc-zgd6z [796.224833ms]
Dec  9 07:55:59.132: INFO: Created: latency-svc-2kjzx
Dec  9 07:55:59.166: INFO: Got endpoints: latency-svc-9jvfr [800.238132ms]
Dec  9 07:55:59.180: INFO: Created: latency-svc-l4lmr
Dec  9 07:55:59.215: INFO: Got endpoints: latency-svc-pjv9b [796.654613ms]
Dec  9 07:55:59.228: INFO: Created: latency-svc-w55nd
Dec  9 07:55:59.267: INFO: Got endpoints: latency-svc-hzjwh [796.726387ms]
Dec  9 07:55:59.278: INFO: Created: latency-svc-v8lz2
Dec  9 07:55:59.318: INFO: Got endpoints: latency-svc-szhbf [797.958087ms]
Dec  9 07:55:59.328: INFO: Created: latency-svc-j7786
Dec  9 07:55:59.369: INFO: Got endpoints: latency-svc-5gzs9 [800.078324ms]
Dec  9 07:55:59.381: INFO: Created: latency-svc-657fg
Dec  9 07:55:59.417: INFO: Got endpoints: latency-svc-8x75c [796.791998ms]
Dec  9 07:55:59.430: INFO: Created: latency-svc-gfbcd
Dec  9 07:55:59.469: INFO: Got endpoints: latency-svc-jjzqz [801.593705ms]
Dec  9 07:55:59.482: INFO: Created: latency-svc-fnslv
Dec  9 07:55:59.520: INFO: Got endpoints: latency-svc-g829v [800.065976ms]
Dec  9 07:55:59.530: INFO: Created: latency-svc-xhrtc
Dec  9 07:55:59.568: INFO: Got endpoints: latency-svc-wznqq [792.934889ms]
Dec  9 07:55:59.580: INFO: Created: latency-svc-2pw68
Dec  9 07:55:59.618: INFO: Got endpoints: latency-svc-x2zqb [796.547207ms]
Dec  9 07:55:59.631: INFO: Created: latency-svc-dzkhl
Dec  9 07:55:59.667: INFO: Got endpoints: latency-svc-z8t4p [800.77319ms]
Dec  9 07:55:59.681: INFO: Created: latency-svc-nj4zc
Dec  9 07:55:59.720: INFO: Got endpoints: latency-svc-sr2qj [801.015681ms]
Dec  9 07:55:59.731: INFO: Created: latency-svc-s9gcp
Dec  9 07:55:59.768: INFO: Got endpoints: latency-svc-tz9gw [792.192273ms]
Dec  9 07:55:59.779: INFO: Created: latency-svc-xqrvc
Dec  9 07:55:59.819: INFO: Got endpoints: latency-svc-gtw5w [750.862394ms]
Dec  9 07:55:59.830: INFO: Created: latency-svc-6kfrw
Dec  9 07:55:59.867: INFO: Got endpoints: latency-svc-2kjzx [748.789977ms]
Dec  9 07:55:59.893: INFO: Created: latency-svc-phnp5
Dec  9 07:55:59.918: INFO: Got endpoints: latency-svc-l4lmr [751.27669ms]
Dec  9 07:55:59.930: INFO: Created: latency-svc-bx9kd
Dec  9 07:55:59.969: INFO: Got endpoints: latency-svc-w55nd [752.575737ms]
Dec  9 07:55:59.984: INFO: Created: latency-svc-wrhpx
Dec  9 07:56:00.020: INFO: Got endpoints: latency-svc-v8lz2 [750.894097ms]
Dec  9 07:56:00.036: INFO: Created: latency-svc-xvn4n
Dec  9 07:56:00.067: INFO: Got endpoints: latency-svc-j7786 [749.00773ms]
Dec  9 07:56:00.081: INFO: Created: latency-svc-lpfq7
Dec  9 07:56:00.125: INFO: Got endpoints: latency-svc-657fg [755.490424ms]
Dec  9 07:56:00.149: INFO: Created: latency-svc-b675t
Dec  9 07:56:00.173: INFO: Got endpoints: latency-svc-gfbcd [754.880664ms]
Dec  9 07:56:00.185: INFO: Created: latency-svc-7rxnl
Dec  9 07:56:00.221: INFO: Got endpoints: latency-svc-fnslv [750.920023ms]
Dec  9 07:56:00.234: INFO: Created: latency-svc-h4ng8
Dec  9 07:56:00.269: INFO: Got endpoints: latency-svc-xhrtc [749.429705ms]
Dec  9 07:56:00.282: INFO: Created: latency-svc-pm6qx
Dec  9 07:56:00.321: INFO: Got endpoints: latency-svc-2pw68 [753.49522ms]
Dec  9 07:56:00.334: INFO: Created: latency-svc-m5j5r
Dec  9 07:56:00.368: INFO: Got endpoints: latency-svc-dzkhl [749.922763ms]
Dec  9 07:56:00.379: INFO: Created: latency-svc-88rdd
Dec  9 07:56:00.422: INFO: Got endpoints: latency-svc-nj4zc [753.998345ms]
Dec  9 07:56:00.434: INFO: Created: latency-svc-nr4wn
Dec  9 07:56:00.468: INFO: Got endpoints: latency-svc-s9gcp [748.344377ms]
Dec  9 07:56:00.481: INFO: Created: latency-svc-knx57
Dec  9 07:56:00.518: INFO: Got endpoints: latency-svc-xqrvc [750.374266ms]
Dec  9 07:56:00.529: INFO: Created: latency-svc-9wz6t
Dec  9 07:56:00.568: INFO: Got endpoints: latency-svc-6kfrw [749.17672ms]
Dec  9 07:56:00.581: INFO: Created: latency-svc-pv6x8
Dec  9 07:56:00.621: INFO: Got endpoints: latency-svc-phnp5 [753.663358ms]
Dec  9 07:56:00.632: INFO: Created: latency-svc-mlbqs
Dec  9 07:56:00.671: INFO: Got endpoints: latency-svc-bx9kd [752.85379ms]
Dec  9 07:56:00.681: INFO: Created: latency-svc-sjhtg
Dec  9 07:56:00.719: INFO: Got endpoints: latency-svc-wrhpx [749.640372ms]
Dec  9 07:56:00.729: INFO: Created: latency-svc-42wq4
Dec  9 07:56:00.767: INFO: Got endpoints: latency-svc-xvn4n [747.303219ms]
Dec  9 07:56:00.779: INFO: Created: latency-svc-fwr47
Dec  9 07:56:00.817: INFO: Got endpoints: latency-svc-lpfq7 [749.367843ms]
Dec  9 07:56:00.828: INFO: Created: latency-svc-st6l9
Dec  9 07:56:00.871: INFO: Got endpoints: latency-svc-b675t [746.267514ms]
Dec  9 07:56:00.881: INFO: Created: latency-svc-drgx7
Dec  9 07:56:00.918: INFO: Got endpoints: latency-svc-7rxnl [744.400559ms]
Dec  9 07:56:00.938: INFO: Created: latency-svc-h6nl7
Dec  9 07:56:00.967: INFO: Got endpoints: latency-svc-h4ng8 [745.543998ms]
Dec  9 07:56:00.979: INFO: Created: latency-svc-vxwv2
Dec  9 07:56:01.020: INFO: Got endpoints: latency-svc-pm6qx [750.364352ms]
Dec  9 07:56:01.034: INFO: Created: latency-svc-fprsw
Dec  9 07:56:01.074: INFO: Got endpoints: latency-svc-m5j5r [752.252239ms]
Dec  9 07:56:01.085: INFO: Created: latency-svc-pfhww
Dec  9 07:56:01.118: INFO: Got endpoints: latency-svc-88rdd [749.347451ms]
Dec  9 07:56:01.132: INFO: Created: latency-svc-6hqf9
Dec  9 07:56:01.165: INFO: Got endpoints: latency-svc-nr4wn [743.554719ms]
Dec  9 07:56:01.176: INFO: Created: latency-svc-279sj
Dec  9 07:56:01.218: INFO: Got endpoints: latency-svc-knx57 [749.3922ms]
Dec  9 07:56:01.229: INFO: Created: latency-svc-5s2xn
Dec  9 07:56:01.270: INFO: Got endpoints: latency-svc-9wz6t [751.729386ms]
Dec  9 07:56:01.281: INFO: Created: latency-svc-f6jx4
Dec  9 07:56:01.321: INFO: Got endpoints: latency-svc-pv6x8 [751.106407ms]
Dec  9 07:56:01.336: INFO: Created: latency-svc-p6zfv
Dec  9 07:56:01.369: INFO: Got endpoints: latency-svc-mlbqs [748.232717ms]
Dec  9 07:56:01.383: INFO: Created: latency-svc-jwczb
Dec  9 07:56:01.419: INFO: Got endpoints: latency-svc-sjhtg [747.649999ms]
Dec  9 07:56:01.430: INFO: Created: latency-svc-4xtd6
Dec  9 07:56:01.468: INFO: Got endpoints: latency-svc-42wq4 [749.202493ms]
Dec  9 07:56:01.482: INFO: Created: latency-svc-6mzhq
Dec  9 07:56:01.517: INFO: Got endpoints: latency-svc-fwr47 [749.387171ms]
Dec  9 07:56:01.532: INFO: Created: latency-svc-fcwht
Dec  9 07:56:01.569: INFO: Got endpoints: latency-svc-st6l9 [752.101948ms]
Dec  9 07:56:01.581: INFO: Created: latency-svc-xn5dp
Dec  9 07:56:01.618: INFO: Got endpoints: latency-svc-drgx7 [746.408908ms]
Dec  9 07:56:01.631: INFO: Created: latency-svc-fgws6
Dec  9 07:56:01.669: INFO: Got endpoints: latency-svc-h6nl7 [750.583266ms]
Dec  9 07:56:01.682: INFO: Created: latency-svc-cnzc2
Dec  9 07:56:01.726: INFO: Got endpoints: latency-svc-vxwv2 [758.482005ms]
Dec  9 07:56:01.738: INFO: Created: latency-svc-rnr2n
Dec  9 07:56:01.771: INFO: Got endpoints: latency-svc-fprsw [751.161221ms]
Dec  9 07:56:01.784: INFO: Created: latency-svc-flzn6
Dec  9 07:56:01.819: INFO: Got endpoints: latency-svc-pfhww [744.700246ms]
Dec  9 07:56:01.834: INFO: Created: latency-svc-qt92w
Dec  9 07:56:01.868: INFO: Got endpoints: latency-svc-6hqf9 [750.514068ms]
Dec  9 07:56:01.881: INFO: Created: latency-svc-dzbq9
Dec  9 07:56:01.919: INFO: Got endpoints: latency-svc-279sj [752.899367ms]
Dec  9 07:56:01.933: INFO: Created: latency-svc-mrwk4
Dec  9 07:56:01.973: INFO: Got endpoints: latency-svc-5s2xn [754.277364ms]
Dec  9 07:56:01.988: INFO: Created: latency-svc-sqj5c
Dec  9 07:56:02.022: INFO: Got endpoints: latency-svc-f6jx4 [751.613703ms]
Dec  9 07:56:02.035: INFO: Created: latency-svc-zkqzf
Dec  9 07:56:02.069: INFO: Got endpoints: latency-svc-p6zfv [748.430885ms]
Dec  9 07:56:02.099: INFO: Created: latency-svc-6zh8m
Dec  9 07:56:02.142: INFO: Got endpoints: latency-svc-jwczb [772.731651ms]
Dec  9 07:56:02.156: INFO: Created: latency-svc-c2hfn
Dec  9 07:56:02.176: INFO: Got endpoints: latency-svc-4xtd6 [757.414582ms]
Dec  9 07:56:02.194: INFO: Created: latency-svc-n2v8m
Dec  9 07:56:02.221: INFO: Got endpoints: latency-svc-6mzhq [752.584578ms]
Dec  9 07:56:02.237: INFO: Created: latency-svc-d5j6k
Dec  9 07:56:02.271: INFO: Got endpoints: latency-svc-fcwht [753.491531ms]
Dec  9 07:56:02.287: INFO: Created: latency-svc-xzt77
Dec  9 07:56:02.323: INFO: Got endpoints: latency-svc-xn5dp [753.692758ms]
Dec  9 07:56:02.348: INFO: Created: latency-svc-d6vfw
Dec  9 07:56:02.373: INFO: Got endpoints: latency-svc-fgws6 [754.486953ms]
Dec  9 07:56:02.400: INFO: Created: latency-svc-9wms9
Dec  9 07:56:02.426: INFO: Got endpoints: latency-svc-cnzc2 [756.690572ms]
Dec  9 07:56:02.438: INFO: Created: latency-svc-89rh8
Dec  9 07:56:02.479: INFO: Got endpoints: latency-svc-rnr2n [753.020324ms]
Dec  9 07:56:02.498: INFO: Created: latency-svc-ncxfg
Dec  9 07:56:02.528: INFO: Got endpoints: latency-svc-flzn6 [756.56297ms]
Dec  9 07:56:02.559: INFO: Created: latency-svc-q5m6w
Dec  9 07:56:02.578: INFO: Got endpoints: latency-svc-qt92w [758.321411ms]
Dec  9 07:56:02.604: INFO: Created: latency-svc-wrjl8
Dec  9 07:56:02.624: INFO: Got endpoints: latency-svc-dzbq9 [754.565885ms]
Dec  9 07:56:02.638: INFO: Created: latency-svc-tsg2x
Dec  9 07:56:02.685: INFO: Got endpoints: latency-svc-mrwk4 [766.114716ms]
Dec  9 07:56:02.709: INFO: Created: latency-svc-tlfj8
Dec  9 07:56:02.725: INFO: Got endpoints: latency-svc-sqj5c [752.311964ms]
Dec  9 07:56:02.749: INFO: Created: latency-svc-vtxsp
Dec  9 07:56:02.793: INFO: Got endpoints: latency-svc-zkqzf [770.931669ms]
Dec  9 07:56:02.831: INFO: Created: latency-svc-4l5bm
Dec  9 07:56:02.847: INFO: Got endpoints: latency-svc-6zh8m [777.641512ms]
Dec  9 07:56:02.884: INFO: Got endpoints: latency-svc-c2hfn [741.426394ms]
Dec  9 07:56:02.905: INFO: Created: latency-svc-xgzf9
Dec  9 07:56:02.961: INFO: Got endpoints: latency-svc-n2v8m [784.327226ms]
Dec  9 07:56:02.988: INFO: Created: latency-svc-dtwnl
Dec  9 07:56:02.989: INFO: Got endpoints: latency-svc-d5j6k [767.810728ms]
Dec  9 07:56:03.029: INFO: Created: latency-svc-hqr5s
Dec  9 07:56:03.057: INFO: Created: latency-svc-5dknc
Dec  9 07:56:03.058: INFO: Got endpoints: latency-svc-xzt77 [786.29583ms]
Dec  9 07:56:03.114: INFO: Got endpoints: latency-svc-d6vfw [789.905213ms]
Dec  9 07:56:03.116: INFO: Created: latency-svc-qcscw
Dec  9 07:56:03.118: INFO: Got endpoints: latency-svc-9wms9 [745.285058ms]
Dec  9 07:56:03.135: INFO: Created: latency-svc-66jv4
Dec  9 07:56:03.182: INFO: Created: latency-svc-9nmjf
Dec  9 07:56:03.184: INFO: Got endpoints: latency-svc-89rh8 [757.956617ms]
Dec  9 07:56:03.205: INFO: Created: latency-svc-v6mrw
Dec  9 07:56:03.220: INFO: Got endpoints: latency-svc-ncxfg [740.781652ms]
Dec  9 07:56:03.235: INFO: Created: latency-svc-ldm8c
Dec  9 07:56:03.272: INFO: Got endpoints: latency-svc-q5m6w [743.729367ms]
Dec  9 07:56:03.320: INFO: Got endpoints: latency-svc-wrjl8 [740.292719ms]
Dec  9 07:56:03.372: INFO: Got endpoints: latency-svc-tsg2x [747.864472ms]
Dec  9 07:56:03.418: INFO: Got endpoints: latency-svc-tlfj8 [732.922934ms]
Dec  9 07:56:03.475: INFO: Got endpoints: latency-svc-vtxsp [748.453924ms]
Dec  9 07:56:03.518: INFO: Got endpoints: latency-svc-4l5bm [724.23261ms]
Dec  9 07:56:03.569: INFO: Got endpoints: latency-svc-xgzf9 [721.438635ms]
Dec  9 07:56:03.622: INFO: Got endpoints: latency-svc-dtwnl [737.501617ms]
Dec  9 07:56:03.670: INFO: Got endpoints: latency-svc-hqr5s [707.518601ms]
Dec  9 07:56:03.717: INFO: Got endpoints: latency-svc-5dknc [727.258708ms]
Dec  9 07:56:03.768: INFO: Got endpoints: latency-svc-qcscw [710.546986ms]
Dec  9 07:56:03.821: INFO: Got endpoints: latency-svc-66jv4 [706.836583ms]
Dec  9 07:56:03.872: INFO: Got endpoints: latency-svc-9nmjf [724.079405ms]
Dec  9 07:56:03.917: INFO: Got endpoints: latency-svc-v6mrw [733.025381ms]
Dec  9 07:56:03.967: INFO: Got endpoints: latency-svc-ldm8c [746.114421ms]
Dec  9 07:56:03.967: INFO: Latencies: [46.138904ms 66.953004ms 88.712216ms 94.289999ms 96.419308ms 97.934357ms 99.211574ms 110.036477ms 111.743976ms 122.356498ms 124.459971ms 142.95537ms 143.610124ms 147.610136ms 157.964142ms 158.411838ms 160.247763ms 173.539278ms 174.727806ms 176.733843ms 187.914002ms 219.24754ms 251.082509ms 267.410847ms 267.781804ms 275.350312ms 277.862975ms 278.023147ms 280.180449ms 285.461579ms 286.363705ms 289.160819ms 296.54554ms 299.446823ms 299.621352ms 299.79813ms 300.272008ms 302.752433ms 303.859011ms 304.077474ms 305.015448ms 317.071819ms 325.65433ms 339.966146ms 346.716412ms 346.82052ms 346.822172ms 348.259669ms 348.496075ms 364.757525ms 411.669877ms 443.253604ms 481.962572ms 643.984563ms 687.893239ms 689.765216ms 706.836583ms 706.867601ms 707.518601ms 710.546986ms 721.40526ms 721.438635ms 724.079405ms 724.23261ms 727.258708ms 732.922934ms 733.025381ms 735.657349ms 737.016869ms 737.501617ms 739.768402ms 740.292719ms 740.781652ms 741.38271ms 741.426394ms 743.554719ms 743.729367ms 744.400559ms 744.50891ms 744.540288ms 744.700246ms 744.788148ms 745.285058ms 745.543998ms 746.114421ms 746.267514ms 746.408908ms 746.76878ms 746.937086ms 747.068502ms 747.149154ms 747.157171ms 747.164902ms 747.303219ms 747.505951ms 747.649999ms 747.864472ms 748.232717ms 748.293884ms 748.344377ms 748.412051ms 748.430885ms 748.453924ms 748.789977ms 749.00773ms 749.17672ms 749.202493ms 749.202601ms 749.347451ms 749.353064ms 749.367843ms 749.387171ms 749.3922ms 749.399376ms 749.429705ms 749.463644ms 749.532163ms 749.640372ms 749.673739ms 749.754731ms 749.76015ms 749.922763ms 749.963416ms 750.364352ms 750.374266ms 750.42932ms 750.514068ms 750.532897ms 750.583266ms 750.646064ms 750.862394ms 750.894097ms 750.920023ms 751.106407ms 751.148131ms 751.161221ms 751.27669ms 751.583516ms 751.613703ms 751.708233ms 751.729386ms 751.887724ms 752.101948ms 752.252239ms 752.311964ms 752.409695ms 752.542842ms 752.575737ms 752.584578ms 752.740521ms 752.85379ms 752.899367ms 753.020324ms 753.491531ms 753.49522ms 753.663358ms 753.692758ms 753.850757ms 753.998345ms 754.277364ms 754.486953ms 754.565885ms 754.880664ms 754.927078ms 755.058974ms 755.490424ms 756.420699ms 756.56297ms 756.690572ms 756.965623ms 757.20379ms 757.414582ms 757.956617ms 758.321411ms 758.482005ms 766.114716ms 767.810728ms 770.931669ms 772.731651ms 777.641512ms 784.327226ms 786.29583ms 789.905213ms 792.192273ms 792.761681ms 792.934889ms 796.224833ms 796.547207ms 796.654613ms 796.726387ms 796.791998ms 797.958087ms 800.065976ms 800.078324ms 800.238132ms 800.77319ms 801.015681ms 801.593705ms 808.034065ms 855.490999ms]
Dec  9 07:56:03.968: INFO: 50 %ile: 748.412051ms
Dec  9 07:56:03.968: INFO: 90 %ile: 784.327226ms
Dec  9 07:56:03.968: INFO: 99 %ile: 808.034065ms
Dec  9 07:56:03.968: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  test/e2e/framework/node/init/init.go:32
Dec  9 07:56:03.969: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Service endpoints latency
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Service endpoints latency
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Service endpoints latency
  tear down framework | framework.go:193
STEP: Destroying namespace "svc-latency-6592" for this suite. 12/09/22 07:56:03.974
------------------------------
â€¢ [SLOW TEST] [10.825 seconds]
[sig-network] Service endpoints latency
test/e2e/network/common/framework.go:23
  should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Service endpoints latency
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:55:53.157
    Dec  9 07:55:53.157: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename svc-latency 12/09/22 07:55:53.158
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:55:53.178
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:55:53.182
    [BeforeEach] [sig-network] Service endpoints latency
      test/e2e/framework/metrics/init/init.go:31
    [It] should not be very high  [Conformance]
      test/e2e/network/service_latency.go:59
    Dec  9 07:55:53.186: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: creating replication controller svc-latency-rc in namespace svc-latency-6592 12/09/22 07:55:53.187
    I1209 07:55:53.194575      20 runners.go:193] Created replication controller with name: svc-latency-rc, namespace: svc-latency-6592, replica count: 1
    I1209 07:55:54.246318      20 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I1209 07:55:55.246601      20 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Dec  9 07:55:55.361: INFO: Created: latency-svc-fgfpr
    Dec  9 07:55:55.369: INFO: Got endpoints: latency-svc-fgfpr [21.760851ms]
    Dec  9 07:55:55.399: INFO: Created: latency-svc-mm42v
    Dec  9 07:55:55.415: INFO: Created: latency-svc-559q2
    Dec  9 07:55:55.416: INFO: Got endpoints: latency-svc-mm42v [46.138904ms]
    Dec  9 07:55:55.428: INFO: Created: latency-svc-kqf4p
    Dec  9 07:55:55.435: INFO: Created: latency-svc-2wjjm
    Dec  9 07:55:55.437: INFO: Got endpoints: latency-svc-559q2 [66.953004ms]
    Dec  9 07:55:55.463: INFO: Created: latency-svc-kdvvc
    Dec  9 07:55:55.465: INFO: Got endpoints: latency-svc-kqf4p [94.289999ms]
    Dec  9 07:55:55.467: INFO: Got endpoints: latency-svc-2wjjm [96.419308ms]
    Dec  9 07:55:55.468: INFO: Got endpoints: latency-svc-kdvvc [97.934357ms]
    Dec  9 07:55:55.469: INFO: Created: latency-svc-9kr8q
    Dec  9 07:55:55.495: INFO: Got endpoints: latency-svc-9kr8q [124.459971ms]
    Dec  9 07:55:55.498: INFO: Created: latency-svc-lx22l
    Dec  9 07:55:55.514: INFO: Got endpoints: latency-svc-lx22l [142.95537ms]
    Dec  9 07:55:55.519: INFO: Created: latency-svc-4qzbg
    Dec  9 07:55:55.529: INFO: Got endpoints: latency-svc-4qzbg [157.964142ms]
    Dec  9 07:55:55.594: INFO: Created: latency-svc-j9gnm
    Dec  9 07:55:55.594: INFO: Created: latency-svc-pdjrv
    Dec  9 07:55:55.595: INFO: Created: latency-svc-cj968
    Dec  9 07:55:55.596: INFO: Created: latency-svc-wh2cf
    Dec  9 07:55:55.597: INFO: Created: latency-svc-lj7cf
    Dec  9 07:55:55.598: INFO: Created: latency-svc-d54kb
    Dec  9 07:55:55.598: INFO: Created: latency-svc-qwsv5
    Dec  9 07:55:55.598: INFO: Created: latency-svc-zlnvk
    Dec  9 07:55:55.596: INFO: Created: latency-svc-p9qvh
    Dec  9 07:55:55.598: INFO: Created: latency-svc-bfzrv
    Dec  9 07:55:55.598: INFO: Created: latency-svc-7pq6g
    Dec  9 07:55:55.599: INFO: Created: latency-svc-sr7gl
    Dec  9 07:55:55.599: INFO: Created: latency-svc-89ss9
    Dec  9 07:55:55.599: INFO: Created: latency-svc-5xps6
    Dec  9 07:55:55.605: INFO: Created: latency-svc-k7zdg
    Dec  9 07:55:55.671: INFO: Got endpoints: latency-svc-7pq6g [299.79813ms]
    Dec  9 07:55:55.672: INFO: Got endpoints: latency-svc-pdjrv [176.733843ms]
    Dec  9 07:55:55.673: INFO: Got endpoints: latency-svc-k7zdg [143.610124ms]
    Dec  9 07:55:55.674: INFO: Got endpoints: latency-svc-bfzrv [302.752433ms]
    Dec  9 07:55:55.674: INFO: Got endpoints: latency-svc-sr7gl [160.247763ms]
    Dec  9 07:55:55.719: INFO: Got endpoints: latency-svc-p9qvh [346.822172ms]
    Dec  9 07:55:55.720: INFO: Got endpoints: latency-svc-wh2cf [346.82052ms]
    Dec  9 07:55:55.720: INFO: Got endpoints: latency-svc-89ss9 [348.496075ms]
    Dec  9 07:55:55.720: INFO: Got endpoints: latency-svc-lj7cf [346.716412ms]
    Dec  9 07:55:55.721: INFO: Got endpoints: latency-svc-5xps6 [348.259669ms]
    Dec  9 07:55:55.743: INFO: Created: latency-svc-7v24p
    Dec  9 07:55:55.744: INFO: Got endpoints: latency-svc-j9gnm [275.350312ms]
    Dec  9 07:55:55.746: INFO: Got endpoints: latency-svc-d54kb [325.65433ms]
    Dec  9 07:55:55.753: INFO: Got endpoints: latency-svc-cj968 [285.461579ms]
    Dec  9 07:55:55.754: INFO: Got endpoints: latency-svc-zlnvk [286.363705ms]
    Dec  9 07:55:55.754: INFO: Got endpoints: latency-svc-qwsv5 [317.071819ms]
    Dec  9 07:55:55.761: INFO: Got endpoints: latency-svc-7v24p [88.712216ms]
    Dec  9 07:55:55.766: INFO: Created: latency-svc-smpk2
    Dec  9 07:55:55.781: INFO: Created: latency-svc-j8k6p
    Dec  9 07:55:55.784: INFO: Got endpoints: latency-svc-smpk2 [111.743976ms]
    Dec  9 07:55:55.796: INFO: Got endpoints: latency-svc-j8k6p [122.356498ms]
    Dec  9 07:55:55.863: INFO: Created: latency-svc-crjqx
    Dec  9 07:55:55.875: INFO: Created: latency-svc-cttr2
    Dec  9 07:55:55.881: INFO: Created: latency-svc-r7ltb
    Dec  9 07:55:55.882: INFO: Created: latency-svc-nqdhk
    Dec  9 07:55:55.884: INFO: Created: latency-svc-xq6jm
    Dec  9 07:55:55.886: INFO: Created: latency-svc-882l9
    Dec  9 07:55:55.894: INFO: Created: latency-svc-gxz6r
    Dec  9 07:55:55.898: INFO: Created: latency-svc-gh8w8
    Dec  9 07:55:55.919: INFO: Created: latency-svc-p2nbc
    Dec  9 07:55:55.920: INFO: Created: latency-svc-5pndz
    Dec  9 07:55:55.922: INFO: Created: latency-svc-z6m82
    Dec  9 07:55:55.922: INFO: Created: latency-svc-rmt69
    Dec  9 07:55:55.923: INFO: Created: latency-svc-95ncs
    Dec  9 07:55:55.924: INFO: Created: latency-svc-jr54z
    Dec  9 07:55:55.929: INFO: Created: latency-svc-w26kc
    Dec  9 07:55:55.971: INFO: Got endpoints: latency-svc-882l9 [173.539278ms]
    Dec  9 07:55:55.974: INFO: Got endpoints: latency-svc-xq6jm [300.272008ms]
    Dec  9 07:55:55.973: INFO: Got endpoints: latency-svc-crjqx [299.621352ms]
    Dec  9 07:55:55.973: INFO: Got endpoints: latency-svc-gh8w8 [251.082509ms]
    Dec  9 07:55:55.973: INFO: Got endpoints: latency-svc-nqdhk [187.914002ms]
    Dec  9 07:55:56.010: INFO: Got endpoints: latency-svc-p2nbc [289.160819ms]
    Dec  9 07:55:56.011: INFO: Got endpoints: latency-svc-gxz6r [280.180449ms]
    Dec  9 07:55:56.012: INFO: Got endpoints: latency-svc-jr54z [267.781804ms]
    Dec  9 07:55:56.012: INFO: Got endpoints: latency-svc-5pndz [277.862975ms]
    Dec  9 07:55:56.013: INFO: Got endpoints: latency-svc-z6m82 [278.023147ms]
    Dec  9 07:55:56.050: INFO: Got endpoints: latency-svc-95ncs [299.446823ms]
    Dec  9 07:55:56.055: INFO: Created: latency-svc-zh798
    Dec  9 07:55:56.058: INFO: Got endpoints: latency-svc-cttr2 [296.54554ms]
    Dec  9 07:55:56.058: INFO: Got endpoints: latency-svc-w26kc [304.077474ms]
    Dec  9 07:55:56.058: INFO: Got endpoints: latency-svc-r7ltb [305.015448ms]
    Dec  9 07:55:56.058: INFO: Got endpoints: latency-svc-rmt69 [303.859011ms]
    Dec  9 07:55:56.072: INFO: Got endpoints: latency-svc-zh798 [99.211574ms]
    Dec  9 07:55:56.072: INFO: Created: latency-svc-ccrsx
    Dec  9 07:55:56.079: INFO: Created: latency-svc-574nl
    Dec  9 07:55:56.086: INFO: Got endpoints: latency-svc-ccrsx [110.036477ms]
    Dec  9 07:55:56.122: INFO: Got endpoints: latency-svc-574nl [147.610136ms]
    Dec  9 07:55:56.152: INFO: Created: latency-svc-mx7wz
    Dec  9 07:55:56.158: INFO: Created: latency-svc-cjwx2
    Dec  9 07:55:56.161: INFO: Created: latency-svc-pp7qh
    Dec  9 07:55:56.162: INFO: Created: latency-svc-z4zjk
    Dec  9 07:55:56.172: INFO: Created: latency-svc-jbqfk
    Dec  9 07:55:56.165: INFO: Created: latency-svc-tq86f
    Dec  9 07:55:56.166: INFO: Created: latency-svc-5jkzj
    Dec  9 07:55:56.167: INFO: Created: latency-svc-qx4xm
    Dec  9 07:55:56.167: INFO: Created: latency-svc-t6tcw
    Dec  9 07:55:56.168: INFO: Created: latency-svc-d84ph
    Dec  9 07:55:56.167: INFO: Created: latency-svc-r9pp4
    Dec  9 07:55:56.170: INFO: Created: latency-svc-68ql7
    Dec  9 07:55:56.170: INFO: Created: latency-svc-7b9sp
    Dec  9 07:55:56.172: INFO: Created: latency-svc-98gpd
    Dec  9 07:55:56.173: INFO: Created: latency-svc-lvt4f
    Dec  9 07:55:56.232: INFO: Got endpoints: latency-svc-z4zjk [219.24754ms]
    Dec  9 07:55:56.233: INFO: Got endpoints: latency-svc-mx7wz [174.727806ms]
    Dec  9 07:55:56.282: INFO: Got endpoints: latency-svc-t6tcw [158.411838ms]
    Dec  9 07:55:56.286: INFO: Created: latency-svc-9984m
    Dec  9 07:55:56.301: INFO: Created: latency-svc-2vx8x
    Dec  9 07:55:56.306: INFO: Created: latency-svc-pfjlj
    Dec  9 07:55:56.328: INFO: Got endpoints: latency-svc-r9pp4 [267.410847ms]
    Dec  9 07:55:56.343: INFO: Created: latency-svc-9jcv4
    Dec  9 07:55:56.399: INFO: Got endpoints: latency-svc-68ql7 [339.966146ms]
    Dec  9 07:55:56.414: INFO: Created: latency-svc-ghvsm
    Dec  9 07:55:56.424: INFO: Got endpoints: latency-svc-jbqfk [364.757525ms]
    Dec  9 07:55:56.439: INFO: Created: latency-svc-5ppvq
    Dec  9 07:55:56.467: INFO: Got endpoints: latency-svc-lvt4f [411.669877ms]
    Dec  9 07:55:56.478: INFO: Created: latency-svc-dz2j2
    Dec  9 07:55:56.516: INFO: Got endpoints: latency-svc-tq86f [443.253604ms]
    Dec  9 07:55:56.528: INFO: Created: latency-svc-jgfzm
    Dec  9 07:55:56.569: INFO: Got endpoints: latency-svc-5jkzj [481.962572ms]
    Dec  9 07:55:56.578: INFO: Created: latency-svc-mh92g
    Dec  9 07:55:56.619: INFO: Got endpoints: latency-svc-qx4xm [643.984563ms]
    Dec  9 07:55:56.628: INFO: Created: latency-svc-fps2f
    Dec  9 07:55:56.668: INFO: Got endpoints: latency-svc-d84ph [689.765216ms]
    Dec  9 07:55:56.681: INFO: Created: latency-svc-kf8g9
    Dec  9 07:55:56.718: INFO: Got endpoints: latency-svc-7b9sp [706.867601ms]
    Dec  9 07:55:56.730: INFO: Created: latency-svc-bw46h
    Dec  9 07:55:56.769: INFO: Got endpoints: latency-svc-98gpd [755.058974ms]
    Dec  9 07:55:56.781: INFO: Created: latency-svc-kmlnj
    Dec  9 07:55:56.821: INFO: Got endpoints: latency-svc-cjwx2 [808.034065ms]
    Dec  9 07:55:56.834: INFO: Created: latency-svc-bsx4w
    Dec  9 07:55:56.869: INFO: Got endpoints: latency-svc-pp7qh [855.490999ms]
    Dec  9 07:55:56.888: INFO: Created: latency-svc-s8p68
    Dec  9 07:55:56.920: INFO: Got endpoints: latency-svc-9984m [687.893239ms]
    Dec  9 07:55:56.932: INFO: Created: latency-svc-ns84s
    Dec  9 07:55:56.970: INFO: Got endpoints: latency-svc-2vx8x [737.016869ms]
    Dec  9 07:55:56.983: INFO: Created: latency-svc-jn9tv
    Dec  9 07:55:57.017: INFO: Got endpoints: latency-svc-pfjlj [735.657349ms]
    Dec  9 07:55:57.031: INFO: Created: latency-svc-ktpcl
    Dec  9 07:55:57.069: INFO: Got endpoints: latency-svc-9jcv4 [741.38271ms]
    Dec  9 07:55:57.081: INFO: Created: latency-svc-sjknm
    Dec  9 07:55:57.121: INFO: Got endpoints: latency-svc-ghvsm [721.40526ms]
    Dec  9 07:55:57.130: INFO: Created: latency-svc-j22s9
    Dec  9 07:55:57.167: INFO: Got endpoints: latency-svc-5ppvq [739.768402ms]
    Dec  9 07:55:57.182: INFO: Created: latency-svc-fdnlk
    Dec  9 07:55:57.219: INFO: Got endpoints: latency-svc-dz2j2 [751.708233ms]
    Dec  9 07:55:57.231: INFO: Created: latency-svc-9c6rd
    Dec  9 07:55:57.268: INFO: Got endpoints: latency-svc-jgfzm [752.409695ms]
    Dec  9 07:55:57.281: INFO: Created: latency-svc-7krhs
    Dec  9 07:55:57.319: INFO: Got endpoints: latency-svc-mh92g [750.646064ms]
    Dec  9 07:55:57.329: INFO: Created: latency-svc-mcdv6
    Dec  9 07:55:57.369: INFO: Got endpoints: latency-svc-fps2f [749.353064ms]
    Dec  9 07:55:57.379: INFO: Created: latency-svc-dqd9q
    Dec  9 07:55:57.418: INFO: Got endpoints: latency-svc-kf8g9 [749.76015ms]
    Dec  9 07:55:57.428: INFO: Created: latency-svc-pfr47
    Dec  9 07:55:57.468: INFO: Got endpoints: latency-svc-bw46h [749.463644ms]
    Dec  9 07:55:57.478: INFO: Created: latency-svc-xnbrz
    Dec  9 07:55:57.518: INFO: Got endpoints: latency-svc-kmlnj [748.412051ms]
    Dec  9 07:55:57.528: INFO: Created: latency-svc-48p8q
    Dec  9 07:55:57.568: INFO: Got endpoints: latency-svc-bsx4w [744.788148ms]
    Dec  9 07:55:57.577: INFO: Created: latency-svc-22984
    Dec  9 07:55:57.621: INFO: Got endpoints: latency-svc-s8p68 [751.583516ms]
    Dec  9 07:55:57.632: INFO: Created: latency-svc-4bcck
    Dec  9 07:55:57.668: INFO: Got endpoints: latency-svc-ns84s [747.164902ms]
    Dec  9 07:55:57.682: INFO: Created: latency-svc-4fzjh
    Dec  9 07:55:57.718: INFO: Got endpoints: latency-svc-jn9tv [747.505951ms]
    Dec  9 07:55:57.729: INFO: Created: latency-svc-gsqm5
    Dec  9 07:55:57.772: INFO: Got endpoints: latency-svc-ktpcl [754.927078ms]
    Dec  9 07:55:57.782: INFO: Created: latency-svc-n94kz
    Dec  9 07:55:57.819: INFO: Got endpoints: latency-svc-sjknm [749.202601ms]
    Dec  9 07:55:57.828: INFO: Created: latency-svc-8pjrm
    Dec  9 07:55:57.868: INFO: Got endpoints: latency-svc-j22s9 [746.76878ms]
    Dec  9 07:55:57.880: INFO: Created: latency-svc-mnkxg
    Dec  9 07:55:57.920: INFO: Got endpoints: latency-svc-fdnlk [746.937086ms]
    Dec  9 07:55:57.930: INFO: Created: latency-svc-8bf9z
    Dec  9 07:55:57.970: INFO: Got endpoints: latency-svc-9c6rd [749.963416ms]
    Dec  9 07:55:57.981: INFO: Created: latency-svc-7pbf2
    Dec  9 07:55:58.017: INFO: Got endpoints: latency-svc-7krhs [748.293884ms]
    Dec  9 07:55:58.029: INFO: Created: latency-svc-mkzbm
    Dec  9 07:55:58.067: INFO: Got endpoints: latency-svc-mcdv6 [747.149154ms]
    Dec  9 07:55:58.083: INFO: Created: latency-svc-lhkv2
    Dec  9 07:55:58.122: INFO: Got endpoints: latency-svc-dqd9q [752.740521ms]
    Dec  9 07:55:58.132: INFO: Created: latency-svc-sl52x
    Dec  9 07:55:58.168: INFO: Got endpoints: latency-svc-pfr47 [749.673739ms]
    Dec  9 07:55:58.190: INFO: Created: latency-svc-vzfdd
    Dec  9 07:55:58.224: INFO: Got endpoints: latency-svc-xnbrz [756.420699ms]
    Dec  9 07:55:58.250: INFO: Created: latency-svc-lztzx
    Dec  9 07:55:58.275: INFO: Got endpoints: latency-svc-48p8q [756.965623ms]
    Dec  9 07:55:58.295: INFO: Created: latency-svc-4pbr4
    Dec  9 07:55:58.322: INFO: Got endpoints: latency-svc-22984 [753.850757ms]
    Dec  9 07:55:58.333: INFO: Created: latency-svc-zgd6z
    Dec  9 07:55:58.366: INFO: Got endpoints: latency-svc-4bcck [744.540288ms]
    Dec  9 07:55:58.382: INFO: Created: latency-svc-9jvfr
    Dec  9 07:55:58.419: INFO: Got endpoints: latency-svc-4fzjh [750.532897ms]
    Dec  9 07:55:58.431: INFO: Created: latency-svc-pjv9b
    Dec  9 07:55:58.470: INFO: Got endpoints: latency-svc-gsqm5 [751.887724ms]
    Dec  9 07:55:58.481: INFO: Created: latency-svc-hzjwh
    Dec  9 07:55:58.520: INFO: Got endpoints: latency-svc-n94kz [747.068502ms]
    Dec  9 07:55:58.535: INFO: Created: latency-svc-szhbf
    Dec  9 07:55:58.568: INFO: Got endpoints: latency-svc-8pjrm [749.754731ms]
    Dec  9 07:55:58.583: INFO: Created: latency-svc-5gzs9
    Dec  9 07:55:58.620: INFO: Got endpoints: latency-svc-mnkxg [752.542842ms]
    Dec  9 07:55:58.635: INFO: Created: latency-svc-8x75c
    Dec  9 07:55:58.668: INFO: Got endpoints: latency-svc-8bf9z [747.157171ms]
    Dec  9 07:55:58.679: INFO: Created: latency-svc-jjzqz
    Dec  9 07:55:58.719: INFO: Got endpoints: latency-svc-7pbf2 [749.399376ms]
    Dec  9 07:55:58.730: INFO: Created: latency-svc-g829v
    Dec  9 07:55:58.775: INFO: Got endpoints: latency-svc-mkzbm [757.20379ms]
    Dec  9 07:55:58.785: INFO: Created: latency-svc-wznqq
    Dec  9 07:55:58.821: INFO: Got endpoints: latency-svc-lhkv2 [750.42932ms]
    Dec  9 07:55:58.832: INFO: Created: latency-svc-x2zqb
    Dec  9 07:55:58.866: INFO: Got endpoints: latency-svc-sl52x [744.50891ms]
    Dec  9 07:55:58.882: INFO: Created: latency-svc-z8t4p
    Dec  9 07:55:58.918: INFO: Got endpoints: latency-svc-vzfdd [749.532163ms]
    Dec  9 07:55:58.931: INFO: Created: latency-svc-sr2qj
    Dec  9 07:55:58.975: INFO: Got endpoints: latency-svc-lztzx [751.148131ms]
    Dec  9 07:55:58.989: INFO: Created: latency-svc-tz9gw
    Dec  9 07:55:59.068: INFO: Got endpoints: latency-svc-4pbr4 [792.761681ms]
    Dec  9 07:55:59.081: INFO: Created: latency-svc-gtw5w
    Dec  9 07:55:59.118: INFO: Got endpoints: latency-svc-zgd6z [796.224833ms]
    Dec  9 07:55:59.132: INFO: Created: latency-svc-2kjzx
    Dec  9 07:55:59.166: INFO: Got endpoints: latency-svc-9jvfr [800.238132ms]
    Dec  9 07:55:59.180: INFO: Created: latency-svc-l4lmr
    Dec  9 07:55:59.215: INFO: Got endpoints: latency-svc-pjv9b [796.654613ms]
    Dec  9 07:55:59.228: INFO: Created: latency-svc-w55nd
    Dec  9 07:55:59.267: INFO: Got endpoints: latency-svc-hzjwh [796.726387ms]
    Dec  9 07:55:59.278: INFO: Created: latency-svc-v8lz2
    Dec  9 07:55:59.318: INFO: Got endpoints: latency-svc-szhbf [797.958087ms]
    Dec  9 07:55:59.328: INFO: Created: latency-svc-j7786
    Dec  9 07:55:59.369: INFO: Got endpoints: latency-svc-5gzs9 [800.078324ms]
    Dec  9 07:55:59.381: INFO: Created: latency-svc-657fg
    Dec  9 07:55:59.417: INFO: Got endpoints: latency-svc-8x75c [796.791998ms]
    Dec  9 07:55:59.430: INFO: Created: latency-svc-gfbcd
    Dec  9 07:55:59.469: INFO: Got endpoints: latency-svc-jjzqz [801.593705ms]
    Dec  9 07:55:59.482: INFO: Created: latency-svc-fnslv
    Dec  9 07:55:59.520: INFO: Got endpoints: latency-svc-g829v [800.065976ms]
    Dec  9 07:55:59.530: INFO: Created: latency-svc-xhrtc
    Dec  9 07:55:59.568: INFO: Got endpoints: latency-svc-wznqq [792.934889ms]
    Dec  9 07:55:59.580: INFO: Created: latency-svc-2pw68
    Dec  9 07:55:59.618: INFO: Got endpoints: latency-svc-x2zqb [796.547207ms]
    Dec  9 07:55:59.631: INFO: Created: latency-svc-dzkhl
    Dec  9 07:55:59.667: INFO: Got endpoints: latency-svc-z8t4p [800.77319ms]
    Dec  9 07:55:59.681: INFO: Created: latency-svc-nj4zc
    Dec  9 07:55:59.720: INFO: Got endpoints: latency-svc-sr2qj [801.015681ms]
    Dec  9 07:55:59.731: INFO: Created: latency-svc-s9gcp
    Dec  9 07:55:59.768: INFO: Got endpoints: latency-svc-tz9gw [792.192273ms]
    Dec  9 07:55:59.779: INFO: Created: latency-svc-xqrvc
    Dec  9 07:55:59.819: INFO: Got endpoints: latency-svc-gtw5w [750.862394ms]
    Dec  9 07:55:59.830: INFO: Created: latency-svc-6kfrw
    Dec  9 07:55:59.867: INFO: Got endpoints: latency-svc-2kjzx [748.789977ms]
    Dec  9 07:55:59.893: INFO: Created: latency-svc-phnp5
    Dec  9 07:55:59.918: INFO: Got endpoints: latency-svc-l4lmr [751.27669ms]
    Dec  9 07:55:59.930: INFO: Created: latency-svc-bx9kd
    Dec  9 07:55:59.969: INFO: Got endpoints: latency-svc-w55nd [752.575737ms]
    Dec  9 07:55:59.984: INFO: Created: latency-svc-wrhpx
    Dec  9 07:56:00.020: INFO: Got endpoints: latency-svc-v8lz2 [750.894097ms]
    Dec  9 07:56:00.036: INFO: Created: latency-svc-xvn4n
    Dec  9 07:56:00.067: INFO: Got endpoints: latency-svc-j7786 [749.00773ms]
    Dec  9 07:56:00.081: INFO: Created: latency-svc-lpfq7
    Dec  9 07:56:00.125: INFO: Got endpoints: latency-svc-657fg [755.490424ms]
    Dec  9 07:56:00.149: INFO: Created: latency-svc-b675t
    Dec  9 07:56:00.173: INFO: Got endpoints: latency-svc-gfbcd [754.880664ms]
    Dec  9 07:56:00.185: INFO: Created: latency-svc-7rxnl
    Dec  9 07:56:00.221: INFO: Got endpoints: latency-svc-fnslv [750.920023ms]
    Dec  9 07:56:00.234: INFO: Created: latency-svc-h4ng8
    Dec  9 07:56:00.269: INFO: Got endpoints: latency-svc-xhrtc [749.429705ms]
    Dec  9 07:56:00.282: INFO: Created: latency-svc-pm6qx
    Dec  9 07:56:00.321: INFO: Got endpoints: latency-svc-2pw68 [753.49522ms]
    Dec  9 07:56:00.334: INFO: Created: latency-svc-m5j5r
    Dec  9 07:56:00.368: INFO: Got endpoints: latency-svc-dzkhl [749.922763ms]
    Dec  9 07:56:00.379: INFO: Created: latency-svc-88rdd
    Dec  9 07:56:00.422: INFO: Got endpoints: latency-svc-nj4zc [753.998345ms]
    Dec  9 07:56:00.434: INFO: Created: latency-svc-nr4wn
    Dec  9 07:56:00.468: INFO: Got endpoints: latency-svc-s9gcp [748.344377ms]
    Dec  9 07:56:00.481: INFO: Created: latency-svc-knx57
    Dec  9 07:56:00.518: INFO: Got endpoints: latency-svc-xqrvc [750.374266ms]
    Dec  9 07:56:00.529: INFO: Created: latency-svc-9wz6t
    Dec  9 07:56:00.568: INFO: Got endpoints: latency-svc-6kfrw [749.17672ms]
    Dec  9 07:56:00.581: INFO: Created: latency-svc-pv6x8
    Dec  9 07:56:00.621: INFO: Got endpoints: latency-svc-phnp5 [753.663358ms]
    Dec  9 07:56:00.632: INFO: Created: latency-svc-mlbqs
    Dec  9 07:56:00.671: INFO: Got endpoints: latency-svc-bx9kd [752.85379ms]
    Dec  9 07:56:00.681: INFO: Created: latency-svc-sjhtg
    Dec  9 07:56:00.719: INFO: Got endpoints: latency-svc-wrhpx [749.640372ms]
    Dec  9 07:56:00.729: INFO: Created: latency-svc-42wq4
    Dec  9 07:56:00.767: INFO: Got endpoints: latency-svc-xvn4n [747.303219ms]
    Dec  9 07:56:00.779: INFO: Created: latency-svc-fwr47
    Dec  9 07:56:00.817: INFO: Got endpoints: latency-svc-lpfq7 [749.367843ms]
    Dec  9 07:56:00.828: INFO: Created: latency-svc-st6l9
    Dec  9 07:56:00.871: INFO: Got endpoints: latency-svc-b675t [746.267514ms]
    Dec  9 07:56:00.881: INFO: Created: latency-svc-drgx7
    Dec  9 07:56:00.918: INFO: Got endpoints: latency-svc-7rxnl [744.400559ms]
    Dec  9 07:56:00.938: INFO: Created: latency-svc-h6nl7
    Dec  9 07:56:00.967: INFO: Got endpoints: latency-svc-h4ng8 [745.543998ms]
    Dec  9 07:56:00.979: INFO: Created: latency-svc-vxwv2
    Dec  9 07:56:01.020: INFO: Got endpoints: latency-svc-pm6qx [750.364352ms]
    Dec  9 07:56:01.034: INFO: Created: latency-svc-fprsw
    Dec  9 07:56:01.074: INFO: Got endpoints: latency-svc-m5j5r [752.252239ms]
    Dec  9 07:56:01.085: INFO: Created: latency-svc-pfhww
    Dec  9 07:56:01.118: INFO: Got endpoints: latency-svc-88rdd [749.347451ms]
    Dec  9 07:56:01.132: INFO: Created: latency-svc-6hqf9
    Dec  9 07:56:01.165: INFO: Got endpoints: latency-svc-nr4wn [743.554719ms]
    Dec  9 07:56:01.176: INFO: Created: latency-svc-279sj
    Dec  9 07:56:01.218: INFO: Got endpoints: latency-svc-knx57 [749.3922ms]
    Dec  9 07:56:01.229: INFO: Created: latency-svc-5s2xn
    Dec  9 07:56:01.270: INFO: Got endpoints: latency-svc-9wz6t [751.729386ms]
    Dec  9 07:56:01.281: INFO: Created: latency-svc-f6jx4
    Dec  9 07:56:01.321: INFO: Got endpoints: latency-svc-pv6x8 [751.106407ms]
    Dec  9 07:56:01.336: INFO: Created: latency-svc-p6zfv
    Dec  9 07:56:01.369: INFO: Got endpoints: latency-svc-mlbqs [748.232717ms]
    Dec  9 07:56:01.383: INFO: Created: latency-svc-jwczb
    Dec  9 07:56:01.419: INFO: Got endpoints: latency-svc-sjhtg [747.649999ms]
    Dec  9 07:56:01.430: INFO: Created: latency-svc-4xtd6
    Dec  9 07:56:01.468: INFO: Got endpoints: latency-svc-42wq4 [749.202493ms]
    Dec  9 07:56:01.482: INFO: Created: latency-svc-6mzhq
    Dec  9 07:56:01.517: INFO: Got endpoints: latency-svc-fwr47 [749.387171ms]
    Dec  9 07:56:01.532: INFO: Created: latency-svc-fcwht
    Dec  9 07:56:01.569: INFO: Got endpoints: latency-svc-st6l9 [752.101948ms]
    Dec  9 07:56:01.581: INFO: Created: latency-svc-xn5dp
    Dec  9 07:56:01.618: INFO: Got endpoints: latency-svc-drgx7 [746.408908ms]
    Dec  9 07:56:01.631: INFO: Created: latency-svc-fgws6
    Dec  9 07:56:01.669: INFO: Got endpoints: latency-svc-h6nl7 [750.583266ms]
    Dec  9 07:56:01.682: INFO: Created: latency-svc-cnzc2
    Dec  9 07:56:01.726: INFO: Got endpoints: latency-svc-vxwv2 [758.482005ms]
    Dec  9 07:56:01.738: INFO: Created: latency-svc-rnr2n
    Dec  9 07:56:01.771: INFO: Got endpoints: latency-svc-fprsw [751.161221ms]
    Dec  9 07:56:01.784: INFO: Created: latency-svc-flzn6
    Dec  9 07:56:01.819: INFO: Got endpoints: latency-svc-pfhww [744.700246ms]
    Dec  9 07:56:01.834: INFO: Created: latency-svc-qt92w
    Dec  9 07:56:01.868: INFO: Got endpoints: latency-svc-6hqf9 [750.514068ms]
    Dec  9 07:56:01.881: INFO: Created: latency-svc-dzbq9
    Dec  9 07:56:01.919: INFO: Got endpoints: latency-svc-279sj [752.899367ms]
    Dec  9 07:56:01.933: INFO: Created: latency-svc-mrwk4
    Dec  9 07:56:01.973: INFO: Got endpoints: latency-svc-5s2xn [754.277364ms]
    Dec  9 07:56:01.988: INFO: Created: latency-svc-sqj5c
    Dec  9 07:56:02.022: INFO: Got endpoints: latency-svc-f6jx4 [751.613703ms]
    Dec  9 07:56:02.035: INFO: Created: latency-svc-zkqzf
    Dec  9 07:56:02.069: INFO: Got endpoints: latency-svc-p6zfv [748.430885ms]
    Dec  9 07:56:02.099: INFO: Created: latency-svc-6zh8m
    Dec  9 07:56:02.142: INFO: Got endpoints: latency-svc-jwczb [772.731651ms]
    Dec  9 07:56:02.156: INFO: Created: latency-svc-c2hfn
    Dec  9 07:56:02.176: INFO: Got endpoints: latency-svc-4xtd6 [757.414582ms]
    Dec  9 07:56:02.194: INFO: Created: latency-svc-n2v8m
    Dec  9 07:56:02.221: INFO: Got endpoints: latency-svc-6mzhq [752.584578ms]
    Dec  9 07:56:02.237: INFO: Created: latency-svc-d5j6k
    Dec  9 07:56:02.271: INFO: Got endpoints: latency-svc-fcwht [753.491531ms]
    Dec  9 07:56:02.287: INFO: Created: latency-svc-xzt77
    Dec  9 07:56:02.323: INFO: Got endpoints: latency-svc-xn5dp [753.692758ms]
    Dec  9 07:56:02.348: INFO: Created: latency-svc-d6vfw
    Dec  9 07:56:02.373: INFO: Got endpoints: latency-svc-fgws6 [754.486953ms]
    Dec  9 07:56:02.400: INFO: Created: latency-svc-9wms9
    Dec  9 07:56:02.426: INFO: Got endpoints: latency-svc-cnzc2 [756.690572ms]
    Dec  9 07:56:02.438: INFO: Created: latency-svc-89rh8
    Dec  9 07:56:02.479: INFO: Got endpoints: latency-svc-rnr2n [753.020324ms]
    Dec  9 07:56:02.498: INFO: Created: latency-svc-ncxfg
    Dec  9 07:56:02.528: INFO: Got endpoints: latency-svc-flzn6 [756.56297ms]
    Dec  9 07:56:02.559: INFO: Created: latency-svc-q5m6w
    Dec  9 07:56:02.578: INFO: Got endpoints: latency-svc-qt92w [758.321411ms]
    Dec  9 07:56:02.604: INFO: Created: latency-svc-wrjl8
    Dec  9 07:56:02.624: INFO: Got endpoints: latency-svc-dzbq9 [754.565885ms]
    Dec  9 07:56:02.638: INFO: Created: latency-svc-tsg2x
    Dec  9 07:56:02.685: INFO: Got endpoints: latency-svc-mrwk4 [766.114716ms]
    Dec  9 07:56:02.709: INFO: Created: latency-svc-tlfj8
    Dec  9 07:56:02.725: INFO: Got endpoints: latency-svc-sqj5c [752.311964ms]
    Dec  9 07:56:02.749: INFO: Created: latency-svc-vtxsp
    Dec  9 07:56:02.793: INFO: Got endpoints: latency-svc-zkqzf [770.931669ms]
    Dec  9 07:56:02.831: INFO: Created: latency-svc-4l5bm
    Dec  9 07:56:02.847: INFO: Got endpoints: latency-svc-6zh8m [777.641512ms]
    Dec  9 07:56:02.884: INFO: Got endpoints: latency-svc-c2hfn [741.426394ms]
    Dec  9 07:56:02.905: INFO: Created: latency-svc-xgzf9
    Dec  9 07:56:02.961: INFO: Got endpoints: latency-svc-n2v8m [784.327226ms]
    Dec  9 07:56:02.988: INFO: Created: latency-svc-dtwnl
    Dec  9 07:56:02.989: INFO: Got endpoints: latency-svc-d5j6k [767.810728ms]
    Dec  9 07:56:03.029: INFO: Created: latency-svc-hqr5s
    Dec  9 07:56:03.057: INFO: Created: latency-svc-5dknc
    Dec  9 07:56:03.058: INFO: Got endpoints: latency-svc-xzt77 [786.29583ms]
    Dec  9 07:56:03.114: INFO: Got endpoints: latency-svc-d6vfw [789.905213ms]
    Dec  9 07:56:03.116: INFO: Created: latency-svc-qcscw
    Dec  9 07:56:03.118: INFO: Got endpoints: latency-svc-9wms9 [745.285058ms]
    Dec  9 07:56:03.135: INFO: Created: latency-svc-66jv4
    Dec  9 07:56:03.182: INFO: Created: latency-svc-9nmjf
    Dec  9 07:56:03.184: INFO: Got endpoints: latency-svc-89rh8 [757.956617ms]
    Dec  9 07:56:03.205: INFO: Created: latency-svc-v6mrw
    Dec  9 07:56:03.220: INFO: Got endpoints: latency-svc-ncxfg [740.781652ms]
    Dec  9 07:56:03.235: INFO: Created: latency-svc-ldm8c
    Dec  9 07:56:03.272: INFO: Got endpoints: latency-svc-q5m6w [743.729367ms]
    Dec  9 07:56:03.320: INFO: Got endpoints: latency-svc-wrjl8 [740.292719ms]
    Dec  9 07:56:03.372: INFO: Got endpoints: latency-svc-tsg2x [747.864472ms]
    Dec  9 07:56:03.418: INFO: Got endpoints: latency-svc-tlfj8 [732.922934ms]
    Dec  9 07:56:03.475: INFO: Got endpoints: latency-svc-vtxsp [748.453924ms]
    Dec  9 07:56:03.518: INFO: Got endpoints: latency-svc-4l5bm [724.23261ms]
    Dec  9 07:56:03.569: INFO: Got endpoints: latency-svc-xgzf9 [721.438635ms]
    Dec  9 07:56:03.622: INFO: Got endpoints: latency-svc-dtwnl [737.501617ms]
    Dec  9 07:56:03.670: INFO: Got endpoints: latency-svc-hqr5s [707.518601ms]
    Dec  9 07:56:03.717: INFO: Got endpoints: latency-svc-5dknc [727.258708ms]
    Dec  9 07:56:03.768: INFO: Got endpoints: latency-svc-qcscw [710.546986ms]
    Dec  9 07:56:03.821: INFO: Got endpoints: latency-svc-66jv4 [706.836583ms]
    Dec  9 07:56:03.872: INFO: Got endpoints: latency-svc-9nmjf [724.079405ms]
    Dec  9 07:56:03.917: INFO: Got endpoints: latency-svc-v6mrw [733.025381ms]
    Dec  9 07:56:03.967: INFO: Got endpoints: latency-svc-ldm8c [746.114421ms]
    Dec  9 07:56:03.967: INFO: Latencies: [46.138904ms 66.953004ms 88.712216ms 94.289999ms 96.419308ms 97.934357ms 99.211574ms 110.036477ms 111.743976ms 122.356498ms 124.459971ms 142.95537ms 143.610124ms 147.610136ms 157.964142ms 158.411838ms 160.247763ms 173.539278ms 174.727806ms 176.733843ms 187.914002ms 219.24754ms 251.082509ms 267.410847ms 267.781804ms 275.350312ms 277.862975ms 278.023147ms 280.180449ms 285.461579ms 286.363705ms 289.160819ms 296.54554ms 299.446823ms 299.621352ms 299.79813ms 300.272008ms 302.752433ms 303.859011ms 304.077474ms 305.015448ms 317.071819ms 325.65433ms 339.966146ms 346.716412ms 346.82052ms 346.822172ms 348.259669ms 348.496075ms 364.757525ms 411.669877ms 443.253604ms 481.962572ms 643.984563ms 687.893239ms 689.765216ms 706.836583ms 706.867601ms 707.518601ms 710.546986ms 721.40526ms 721.438635ms 724.079405ms 724.23261ms 727.258708ms 732.922934ms 733.025381ms 735.657349ms 737.016869ms 737.501617ms 739.768402ms 740.292719ms 740.781652ms 741.38271ms 741.426394ms 743.554719ms 743.729367ms 744.400559ms 744.50891ms 744.540288ms 744.700246ms 744.788148ms 745.285058ms 745.543998ms 746.114421ms 746.267514ms 746.408908ms 746.76878ms 746.937086ms 747.068502ms 747.149154ms 747.157171ms 747.164902ms 747.303219ms 747.505951ms 747.649999ms 747.864472ms 748.232717ms 748.293884ms 748.344377ms 748.412051ms 748.430885ms 748.453924ms 748.789977ms 749.00773ms 749.17672ms 749.202493ms 749.202601ms 749.347451ms 749.353064ms 749.367843ms 749.387171ms 749.3922ms 749.399376ms 749.429705ms 749.463644ms 749.532163ms 749.640372ms 749.673739ms 749.754731ms 749.76015ms 749.922763ms 749.963416ms 750.364352ms 750.374266ms 750.42932ms 750.514068ms 750.532897ms 750.583266ms 750.646064ms 750.862394ms 750.894097ms 750.920023ms 751.106407ms 751.148131ms 751.161221ms 751.27669ms 751.583516ms 751.613703ms 751.708233ms 751.729386ms 751.887724ms 752.101948ms 752.252239ms 752.311964ms 752.409695ms 752.542842ms 752.575737ms 752.584578ms 752.740521ms 752.85379ms 752.899367ms 753.020324ms 753.491531ms 753.49522ms 753.663358ms 753.692758ms 753.850757ms 753.998345ms 754.277364ms 754.486953ms 754.565885ms 754.880664ms 754.927078ms 755.058974ms 755.490424ms 756.420699ms 756.56297ms 756.690572ms 756.965623ms 757.20379ms 757.414582ms 757.956617ms 758.321411ms 758.482005ms 766.114716ms 767.810728ms 770.931669ms 772.731651ms 777.641512ms 784.327226ms 786.29583ms 789.905213ms 792.192273ms 792.761681ms 792.934889ms 796.224833ms 796.547207ms 796.654613ms 796.726387ms 796.791998ms 797.958087ms 800.065976ms 800.078324ms 800.238132ms 800.77319ms 801.015681ms 801.593705ms 808.034065ms 855.490999ms]
    Dec  9 07:56:03.968: INFO: 50 %ile: 748.412051ms
    Dec  9 07:56:03.968: INFO: 90 %ile: 784.327226ms
    Dec  9 07:56:03.968: INFO: 99 %ile: 808.034065ms
    Dec  9 07:56:03.968: INFO: Total sample count: 200
    [AfterEach] [sig-network] Service endpoints latency
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:56:03.969: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Service endpoints latency
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Service endpoints latency
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Service endpoints latency
      tear down framework | framework.go:193
    STEP: Destroying namespace "svc-latency-6592" for this suite. 12/09/22 07:56:03.974
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes
  should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:56:03.983
Dec  9 07:56:03.983: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename emptydir-wrapper 12/09/22 07:56:03.984
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:56:04.003
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:56:04.014
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189
STEP: Creating 50 configmaps 12/09/22 07:56:04.02
STEP: Creating RC which spawns configmap-volume pods 12/09/22 07:56:04.332
Dec  9 07:56:04.362: INFO: Pod name wrapped-volume-race-82d05535-52e0-4f8d-967b-41910560c8a2: Found 0 pods out of 5
Dec  9 07:56:09.385: INFO: Pod name wrapped-volume-race-82d05535-52e0-4f8d-967b-41910560c8a2: Found 5 pods out of 5
STEP: Ensuring each pod is running 12/09/22 07:56:09.386
Dec  9 07:56:09.386: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-82d05535-52e0-4f8d-967b-41910560c8a2-2g4wj" in namespace "emptydir-wrapper-3189" to be "running"
Dec  9 07:56:09.408: INFO: Pod "wrapped-volume-race-82d05535-52e0-4f8d-967b-41910560c8a2-2g4wj": Phase="Pending", Reason="", readiness=false. Elapsed: 21.633887ms
Dec  9 07:56:11.412: INFO: Pod "wrapped-volume-race-82d05535-52e0-4f8d-967b-41910560c8a2-2g4wj": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025660592s
Dec  9 07:56:13.415: INFO: Pod "wrapped-volume-race-82d05535-52e0-4f8d-967b-41910560c8a2-2g4wj": Phase="Pending", Reason="", readiness=false. Elapsed: 4.028543516s
Dec  9 07:56:15.414: INFO: Pod "wrapped-volume-race-82d05535-52e0-4f8d-967b-41910560c8a2-2g4wj": Phase="Pending", Reason="", readiness=false. Elapsed: 6.027141741s
Dec  9 07:56:17.414: INFO: Pod "wrapped-volume-race-82d05535-52e0-4f8d-967b-41910560c8a2-2g4wj": Phase="Pending", Reason="", readiness=false. Elapsed: 8.027353549s
Dec  9 07:56:19.414: INFO: Pod "wrapped-volume-race-82d05535-52e0-4f8d-967b-41910560c8a2-2g4wj": Phase="Running", Reason="", readiness=true. Elapsed: 10.027247946s
Dec  9 07:56:19.414: INFO: Pod "wrapped-volume-race-82d05535-52e0-4f8d-967b-41910560c8a2-2g4wj" satisfied condition "running"
Dec  9 07:56:19.414: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-82d05535-52e0-4f8d-967b-41910560c8a2-2tctv" in namespace "emptydir-wrapper-3189" to be "running"
Dec  9 07:56:19.417: INFO: Pod "wrapped-volume-race-82d05535-52e0-4f8d-967b-41910560c8a2-2tctv": Phase="Running", Reason="", readiness=true. Elapsed: 3.128304ms
Dec  9 07:56:19.417: INFO: Pod "wrapped-volume-race-82d05535-52e0-4f8d-967b-41910560c8a2-2tctv" satisfied condition "running"
Dec  9 07:56:19.417: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-82d05535-52e0-4f8d-967b-41910560c8a2-hvjxf" in namespace "emptydir-wrapper-3189" to be "running"
Dec  9 07:56:19.420: INFO: Pod "wrapped-volume-race-82d05535-52e0-4f8d-967b-41910560c8a2-hvjxf": Phase="Running", Reason="", readiness=true. Elapsed: 3.060476ms
Dec  9 07:56:19.420: INFO: Pod "wrapped-volume-race-82d05535-52e0-4f8d-967b-41910560c8a2-hvjxf" satisfied condition "running"
Dec  9 07:56:19.420: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-82d05535-52e0-4f8d-967b-41910560c8a2-j7wlr" in namespace "emptydir-wrapper-3189" to be "running"
Dec  9 07:56:19.423: INFO: Pod "wrapped-volume-race-82d05535-52e0-4f8d-967b-41910560c8a2-j7wlr": Phase="Running", Reason="", readiness=true. Elapsed: 2.942467ms
Dec  9 07:56:19.423: INFO: Pod "wrapped-volume-race-82d05535-52e0-4f8d-967b-41910560c8a2-j7wlr" satisfied condition "running"
Dec  9 07:56:19.423: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-82d05535-52e0-4f8d-967b-41910560c8a2-lg7bx" in namespace "emptydir-wrapper-3189" to be "running"
Dec  9 07:56:19.426: INFO: Pod "wrapped-volume-race-82d05535-52e0-4f8d-967b-41910560c8a2-lg7bx": Phase="Running", Reason="", readiness=true. Elapsed: 2.893602ms
Dec  9 07:56:19.426: INFO: Pod "wrapped-volume-race-82d05535-52e0-4f8d-967b-41910560c8a2-lg7bx" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-82d05535-52e0-4f8d-967b-41910560c8a2 in namespace emptydir-wrapper-3189, will wait for the garbage collector to delete the pods 12/09/22 07:56:19.426
Dec  9 07:56:19.492: INFO: Deleting ReplicationController wrapped-volume-race-82d05535-52e0-4f8d-967b-41910560c8a2 took: 11.743883ms
Dec  9 07:56:19.594: INFO: Terminating ReplicationController wrapped-volume-race-82d05535-52e0-4f8d-967b-41910560c8a2 pods took: 102.262457ms
STEP: Creating RC which spawns configmap-volume pods 12/09/22 07:56:23.498
Dec  9 07:56:23.515: INFO: Pod name wrapped-volume-race-c63f10eb-8a79-480e-921a-f39e89f3194f: Found 0 pods out of 5
Dec  9 07:56:28.524: INFO: Pod name wrapped-volume-race-c63f10eb-8a79-480e-921a-f39e89f3194f: Found 5 pods out of 5
STEP: Ensuring each pod is running 12/09/22 07:56:28.524
Dec  9 07:56:28.524: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-c63f10eb-8a79-480e-921a-f39e89f3194f-27dtq" in namespace "emptydir-wrapper-3189" to be "running"
Dec  9 07:56:28.529: INFO: Pod "wrapped-volume-race-c63f10eb-8a79-480e-921a-f39e89f3194f-27dtq": Phase="Pending", Reason="", readiness=false. Elapsed: 4.43418ms
Dec  9 07:56:30.538: INFO: Pod "wrapped-volume-race-c63f10eb-8a79-480e-921a-f39e89f3194f-27dtq": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013310742s
Dec  9 07:56:32.533: INFO: Pod "wrapped-volume-race-c63f10eb-8a79-480e-921a-f39e89f3194f-27dtq": Phase="Pending", Reason="", readiness=false. Elapsed: 4.00911862s
Dec  9 07:56:34.540: INFO: Pod "wrapped-volume-race-c63f10eb-8a79-480e-921a-f39e89f3194f-27dtq": Phase="Pending", Reason="", readiness=false. Elapsed: 6.015736364s
Dec  9 07:56:36.540: INFO: Pod "wrapped-volume-race-c63f10eb-8a79-480e-921a-f39e89f3194f-27dtq": Phase="Pending", Reason="", readiness=false. Elapsed: 8.015620955s
Dec  9 07:56:38.533: INFO: Pod "wrapped-volume-race-c63f10eb-8a79-480e-921a-f39e89f3194f-27dtq": Phase="Running", Reason="", readiness=true. Elapsed: 10.00843399s
Dec  9 07:56:38.533: INFO: Pod "wrapped-volume-race-c63f10eb-8a79-480e-921a-f39e89f3194f-27dtq" satisfied condition "running"
Dec  9 07:56:38.533: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-c63f10eb-8a79-480e-921a-f39e89f3194f-2gczh" in namespace "emptydir-wrapper-3189" to be "running"
Dec  9 07:56:38.536: INFO: Pod "wrapped-volume-race-c63f10eb-8a79-480e-921a-f39e89f3194f-2gczh": Phase="Running", Reason="", readiness=true. Elapsed: 3.008384ms
Dec  9 07:56:38.536: INFO: Pod "wrapped-volume-race-c63f10eb-8a79-480e-921a-f39e89f3194f-2gczh" satisfied condition "running"
Dec  9 07:56:38.536: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-c63f10eb-8a79-480e-921a-f39e89f3194f-n2ghw" in namespace "emptydir-wrapper-3189" to be "running"
Dec  9 07:56:38.539: INFO: Pod "wrapped-volume-race-c63f10eb-8a79-480e-921a-f39e89f3194f-n2ghw": Phase="Running", Reason="", readiness=true. Elapsed: 2.862943ms
Dec  9 07:56:38.539: INFO: Pod "wrapped-volume-race-c63f10eb-8a79-480e-921a-f39e89f3194f-n2ghw" satisfied condition "running"
Dec  9 07:56:38.539: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-c63f10eb-8a79-480e-921a-f39e89f3194f-qhxqw" in namespace "emptydir-wrapper-3189" to be "running"
Dec  9 07:56:38.542: INFO: Pod "wrapped-volume-race-c63f10eb-8a79-480e-921a-f39e89f3194f-qhxqw": Phase="Running", Reason="", readiness=true. Elapsed: 3.020488ms
Dec  9 07:56:38.542: INFO: Pod "wrapped-volume-race-c63f10eb-8a79-480e-921a-f39e89f3194f-qhxqw" satisfied condition "running"
Dec  9 07:56:38.542: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-c63f10eb-8a79-480e-921a-f39e89f3194f-t9smr" in namespace "emptydir-wrapper-3189" to be "running"
Dec  9 07:56:38.545: INFO: Pod "wrapped-volume-race-c63f10eb-8a79-480e-921a-f39e89f3194f-t9smr": Phase="Running", Reason="", readiness=true. Elapsed: 3.082096ms
Dec  9 07:56:38.545: INFO: Pod "wrapped-volume-race-c63f10eb-8a79-480e-921a-f39e89f3194f-t9smr" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-c63f10eb-8a79-480e-921a-f39e89f3194f in namespace emptydir-wrapper-3189, will wait for the garbage collector to delete the pods 12/09/22 07:56:38.545
Dec  9 07:56:38.607: INFO: Deleting ReplicationController wrapped-volume-race-c63f10eb-8a79-480e-921a-f39e89f3194f took: 8.486053ms
Dec  9 07:56:38.707: INFO: Terminating ReplicationController wrapped-volume-race-c63f10eb-8a79-480e-921a-f39e89f3194f pods took: 100.508376ms
STEP: Creating RC which spawns configmap-volume pods 12/09/22 07:56:42.618
Dec  9 07:56:42.633: INFO: Pod name wrapped-volume-race-adfa6c9c-3d11-4ff5-895a-525af7a1590b: Found 0 pods out of 5
Dec  9 07:56:47.639: INFO: Pod name wrapped-volume-race-adfa6c9c-3d11-4ff5-895a-525af7a1590b: Found 5 pods out of 5
STEP: Ensuring each pod is running 12/09/22 07:56:47.639
Dec  9 07:56:47.639: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-adfa6c9c-3d11-4ff5-895a-525af7a1590b-2b6rp" in namespace "emptydir-wrapper-3189" to be "running"
Dec  9 07:56:47.642: INFO: Pod "wrapped-volume-race-adfa6c9c-3d11-4ff5-895a-525af7a1590b-2b6rp": Phase="Pending", Reason="", readiness=false. Elapsed: 3.048873ms
Dec  9 07:56:49.647: INFO: Pod "wrapped-volume-race-adfa6c9c-3d11-4ff5-895a-525af7a1590b-2b6rp": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007283607s
Dec  9 07:56:51.647: INFO: Pod "wrapped-volume-race-adfa6c9c-3d11-4ff5-895a-525af7a1590b-2b6rp": Phase="Pending", Reason="", readiness=false. Elapsed: 4.00724685s
Dec  9 07:56:53.647: INFO: Pod "wrapped-volume-race-adfa6c9c-3d11-4ff5-895a-525af7a1590b-2b6rp": Phase="Pending", Reason="", readiness=false. Elapsed: 6.00733932s
Dec  9 07:56:55.667: INFO: Pod "wrapped-volume-race-adfa6c9c-3d11-4ff5-895a-525af7a1590b-2b6rp": Phase="Pending", Reason="", readiness=false. Elapsed: 8.027717774s
Dec  9 07:56:57.648: INFO: Pod "wrapped-volume-race-adfa6c9c-3d11-4ff5-895a-525af7a1590b-2b6rp": Phase="Running", Reason="", readiness=true. Elapsed: 10.008171723s
Dec  9 07:56:57.648: INFO: Pod "wrapped-volume-race-adfa6c9c-3d11-4ff5-895a-525af7a1590b-2b6rp" satisfied condition "running"
Dec  9 07:56:57.648: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-adfa6c9c-3d11-4ff5-895a-525af7a1590b-99x9q" in namespace "emptydir-wrapper-3189" to be "running"
Dec  9 07:56:57.651: INFO: Pod "wrapped-volume-race-adfa6c9c-3d11-4ff5-895a-525af7a1590b-99x9q": Phase="Pending", Reason="", readiness=false. Elapsed: 3.082049ms
Dec  9 07:56:59.655: INFO: Pod "wrapped-volume-race-adfa6c9c-3d11-4ff5-895a-525af7a1590b-99x9q": Phase="Running", Reason="", readiness=true. Elapsed: 2.007799642s
Dec  9 07:56:59.656: INFO: Pod "wrapped-volume-race-adfa6c9c-3d11-4ff5-895a-525af7a1590b-99x9q" satisfied condition "running"
Dec  9 07:56:59.656: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-adfa6c9c-3d11-4ff5-895a-525af7a1590b-nfbml" in namespace "emptydir-wrapper-3189" to be "running"
Dec  9 07:56:59.659: INFO: Pod "wrapped-volume-race-adfa6c9c-3d11-4ff5-895a-525af7a1590b-nfbml": Phase="Running", Reason="", readiness=true. Elapsed: 3.25556ms
Dec  9 07:56:59.659: INFO: Pod "wrapped-volume-race-adfa6c9c-3d11-4ff5-895a-525af7a1590b-nfbml" satisfied condition "running"
Dec  9 07:56:59.659: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-adfa6c9c-3d11-4ff5-895a-525af7a1590b-nq9mh" in namespace "emptydir-wrapper-3189" to be "running"
Dec  9 07:56:59.662: INFO: Pod "wrapped-volume-race-adfa6c9c-3d11-4ff5-895a-525af7a1590b-nq9mh": Phase="Running", Reason="", readiness=true. Elapsed: 3.455919ms
Dec  9 07:56:59.662: INFO: Pod "wrapped-volume-race-adfa6c9c-3d11-4ff5-895a-525af7a1590b-nq9mh" satisfied condition "running"
Dec  9 07:56:59.663: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-adfa6c9c-3d11-4ff5-895a-525af7a1590b-vsdqh" in namespace "emptydir-wrapper-3189" to be "running"
Dec  9 07:56:59.665: INFO: Pod "wrapped-volume-race-adfa6c9c-3d11-4ff5-895a-525af7a1590b-vsdqh": Phase="Running", Reason="", readiness=true. Elapsed: 2.673383ms
Dec  9 07:56:59.665: INFO: Pod "wrapped-volume-race-adfa6c9c-3d11-4ff5-895a-525af7a1590b-vsdqh" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-adfa6c9c-3d11-4ff5-895a-525af7a1590b in namespace emptydir-wrapper-3189, will wait for the garbage collector to delete the pods 12/09/22 07:56:59.665
Dec  9 07:56:59.727: INFO: Deleting ReplicationController wrapped-volume-race-adfa6c9c-3d11-4ff5-895a-525af7a1590b took: 7.939009ms
Dec  9 07:56:59.827: INFO: Terminating ReplicationController wrapped-volume-race-adfa6c9c-3d11-4ff5-895a-525af7a1590b pods took: 100.917835ms
STEP: Cleaning up the configMaps 12/09/22 07:57:02.528
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/node/init/init.go:32
Dec  9 07:57:02.815: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-wrapper-3189" for this suite. 12/09/22 07:57:02.828
------------------------------
â€¢ [SLOW TEST] [58.876 seconds]
[sig-storage] EmptyDir wrapper volumes
test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:56:03.983
    Dec  9 07:56:03.983: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename emptydir-wrapper 12/09/22 07:56:03.984
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:56:04.003
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:56:04.014
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should not cause race condition when used for configmaps [Serial] [Conformance]
      test/e2e/storage/empty_dir_wrapper.go:189
    STEP: Creating 50 configmaps 12/09/22 07:56:04.02
    STEP: Creating RC which spawns configmap-volume pods 12/09/22 07:56:04.332
    Dec  9 07:56:04.362: INFO: Pod name wrapped-volume-race-82d05535-52e0-4f8d-967b-41910560c8a2: Found 0 pods out of 5
    Dec  9 07:56:09.385: INFO: Pod name wrapped-volume-race-82d05535-52e0-4f8d-967b-41910560c8a2: Found 5 pods out of 5
    STEP: Ensuring each pod is running 12/09/22 07:56:09.386
    Dec  9 07:56:09.386: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-82d05535-52e0-4f8d-967b-41910560c8a2-2g4wj" in namespace "emptydir-wrapper-3189" to be "running"
    Dec  9 07:56:09.408: INFO: Pod "wrapped-volume-race-82d05535-52e0-4f8d-967b-41910560c8a2-2g4wj": Phase="Pending", Reason="", readiness=false. Elapsed: 21.633887ms
    Dec  9 07:56:11.412: INFO: Pod "wrapped-volume-race-82d05535-52e0-4f8d-967b-41910560c8a2-2g4wj": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025660592s
    Dec  9 07:56:13.415: INFO: Pod "wrapped-volume-race-82d05535-52e0-4f8d-967b-41910560c8a2-2g4wj": Phase="Pending", Reason="", readiness=false. Elapsed: 4.028543516s
    Dec  9 07:56:15.414: INFO: Pod "wrapped-volume-race-82d05535-52e0-4f8d-967b-41910560c8a2-2g4wj": Phase="Pending", Reason="", readiness=false. Elapsed: 6.027141741s
    Dec  9 07:56:17.414: INFO: Pod "wrapped-volume-race-82d05535-52e0-4f8d-967b-41910560c8a2-2g4wj": Phase="Pending", Reason="", readiness=false. Elapsed: 8.027353549s
    Dec  9 07:56:19.414: INFO: Pod "wrapped-volume-race-82d05535-52e0-4f8d-967b-41910560c8a2-2g4wj": Phase="Running", Reason="", readiness=true. Elapsed: 10.027247946s
    Dec  9 07:56:19.414: INFO: Pod "wrapped-volume-race-82d05535-52e0-4f8d-967b-41910560c8a2-2g4wj" satisfied condition "running"
    Dec  9 07:56:19.414: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-82d05535-52e0-4f8d-967b-41910560c8a2-2tctv" in namespace "emptydir-wrapper-3189" to be "running"
    Dec  9 07:56:19.417: INFO: Pod "wrapped-volume-race-82d05535-52e0-4f8d-967b-41910560c8a2-2tctv": Phase="Running", Reason="", readiness=true. Elapsed: 3.128304ms
    Dec  9 07:56:19.417: INFO: Pod "wrapped-volume-race-82d05535-52e0-4f8d-967b-41910560c8a2-2tctv" satisfied condition "running"
    Dec  9 07:56:19.417: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-82d05535-52e0-4f8d-967b-41910560c8a2-hvjxf" in namespace "emptydir-wrapper-3189" to be "running"
    Dec  9 07:56:19.420: INFO: Pod "wrapped-volume-race-82d05535-52e0-4f8d-967b-41910560c8a2-hvjxf": Phase="Running", Reason="", readiness=true. Elapsed: 3.060476ms
    Dec  9 07:56:19.420: INFO: Pod "wrapped-volume-race-82d05535-52e0-4f8d-967b-41910560c8a2-hvjxf" satisfied condition "running"
    Dec  9 07:56:19.420: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-82d05535-52e0-4f8d-967b-41910560c8a2-j7wlr" in namespace "emptydir-wrapper-3189" to be "running"
    Dec  9 07:56:19.423: INFO: Pod "wrapped-volume-race-82d05535-52e0-4f8d-967b-41910560c8a2-j7wlr": Phase="Running", Reason="", readiness=true. Elapsed: 2.942467ms
    Dec  9 07:56:19.423: INFO: Pod "wrapped-volume-race-82d05535-52e0-4f8d-967b-41910560c8a2-j7wlr" satisfied condition "running"
    Dec  9 07:56:19.423: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-82d05535-52e0-4f8d-967b-41910560c8a2-lg7bx" in namespace "emptydir-wrapper-3189" to be "running"
    Dec  9 07:56:19.426: INFO: Pod "wrapped-volume-race-82d05535-52e0-4f8d-967b-41910560c8a2-lg7bx": Phase="Running", Reason="", readiness=true. Elapsed: 2.893602ms
    Dec  9 07:56:19.426: INFO: Pod "wrapped-volume-race-82d05535-52e0-4f8d-967b-41910560c8a2-lg7bx" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-82d05535-52e0-4f8d-967b-41910560c8a2 in namespace emptydir-wrapper-3189, will wait for the garbage collector to delete the pods 12/09/22 07:56:19.426
    Dec  9 07:56:19.492: INFO: Deleting ReplicationController wrapped-volume-race-82d05535-52e0-4f8d-967b-41910560c8a2 took: 11.743883ms
    Dec  9 07:56:19.594: INFO: Terminating ReplicationController wrapped-volume-race-82d05535-52e0-4f8d-967b-41910560c8a2 pods took: 102.262457ms
    STEP: Creating RC which spawns configmap-volume pods 12/09/22 07:56:23.498
    Dec  9 07:56:23.515: INFO: Pod name wrapped-volume-race-c63f10eb-8a79-480e-921a-f39e89f3194f: Found 0 pods out of 5
    Dec  9 07:56:28.524: INFO: Pod name wrapped-volume-race-c63f10eb-8a79-480e-921a-f39e89f3194f: Found 5 pods out of 5
    STEP: Ensuring each pod is running 12/09/22 07:56:28.524
    Dec  9 07:56:28.524: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-c63f10eb-8a79-480e-921a-f39e89f3194f-27dtq" in namespace "emptydir-wrapper-3189" to be "running"
    Dec  9 07:56:28.529: INFO: Pod "wrapped-volume-race-c63f10eb-8a79-480e-921a-f39e89f3194f-27dtq": Phase="Pending", Reason="", readiness=false. Elapsed: 4.43418ms
    Dec  9 07:56:30.538: INFO: Pod "wrapped-volume-race-c63f10eb-8a79-480e-921a-f39e89f3194f-27dtq": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013310742s
    Dec  9 07:56:32.533: INFO: Pod "wrapped-volume-race-c63f10eb-8a79-480e-921a-f39e89f3194f-27dtq": Phase="Pending", Reason="", readiness=false. Elapsed: 4.00911862s
    Dec  9 07:56:34.540: INFO: Pod "wrapped-volume-race-c63f10eb-8a79-480e-921a-f39e89f3194f-27dtq": Phase="Pending", Reason="", readiness=false. Elapsed: 6.015736364s
    Dec  9 07:56:36.540: INFO: Pod "wrapped-volume-race-c63f10eb-8a79-480e-921a-f39e89f3194f-27dtq": Phase="Pending", Reason="", readiness=false. Elapsed: 8.015620955s
    Dec  9 07:56:38.533: INFO: Pod "wrapped-volume-race-c63f10eb-8a79-480e-921a-f39e89f3194f-27dtq": Phase="Running", Reason="", readiness=true. Elapsed: 10.00843399s
    Dec  9 07:56:38.533: INFO: Pod "wrapped-volume-race-c63f10eb-8a79-480e-921a-f39e89f3194f-27dtq" satisfied condition "running"
    Dec  9 07:56:38.533: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-c63f10eb-8a79-480e-921a-f39e89f3194f-2gczh" in namespace "emptydir-wrapper-3189" to be "running"
    Dec  9 07:56:38.536: INFO: Pod "wrapped-volume-race-c63f10eb-8a79-480e-921a-f39e89f3194f-2gczh": Phase="Running", Reason="", readiness=true. Elapsed: 3.008384ms
    Dec  9 07:56:38.536: INFO: Pod "wrapped-volume-race-c63f10eb-8a79-480e-921a-f39e89f3194f-2gczh" satisfied condition "running"
    Dec  9 07:56:38.536: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-c63f10eb-8a79-480e-921a-f39e89f3194f-n2ghw" in namespace "emptydir-wrapper-3189" to be "running"
    Dec  9 07:56:38.539: INFO: Pod "wrapped-volume-race-c63f10eb-8a79-480e-921a-f39e89f3194f-n2ghw": Phase="Running", Reason="", readiness=true. Elapsed: 2.862943ms
    Dec  9 07:56:38.539: INFO: Pod "wrapped-volume-race-c63f10eb-8a79-480e-921a-f39e89f3194f-n2ghw" satisfied condition "running"
    Dec  9 07:56:38.539: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-c63f10eb-8a79-480e-921a-f39e89f3194f-qhxqw" in namespace "emptydir-wrapper-3189" to be "running"
    Dec  9 07:56:38.542: INFO: Pod "wrapped-volume-race-c63f10eb-8a79-480e-921a-f39e89f3194f-qhxqw": Phase="Running", Reason="", readiness=true. Elapsed: 3.020488ms
    Dec  9 07:56:38.542: INFO: Pod "wrapped-volume-race-c63f10eb-8a79-480e-921a-f39e89f3194f-qhxqw" satisfied condition "running"
    Dec  9 07:56:38.542: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-c63f10eb-8a79-480e-921a-f39e89f3194f-t9smr" in namespace "emptydir-wrapper-3189" to be "running"
    Dec  9 07:56:38.545: INFO: Pod "wrapped-volume-race-c63f10eb-8a79-480e-921a-f39e89f3194f-t9smr": Phase="Running", Reason="", readiness=true. Elapsed: 3.082096ms
    Dec  9 07:56:38.545: INFO: Pod "wrapped-volume-race-c63f10eb-8a79-480e-921a-f39e89f3194f-t9smr" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-c63f10eb-8a79-480e-921a-f39e89f3194f in namespace emptydir-wrapper-3189, will wait for the garbage collector to delete the pods 12/09/22 07:56:38.545
    Dec  9 07:56:38.607: INFO: Deleting ReplicationController wrapped-volume-race-c63f10eb-8a79-480e-921a-f39e89f3194f took: 8.486053ms
    Dec  9 07:56:38.707: INFO: Terminating ReplicationController wrapped-volume-race-c63f10eb-8a79-480e-921a-f39e89f3194f pods took: 100.508376ms
    STEP: Creating RC which spawns configmap-volume pods 12/09/22 07:56:42.618
    Dec  9 07:56:42.633: INFO: Pod name wrapped-volume-race-adfa6c9c-3d11-4ff5-895a-525af7a1590b: Found 0 pods out of 5
    Dec  9 07:56:47.639: INFO: Pod name wrapped-volume-race-adfa6c9c-3d11-4ff5-895a-525af7a1590b: Found 5 pods out of 5
    STEP: Ensuring each pod is running 12/09/22 07:56:47.639
    Dec  9 07:56:47.639: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-adfa6c9c-3d11-4ff5-895a-525af7a1590b-2b6rp" in namespace "emptydir-wrapper-3189" to be "running"
    Dec  9 07:56:47.642: INFO: Pod "wrapped-volume-race-adfa6c9c-3d11-4ff5-895a-525af7a1590b-2b6rp": Phase="Pending", Reason="", readiness=false. Elapsed: 3.048873ms
    Dec  9 07:56:49.647: INFO: Pod "wrapped-volume-race-adfa6c9c-3d11-4ff5-895a-525af7a1590b-2b6rp": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007283607s
    Dec  9 07:56:51.647: INFO: Pod "wrapped-volume-race-adfa6c9c-3d11-4ff5-895a-525af7a1590b-2b6rp": Phase="Pending", Reason="", readiness=false. Elapsed: 4.00724685s
    Dec  9 07:56:53.647: INFO: Pod "wrapped-volume-race-adfa6c9c-3d11-4ff5-895a-525af7a1590b-2b6rp": Phase="Pending", Reason="", readiness=false. Elapsed: 6.00733932s
    Dec  9 07:56:55.667: INFO: Pod "wrapped-volume-race-adfa6c9c-3d11-4ff5-895a-525af7a1590b-2b6rp": Phase="Pending", Reason="", readiness=false. Elapsed: 8.027717774s
    Dec  9 07:56:57.648: INFO: Pod "wrapped-volume-race-adfa6c9c-3d11-4ff5-895a-525af7a1590b-2b6rp": Phase="Running", Reason="", readiness=true. Elapsed: 10.008171723s
    Dec  9 07:56:57.648: INFO: Pod "wrapped-volume-race-adfa6c9c-3d11-4ff5-895a-525af7a1590b-2b6rp" satisfied condition "running"
    Dec  9 07:56:57.648: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-adfa6c9c-3d11-4ff5-895a-525af7a1590b-99x9q" in namespace "emptydir-wrapper-3189" to be "running"
    Dec  9 07:56:57.651: INFO: Pod "wrapped-volume-race-adfa6c9c-3d11-4ff5-895a-525af7a1590b-99x9q": Phase="Pending", Reason="", readiness=false. Elapsed: 3.082049ms
    Dec  9 07:56:59.655: INFO: Pod "wrapped-volume-race-adfa6c9c-3d11-4ff5-895a-525af7a1590b-99x9q": Phase="Running", Reason="", readiness=true. Elapsed: 2.007799642s
    Dec  9 07:56:59.656: INFO: Pod "wrapped-volume-race-adfa6c9c-3d11-4ff5-895a-525af7a1590b-99x9q" satisfied condition "running"
    Dec  9 07:56:59.656: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-adfa6c9c-3d11-4ff5-895a-525af7a1590b-nfbml" in namespace "emptydir-wrapper-3189" to be "running"
    Dec  9 07:56:59.659: INFO: Pod "wrapped-volume-race-adfa6c9c-3d11-4ff5-895a-525af7a1590b-nfbml": Phase="Running", Reason="", readiness=true. Elapsed: 3.25556ms
    Dec  9 07:56:59.659: INFO: Pod "wrapped-volume-race-adfa6c9c-3d11-4ff5-895a-525af7a1590b-nfbml" satisfied condition "running"
    Dec  9 07:56:59.659: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-adfa6c9c-3d11-4ff5-895a-525af7a1590b-nq9mh" in namespace "emptydir-wrapper-3189" to be "running"
    Dec  9 07:56:59.662: INFO: Pod "wrapped-volume-race-adfa6c9c-3d11-4ff5-895a-525af7a1590b-nq9mh": Phase="Running", Reason="", readiness=true. Elapsed: 3.455919ms
    Dec  9 07:56:59.662: INFO: Pod "wrapped-volume-race-adfa6c9c-3d11-4ff5-895a-525af7a1590b-nq9mh" satisfied condition "running"
    Dec  9 07:56:59.663: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-adfa6c9c-3d11-4ff5-895a-525af7a1590b-vsdqh" in namespace "emptydir-wrapper-3189" to be "running"
    Dec  9 07:56:59.665: INFO: Pod "wrapped-volume-race-adfa6c9c-3d11-4ff5-895a-525af7a1590b-vsdqh": Phase="Running", Reason="", readiness=true. Elapsed: 2.673383ms
    Dec  9 07:56:59.665: INFO: Pod "wrapped-volume-race-adfa6c9c-3d11-4ff5-895a-525af7a1590b-vsdqh" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-adfa6c9c-3d11-4ff5-895a-525af7a1590b in namespace emptydir-wrapper-3189, will wait for the garbage collector to delete the pods 12/09/22 07:56:59.665
    Dec  9 07:56:59.727: INFO: Deleting ReplicationController wrapped-volume-race-adfa6c9c-3d11-4ff5-895a-525af7a1590b took: 7.939009ms
    Dec  9 07:56:59.827: INFO: Terminating ReplicationController wrapped-volume-race-adfa6c9c-3d11-4ff5-895a-525af7a1590b pods took: 100.917835ms
    STEP: Cleaning up the configMaps 12/09/22 07:57:02.528
    [AfterEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/node/init/init.go:32
    Dec  9 07:57:02.815: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-wrapper-3189" for this suite. 12/09/22 07:57:02.828
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-apps] CronJob
  should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 07:57:02.86
Dec  9 07:57:02.860: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename cronjob 12/09/22 07:57:02.868
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:57:02.91
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:57:02.924
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96
STEP: Creating a suspended cronjob 12/09/22 07:57:02.929
STEP: Ensuring no jobs are scheduled 12/09/22 07:57:02.934
STEP: Ensuring no job exists by listing jobs explicitly 12/09/22 08:02:02.941
STEP: Removing cronjob 12/09/22 08:02:02.947
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Dec  9 08:02:02.957: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-7079" for this suite. 12/09/22 08:02:02.965
------------------------------
â€¢ [SLOW TEST] [300.117 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 07:57:02.86
    Dec  9 07:57:02.860: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename cronjob 12/09/22 07:57:02.868
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 07:57:02.91
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 07:57:02.924
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should not schedule jobs when suspended [Slow] [Conformance]
      test/e2e/apps/cronjob.go:96
    STEP: Creating a suspended cronjob 12/09/22 07:57:02.929
    STEP: Ensuring no jobs are scheduled 12/09/22 07:57:02.934
    STEP: Ensuring no job exists by listing jobs explicitly 12/09/22 08:02:02.941
    STEP: Removing cronjob 12/09/22 08:02:02.947
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Dec  9 08:02:02.957: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-7079" for this suite. 12/09/22 08:02:02.965
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-apps] DisruptionController
  should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:347
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 08:02:02.979
Dec  9 08:02:02.979: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename disruption 12/09/22 08:02:02.982
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:02:03.034
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:02:03.04
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[It] should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:347
STEP: Creating a pdb that targets all three pods in a test replica set 12/09/22 08:02:03.046
STEP: Waiting for the pdb to be processed 12/09/22 08:02:03.052
STEP: First trying to evict a pod which shouldn't be evictable 12/09/22 08:02:05.081
STEP: Waiting for all pods to be running 12/09/22 08:02:05.081
Dec  9 08:02:05.088: INFO: pods: 0 < 3
Dec  9 08:02:07.092: INFO: running pods: 0 < 3
STEP: locating a running pod 12/09/22 08:02:09.094
STEP: Updating the pdb to allow a pod to be evicted 12/09/22 08:02:09.109
STEP: Waiting for the pdb to be processed 12/09/22 08:02:09.118
STEP: Trying to evict the same pod we tried earlier which should now be evictable 12/09/22 08:02:11.148
STEP: Waiting for all pods to be running 12/09/22 08:02:11.148
STEP: Waiting for the pdb to observed all healthy pods 12/09/22 08:02:11.152
STEP: Patching the pdb to disallow a pod to be evicted 12/09/22 08:02:11.186
STEP: Waiting for the pdb to be processed 12/09/22 08:02:11.233
STEP: Waiting for all pods to be running 12/09/22 08:02:11.252
Dec  9 08:02:11.258: INFO: running pods: 2 < 3
STEP: locating a running pod 12/09/22 08:02:13.262
STEP: Deleting the pdb to allow a pod to be evicted 12/09/22 08:02:13.272
STEP: Waiting for the pdb to be deleted 12/09/22 08:02:13.278
STEP: Trying to evict the same pod we tried earlier which should now be evictable 12/09/22 08:02:13.281
STEP: Waiting for all pods to be running 12/09/22 08:02:13.281
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Dec  9 08:02:13.297: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-5931" for this suite. 12/09/22 08:02:13.31
------------------------------
â€¢ [SLOW TEST] [10.375 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:347

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 08:02:02.979
    Dec  9 08:02:02.979: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename disruption 12/09/22 08:02:02.982
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:02:03.034
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:02:03.04
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [It] should block an eviction until the PDB is updated to allow it [Conformance]
      test/e2e/apps/disruption.go:347
    STEP: Creating a pdb that targets all three pods in a test replica set 12/09/22 08:02:03.046
    STEP: Waiting for the pdb to be processed 12/09/22 08:02:03.052
    STEP: First trying to evict a pod which shouldn't be evictable 12/09/22 08:02:05.081
    STEP: Waiting for all pods to be running 12/09/22 08:02:05.081
    Dec  9 08:02:05.088: INFO: pods: 0 < 3
    Dec  9 08:02:07.092: INFO: running pods: 0 < 3
    STEP: locating a running pod 12/09/22 08:02:09.094
    STEP: Updating the pdb to allow a pod to be evicted 12/09/22 08:02:09.109
    STEP: Waiting for the pdb to be processed 12/09/22 08:02:09.118
    STEP: Trying to evict the same pod we tried earlier which should now be evictable 12/09/22 08:02:11.148
    STEP: Waiting for all pods to be running 12/09/22 08:02:11.148
    STEP: Waiting for the pdb to observed all healthy pods 12/09/22 08:02:11.152
    STEP: Patching the pdb to disallow a pod to be evicted 12/09/22 08:02:11.186
    STEP: Waiting for the pdb to be processed 12/09/22 08:02:11.233
    STEP: Waiting for all pods to be running 12/09/22 08:02:11.252
    Dec  9 08:02:11.258: INFO: running pods: 2 < 3
    STEP: locating a running pod 12/09/22 08:02:13.262
    STEP: Deleting the pdb to allow a pod to be evicted 12/09/22 08:02:13.272
    STEP: Waiting for the pdb to be deleted 12/09/22 08:02:13.278
    STEP: Trying to evict the same pod we tried earlier which should now be evictable 12/09/22 08:02:13.281
    STEP: Waiting for all pods to be running 12/09/22 08:02:13.281
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Dec  9 08:02:13.297: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-5931" for this suite. 12/09/22 08:02:13.31
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] Projected configMap
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:174
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 08:02:13.355
Dec  9 08:02:13.355: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename projected 12/09/22 08:02:13.356
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:02:13.411
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:02:13.416
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:174
STEP: Creating configMap with name cm-test-opt-del-168f1916-9776-4aa7-8aa3-b9874dabbac5 12/09/22 08:02:13.423
STEP: Creating configMap with name cm-test-opt-upd-1b67cd66-6241-4a5a-9637-fe8cec2db365 12/09/22 08:02:13.428
STEP: Creating the pod 12/09/22 08:02:13.431
Dec  9 08:02:13.441: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-e96583a5-2bc5-401d-a14e-53a6cf7b6220" in namespace "projected-6354" to be "running and ready"
Dec  9 08:02:13.447: INFO: Pod "pod-projected-configmaps-e96583a5-2bc5-401d-a14e-53a6cf7b6220": Phase="Pending", Reason="", readiness=false. Elapsed: 5.488808ms
Dec  9 08:02:13.447: INFO: The phase of Pod pod-projected-configmaps-e96583a5-2bc5-401d-a14e-53a6cf7b6220 is Pending, waiting for it to be Running (with Ready = true)
Dec  9 08:02:15.451: INFO: Pod "pod-projected-configmaps-e96583a5-2bc5-401d-a14e-53a6cf7b6220": Phase="Running", Reason="", readiness=true. Elapsed: 2.009618689s
Dec  9 08:02:15.451: INFO: The phase of Pod pod-projected-configmaps-e96583a5-2bc5-401d-a14e-53a6cf7b6220 is Running (Ready = true)
Dec  9 08:02:15.451: INFO: Pod "pod-projected-configmaps-e96583a5-2bc5-401d-a14e-53a6cf7b6220" satisfied condition "running and ready"
STEP: Deleting configmap cm-test-opt-del-168f1916-9776-4aa7-8aa3-b9874dabbac5 12/09/22 08:02:15.476
STEP: Updating configmap cm-test-opt-upd-1b67cd66-6241-4a5a-9637-fe8cec2db365 12/09/22 08:02:15.483
STEP: Creating configMap with name cm-test-opt-create-0afdba34-744e-4a6f-a3ee-ece2bb0ba7d5 12/09/22 08:02:15.489
STEP: waiting to observe update in volume 12/09/22 08:02:15.494
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Dec  9 08:02:17.521: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-6354" for this suite. 12/09/22 08:02:17.526
------------------------------
â€¢ [4.176 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:174

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 08:02:13.355
    Dec  9 08:02:13.355: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename projected 12/09/22 08:02:13.356
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:02:13.411
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:02:13.416
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:174
    STEP: Creating configMap with name cm-test-opt-del-168f1916-9776-4aa7-8aa3-b9874dabbac5 12/09/22 08:02:13.423
    STEP: Creating configMap with name cm-test-opt-upd-1b67cd66-6241-4a5a-9637-fe8cec2db365 12/09/22 08:02:13.428
    STEP: Creating the pod 12/09/22 08:02:13.431
    Dec  9 08:02:13.441: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-e96583a5-2bc5-401d-a14e-53a6cf7b6220" in namespace "projected-6354" to be "running and ready"
    Dec  9 08:02:13.447: INFO: Pod "pod-projected-configmaps-e96583a5-2bc5-401d-a14e-53a6cf7b6220": Phase="Pending", Reason="", readiness=false. Elapsed: 5.488808ms
    Dec  9 08:02:13.447: INFO: The phase of Pod pod-projected-configmaps-e96583a5-2bc5-401d-a14e-53a6cf7b6220 is Pending, waiting for it to be Running (with Ready = true)
    Dec  9 08:02:15.451: INFO: Pod "pod-projected-configmaps-e96583a5-2bc5-401d-a14e-53a6cf7b6220": Phase="Running", Reason="", readiness=true. Elapsed: 2.009618689s
    Dec  9 08:02:15.451: INFO: The phase of Pod pod-projected-configmaps-e96583a5-2bc5-401d-a14e-53a6cf7b6220 is Running (Ready = true)
    Dec  9 08:02:15.451: INFO: Pod "pod-projected-configmaps-e96583a5-2bc5-401d-a14e-53a6cf7b6220" satisfied condition "running and ready"
    STEP: Deleting configmap cm-test-opt-del-168f1916-9776-4aa7-8aa3-b9874dabbac5 12/09/22 08:02:15.476
    STEP: Updating configmap cm-test-opt-upd-1b67cd66-6241-4a5a-9637-fe8cec2db365 12/09/22 08:02:15.483
    STEP: Creating configMap with name cm-test-opt-create-0afdba34-744e-4a6f-a3ee-ece2bb0ba7d5 12/09/22 08:02:15.489
    STEP: waiting to observe update in volume 12/09/22 08:02:15.494
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Dec  9 08:02:17.521: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-6354" for this suite. 12/09/22 08:02:17.526
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:141
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 08:02:17.534
Dec  9 08:02:17.534: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename disruption 12/09/22 08:02:17.534
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:02:17.551
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:02:17.555
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[It] should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:141
STEP: Waiting for the pdb to be processed 12/09/22 08:02:17.563
STEP: Waiting for all pods to be running 12/09/22 08:02:19.611
Dec  9 08:02:19.623: INFO: running pods: 0 < 3
Dec  9 08:02:21.634: INFO: running pods: 1 < 3
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Dec  9 08:02:23.630: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-6611" for this suite. 12/09/22 08:02:23.634
------------------------------
â€¢ [SLOW TEST] [6.106 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:141

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 08:02:17.534
    Dec  9 08:02:17.534: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename disruption 12/09/22 08:02:17.534
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:02:17.551
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:02:17.555
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [It] should observe PodDisruptionBudget status updated [Conformance]
      test/e2e/apps/disruption.go:141
    STEP: Waiting for the pdb to be processed 12/09/22 08:02:17.563
    STEP: Waiting for all pods to be running 12/09/22 08:02:19.611
    Dec  9 08:02:19.623: INFO: running pods: 0 < 3
    Dec  9 08:02:21.634: INFO: running pods: 1 < 3
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Dec  9 08:02:23.630: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-6611" for this suite. 12/09/22 08:02:23.634
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] ConfigMap
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:240
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 08:02:23.639
Dec  9 08:02:23.640: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename configmap 12/09/22 08:02:23.64
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:02:23.663
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:02:23.668
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:240
STEP: Creating configMap with name cm-test-opt-del-d7097259-bb4b-4a5b-aa65-c192db6d4636 12/09/22 08:02:23.674
STEP: Creating configMap with name cm-test-opt-upd-da54d7d1-798d-4e4d-b9ab-2a71730b540b 12/09/22 08:02:23.678
STEP: Creating the pod 12/09/22 08:02:23.683
Dec  9 08:02:23.691: INFO: Waiting up to 5m0s for pod "pod-configmaps-bd5347fe-6dcb-45f5-bb8c-ec9530227a40" in namespace "configmap-6458" to be "running and ready"
Dec  9 08:02:23.694: INFO: Pod "pod-configmaps-bd5347fe-6dcb-45f5-bb8c-ec9530227a40": Phase="Pending", Reason="", readiness=false. Elapsed: 2.989921ms
Dec  9 08:02:23.694: INFO: The phase of Pod pod-configmaps-bd5347fe-6dcb-45f5-bb8c-ec9530227a40 is Pending, waiting for it to be Running (with Ready = true)
Dec  9 08:02:25.699: INFO: Pod "pod-configmaps-bd5347fe-6dcb-45f5-bb8c-ec9530227a40": Phase="Running", Reason="", readiness=true. Elapsed: 2.007558422s
Dec  9 08:02:25.699: INFO: The phase of Pod pod-configmaps-bd5347fe-6dcb-45f5-bb8c-ec9530227a40 is Running (Ready = true)
Dec  9 08:02:25.699: INFO: Pod "pod-configmaps-bd5347fe-6dcb-45f5-bb8c-ec9530227a40" satisfied condition "running and ready"
STEP: Deleting configmap cm-test-opt-del-d7097259-bb4b-4a5b-aa65-c192db6d4636 12/09/22 08:02:25.719
STEP: Updating configmap cm-test-opt-upd-da54d7d1-798d-4e4d-b9ab-2a71730b540b 12/09/22 08:02:25.724
STEP: Creating configMap with name cm-test-opt-create-af3d9145-cd0a-4694-8dbd-a999d12d0338 12/09/22 08:02:25.729
STEP: waiting to observe update in volume 12/09/22 08:02:25.735
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Dec  9 08:02:27.755: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-6458" for this suite. 12/09/22 08:02:27.758
------------------------------
â€¢ [4.125 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:240

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 08:02:23.639
    Dec  9 08:02:23.640: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename configmap 12/09/22 08:02:23.64
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:02:23.663
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:02:23.668
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:240
    STEP: Creating configMap with name cm-test-opt-del-d7097259-bb4b-4a5b-aa65-c192db6d4636 12/09/22 08:02:23.674
    STEP: Creating configMap with name cm-test-opt-upd-da54d7d1-798d-4e4d-b9ab-2a71730b540b 12/09/22 08:02:23.678
    STEP: Creating the pod 12/09/22 08:02:23.683
    Dec  9 08:02:23.691: INFO: Waiting up to 5m0s for pod "pod-configmaps-bd5347fe-6dcb-45f5-bb8c-ec9530227a40" in namespace "configmap-6458" to be "running and ready"
    Dec  9 08:02:23.694: INFO: Pod "pod-configmaps-bd5347fe-6dcb-45f5-bb8c-ec9530227a40": Phase="Pending", Reason="", readiness=false. Elapsed: 2.989921ms
    Dec  9 08:02:23.694: INFO: The phase of Pod pod-configmaps-bd5347fe-6dcb-45f5-bb8c-ec9530227a40 is Pending, waiting for it to be Running (with Ready = true)
    Dec  9 08:02:25.699: INFO: Pod "pod-configmaps-bd5347fe-6dcb-45f5-bb8c-ec9530227a40": Phase="Running", Reason="", readiness=true. Elapsed: 2.007558422s
    Dec  9 08:02:25.699: INFO: The phase of Pod pod-configmaps-bd5347fe-6dcb-45f5-bb8c-ec9530227a40 is Running (Ready = true)
    Dec  9 08:02:25.699: INFO: Pod "pod-configmaps-bd5347fe-6dcb-45f5-bb8c-ec9530227a40" satisfied condition "running and ready"
    STEP: Deleting configmap cm-test-opt-del-d7097259-bb4b-4a5b-aa65-c192db6d4636 12/09/22 08:02:25.719
    STEP: Updating configmap cm-test-opt-upd-da54d7d1-798d-4e4d-b9ab-2a71730b540b 12/09/22 08:02:25.724
    STEP: Creating configMap with name cm-test-opt-create-af3d9145-cd0a-4694-8dbd-a999d12d0338 12/09/22 08:02:25.729
    STEP: waiting to observe update in volume 12/09/22 08:02:25.735
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Dec  9 08:02:27.755: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-6458" for this suite. 12/09/22 08:02:27.758
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:309
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 08:02:27.768
Dec  9 08:02:27.769: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename crd-publish-openapi 12/09/22 08:02:27.771
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:02:27.789
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:02:27.793
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:309
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation 12/09/22 08:02:27.797
Dec  9 08:02:27.797: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation 12/09/22 08:02:33.976
Dec  9 08:02:33.976: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
Dec  9 08:02:35.454: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Dec  9 08:02:41.988: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-9488" for this suite. 12/09/22 08:02:41.997
------------------------------
â€¢ [SLOW TEST] [14.237 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:309

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 08:02:27.768
    Dec  9 08:02:27.769: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename crd-publish-openapi 12/09/22 08:02:27.771
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:02:27.789
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:02:27.793
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for multiple CRDs of same group but different versions [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:309
    STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation 12/09/22 08:02:27.797
    Dec  9 08:02:27.797: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation 12/09/22 08:02:33.976
    Dec  9 08:02:33.976: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    Dec  9 08:02:35.454: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Dec  9 08:02:41.988: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-9488" for this suite. 12/09/22 08:02:41.997
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:357
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 08:02:42.005
Dec  9 08:02:42.006: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename crd-publish-openapi 12/09/22 08:02:42.007
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:02:42.024
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:02:42.027
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:357
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation 12/09/22 08:02:42.031
Dec  9 08:02:42.032: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
Dec  9 08:02:43.622: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Dec  9 08:02:49.897: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-5058" for this suite. 12/09/22 08:02:49.906
------------------------------
â€¢ [SLOW TEST] [7.907 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:357

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 08:02:42.005
    Dec  9 08:02:42.006: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename crd-publish-openapi 12/09/22 08:02:42.007
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:02:42.024
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:02:42.027
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for multiple CRDs of same group and version but different kinds [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:357
    STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation 12/09/22 08:02:42.031
    Dec  9 08:02:42.032: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    Dec  9 08:02:43.622: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Dec  9 08:02:49.897: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-5058" for this suite. 12/09/22 08:02:49.906
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:79
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 08:02:49.914
Dec  9 08:02:49.914: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename secrets 12/09/22 08:02:49.915
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:02:49.931
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:02:49.935
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:79
STEP: Creating secret with name secret-test-map-b0802635-267c-48af-803d-90ebe8840b9d 12/09/22 08:02:49.938
STEP: Creating a pod to test consume secrets 12/09/22 08:02:49.943
Dec  9 08:02:49.951: INFO: Waiting up to 5m0s for pod "pod-secrets-820b9fc7-450b-4bc2-ad50-c90be5268592" in namespace "secrets-3967" to be "Succeeded or Failed"
Dec  9 08:02:49.954: INFO: Pod "pod-secrets-820b9fc7-450b-4bc2-ad50-c90be5268592": Phase="Pending", Reason="", readiness=false. Elapsed: 2.787852ms
Dec  9 08:02:51.959: INFO: Pod "pod-secrets-820b9fc7-450b-4bc2-ad50-c90be5268592": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008553947s
Dec  9 08:02:53.957: INFO: Pod "pod-secrets-820b9fc7-450b-4bc2-ad50-c90be5268592": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006510308s
STEP: Saw pod success 12/09/22 08:02:53.957
Dec  9 08:02:53.958: INFO: Pod "pod-secrets-820b9fc7-450b-4bc2-ad50-c90be5268592" satisfied condition "Succeeded or Failed"
Dec  9 08:02:53.960: INFO: Trying to get logs from node ip-10-0-10-179 pod pod-secrets-820b9fc7-450b-4bc2-ad50-c90be5268592 container secret-volume-test: <nil>
STEP: delete the pod 12/09/22 08:02:53.966
Dec  9 08:02:53.976: INFO: Waiting for pod pod-secrets-820b9fc7-450b-4bc2-ad50-c90be5268592 to disappear
Dec  9 08:02:53.983: INFO: Pod pod-secrets-820b9fc7-450b-4bc2-ad50-c90be5268592 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Dec  9 08:02:53.984: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-3967" for this suite. 12/09/22 08:02:53.988
------------------------------
â€¢ [4.081 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:79

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 08:02:49.914
    Dec  9 08:02:49.914: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename secrets 12/09/22 08:02:49.915
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:02:49.931
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:02:49.935
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:79
    STEP: Creating secret with name secret-test-map-b0802635-267c-48af-803d-90ebe8840b9d 12/09/22 08:02:49.938
    STEP: Creating a pod to test consume secrets 12/09/22 08:02:49.943
    Dec  9 08:02:49.951: INFO: Waiting up to 5m0s for pod "pod-secrets-820b9fc7-450b-4bc2-ad50-c90be5268592" in namespace "secrets-3967" to be "Succeeded or Failed"
    Dec  9 08:02:49.954: INFO: Pod "pod-secrets-820b9fc7-450b-4bc2-ad50-c90be5268592": Phase="Pending", Reason="", readiness=false. Elapsed: 2.787852ms
    Dec  9 08:02:51.959: INFO: Pod "pod-secrets-820b9fc7-450b-4bc2-ad50-c90be5268592": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008553947s
    Dec  9 08:02:53.957: INFO: Pod "pod-secrets-820b9fc7-450b-4bc2-ad50-c90be5268592": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006510308s
    STEP: Saw pod success 12/09/22 08:02:53.957
    Dec  9 08:02:53.958: INFO: Pod "pod-secrets-820b9fc7-450b-4bc2-ad50-c90be5268592" satisfied condition "Succeeded or Failed"
    Dec  9 08:02:53.960: INFO: Trying to get logs from node ip-10-0-10-179 pod pod-secrets-820b9fc7-450b-4bc2-ad50-c90be5268592 container secret-volume-test: <nil>
    STEP: delete the pod 12/09/22 08:02:53.966
    Dec  9 08:02:53.976: INFO: Waiting for pod pod-secrets-820b9fc7-450b-4bc2-ad50-c90be5268592 to disappear
    Dec  9 08:02:53.983: INFO: Pod pod-secrets-820b9fc7-450b-4bc2-ad50-c90be5268592 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Dec  9 08:02:53.984: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-3967" for this suite. 12/09/22 08:02:53.988
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:44
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 08:02:53.998
Dec  9 08:02:53.999: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename var-expansion 12/09/22 08:02:53.999
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:02:54.018
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:02:54.023
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:44
STEP: Creating a pod to test env composition 12/09/22 08:02:54.026
Dec  9 08:02:54.034: INFO: Waiting up to 5m0s for pod "var-expansion-709981a1-53d6-4007-8e94-8a6e6e648e40" in namespace "var-expansion-7953" to be "Succeeded or Failed"
Dec  9 08:02:54.037: INFO: Pod "var-expansion-709981a1-53d6-4007-8e94-8a6e6e648e40": Phase="Pending", Reason="", readiness=false. Elapsed: 2.934066ms
Dec  9 08:02:56.041: INFO: Pod "var-expansion-709981a1-53d6-4007-8e94-8a6e6e648e40": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006506124s
Dec  9 08:02:58.041: INFO: Pod "var-expansion-709981a1-53d6-4007-8e94-8a6e6e648e40": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00684827s
STEP: Saw pod success 12/09/22 08:02:58.041
Dec  9 08:02:58.041: INFO: Pod "var-expansion-709981a1-53d6-4007-8e94-8a6e6e648e40" satisfied condition "Succeeded or Failed"
Dec  9 08:02:58.044: INFO: Trying to get logs from node ip-10-0-10-179 pod var-expansion-709981a1-53d6-4007-8e94-8a6e6e648e40 container dapi-container: <nil>
STEP: delete the pod 12/09/22 08:02:58.049
Dec  9 08:02:58.058: INFO: Waiting for pod var-expansion-709981a1-53d6-4007-8e94-8a6e6e648e40 to disappear
Dec  9 08:02:58.061: INFO: Pod var-expansion-709981a1-53d6-4007-8e94-8a6e6e648e40 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Dec  9 08:02:58.061: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-7953" for this suite. 12/09/22 08:02:58.065
------------------------------
â€¢ [4.072 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:44

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 08:02:53.998
    Dec  9 08:02:53.999: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename var-expansion 12/09/22 08:02:53.999
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:02:54.018
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:02:54.023
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:44
    STEP: Creating a pod to test env composition 12/09/22 08:02:54.026
    Dec  9 08:02:54.034: INFO: Waiting up to 5m0s for pod "var-expansion-709981a1-53d6-4007-8e94-8a6e6e648e40" in namespace "var-expansion-7953" to be "Succeeded or Failed"
    Dec  9 08:02:54.037: INFO: Pod "var-expansion-709981a1-53d6-4007-8e94-8a6e6e648e40": Phase="Pending", Reason="", readiness=false. Elapsed: 2.934066ms
    Dec  9 08:02:56.041: INFO: Pod "var-expansion-709981a1-53d6-4007-8e94-8a6e6e648e40": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006506124s
    Dec  9 08:02:58.041: INFO: Pod "var-expansion-709981a1-53d6-4007-8e94-8a6e6e648e40": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00684827s
    STEP: Saw pod success 12/09/22 08:02:58.041
    Dec  9 08:02:58.041: INFO: Pod "var-expansion-709981a1-53d6-4007-8e94-8a6e6e648e40" satisfied condition "Succeeded or Failed"
    Dec  9 08:02:58.044: INFO: Trying to get logs from node ip-10-0-10-179 pod var-expansion-709981a1-53d6-4007-8e94-8a6e6e648e40 container dapi-container: <nil>
    STEP: delete the pod 12/09/22 08:02:58.049
    Dec  9 08:02:58.058: INFO: Waiting for pod var-expansion-709981a1-53d6-4007-8e94-8a6e6e648e40 to disappear
    Dec  9 08:02:58.061: INFO: Pod var-expansion-709981a1-53d6-4007-8e94-8a6e6e648e40 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Dec  9 08:02:58.061: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-7953" for this suite. 12/09/22 08:02:58.065
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:690
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 08:02:58.072
Dec  9 08:02:58.072: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename resourcequota 12/09/22 08:02:58.072
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:02:58.093
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:02:58.099
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:690
STEP: Creating a ResourceQuota with terminating scope 12/09/22 08:02:58.103
STEP: Ensuring ResourceQuota status is calculated 12/09/22 08:02:58.107
STEP: Creating a ResourceQuota with not terminating scope 12/09/22 08:03:00.111
STEP: Ensuring ResourceQuota status is calculated 12/09/22 08:03:00.117
STEP: Creating a long running pod 12/09/22 08:03:02.121
STEP: Ensuring resource quota with not terminating scope captures the pod usage 12/09/22 08:03:02.154
STEP: Ensuring resource quota with terminating scope ignored the pod usage 12/09/22 08:03:04.166
STEP: Deleting the pod 12/09/22 08:03:06.18
STEP: Ensuring resource quota status released the pod usage 12/09/22 08:03:06.215
STEP: Creating a terminating pod 12/09/22 08:03:08.219
STEP: Ensuring resource quota with terminating scope captures the pod usage 12/09/22 08:03:08.232
STEP: Ensuring resource quota with not terminating scope ignored the pod usage 12/09/22 08:03:10.237
STEP: Deleting the pod 12/09/22 08:03:12.246
STEP: Ensuring resource quota status released the pod usage 12/09/22 08:03:12.277
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Dec  9 08:03:14.281: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-2430" for this suite. 12/09/22 08:03:14.284
------------------------------
â€¢ [SLOW TEST] [16.221 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:690

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 08:02:58.072
    Dec  9 08:02:58.072: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename resourcequota 12/09/22 08:02:58.072
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:02:58.093
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:02:58.099
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should verify ResourceQuota with terminating scopes. [Conformance]
      test/e2e/apimachinery/resource_quota.go:690
    STEP: Creating a ResourceQuota with terminating scope 12/09/22 08:02:58.103
    STEP: Ensuring ResourceQuota status is calculated 12/09/22 08:02:58.107
    STEP: Creating a ResourceQuota with not terminating scope 12/09/22 08:03:00.111
    STEP: Ensuring ResourceQuota status is calculated 12/09/22 08:03:00.117
    STEP: Creating a long running pod 12/09/22 08:03:02.121
    STEP: Ensuring resource quota with not terminating scope captures the pod usage 12/09/22 08:03:02.154
    STEP: Ensuring resource quota with terminating scope ignored the pod usage 12/09/22 08:03:04.166
    STEP: Deleting the pod 12/09/22 08:03:06.18
    STEP: Ensuring resource quota status released the pod usage 12/09/22 08:03:06.215
    STEP: Creating a terminating pod 12/09/22 08:03:08.219
    STEP: Ensuring resource quota with terminating scope captures the pod usage 12/09/22 08:03:08.232
    STEP: Ensuring resource quota with not terminating scope ignored the pod usage 12/09/22 08:03:10.237
    STEP: Deleting the pod 12/09/22 08:03:12.246
    STEP: Ensuring resource quota status released the pod usage 12/09/22 08:03:12.277
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Dec  9 08:03:14.281: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-2430" for this suite. 12/09/22 08:03:14.284
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-scheduling] LimitRange
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:61
[BeforeEach] [sig-scheduling] LimitRange
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 08:03:14.294
Dec  9 08:03:14.294: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename limitrange 12/09/22 08:03:14.296
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:03:14.317
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:03:14.321
[BeforeEach] [sig-scheduling] LimitRange
  test/e2e/framework/metrics/init/init.go:31
[It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:61
STEP: Creating a LimitRange 12/09/22 08:03:14.325
STEP: Setting up watch 12/09/22 08:03:14.326
STEP: Submitting a LimitRange 12/09/22 08:03:14.43
STEP: Verifying LimitRange creation was observed 12/09/22 08:03:14.44
STEP: Fetching the LimitRange to ensure it has proper values 12/09/22 08:03:14.442
Dec  9 08:03:14.445: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Dec  9 08:03:14.445: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with no resource requirements 12/09/22 08:03:14.445
STEP: Ensuring Pod has resource requirements applied from LimitRange 12/09/22 08:03:14.454
Dec  9 08:03:14.458: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Dec  9 08:03:14.459: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with partial resource requirements 12/09/22 08:03:14.459
STEP: Ensuring Pod has merged resource requirements applied from LimitRange 12/09/22 08:03:14.471
Dec  9 08:03:14.475: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
Dec  9 08:03:14.475: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Failing to create a Pod with less than min resources 12/09/22 08:03:14.475
STEP: Failing to create a Pod with more than max resources 12/09/22 08:03:14.486
STEP: Updating a LimitRange 12/09/22 08:03:14.493
STEP: Verifying LimitRange updating is effective 12/09/22 08:03:14.498
STEP: Creating a Pod with less than former min resources 12/09/22 08:03:16.504
STEP: Failing to create a Pod with more than max resources 12/09/22 08:03:16.51
STEP: Deleting a LimitRange 12/09/22 08:03:16.516
STEP: Verifying the LimitRange was deleted 12/09/22 08:03:16.526
Dec  9 08:03:21.530: INFO: limitRange is already deleted
STEP: Creating a Pod with more than former max resources 12/09/22 08:03:21.53
[AfterEach] [sig-scheduling] LimitRange
  test/e2e/framework/node/init/init.go:32
Dec  9 08:03:21.538: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  tear down framework | framework.go:193
STEP: Destroying namespace "limitrange-9183" for this suite. 12/09/22 08:03:21.55
------------------------------
â€¢ [SLOW TEST] [7.262 seconds]
[sig-scheduling] LimitRange
test/e2e/scheduling/framework.go:40
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:61

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] LimitRange
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 08:03:14.294
    Dec  9 08:03:14.294: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename limitrange 12/09/22 08:03:14.296
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:03:14.317
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:03:14.321
    [BeforeEach] [sig-scheduling] LimitRange
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
      test/e2e/scheduling/limit_range.go:61
    STEP: Creating a LimitRange 12/09/22 08:03:14.325
    STEP: Setting up watch 12/09/22 08:03:14.326
    STEP: Submitting a LimitRange 12/09/22 08:03:14.43
    STEP: Verifying LimitRange creation was observed 12/09/22 08:03:14.44
    STEP: Fetching the LimitRange to ensure it has proper values 12/09/22 08:03:14.442
    Dec  9 08:03:14.445: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
    Dec  9 08:03:14.445: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Creating a Pod with no resource requirements 12/09/22 08:03:14.445
    STEP: Ensuring Pod has resource requirements applied from LimitRange 12/09/22 08:03:14.454
    Dec  9 08:03:14.458: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
    Dec  9 08:03:14.459: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Creating a Pod with partial resource requirements 12/09/22 08:03:14.459
    STEP: Ensuring Pod has merged resource requirements applied from LimitRange 12/09/22 08:03:14.471
    Dec  9 08:03:14.475: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
    Dec  9 08:03:14.475: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Failing to create a Pod with less than min resources 12/09/22 08:03:14.475
    STEP: Failing to create a Pod with more than max resources 12/09/22 08:03:14.486
    STEP: Updating a LimitRange 12/09/22 08:03:14.493
    STEP: Verifying LimitRange updating is effective 12/09/22 08:03:14.498
    STEP: Creating a Pod with less than former min resources 12/09/22 08:03:16.504
    STEP: Failing to create a Pod with more than max resources 12/09/22 08:03:16.51
    STEP: Deleting a LimitRange 12/09/22 08:03:16.516
    STEP: Verifying the LimitRange was deleted 12/09/22 08:03:16.526
    Dec  9 08:03:21.530: INFO: limitRange is already deleted
    STEP: Creating a Pod with more than former max resources 12/09/22 08:03:21.53
    [AfterEach] [sig-scheduling] LimitRange
      test/e2e/framework/node/init/init.go:32
    Dec  9 08:03:21.538: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      tear down framework | framework.go:193
    STEP: Destroying namespace "limitrange-9183" for this suite. 12/09/22 08:03:21.55
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl diff
  should check if kubectl diff finds a difference for Deployments [Conformance]
  test/e2e/kubectl/kubectl.go:931
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 08:03:21.558
Dec  9 08:03:21.558: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename kubectl 12/09/22 08:03:21.559
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:03:21.574
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:03:21.579
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if kubectl diff finds a difference for Deployments [Conformance]
  test/e2e/kubectl/kubectl.go:931
STEP: create deployment with httpd image 12/09/22 08:03:21.585
Dec  9 08:03:21.587: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=kubectl-2013 create -f -'
Dec  9 08:03:22.391: INFO: stderr: ""
Dec  9 08:03:22.391: INFO: stdout: "deployment.apps/httpd-deployment created\n"
STEP: verify diff finds difference between live and declared image 12/09/22 08:03:22.391
Dec  9 08:03:22.391: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=kubectl-2013 diff -f -'
Dec  9 08:03:22.655: INFO: rc: 1
Dec  9 08:03:22.655: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=kubectl-2013 delete -f -'
Dec  9 08:03:22.745: INFO: stderr: ""
Dec  9 08:03:22.745: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Dec  9 08:03:22.745: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-2013" for this suite. 12/09/22 08:03:22.758
------------------------------
â€¢ [1.205 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl diff
  test/e2e/kubectl/kubectl.go:925
    should check if kubectl diff finds a difference for Deployments [Conformance]
    test/e2e/kubectl/kubectl.go:931

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 08:03:21.558
    Dec  9 08:03:21.558: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename kubectl 12/09/22 08:03:21.559
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:03:21.574
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:03:21.579
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if kubectl diff finds a difference for Deployments [Conformance]
      test/e2e/kubectl/kubectl.go:931
    STEP: create deployment with httpd image 12/09/22 08:03:21.585
    Dec  9 08:03:21.587: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=kubectl-2013 create -f -'
    Dec  9 08:03:22.391: INFO: stderr: ""
    Dec  9 08:03:22.391: INFO: stdout: "deployment.apps/httpd-deployment created\n"
    STEP: verify diff finds difference between live and declared image 12/09/22 08:03:22.391
    Dec  9 08:03:22.391: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=kubectl-2013 diff -f -'
    Dec  9 08:03:22.655: INFO: rc: 1
    Dec  9 08:03:22.655: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=kubectl-2013 delete -f -'
    Dec  9 08:03:22.745: INFO: stderr: ""
    Dec  9 08:03:22.745: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Dec  9 08:03:22.745: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-2013" for this suite. 12/09/22 08:03:22.758
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 08:03:22.765
Dec  9 08:03:22.765: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename dns 12/09/22 08:03:22.766
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:03:22.781
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:03:22.788
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137
STEP: Creating a test headless service 12/09/22 08:03:22.791
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-941.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-941.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-941.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-941.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-941.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-941.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-941.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-941.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-941.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-941.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-941.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-941.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 134.56.3.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.3.56.134_udp@PTR;check="$$(dig +tcp +noall +answer +search 134.56.3.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.3.56.134_tcp@PTR;sleep 1; done
 12/09/22 08:03:22.816
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-941.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-941.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-941.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-941.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-941.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-941.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-941.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-941.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-941.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-941.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-941.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-941.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 134.56.3.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.3.56.134_udp@PTR;check="$$(dig +tcp +noall +answer +search 134.56.3.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.3.56.134_tcp@PTR;sleep 1; done
 12/09/22 08:03:22.816
STEP: creating a pod to probe DNS 12/09/22 08:03:22.816
STEP: submitting the pod to kubernetes 12/09/22 08:03:22.817
Dec  9 08:03:22.833: INFO: Waiting up to 15m0s for pod "dns-test-eeb6fe0a-9395-402e-9fc2-20d39ba51235" in namespace "dns-941" to be "running"
Dec  9 08:03:22.839: INFO: Pod "dns-test-eeb6fe0a-9395-402e-9fc2-20d39ba51235": Phase="Pending", Reason="", readiness=false. Elapsed: 5.261512ms
Dec  9 08:03:24.847: INFO: Pod "dns-test-eeb6fe0a-9395-402e-9fc2-20d39ba51235": Phase="Running", Reason="", readiness=true. Elapsed: 2.013831192s
Dec  9 08:03:24.847: INFO: Pod "dns-test-eeb6fe0a-9395-402e-9fc2-20d39ba51235" satisfied condition "running"
STEP: retrieving the pod 12/09/22 08:03:24.847
STEP: looking for the results for each expected name from probers 12/09/22 08:03:24.851
Dec  9 08:03:24.855: INFO: Unable to read wheezy_udp@dns-test-service.dns-941.svc.cluster.local from pod dns-941/dns-test-eeb6fe0a-9395-402e-9fc2-20d39ba51235: the server could not find the requested resource (get pods dns-test-eeb6fe0a-9395-402e-9fc2-20d39ba51235)
Dec  9 08:03:24.858: INFO: Unable to read wheezy_tcp@dns-test-service.dns-941.svc.cluster.local from pod dns-941/dns-test-eeb6fe0a-9395-402e-9fc2-20d39ba51235: the server could not find the requested resource (get pods dns-test-eeb6fe0a-9395-402e-9fc2-20d39ba51235)
Dec  9 08:03:24.861: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-941.svc.cluster.local from pod dns-941/dns-test-eeb6fe0a-9395-402e-9fc2-20d39ba51235: the server could not find the requested resource (get pods dns-test-eeb6fe0a-9395-402e-9fc2-20d39ba51235)
Dec  9 08:03:24.864: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-941.svc.cluster.local from pod dns-941/dns-test-eeb6fe0a-9395-402e-9fc2-20d39ba51235: the server could not find the requested resource (get pods dns-test-eeb6fe0a-9395-402e-9fc2-20d39ba51235)
Dec  9 08:03:24.877: INFO: Unable to read jessie_udp@dns-test-service.dns-941.svc.cluster.local from pod dns-941/dns-test-eeb6fe0a-9395-402e-9fc2-20d39ba51235: the server could not find the requested resource (get pods dns-test-eeb6fe0a-9395-402e-9fc2-20d39ba51235)
Dec  9 08:03:24.880: INFO: Unable to read jessie_tcp@dns-test-service.dns-941.svc.cluster.local from pod dns-941/dns-test-eeb6fe0a-9395-402e-9fc2-20d39ba51235: the server could not find the requested resource (get pods dns-test-eeb6fe0a-9395-402e-9fc2-20d39ba51235)
Dec  9 08:03:24.882: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-941.svc.cluster.local from pod dns-941/dns-test-eeb6fe0a-9395-402e-9fc2-20d39ba51235: the server could not find the requested resource (get pods dns-test-eeb6fe0a-9395-402e-9fc2-20d39ba51235)
Dec  9 08:03:24.885: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-941.svc.cluster.local from pod dns-941/dns-test-eeb6fe0a-9395-402e-9fc2-20d39ba51235: the server could not find the requested resource (get pods dns-test-eeb6fe0a-9395-402e-9fc2-20d39ba51235)
Dec  9 08:03:24.899: INFO: Lookups using dns-941/dns-test-eeb6fe0a-9395-402e-9fc2-20d39ba51235 failed for: [wheezy_udp@dns-test-service.dns-941.svc.cluster.local wheezy_tcp@dns-test-service.dns-941.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-941.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-941.svc.cluster.local jessie_udp@dns-test-service.dns-941.svc.cluster.local jessie_tcp@dns-test-service.dns-941.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-941.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-941.svc.cluster.local]

Dec  9 08:03:29.912: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-941.svc.cluster.local from pod dns-941/dns-test-eeb6fe0a-9395-402e-9fc2-20d39ba51235: the server could not find the requested resource (get pods dns-test-eeb6fe0a-9395-402e-9fc2-20d39ba51235)
Dec  9 08:03:29.915: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-941.svc.cluster.local from pod dns-941/dns-test-eeb6fe0a-9395-402e-9fc2-20d39ba51235: the server could not find the requested resource (get pods dns-test-eeb6fe0a-9395-402e-9fc2-20d39ba51235)
Dec  9 08:03:29.956: INFO: Lookups using dns-941/dns-test-eeb6fe0a-9395-402e-9fc2-20d39ba51235 failed for: [wheezy_udp@_http._tcp.dns-test-service.dns-941.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-941.svc.cluster.local]

Dec  9 08:03:34.954: INFO: DNS probes using dns-941/dns-test-eeb6fe0a-9395-402e-9fc2-20d39ba51235 succeeded

STEP: deleting the pod 12/09/22 08:03:34.954
STEP: deleting the test service 12/09/22 08:03:34.984
STEP: deleting the test headless service 12/09/22 08:03:35.022
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Dec  9 08:03:35.054: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-941" for this suite. 12/09/22 08:03:35.072
------------------------------
â€¢ [SLOW TEST] [12.316 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 08:03:22.765
    Dec  9 08:03:22.765: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename dns 12/09/22 08:03:22.766
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:03:22.781
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:03:22.788
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for services  [Conformance]
      test/e2e/network/dns.go:137
    STEP: Creating a test headless service 12/09/22 08:03:22.791
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-941.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-941.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-941.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-941.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-941.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-941.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-941.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-941.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-941.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-941.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-941.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-941.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 134.56.3.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.3.56.134_udp@PTR;check="$$(dig +tcp +noall +answer +search 134.56.3.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.3.56.134_tcp@PTR;sleep 1; done
     12/09/22 08:03:22.816
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-941.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-941.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-941.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-941.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-941.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-941.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-941.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-941.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-941.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-941.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-941.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-941.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 134.56.3.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.3.56.134_udp@PTR;check="$$(dig +tcp +noall +answer +search 134.56.3.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.3.56.134_tcp@PTR;sleep 1; done
     12/09/22 08:03:22.816
    STEP: creating a pod to probe DNS 12/09/22 08:03:22.816
    STEP: submitting the pod to kubernetes 12/09/22 08:03:22.817
    Dec  9 08:03:22.833: INFO: Waiting up to 15m0s for pod "dns-test-eeb6fe0a-9395-402e-9fc2-20d39ba51235" in namespace "dns-941" to be "running"
    Dec  9 08:03:22.839: INFO: Pod "dns-test-eeb6fe0a-9395-402e-9fc2-20d39ba51235": Phase="Pending", Reason="", readiness=false. Elapsed: 5.261512ms
    Dec  9 08:03:24.847: INFO: Pod "dns-test-eeb6fe0a-9395-402e-9fc2-20d39ba51235": Phase="Running", Reason="", readiness=true. Elapsed: 2.013831192s
    Dec  9 08:03:24.847: INFO: Pod "dns-test-eeb6fe0a-9395-402e-9fc2-20d39ba51235" satisfied condition "running"
    STEP: retrieving the pod 12/09/22 08:03:24.847
    STEP: looking for the results for each expected name from probers 12/09/22 08:03:24.851
    Dec  9 08:03:24.855: INFO: Unable to read wheezy_udp@dns-test-service.dns-941.svc.cluster.local from pod dns-941/dns-test-eeb6fe0a-9395-402e-9fc2-20d39ba51235: the server could not find the requested resource (get pods dns-test-eeb6fe0a-9395-402e-9fc2-20d39ba51235)
    Dec  9 08:03:24.858: INFO: Unable to read wheezy_tcp@dns-test-service.dns-941.svc.cluster.local from pod dns-941/dns-test-eeb6fe0a-9395-402e-9fc2-20d39ba51235: the server could not find the requested resource (get pods dns-test-eeb6fe0a-9395-402e-9fc2-20d39ba51235)
    Dec  9 08:03:24.861: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-941.svc.cluster.local from pod dns-941/dns-test-eeb6fe0a-9395-402e-9fc2-20d39ba51235: the server could not find the requested resource (get pods dns-test-eeb6fe0a-9395-402e-9fc2-20d39ba51235)
    Dec  9 08:03:24.864: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-941.svc.cluster.local from pod dns-941/dns-test-eeb6fe0a-9395-402e-9fc2-20d39ba51235: the server could not find the requested resource (get pods dns-test-eeb6fe0a-9395-402e-9fc2-20d39ba51235)
    Dec  9 08:03:24.877: INFO: Unable to read jessie_udp@dns-test-service.dns-941.svc.cluster.local from pod dns-941/dns-test-eeb6fe0a-9395-402e-9fc2-20d39ba51235: the server could not find the requested resource (get pods dns-test-eeb6fe0a-9395-402e-9fc2-20d39ba51235)
    Dec  9 08:03:24.880: INFO: Unable to read jessie_tcp@dns-test-service.dns-941.svc.cluster.local from pod dns-941/dns-test-eeb6fe0a-9395-402e-9fc2-20d39ba51235: the server could not find the requested resource (get pods dns-test-eeb6fe0a-9395-402e-9fc2-20d39ba51235)
    Dec  9 08:03:24.882: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-941.svc.cluster.local from pod dns-941/dns-test-eeb6fe0a-9395-402e-9fc2-20d39ba51235: the server could not find the requested resource (get pods dns-test-eeb6fe0a-9395-402e-9fc2-20d39ba51235)
    Dec  9 08:03:24.885: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-941.svc.cluster.local from pod dns-941/dns-test-eeb6fe0a-9395-402e-9fc2-20d39ba51235: the server could not find the requested resource (get pods dns-test-eeb6fe0a-9395-402e-9fc2-20d39ba51235)
    Dec  9 08:03:24.899: INFO: Lookups using dns-941/dns-test-eeb6fe0a-9395-402e-9fc2-20d39ba51235 failed for: [wheezy_udp@dns-test-service.dns-941.svc.cluster.local wheezy_tcp@dns-test-service.dns-941.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-941.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-941.svc.cluster.local jessie_udp@dns-test-service.dns-941.svc.cluster.local jessie_tcp@dns-test-service.dns-941.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-941.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-941.svc.cluster.local]

    Dec  9 08:03:29.912: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-941.svc.cluster.local from pod dns-941/dns-test-eeb6fe0a-9395-402e-9fc2-20d39ba51235: the server could not find the requested resource (get pods dns-test-eeb6fe0a-9395-402e-9fc2-20d39ba51235)
    Dec  9 08:03:29.915: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-941.svc.cluster.local from pod dns-941/dns-test-eeb6fe0a-9395-402e-9fc2-20d39ba51235: the server could not find the requested resource (get pods dns-test-eeb6fe0a-9395-402e-9fc2-20d39ba51235)
    Dec  9 08:03:29.956: INFO: Lookups using dns-941/dns-test-eeb6fe0a-9395-402e-9fc2-20d39ba51235 failed for: [wheezy_udp@_http._tcp.dns-test-service.dns-941.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-941.svc.cluster.local]

    Dec  9 08:03:34.954: INFO: DNS probes using dns-941/dns-test-eeb6fe0a-9395-402e-9fc2-20d39ba51235 succeeded

    STEP: deleting the pod 12/09/22 08:03:34.954
    STEP: deleting the test service 12/09/22 08:03:34.984
    STEP: deleting the test headless service 12/09/22 08:03:35.022
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Dec  9 08:03:35.054: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-941" for this suite. 12/09/22 08:03:35.072
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:105
[BeforeEach] [sig-network] Networking
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 08:03:35.084
Dec  9 08:03:35.084: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename pod-network-test 12/09/22 08:03:35.085
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:03:35.106
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:03:35.111
[BeforeEach] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:31
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:105
STEP: Performing setup for networking test in namespace pod-network-test-4444 12/09/22 08:03:35.117
STEP: creating a selector 12/09/22 08:03:35.117
STEP: Creating the service pods in kubernetes 12/09/22 08:03:35.117
Dec  9 08:03:35.117: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Dec  9 08:03:35.148: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-4444" to be "running and ready"
Dec  9 08:03:35.154: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 5.764763ms
Dec  9 08:03:35.155: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Dec  9 08:03:37.159: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.010307322s
Dec  9 08:03:37.159: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Dec  9 08:03:39.159: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.010544188s
Dec  9 08:03:39.159: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Dec  9 08:03:41.158: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.009266208s
Dec  9 08:03:41.158: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Dec  9 08:03:43.160: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.011028905s
Dec  9 08:03:43.160: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Dec  9 08:03:45.159: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.009938716s
Dec  9 08:03:45.159: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Dec  9 08:03:47.158: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.0098256s
Dec  9 08:03:47.159: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Dec  9 08:03:47.159: INFO: Pod "netserver-0" satisfied condition "running and ready"
Dec  9 08:03:47.165: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-4444" to be "running and ready"
Dec  9 08:03:47.173: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 8.608514ms
Dec  9 08:03:47.173: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Dec  9 08:03:47.173: INFO: Pod "netserver-1" satisfied condition "running and ready"
STEP: Creating test pods 12/09/22 08:03:47.176
Dec  9 08:03:47.189: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-4444" to be "running"
Dec  9 08:03:47.201: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 12.120608ms
Dec  9 08:03:49.207: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.017453423s
Dec  9 08:03:49.207: INFO: Pod "test-container-pod" satisfied condition "running"
Dec  9 08:03:49.209: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-4444" to be "running"
Dec  9 08:03:49.212: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.486328ms
Dec  9 08:03:49.212: INFO: Pod "host-test-container-pod" satisfied condition "running"
Dec  9 08:03:49.214: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
Dec  9 08:03:49.214: INFO: Going to poll 10.2.136.84 on port 8083 at least 0 times, with a maximum of 34 tries before failing
Dec  9 08:03:49.217: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.2.136.84:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-4444 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec  9 08:03:49.217: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
Dec  9 08:03:49.217: INFO: ExecWithOptions: Clientset creation
Dec  9 08:03:49.218: INFO: ExecWithOptions: execute(POST https://10.3.0.1:443/api/v1/namespaces/pod-network-test-4444/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.2.136.84%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Dec  9 08:03:49.290: INFO: Found all 1 expected endpoints: [netserver-0]
Dec  9 08:03:49.290: INFO: Going to poll 10.2.166.152 on port 8083 at least 0 times, with a maximum of 34 tries before failing
Dec  9 08:03:49.293: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.2.166.152:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-4444 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec  9 08:03:49.293: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
Dec  9 08:03:49.294: INFO: ExecWithOptions: Clientset creation
Dec  9 08:03:49.294: INFO: ExecWithOptions: execute(POST https://10.3.0.1:443/api/v1/namespaces/pod-network-test-4444/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.2.166.152%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Dec  9 08:03:49.351: INFO: Found all 1 expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  test/e2e/framework/node/init/init.go:32
Dec  9 08:03:49.352: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Networking
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Networking
  tear down framework | framework.go:193
STEP: Destroying namespace "pod-network-test-4444" for this suite. 12/09/22 08:03:49.356
------------------------------
â€¢ [SLOW TEST] [14.278 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:105

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 08:03:35.084
    Dec  9 08:03:35.084: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename pod-network-test 12/09/22 08:03:35.085
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:03:35.106
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:03:35.111
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:31
    [It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:105
    STEP: Performing setup for networking test in namespace pod-network-test-4444 12/09/22 08:03:35.117
    STEP: creating a selector 12/09/22 08:03:35.117
    STEP: Creating the service pods in kubernetes 12/09/22 08:03:35.117
    Dec  9 08:03:35.117: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Dec  9 08:03:35.148: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-4444" to be "running and ready"
    Dec  9 08:03:35.154: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 5.764763ms
    Dec  9 08:03:35.155: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Dec  9 08:03:37.159: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.010307322s
    Dec  9 08:03:37.159: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Dec  9 08:03:39.159: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.010544188s
    Dec  9 08:03:39.159: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Dec  9 08:03:41.158: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.009266208s
    Dec  9 08:03:41.158: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Dec  9 08:03:43.160: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.011028905s
    Dec  9 08:03:43.160: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Dec  9 08:03:45.159: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.009938716s
    Dec  9 08:03:45.159: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Dec  9 08:03:47.158: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.0098256s
    Dec  9 08:03:47.159: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Dec  9 08:03:47.159: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Dec  9 08:03:47.165: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-4444" to be "running and ready"
    Dec  9 08:03:47.173: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 8.608514ms
    Dec  9 08:03:47.173: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Dec  9 08:03:47.173: INFO: Pod "netserver-1" satisfied condition "running and ready"
    STEP: Creating test pods 12/09/22 08:03:47.176
    Dec  9 08:03:47.189: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-4444" to be "running"
    Dec  9 08:03:47.201: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 12.120608ms
    Dec  9 08:03:49.207: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.017453423s
    Dec  9 08:03:49.207: INFO: Pod "test-container-pod" satisfied condition "running"
    Dec  9 08:03:49.209: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-4444" to be "running"
    Dec  9 08:03:49.212: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.486328ms
    Dec  9 08:03:49.212: INFO: Pod "host-test-container-pod" satisfied condition "running"
    Dec  9 08:03:49.214: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
    Dec  9 08:03:49.214: INFO: Going to poll 10.2.136.84 on port 8083 at least 0 times, with a maximum of 34 tries before failing
    Dec  9 08:03:49.217: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.2.136.84:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-4444 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Dec  9 08:03:49.217: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    Dec  9 08:03:49.217: INFO: ExecWithOptions: Clientset creation
    Dec  9 08:03:49.218: INFO: ExecWithOptions: execute(POST https://10.3.0.1:443/api/v1/namespaces/pod-network-test-4444/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.2.136.84%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Dec  9 08:03:49.290: INFO: Found all 1 expected endpoints: [netserver-0]
    Dec  9 08:03:49.290: INFO: Going to poll 10.2.166.152 on port 8083 at least 0 times, with a maximum of 34 tries before failing
    Dec  9 08:03:49.293: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.2.166.152:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-4444 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Dec  9 08:03:49.293: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    Dec  9 08:03:49.294: INFO: ExecWithOptions: Clientset creation
    Dec  9 08:03:49.294: INFO: ExecWithOptions: execute(POST https://10.3.0.1:443/api/v1/namespaces/pod-network-test-4444/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.2.166.152%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Dec  9 08:03:49.351: INFO: Found all 1 expected endpoints: [netserver-1]
    [AfterEach] [sig-network] Networking
      test/e2e/framework/node/init/init.go:32
    Dec  9 08:03:49.352: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Networking
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Networking
      tear down framework | framework.go:193
    STEP: Destroying namespace "pod-network-test-4444" for this suite. 12/09/22 08:03:49.356
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-cli] Kubectl client Guestbook application
  should create and stop a working application  [Conformance]
  test/e2e/kubectl/kubectl.go:394
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 08:03:49.362
Dec  9 08:03:49.362: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename kubectl 12/09/22 08:03:49.363
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:03:49.39
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:03:49.394
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should create and stop a working application  [Conformance]
  test/e2e/kubectl/kubectl.go:394
STEP: creating all guestbook components 12/09/22 08:03:49.398
Dec  9 08:03:49.398: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-replica
  labels:
    app: agnhost
    role: replica
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: replica
    tier: backend

Dec  9 08:03:49.399: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=kubectl-7458 create -f -'
Dec  9 08:03:49.736: INFO: stderr: ""
Dec  9 08:03:49.736: INFO: stdout: "service/agnhost-replica created\n"
Dec  9 08:03:49.736: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-primary
  labels:
    app: agnhost
    role: primary
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: primary
    tier: backend

Dec  9 08:03:49.736: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=kubectl-7458 create -f -'
Dec  9 08:03:50.131: INFO: stderr: ""
Dec  9 08:03:50.131: INFO: stdout: "service/agnhost-primary created\n"
Dec  9 08:03:50.131: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Dec  9 08:03:50.131: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=kubectl-7458 create -f -'
Dec  9 08:03:50.440: INFO: stderr: ""
Dec  9 08:03:50.440: INFO: stdout: "service/frontend created\n"
Dec  9 08:03:50.441: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: registry.k8s.io/e2e-test-images/agnhost:2.43
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

Dec  9 08:03:50.441: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=kubectl-7458 create -f -'
Dec  9 08:03:50.709: INFO: stderr: ""
Dec  9 08:03:50.709: INFO: stdout: "deployment.apps/frontend created\n"
Dec  9 08:03:50.710: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-primary
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: primary
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      containers:
      - name: primary
        image: registry.k8s.io/e2e-test-images/agnhost:2.43
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Dec  9 08:03:50.710: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=kubectl-7458 create -f -'
Dec  9 08:03:50.914: INFO: stderr: ""
Dec  9 08:03:50.914: INFO: stdout: "deployment.apps/agnhost-primary created\n"
Dec  9 08:03:50.914: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-replica
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: replica
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      containers:
      - name: replica
        image: registry.k8s.io/e2e-test-images/agnhost:2.43
        args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Dec  9 08:03:50.914: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=kubectl-7458 create -f -'
Dec  9 08:03:51.123: INFO: stderr: ""
Dec  9 08:03:51.124: INFO: stdout: "deployment.apps/agnhost-replica created\n"
STEP: validating guestbook app 12/09/22 08:03:51.124
Dec  9 08:03:51.124: INFO: Waiting for all frontend pods to be Running.
Dec  9 08:03:56.175: INFO: Waiting for frontend to serve content.
Dec  9 08:03:56.203: INFO: Trying to add a new entry to the guestbook.
Dec  9 08:03:56.247: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources 12/09/22 08:03:56.269
Dec  9 08:03:56.270: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=kubectl-7458 delete --grace-period=0 --force -f -'
Dec  9 08:03:56.482: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec  9 08:03:56.482: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
STEP: using delete to clean up resources 12/09/22 08:03:56.482
Dec  9 08:03:56.483: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=kubectl-7458 delete --grace-period=0 --force -f -'
Dec  9 08:03:56.663: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec  9 08:03:56.663: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources 12/09/22 08:03:56.663
Dec  9 08:03:56.663: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=kubectl-7458 delete --grace-period=0 --force -f -'
Dec  9 08:03:56.814: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec  9 08:03:56.814: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources 12/09/22 08:03:56.814
Dec  9 08:03:56.815: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=kubectl-7458 delete --grace-period=0 --force -f -'
Dec  9 08:03:56.957: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec  9 08:03:56.957: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources 12/09/22 08:03:56.957
Dec  9 08:03:56.957: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=kubectl-7458 delete --grace-period=0 --force -f -'
Dec  9 08:03:57.140: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec  9 08:03:57.140: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources 12/09/22 08:03:57.14
Dec  9 08:03:57.140: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=kubectl-7458 delete --grace-period=0 --force -f -'
Dec  9 08:03:57.326: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec  9 08:03:57.326: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Dec  9 08:03:57.326: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-7458" for this suite. 12/09/22 08:03:57.337
------------------------------
â€¢ [SLOW TEST] [7.983 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Guestbook application
  test/e2e/kubectl/kubectl.go:369
    should create and stop a working application  [Conformance]
    test/e2e/kubectl/kubectl.go:394

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 08:03:49.362
    Dec  9 08:03:49.362: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename kubectl 12/09/22 08:03:49.363
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:03:49.39
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:03:49.394
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should create and stop a working application  [Conformance]
      test/e2e/kubectl/kubectl.go:394
    STEP: creating all guestbook components 12/09/22 08:03:49.398
    Dec  9 08:03:49.398: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: agnhost-replica
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      ports:
      - port: 6379
      selector:
        app: agnhost
        role: replica
        tier: backend

    Dec  9 08:03:49.399: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=kubectl-7458 create -f -'
    Dec  9 08:03:49.736: INFO: stderr: ""
    Dec  9 08:03:49.736: INFO: stdout: "service/agnhost-replica created\n"
    Dec  9 08:03:49.736: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: agnhost-primary
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      ports:
      - port: 6379
        targetPort: 6379
      selector:
        app: agnhost
        role: primary
        tier: backend

    Dec  9 08:03:49.736: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=kubectl-7458 create -f -'
    Dec  9 08:03:50.131: INFO: stderr: ""
    Dec  9 08:03:50.131: INFO: stdout: "service/agnhost-primary created\n"
    Dec  9 08:03:50.131: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: frontend
      labels:
        app: guestbook
        tier: frontend
    spec:
      # if your cluster supports it, uncomment the following to automatically create
      # an external load-balanced IP for the frontend service.
      # type: LoadBalancer
      ports:
      - port: 80
      selector:
        app: guestbook
        tier: frontend

    Dec  9 08:03:50.131: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=kubectl-7458 create -f -'
    Dec  9 08:03:50.440: INFO: stderr: ""
    Dec  9 08:03:50.440: INFO: stdout: "service/frontend created\n"
    Dec  9 08:03:50.441: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: frontend
    spec:
      replicas: 3
      selector:
        matchLabels:
          app: guestbook
          tier: frontend
      template:
        metadata:
          labels:
            app: guestbook
            tier: frontend
        spec:
          containers:
          - name: guestbook-frontend
            image: registry.k8s.io/e2e-test-images/agnhost:2.43
            args: [ "guestbook", "--backend-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 80

    Dec  9 08:03:50.441: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=kubectl-7458 create -f -'
    Dec  9 08:03:50.709: INFO: stderr: ""
    Dec  9 08:03:50.709: INFO: stdout: "deployment.apps/frontend created\n"
    Dec  9 08:03:50.710: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: agnhost-primary
    spec:
      replicas: 1
      selector:
        matchLabels:
          app: agnhost
          role: primary
          tier: backend
      template:
        metadata:
          labels:
            app: agnhost
            role: primary
            tier: backend
        spec:
          containers:
          - name: primary
            image: registry.k8s.io/e2e-test-images/agnhost:2.43
            args: [ "guestbook", "--http-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 6379

    Dec  9 08:03:50.710: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=kubectl-7458 create -f -'
    Dec  9 08:03:50.914: INFO: stderr: ""
    Dec  9 08:03:50.914: INFO: stdout: "deployment.apps/agnhost-primary created\n"
    Dec  9 08:03:50.914: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: agnhost-replica
    spec:
      replicas: 2
      selector:
        matchLabels:
          app: agnhost
          role: replica
          tier: backend
      template:
        metadata:
          labels:
            app: agnhost
            role: replica
            tier: backend
        spec:
          containers:
          - name: replica
            image: registry.k8s.io/e2e-test-images/agnhost:2.43
            args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 6379

    Dec  9 08:03:50.914: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=kubectl-7458 create -f -'
    Dec  9 08:03:51.123: INFO: stderr: ""
    Dec  9 08:03:51.124: INFO: stdout: "deployment.apps/agnhost-replica created\n"
    STEP: validating guestbook app 12/09/22 08:03:51.124
    Dec  9 08:03:51.124: INFO: Waiting for all frontend pods to be Running.
    Dec  9 08:03:56.175: INFO: Waiting for frontend to serve content.
    Dec  9 08:03:56.203: INFO: Trying to add a new entry to the guestbook.
    Dec  9 08:03:56.247: INFO: Verifying that added entry can be retrieved.
    STEP: using delete to clean up resources 12/09/22 08:03:56.269
    Dec  9 08:03:56.270: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=kubectl-7458 delete --grace-period=0 --force -f -'
    Dec  9 08:03:56.482: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Dec  9 08:03:56.482: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
    STEP: using delete to clean up resources 12/09/22 08:03:56.482
    Dec  9 08:03:56.483: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=kubectl-7458 delete --grace-period=0 --force -f -'
    Dec  9 08:03:56.663: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Dec  9 08:03:56.663: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
    STEP: using delete to clean up resources 12/09/22 08:03:56.663
    Dec  9 08:03:56.663: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=kubectl-7458 delete --grace-period=0 --force -f -'
    Dec  9 08:03:56.814: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Dec  9 08:03:56.814: INFO: stdout: "service \"frontend\" force deleted\n"
    STEP: using delete to clean up resources 12/09/22 08:03:56.814
    Dec  9 08:03:56.815: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=kubectl-7458 delete --grace-period=0 --force -f -'
    Dec  9 08:03:56.957: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Dec  9 08:03:56.957: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
    STEP: using delete to clean up resources 12/09/22 08:03:56.957
    Dec  9 08:03:56.957: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=kubectl-7458 delete --grace-period=0 --force -f -'
    Dec  9 08:03:57.140: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Dec  9 08:03:57.140: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
    STEP: using delete to clean up resources 12/09/22 08:03:57.14
    Dec  9 08:03:57.140: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=kubectl-7458 delete --grace-period=0 --force -f -'
    Dec  9 08:03:57.326: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Dec  9 08:03:57.326: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Dec  9 08:03:57.326: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-7458" for this suite. 12/09/22 08:03:57.337
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:248
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 08:03:57.345
Dec  9 08:03:57.345: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename container-runtime 12/09/22 08:03:57.352
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:03:57.369
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:03:57.377
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:248
STEP: create the container 12/09/22 08:03:57.38
STEP: wait for the container to reach Succeeded 12/09/22 08:03:57.404
STEP: get the container status 12/09/22 08:04:01.433
STEP: the container should be terminated 12/09/22 08:04:01.436
STEP: the termination message should be set 12/09/22 08:04:01.436
Dec  9 08:04:01.436: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container 12/09/22 08:04:01.436
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Dec  9 08:04:01.453: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-1746" for this suite. 12/09/22 08:04:01.458
------------------------------
â€¢ [4.119 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    on terminated container
    test/e2e/common/node/runtime.go:137
      should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 08:03:57.345
    Dec  9 08:03:57.345: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename container-runtime 12/09/22 08:03:57.352
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:03:57.369
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:03:57.377
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:248
    STEP: create the container 12/09/22 08:03:57.38
    STEP: wait for the container to reach Succeeded 12/09/22 08:03:57.404
    STEP: get the container status 12/09/22 08:04:01.433
    STEP: the container should be terminated 12/09/22 08:04:01.436
    STEP: the termination message should be set 12/09/22 08:04:01.436
    Dec  9 08:04:01.436: INFO: Expected: &{OK} to match Container's Termination Message: OK --
    STEP: delete the container 12/09/22 08:04:01.436
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Dec  9 08:04:01.453: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-1746" for this suite. 12/09/22 08:04:01.458
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:130
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 08:04:01.471
Dec  9 08:04:01.472: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename downward-api 12/09/22 08:04:01.477
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:04:01.517
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:04:01.524
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:130
STEP: Creating the pod 12/09/22 08:04:01.538
Dec  9 08:04:01.549: INFO: Waiting up to 5m0s for pod "labelsupdate2a2ad52b-22c2-4c96-adf7-93198846b8df" in namespace "downward-api-6537" to be "running and ready"
Dec  9 08:04:01.562: INFO: Pod "labelsupdate2a2ad52b-22c2-4c96-adf7-93198846b8df": Phase="Pending", Reason="", readiness=false. Elapsed: 12.839196ms
Dec  9 08:04:01.562: INFO: The phase of Pod labelsupdate2a2ad52b-22c2-4c96-adf7-93198846b8df is Pending, waiting for it to be Running (with Ready = true)
Dec  9 08:04:03.566: INFO: Pod "labelsupdate2a2ad52b-22c2-4c96-adf7-93198846b8df": Phase="Running", Reason="", readiness=true. Elapsed: 2.016274881s
Dec  9 08:04:03.566: INFO: The phase of Pod labelsupdate2a2ad52b-22c2-4c96-adf7-93198846b8df is Running (Ready = true)
Dec  9 08:04:03.566: INFO: Pod "labelsupdate2a2ad52b-22c2-4c96-adf7-93198846b8df" satisfied condition "running and ready"
Dec  9 08:04:04.088: INFO: Successfully updated pod "labelsupdate2a2ad52b-22c2-4c96-adf7-93198846b8df"
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Dec  9 08:04:06.104: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-6537" for this suite. 12/09/22 08:04:06.112
------------------------------
â€¢ [4.655 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:130

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 08:04:01.471
    Dec  9 08:04:01.472: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename downward-api 12/09/22 08:04:01.477
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:04:01.517
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:04:01.524
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should update labels on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:130
    STEP: Creating the pod 12/09/22 08:04:01.538
    Dec  9 08:04:01.549: INFO: Waiting up to 5m0s for pod "labelsupdate2a2ad52b-22c2-4c96-adf7-93198846b8df" in namespace "downward-api-6537" to be "running and ready"
    Dec  9 08:04:01.562: INFO: Pod "labelsupdate2a2ad52b-22c2-4c96-adf7-93198846b8df": Phase="Pending", Reason="", readiness=false. Elapsed: 12.839196ms
    Dec  9 08:04:01.562: INFO: The phase of Pod labelsupdate2a2ad52b-22c2-4c96-adf7-93198846b8df is Pending, waiting for it to be Running (with Ready = true)
    Dec  9 08:04:03.566: INFO: Pod "labelsupdate2a2ad52b-22c2-4c96-adf7-93198846b8df": Phase="Running", Reason="", readiness=true. Elapsed: 2.016274881s
    Dec  9 08:04:03.566: INFO: The phase of Pod labelsupdate2a2ad52b-22c2-4c96-adf7-93198846b8df is Running (Ready = true)
    Dec  9 08:04:03.566: INFO: Pod "labelsupdate2a2ad52b-22c2-4c96-adf7-93198846b8df" satisfied condition "running and ready"
    Dec  9 08:04:04.088: INFO: Successfully updated pod "labelsupdate2a2ad52b-22c2-4c96-adf7-93198846b8df"
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Dec  9 08:04:06.104: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-6537" for this suite. 12/09/22 08:04:06.112
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Downward API volume
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:249
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 08:04:06.127
Dec  9 08:04:06.127: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename downward-api 12/09/22 08:04:06.127
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:04:06.354
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:04:06.379
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:249
STEP: Creating a pod to test downward API volume plugin 12/09/22 08:04:06.463
Dec  9 08:04:06.512: INFO: Waiting up to 5m0s for pod "downwardapi-volume-58f61276-a0ce-46a2-84f0-814e2c918933" in namespace "downward-api-599" to be "Succeeded or Failed"
Dec  9 08:04:06.524: INFO: Pod "downwardapi-volume-58f61276-a0ce-46a2-84f0-814e2c918933": Phase="Pending", Reason="", readiness=false. Elapsed: 11.909961ms
Dec  9 08:04:08.527: INFO: Pod "downwardapi-volume-58f61276-a0ce-46a2-84f0-814e2c918933": Phase="Running", Reason="", readiness=false. Elapsed: 2.015397s
Dec  9 08:04:10.529: INFO: Pod "downwardapi-volume-58f61276-a0ce-46a2-84f0-814e2c918933": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017403006s
STEP: Saw pod success 12/09/22 08:04:10.529
Dec  9 08:04:10.529: INFO: Pod "downwardapi-volume-58f61276-a0ce-46a2-84f0-814e2c918933" satisfied condition "Succeeded or Failed"
Dec  9 08:04:10.532: INFO: Trying to get logs from node ip-10-0-10-179 pod downwardapi-volume-58f61276-a0ce-46a2-84f0-814e2c918933 container client-container: <nil>
STEP: delete the pod 12/09/22 08:04:10.543
Dec  9 08:04:10.561: INFO: Waiting for pod downwardapi-volume-58f61276-a0ce-46a2-84f0-814e2c918933 to disappear
Dec  9 08:04:10.565: INFO: Pod downwardapi-volume-58f61276-a0ce-46a2-84f0-814e2c918933 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Dec  9 08:04:10.565: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-599" for this suite. 12/09/22 08:04:10.574
------------------------------
â€¢ [4.461 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:249

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 08:04:06.127
    Dec  9 08:04:06.127: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename downward-api 12/09/22 08:04:06.127
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:04:06.354
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:04:06.379
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:249
    STEP: Creating a pod to test downward API volume plugin 12/09/22 08:04:06.463
    Dec  9 08:04:06.512: INFO: Waiting up to 5m0s for pod "downwardapi-volume-58f61276-a0ce-46a2-84f0-814e2c918933" in namespace "downward-api-599" to be "Succeeded or Failed"
    Dec  9 08:04:06.524: INFO: Pod "downwardapi-volume-58f61276-a0ce-46a2-84f0-814e2c918933": Phase="Pending", Reason="", readiness=false. Elapsed: 11.909961ms
    Dec  9 08:04:08.527: INFO: Pod "downwardapi-volume-58f61276-a0ce-46a2-84f0-814e2c918933": Phase="Running", Reason="", readiness=false. Elapsed: 2.015397s
    Dec  9 08:04:10.529: INFO: Pod "downwardapi-volume-58f61276-a0ce-46a2-84f0-814e2c918933": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017403006s
    STEP: Saw pod success 12/09/22 08:04:10.529
    Dec  9 08:04:10.529: INFO: Pod "downwardapi-volume-58f61276-a0ce-46a2-84f0-814e2c918933" satisfied condition "Succeeded or Failed"
    Dec  9 08:04:10.532: INFO: Trying to get logs from node ip-10-0-10-179 pod downwardapi-volume-58f61276-a0ce-46a2-84f0-814e2c918933 container client-container: <nil>
    STEP: delete the pod 12/09/22 08:04:10.543
    Dec  9 08:04:10.561: INFO: Waiting for pod downwardapi-volume-58f61276-a0ce-46a2-84f0-814e2c918933 to disappear
    Dec  9 08:04:10.565: INFO: Pod downwardapi-volume-58f61276-a0ce-46a2-84f0-814e2c918933 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Dec  9 08:04:10.565: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-599" for this suite. 12/09/22 08:04:10.574
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3219
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 08:04:10.592
Dec  9 08:04:10.592: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename services 12/09/22 08:04:10.593
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:04:10.609
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:04:10.613
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3219
STEP: fetching services 12/09/22 08:04:10.617
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Dec  9 08:04:10.620: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-3758" for this suite. 12/09/22 08:04:10.623
------------------------------
â€¢ [0.037 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3219

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 08:04:10.592
    Dec  9 08:04:10.592: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename services 12/09/22 08:04:10.593
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:04:10.609
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:04:10.613
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should find a service from listing all namespaces [Conformance]
      test/e2e/network/service.go:3219
    STEP: fetching services 12/09/22 08:04:10.617
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Dec  9 08:04:10.620: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-3758" for this suite. 12/09/22 08:04:10.623
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:166
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 08:04:10.631
Dec  9 08:04:10.631: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename downward-api 12/09/22 08:04:10.632
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:04:10.648
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:04:10.652
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:166
STEP: Creating a pod to test downward api env vars 12/09/22 08:04:10.656
Dec  9 08:04:10.668: INFO: Waiting up to 5m0s for pod "downward-api-d7798ec1-da93-4582-be1d-03758c5d73f6" in namespace "downward-api-4596" to be "Succeeded or Failed"
Dec  9 08:04:10.673: INFO: Pod "downward-api-d7798ec1-da93-4582-be1d-03758c5d73f6": Phase="Pending", Reason="", readiness=false. Elapsed: 5.061162ms
Dec  9 08:04:12.677: INFO: Pod "downward-api-d7798ec1-da93-4582-be1d-03758c5d73f6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009055842s
Dec  9 08:04:14.677: INFO: Pod "downward-api-d7798ec1-da93-4582-be1d-03758c5d73f6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008831713s
STEP: Saw pod success 12/09/22 08:04:14.677
Dec  9 08:04:14.677: INFO: Pod "downward-api-d7798ec1-da93-4582-be1d-03758c5d73f6" satisfied condition "Succeeded or Failed"
Dec  9 08:04:14.680: INFO: Trying to get logs from node ip-10-0-10-179 pod downward-api-d7798ec1-da93-4582-be1d-03758c5d73f6 container dapi-container: <nil>
STEP: delete the pod 12/09/22 08:04:14.688
Dec  9 08:04:14.700: INFO: Waiting for pod downward-api-d7798ec1-da93-4582-be1d-03758c5d73f6 to disappear
Dec  9 08:04:14.703: INFO: Pod downward-api-d7798ec1-da93-4582-be1d-03758c5d73f6 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Dec  9 08:04:14.703: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-4596" for this suite. 12/09/22 08:04:14.707
------------------------------
â€¢ [4.084 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:166

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 08:04:10.631
    Dec  9 08:04:10.631: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename downward-api 12/09/22 08:04:10.632
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:04:10.648
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:04:10.652
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:166
    STEP: Creating a pod to test downward api env vars 12/09/22 08:04:10.656
    Dec  9 08:04:10.668: INFO: Waiting up to 5m0s for pod "downward-api-d7798ec1-da93-4582-be1d-03758c5d73f6" in namespace "downward-api-4596" to be "Succeeded or Failed"
    Dec  9 08:04:10.673: INFO: Pod "downward-api-d7798ec1-da93-4582-be1d-03758c5d73f6": Phase="Pending", Reason="", readiness=false. Elapsed: 5.061162ms
    Dec  9 08:04:12.677: INFO: Pod "downward-api-d7798ec1-da93-4582-be1d-03758c5d73f6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009055842s
    Dec  9 08:04:14.677: INFO: Pod "downward-api-d7798ec1-da93-4582-be1d-03758c5d73f6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008831713s
    STEP: Saw pod success 12/09/22 08:04:14.677
    Dec  9 08:04:14.677: INFO: Pod "downward-api-d7798ec1-da93-4582-be1d-03758c5d73f6" satisfied condition "Succeeded or Failed"
    Dec  9 08:04:14.680: INFO: Trying to get logs from node ip-10-0-10-179 pod downward-api-d7798ec1-da93-4582-be1d-03758c5d73f6 container dapi-container: <nil>
    STEP: delete the pod 12/09/22 08:04:14.688
    Dec  9 08:04:14.700: INFO: Waiting for pod downward-api-d7798ec1-da93-4582-be1d-03758c5d73f6 to disappear
    Dec  9 08:04:14.703: INFO: Pod downward-api-d7798ec1-da93-4582-be1d-03758c5d73f6 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Dec  9 08:04:14.703: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-4596" for this suite. 12/09/22 08:04:14.707
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:47
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 08:04:14.724
Dec  9 08:04:14.724: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename secrets 12/09/22 08:04:14.725
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:04:14.759
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:04:14.764
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:47
STEP: Creating secret with name secret-test-a8489f4f-47d7-47e4-8080-ae292672ef2d 12/09/22 08:04:14.768
STEP: Creating a pod to test consume secrets 12/09/22 08:04:14.772
Dec  9 08:04:14.783: INFO: Waiting up to 5m0s for pod "pod-secrets-3a0fa0da-6c43-48e4-93f1-935d0fe66d61" in namespace "secrets-9365" to be "Succeeded or Failed"
Dec  9 08:04:14.795: INFO: Pod "pod-secrets-3a0fa0da-6c43-48e4-93f1-935d0fe66d61": Phase="Pending", Reason="", readiness=false. Elapsed: 11.818013ms
Dec  9 08:04:16.800: INFO: Pod "pod-secrets-3a0fa0da-6c43-48e4-93f1-935d0fe66d61": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016825247s
Dec  9 08:04:18.800: INFO: Pod "pod-secrets-3a0fa0da-6c43-48e4-93f1-935d0fe66d61": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017089395s
STEP: Saw pod success 12/09/22 08:04:18.8
Dec  9 08:04:18.801: INFO: Pod "pod-secrets-3a0fa0da-6c43-48e4-93f1-935d0fe66d61" satisfied condition "Succeeded or Failed"
Dec  9 08:04:18.803: INFO: Trying to get logs from node ip-10-0-10-179 pod pod-secrets-3a0fa0da-6c43-48e4-93f1-935d0fe66d61 container secret-volume-test: <nil>
STEP: delete the pod 12/09/22 08:04:18.809
Dec  9 08:04:18.819: INFO: Waiting for pod pod-secrets-3a0fa0da-6c43-48e4-93f1-935d0fe66d61 to disappear
Dec  9 08:04:18.823: INFO: Pod pod-secrets-3a0fa0da-6c43-48e4-93f1-935d0fe66d61 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Dec  9 08:04:18.823: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-9365" for this suite. 12/09/22 08:04:18.828
------------------------------
â€¢ [4.119 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:47

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 08:04:14.724
    Dec  9 08:04:14.724: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename secrets 12/09/22 08:04:14.725
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:04:14.759
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:04:14.764
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:47
    STEP: Creating secret with name secret-test-a8489f4f-47d7-47e4-8080-ae292672ef2d 12/09/22 08:04:14.768
    STEP: Creating a pod to test consume secrets 12/09/22 08:04:14.772
    Dec  9 08:04:14.783: INFO: Waiting up to 5m0s for pod "pod-secrets-3a0fa0da-6c43-48e4-93f1-935d0fe66d61" in namespace "secrets-9365" to be "Succeeded or Failed"
    Dec  9 08:04:14.795: INFO: Pod "pod-secrets-3a0fa0da-6c43-48e4-93f1-935d0fe66d61": Phase="Pending", Reason="", readiness=false. Elapsed: 11.818013ms
    Dec  9 08:04:16.800: INFO: Pod "pod-secrets-3a0fa0da-6c43-48e4-93f1-935d0fe66d61": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016825247s
    Dec  9 08:04:18.800: INFO: Pod "pod-secrets-3a0fa0da-6c43-48e4-93f1-935d0fe66d61": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017089395s
    STEP: Saw pod success 12/09/22 08:04:18.8
    Dec  9 08:04:18.801: INFO: Pod "pod-secrets-3a0fa0da-6c43-48e4-93f1-935d0fe66d61" satisfied condition "Succeeded or Failed"
    Dec  9 08:04:18.803: INFO: Trying to get logs from node ip-10-0-10-179 pod pod-secrets-3a0fa0da-6c43-48e4-93f1-935d0fe66d61 container secret-volume-test: <nil>
    STEP: delete the pod 12/09/22 08:04:18.809
    Dec  9 08:04:18.819: INFO: Waiting for pod pod-secrets-3a0fa0da-6c43-48e4-93f1-935d0fe66d61 to disappear
    Dec  9 08:04:18.823: INFO: Pod pod-secrets-3a0fa0da-6c43-48e4-93f1-935d0fe66d61 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Dec  9 08:04:18.823: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-9365" for this suite. 12/09/22 08:04:18.828
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod
  should be possible to delete [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:135
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 08:04:18.848
Dec  9 08:04:18.848: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename kubelet-test 12/09/22 08:04:18.849
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:04:18.869
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:04:18.872
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[BeforeEach] when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:85
[It] should be possible to delete [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:135
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Dec  9 08:04:18.947: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-4506" for this suite. 12/09/22 08:04:18.952
------------------------------
â€¢ [0.114 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:82
    should be possible to delete [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:135

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 08:04:18.848
    Dec  9 08:04:18.848: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename kubelet-test 12/09/22 08:04:18.849
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:04:18.869
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:04:18.872
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [BeforeEach] when scheduling a busybox command that always fails in a pod
      test/e2e/common/node/kubelet.go:85
    [It] should be possible to delete [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:135
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Dec  9 08:04:18.947: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-4506" for this suite. 12/09/22 08:04:18.952
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:207
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 08:04:18.962
Dec  9 08:04:18.963: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename emptydir 12/09/22 08:04:18.963
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:04:18.982
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:04:18.985
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:207
STEP: Creating a pod to test emptydir 0666 on node default medium 12/09/22 08:04:18.988
Dec  9 08:04:18.998: INFO: Waiting up to 5m0s for pod "pod-c9b3187a-e2c2-45fe-89f4-2b8f5e788b64" in namespace "emptydir-1499" to be "Succeeded or Failed"
Dec  9 08:04:19.007: INFO: Pod "pod-c9b3187a-e2c2-45fe-89f4-2b8f5e788b64": Phase="Pending", Reason="", readiness=false. Elapsed: 8.372453ms
Dec  9 08:04:21.011: INFO: Pod "pod-c9b3187a-e2c2-45fe-89f4-2b8f5e788b64": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012961312s
Dec  9 08:04:23.011: INFO: Pod "pod-c9b3187a-e2c2-45fe-89f4-2b8f5e788b64": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012091021s
STEP: Saw pod success 12/09/22 08:04:23.011
Dec  9 08:04:23.011: INFO: Pod "pod-c9b3187a-e2c2-45fe-89f4-2b8f5e788b64" satisfied condition "Succeeded or Failed"
Dec  9 08:04:23.014: INFO: Trying to get logs from node ip-10-0-10-179 pod pod-c9b3187a-e2c2-45fe-89f4-2b8f5e788b64 container test-container: <nil>
STEP: delete the pod 12/09/22 08:04:23.019
Dec  9 08:04:23.028: INFO: Waiting for pod pod-c9b3187a-e2c2-45fe-89f4-2b8f5e788b64 to disappear
Dec  9 08:04:23.032: INFO: Pod pod-c9b3187a-e2c2-45fe-89f4-2b8f5e788b64 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Dec  9 08:04:23.032: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-1499" for this suite. 12/09/22 08:04:23.036
------------------------------
â€¢ [4.081 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 08:04:18.962
    Dec  9 08:04:18.963: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename emptydir 12/09/22 08:04:18.963
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:04:18.982
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:04:18.985
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:207
    STEP: Creating a pod to test emptydir 0666 on node default medium 12/09/22 08:04:18.988
    Dec  9 08:04:18.998: INFO: Waiting up to 5m0s for pod "pod-c9b3187a-e2c2-45fe-89f4-2b8f5e788b64" in namespace "emptydir-1499" to be "Succeeded or Failed"
    Dec  9 08:04:19.007: INFO: Pod "pod-c9b3187a-e2c2-45fe-89f4-2b8f5e788b64": Phase="Pending", Reason="", readiness=false. Elapsed: 8.372453ms
    Dec  9 08:04:21.011: INFO: Pod "pod-c9b3187a-e2c2-45fe-89f4-2b8f5e788b64": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012961312s
    Dec  9 08:04:23.011: INFO: Pod "pod-c9b3187a-e2c2-45fe-89f4-2b8f5e788b64": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012091021s
    STEP: Saw pod success 12/09/22 08:04:23.011
    Dec  9 08:04:23.011: INFO: Pod "pod-c9b3187a-e2c2-45fe-89f4-2b8f5e788b64" satisfied condition "Succeeded or Failed"
    Dec  9 08:04:23.014: INFO: Trying to get logs from node ip-10-0-10-179 pod pod-c9b3187a-e2c2-45fe-89f4-2b8f5e788b64 container test-container: <nil>
    STEP: delete the pod 12/09/22 08:04:23.019
    Dec  9 08:04:23.028: INFO: Waiting for pod pod-c9b3187a-e2c2-45fe-89f4-2b8f5e788b64 to disappear
    Dec  9 08:04:23.032: INFO: Pod pod-c9b3187a-e2c2-45fe-89f4-2b8f5e788b64 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Dec  9 08:04:23.032: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-1499" for this suite. 12/09/22 08:04:23.036
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] Pods
  should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:344
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 08:04:23.045
Dec  9 08:04:23.045: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename pods 12/09/22 08:04:23.047
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:04:23.064
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:04:23.069
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:344
STEP: creating the pod 12/09/22 08:04:23.073
STEP: submitting the pod to kubernetes 12/09/22 08:04:23.073
Dec  9 08:04:23.082: INFO: Waiting up to 5m0s for pod "pod-update-f1fd8c1d-8103-4499-be2c-5e0ab5fd9942" in namespace "pods-4325" to be "running and ready"
Dec  9 08:04:23.086: INFO: Pod "pod-update-f1fd8c1d-8103-4499-be2c-5e0ab5fd9942": Phase="Pending", Reason="", readiness=false. Elapsed: 4.63862ms
Dec  9 08:04:23.087: INFO: The phase of Pod pod-update-f1fd8c1d-8103-4499-be2c-5e0ab5fd9942 is Pending, waiting for it to be Running (with Ready = true)
Dec  9 08:04:25.090: INFO: Pod "pod-update-f1fd8c1d-8103-4499-be2c-5e0ab5fd9942": Phase="Running", Reason="", readiness=true. Elapsed: 2.008732755s
Dec  9 08:04:25.091: INFO: The phase of Pod pod-update-f1fd8c1d-8103-4499-be2c-5e0ab5fd9942 is Running (Ready = true)
Dec  9 08:04:25.091: INFO: Pod "pod-update-f1fd8c1d-8103-4499-be2c-5e0ab5fd9942" satisfied condition "running and ready"
STEP: verifying the pod is in kubernetes 12/09/22 08:04:25.093
STEP: updating the pod 12/09/22 08:04:25.096
Dec  9 08:04:25.606: INFO: Successfully updated pod "pod-update-f1fd8c1d-8103-4499-be2c-5e0ab5fd9942"
Dec  9 08:04:25.606: INFO: Waiting up to 5m0s for pod "pod-update-f1fd8c1d-8103-4499-be2c-5e0ab5fd9942" in namespace "pods-4325" to be "running"
Dec  9 08:04:25.609: INFO: Pod "pod-update-f1fd8c1d-8103-4499-be2c-5e0ab5fd9942": Phase="Running", Reason="", readiness=true. Elapsed: 2.403399ms
Dec  9 08:04:25.609: INFO: Pod "pod-update-f1fd8c1d-8103-4499-be2c-5e0ab5fd9942" satisfied condition "running"
STEP: verifying the updated pod is in kubernetes 12/09/22 08:04:25.609
Dec  9 08:04:25.612: INFO: Pod update OK
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Dec  9 08:04:25.612: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-4325" for this suite. 12/09/22 08:04:25.615
------------------------------
â€¢ [2.576 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:344

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 08:04:23.045
    Dec  9 08:04:23.045: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename pods 12/09/22 08:04:23.047
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:04:23.064
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:04:23.069
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should be updated [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:344
    STEP: creating the pod 12/09/22 08:04:23.073
    STEP: submitting the pod to kubernetes 12/09/22 08:04:23.073
    Dec  9 08:04:23.082: INFO: Waiting up to 5m0s for pod "pod-update-f1fd8c1d-8103-4499-be2c-5e0ab5fd9942" in namespace "pods-4325" to be "running and ready"
    Dec  9 08:04:23.086: INFO: Pod "pod-update-f1fd8c1d-8103-4499-be2c-5e0ab5fd9942": Phase="Pending", Reason="", readiness=false. Elapsed: 4.63862ms
    Dec  9 08:04:23.087: INFO: The phase of Pod pod-update-f1fd8c1d-8103-4499-be2c-5e0ab5fd9942 is Pending, waiting for it to be Running (with Ready = true)
    Dec  9 08:04:25.090: INFO: Pod "pod-update-f1fd8c1d-8103-4499-be2c-5e0ab5fd9942": Phase="Running", Reason="", readiness=true. Elapsed: 2.008732755s
    Dec  9 08:04:25.091: INFO: The phase of Pod pod-update-f1fd8c1d-8103-4499-be2c-5e0ab5fd9942 is Running (Ready = true)
    Dec  9 08:04:25.091: INFO: Pod "pod-update-f1fd8c1d-8103-4499-be2c-5e0ab5fd9942" satisfied condition "running and ready"
    STEP: verifying the pod is in kubernetes 12/09/22 08:04:25.093
    STEP: updating the pod 12/09/22 08:04:25.096
    Dec  9 08:04:25.606: INFO: Successfully updated pod "pod-update-f1fd8c1d-8103-4499-be2c-5e0ab5fd9942"
    Dec  9 08:04:25.606: INFO: Waiting up to 5m0s for pod "pod-update-f1fd8c1d-8103-4499-be2c-5e0ab5fd9942" in namespace "pods-4325" to be "running"
    Dec  9 08:04:25.609: INFO: Pod "pod-update-f1fd8c1d-8103-4499-be2c-5e0ab5fd9942": Phase="Running", Reason="", readiness=true. Elapsed: 2.403399ms
    Dec  9 08:04:25.609: INFO: Pod "pod-update-f1fd8c1d-8103-4499-be2c-5e0ab5fd9942" satisfied condition "running"
    STEP: verifying the updated pod is in kubernetes 12/09/22 08:04:25.609
    Dec  9 08:04:25.612: INFO: Pod update OK
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Dec  9 08:04:25.612: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-4325" for this suite. 12/09/22 08:04:25.615
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:167
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 08:04:25.624
Dec  9 08:04:25.624: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename emptydir 12/09/22 08:04:25.625
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:04:25.642
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:04:25.648
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:167
STEP: Creating a pod to test emptydir 0644 on node default medium 12/09/22 08:04:25.652
Dec  9 08:04:25.659: INFO: Waiting up to 5m0s for pod "pod-2bd49103-ff6d-4506-9973-ebab2ade095a" in namespace "emptydir-6796" to be "Succeeded or Failed"
Dec  9 08:04:25.691: INFO: Pod "pod-2bd49103-ff6d-4506-9973-ebab2ade095a": Phase="Pending", Reason="", readiness=false. Elapsed: 31.215928ms
Dec  9 08:04:27.695: INFO: Pod "pod-2bd49103-ff6d-4506-9973-ebab2ade095a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03522867s
Dec  9 08:04:29.696: INFO: Pod "pod-2bd49103-ff6d-4506-9973-ebab2ade095a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.03630436s
STEP: Saw pod success 12/09/22 08:04:29.696
Dec  9 08:04:29.696: INFO: Pod "pod-2bd49103-ff6d-4506-9973-ebab2ade095a" satisfied condition "Succeeded or Failed"
Dec  9 08:04:29.699: INFO: Trying to get logs from node ip-10-0-10-179 pod pod-2bd49103-ff6d-4506-9973-ebab2ade095a container test-container: <nil>
STEP: delete the pod 12/09/22 08:04:29.703
Dec  9 08:04:29.712: INFO: Waiting for pod pod-2bd49103-ff6d-4506-9973-ebab2ade095a to disappear
Dec  9 08:04:29.715: INFO: Pod pod-2bd49103-ff6d-4506-9973-ebab2ade095a no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Dec  9 08:04:29.715: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-6796" for this suite. 12/09/22 08:04:29.719
------------------------------
â€¢ [4.099 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:167

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 08:04:25.624
    Dec  9 08:04:25.624: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename emptydir 12/09/22 08:04:25.625
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:04:25.642
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:04:25.648
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:167
    STEP: Creating a pod to test emptydir 0644 on node default medium 12/09/22 08:04:25.652
    Dec  9 08:04:25.659: INFO: Waiting up to 5m0s for pod "pod-2bd49103-ff6d-4506-9973-ebab2ade095a" in namespace "emptydir-6796" to be "Succeeded or Failed"
    Dec  9 08:04:25.691: INFO: Pod "pod-2bd49103-ff6d-4506-9973-ebab2ade095a": Phase="Pending", Reason="", readiness=false. Elapsed: 31.215928ms
    Dec  9 08:04:27.695: INFO: Pod "pod-2bd49103-ff6d-4506-9973-ebab2ade095a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03522867s
    Dec  9 08:04:29.696: INFO: Pod "pod-2bd49103-ff6d-4506-9973-ebab2ade095a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.03630436s
    STEP: Saw pod success 12/09/22 08:04:29.696
    Dec  9 08:04:29.696: INFO: Pod "pod-2bd49103-ff6d-4506-9973-ebab2ade095a" satisfied condition "Succeeded or Failed"
    Dec  9 08:04:29.699: INFO: Trying to get logs from node ip-10-0-10-179 pod pod-2bd49103-ff6d-4506-9973-ebab2ade095a container test-container: <nil>
    STEP: delete the pod 12/09/22 08:04:29.703
    Dec  9 08:04:29.712: INFO: Waiting for pod pod-2bd49103-ff6d-4506-9973-ebab2ade095a to disappear
    Dec  9 08:04:29.715: INFO: Pod pod-2bd49103-ff6d-4506-9973-ebab2ade095a no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Dec  9 08:04:29.715: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-6796" for this suite. 12/09/22 08:04:29.719
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial]
  validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:129
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 08:04:29.727
Dec  9 08:04:29.727: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename sched-preemption 12/09/22 08:04:29.73
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:04:29.748
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:04:29.754
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:96
Dec  9 08:04:29.770: INFO: Waiting up to 1m0s for all nodes to be ready
Dec  9 08:05:29.795: INFO: Waiting for terminating namespaces to be deleted...
[It] validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:129
STEP: Create pods that use 4/5 of node resources. 12/09/22 08:05:29.799
Dec  9 08:05:29.821: INFO: Created pod: pod0-0-sched-preemption-low-priority
Dec  9 08:05:29.833: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Dec  9 08:05:29.865: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Dec  9 08:05:29.880: INFO: Created pod: pod1-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled. 12/09/22 08:05:29.88
Dec  9 08:05:29.880: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-121" to be "running"
Dec  9 08:05:29.889: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 9.421996ms
Dec  9 08:05:31.894: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013700908s
Dec  9 08:05:33.893: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 4.01350403s
Dec  9 08:05:35.894: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 6.014001966s
Dec  9 08:05:37.893: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 8.013351601s
Dec  9 08:05:37.893: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
Dec  9 08:05:37.893: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-121" to be "running"
Dec  9 08:05:37.896: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.55462ms
Dec  9 08:05:37.896: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
Dec  9 08:05:37.896: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-121" to be "running"
Dec  9 08:05:37.898: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.419427ms
Dec  9 08:05:37.898: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
Dec  9 08:05:37.898: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-121" to be "running"
Dec  9 08:05:37.901: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.225306ms
Dec  9 08:05:37.901: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
STEP: Run a high priority pod that has same requirements as that of lower priority pod 12/09/22 08:05:37.901
Dec  9 08:05:37.905: INFO: Waiting up to 2m0s for pod "preemptor-pod" in namespace "sched-preemption-121" to be "running"
Dec  9 08:05:37.910: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.292232ms
Dec  9 08:05:39.915: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009259391s
Dec  9 08:05:41.915: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.00938509s
Dec  9 08:05:43.915: INFO: Pod "preemptor-pod": Phase="Running", Reason="", readiness=true. Elapsed: 6.009475819s
Dec  9 08:05:43.915: INFO: Pod "preemptor-pod" satisfied condition "running"
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/node/init/init.go:32
Dec  9 08:05:43.928: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:84
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-121" for this suite. 12/09/22 08:05:43.964
------------------------------
â€¢ [SLOW TEST] [74.242 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 08:04:29.727
    Dec  9 08:04:29.727: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename sched-preemption 12/09/22 08:04:29.73
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:04:29.748
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:04:29.754
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:96
    Dec  9 08:04:29.770: INFO: Waiting up to 1m0s for all nodes to be ready
    Dec  9 08:05:29.795: INFO: Waiting for terminating namespaces to be deleted...
    [It] validates basic preemption works [Conformance]
      test/e2e/scheduling/preemption.go:129
    STEP: Create pods that use 4/5 of node resources. 12/09/22 08:05:29.799
    Dec  9 08:05:29.821: INFO: Created pod: pod0-0-sched-preemption-low-priority
    Dec  9 08:05:29.833: INFO: Created pod: pod0-1-sched-preemption-medium-priority
    Dec  9 08:05:29.865: INFO: Created pod: pod1-0-sched-preemption-medium-priority
    Dec  9 08:05:29.880: INFO: Created pod: pod1-1-sched-preemption-medium-priority
    STEP: Wait for pods to be scheduled. 12/09/22 08:05:29.88
    Dec  9 08:05:29.880: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-121" to be "running"
    Dec  9 08:05:29.889: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 9.421996ms
    Dec  9 08:05:31.894: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013700908s
    Dec  9 08:05:33.893: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 4.01350403s
    Dec  9 08:05:35.894: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 6.014001966s
    Dec  9 08:05:37.893: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 8.013351601s
    Dec  9 08:05:37.893: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
    Dec  9 08:05:37.893: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-121" to be "running"
    Dec  9 08:05:37.896: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.55462ms
    Dec  9 08:05:37.896: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
    Dec  9 08:05:37.896: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-121" to be "running"
    Dec  9 08:05:37.898: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.419427ms
    Dec  9 08:05:37.898: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
    Dec  9 08:05:37.898: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-121" to be "running"
    Dec  9 08:05:37.901: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.225306ms
    Dec  9 08:05:37.901: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
    STEP: Run a high priority pod that has same requirements as that of lower priority pod 12/09/22 08:05:37.901
    Dec  9 08:05:37.905: INFO: Waiting up to 2m0s for pod "preemptor-pod" in namespace "sched-preemption-121" to be "running"
    Dec  9 08:05:37.910: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.292232ms
    Dec  9 08:05:39.915: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009259391s
    Dec  9 08:05:41.915: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.00938509s
    Dec  9 08:05:43.915: INFO: Pod "preemptor-pod": Phase="Running", Reason="", readiness=true. Elapsed: 6.009475819s
    Dec  9 08:05:43.915: INFO: Pod "preemptor-pod" satisfied condition "running"
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/node/init/init.go:32
    Dec  9 08:05:43.928: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:84
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-121" for this suite. 12/09/22 08:05:43.964
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:193
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 08:05:43.971
Dec  9 08:05:43.972: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename downward-api 12/09/22 08:05:43.973
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:05:43.99
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:05:43.995
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:193
STEP: Creating a pod to test downward API volume plugin 12/09/22 08:05:43.998
Dec  9 08:05:44.008: INFO: Waiting up to 5m0s for pod "downwardapi-volume-67dbf2db-fb15-451b-8885-5f94437cf2f2" in namespace "downward-api-3188" to be "Succeeded or Failed"
Dec  9 08:05:44.014: INFO: Pod "downwardapi-volume-67dbf2db-fb15-451b-8885-5f94437cf2f2": Phase="Pending", Reason="", readiness=false. Elapsed: 5.386348ms
Dec  9 08:05:46.018: INFO: Pod "downwardapi-volume-67dbf2db-fb15-451b-8885-5f94437cf2f2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009124445s
Dec  9 08:05:48.018: INFO: Pod "downwardapi-volume-67dbf2db-fb15-451b-8885-5f94437cf2f2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009032066s
STEP: Saw pod success 12/09/22 08:05:48.018
Dec  9 08:05:48.018: INFO: Pod "downwardapi-volume-67dbf2db-fb15-451b-8885-5f94437cf2f2" satisfied condition "Succeeded or Failed"
Dec  9 08:05:48.021: INFO: Trying to get logs from node ip-10-0-10-179 pod downwardapi-volume-67dbf2db-fb15-451b-8885-5f94437cf2f2 container client-container: <nil>
STEP: delete the pod 12/09/22 08:05:48.027
Dec  9 08:05:48.036: INFO: Waiting for pod downwardapi-volume-67dbf2db-fb15-451b-8885-5f94437cf2f2 to disappear
Dec  9 08:05:48.039: INFO: Pod downwardapi-volume-67dbf2db-fb15-451b-8885-5f94437cf2f2 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Dec  9 08:05:48.039: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-3188" for this suite. 12/09/22 08:05:48.043
------------------------------
â€¢ [4.079 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 08:05:43.971
    Dec  9 08:05:43.972: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename downward-api 12/09/22 08:05:43.973
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:05:43.99
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:05:43.995
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide container's cpu limit [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:193
    STEP: Creating a pod to test downward API volume plugin 12/09/22 08:05:43.998
    Dec  9 08:05:44.008: INFO: Waiting up to 5m0s for pod "downwardapi-volume-67dbf2db-fb15-451b-8885-5f94437cf2f2" in namespace "downward-api-3188" to be "Succeeded or Failed"
    Dec  9 08:05:44.014: INFO: Pod "downwardapi-volume-67dbf2db-fb15-451b-8885-5f94437cf2f2": Phase="Pending", Reason="", readiness=false. Elapsed: 5.386348ms
    Dec  9 08:05:46.018: INFO: Pod "downwardapi-volume-67dbf2db-fb15-451b-8885-5f94437cf2f2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009124445s
    Dec  9 08:05:48.018: INFO: Pod "downwardapi-volume-67dbf2db-fb15-451b-8885-5f94437cf2f2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009032066s
    STEP: Saw pod success 12/09/22 08:05:48.018
    Dec  9 08:05:48.018: INFO: Pod "downwardapi-volume-67dbf2db-fb15-451b-8885-5f94437cf2f2" satisfied condition "Succeeded or Failed"
    Dec  9 08:05:48.021: INFO: Trying to get logs from node ip-10-0-10-179 pod downwardapi-volume-67dbf2db-fb15-451b-8885-5f94437cf2f2 container client-container: <nil>
    STEP: delete the pod 12/09/22 08:05:48.027
    Dec  9 08:05:48.036: INFO: Waiting for pod downwardapi-volume-67dbf2db-fb15-451b-8885-5f94437cf2f2 to disappear
    Dec  9 08:05:48.039: INFO: Pod downwardapi-volume-67dbf2db-fb15-451b-8885-5f94437cf2f2 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Dec  9 08:05:48.039: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-3188" for this suite. 12/09/22 08:05:48.043
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 08:05:48.052
Dec  9 08:05:48.053: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename deployment 12/09/22 08:05:48.053
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:05:48.07
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:05:48.073
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160
Dec  9 08:05:48.078: INFO: Creating deployment "webserver-deployment"
Dec  9 08:05:48.084: INFO: Waiting for observed generation 1
Dec  9 08:05:50.105: INFO: Waiting for all required pods to come up
Dec  9 08:05:50.164: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running 12/09/22 08:05:50.17
Dec  9 08:05:50.172: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-tq2mk" in namespace "deployment-1288" to be "running"
Dec  9 08:05:50.173: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-mtjcv" in namespace "deployment-1288" to be "running"
Dec  9 08:05:50.173: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-tnhgx" in namespace "deployment-1288" to be "running"
Dec  9 08:05:50.173: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-g8wcm" in namespace "deployment-1288" to be "running"
Dec  9 08:05:50.173: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-27tgc" in namespace "deployment-1288" to be "running"
Dec  9 08:05:50.173: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-bm87k" in namespace "deployment-1288" to be "running"
Dec  9 08:05:50.173: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-crvb5" in namespace "deployment-1288" to be "running"
Dec  9 08:05:50.173: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-fm89k" in namespace "deployment-1288" to be "running"
Dec  9 08:05:50.173: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-msl96" in namespace "deployment-1288" to be "running"
Dec  9 08:05:50.173: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-kwbn7" in namespace "deployment-1288" to be "running"
Dec  9 08:05:50.248: INFO: Pod "webserver-deployment-7f5969cbc7-tnhgx": Phase="Pending", Reason="", readiness=false. Elapsed: 75.154449ms
Dec  9 08:05:50.248: INFO: Pod "webserver-deployment-7f5969cbc7-kwbn7": Phase="Pending", Reason="", readiness=false. Elapsed: 72.662599ms
Dec  9 08:05:50.249: INFO: Pod "webserver-deployment-7f5969cbc7-mtjcv": Phase="Pending", Reason="", readiness=false. Elapsed: 75.928298ms
Dec  9 08:05:50.249: INFO: Pod "webserver-deployment-7f5969cbc7-msl96": Phase="Pending", Reason="", readiness=false. Elapsed: 73.315586ms
Dec  9 08:05:50.249: INFO: Pod "webserver-deployment-7f5969cbc7-bm87k": Phase="Pending", Reason="", readiness=false. Elapsed: 74.672836ms
Dec  9 08:05:50.249: INFO: Pod "webserver-deployment-7f5969cbc7-fm89k": Phase="Running", Reason="", readiness=true. Elapsed: 74.233274ms
Dec  9 08:05:50.249: INFO: Pod "webserver-deployment-7f5969cbc7-27tgc": Phase="Running", Reason="", readiness=true. Elapsed: 75.503829ms
Dec  9 08:05:50.249: INFO: Pod "webserver-deployment-7f5969cbc7-27tgc" satisfied condition "running"
Dec  9 08:05:50.249: INFO: Pod "webserver-deployment-7f5969cbc7-fm89k" satisfied condition "running"
Dec  9 08:05:50.249: INFO: Pod "webserver-deployment-7f5969cbc7-crvb5": Phase="Pending", Reason="", readiness=false. Elapsed: 74.57371ms
Dec  9 08:05:50.249: INFO: Pod "webserver-deployment-7f5969cbc7-g8wcm": Phase="Pending", Reason="", readiness=false. Elapsed: 75.554284ms
Dec  9 08:05:50.249: INFO: Pod "webserver-deployment-7f5969cbc7-tq2mk": Phase="Pending", Reason="", readiness=false. Elapsed: 76.355376ms
Dec  9 08:05:52.262: INFO: Pod "webserver-deployment-7f5969cbc7-bm87k": Phase="Pending", Reason="", readiness=false. Elapsed: 2.087994465s
Dec  9 08:05:52.263: INFO: Pod "webserver-deployment-7f5969cbc7-tq2mk": Phase="Pending", Reason="", readiness=false. Elapsed: 2.089538679s
Dec  9 08:05:52.263: INFO: Pod "webserver-deployment-7f5969cbc7-tnhgx": Phase="Running", Reason="", readiness=true. Elapsed: 2.08983744s
Dec  9 08:05:52.263: INFO: Pod "webserver-deployment-7f5969cbc7-tnhgx" satisfied condition "running"
Dec  9 08:05:52.263: INFO: Pod "webserver-deployment-7f5969cbc7-crvb5": Phase="Running", Reason="", readiness=true. Elapsed: 2.088688524s
Dec  9 08:05:52.263: INFO: Pod "webserver-deployment-7f5969cbc7-crvb5" satisfied condition "running"
Dec  9 08:05:52.263: INFO: Pod "webserver-deployment-7f5969cbc7-mtjcv": Phase="Running", Reason="", readiness=true. Elapsed: 2.090336583s
Dec  9 08:05:52.263: INFO: Pod "webserver-deployment-7f5969cbc7-mtjcv" satisfied condition "running"
Dec  9 08:05:52.263: INFO: Pod "webserver-deployment-7f5969cbc7-kwbn7": Phase="Running", Reason="", readiness=true. Elapsed: 2.087295251s
Dec  9 08:05:52.263: INFO: Pod "webserver-deployment-7f5969cbc7-kwbn7" satisfied condition "running"
Dec  9 08:05:52.263: INFO: Pod "webserver-deployment-7f5969cbc7-g8wcm": Phase="Running", Reason="", readiness=true. Elapsed: 2.089587471s
Dec  9 08:05:52.263: INFO: Pod "webserver-deployment-7f5969cbc7-g8wcm" satisfied condition "running"
Dec  9 08:05:52.263: INFO: Pod "webserver-deployment-7f5969cbc7-msl96": Phase="Pending", Reason="", readiness=false. Elapsed: 2.087570438s
Dec  9 08:05:54.254: INFO: Pod "webserver-deployment-7f5969cbc7-bm87k": Phase="Running", Reason="", readiness=true. Elapsed: 4.07954618s
Dec  9 08:05:54.254: INFO: Pod "webserver-deployment-7f5969cbc7-bm87k" satisfied condition "running"
Dec  9 08:05:54.254: INFO: Pod "webserver-deployment-7f5969cbc7-msl96": Phase="Running", Reason="", readiness=true. Elapsed: 4.07870787s
Dec  9 08:05:54.254: INFO: Pod "webserver-deployment-7f5969cbc7-msl96" satisfied condition "running"
Dec  9 08:05:54.255: INFO: Pod "webserver-deployment-7f5969cbc7-tq2mk": Phase="Running", Reason="", readiness=true. Elapsed: 4.081791097s
Dec  9 08:05:54.255: INFO: Pod "webserver-deployment-7f5969cbc7-tq2mk" satisfied condition "running"
Dec  9 08:05:54.255: INFO: Waiting for deployment "webserver-deployment" to complete
Dec  9 08:05:54.260: INFO: Updating deployment "webserver-deployment" with a non-existent image
Dec  9 08:05:54.270: INFO: Updating deployment webserver-deployment
Dec  9 08:05:54.270: INFO: Waiting for observed generation 2
Dec  9 08:05:56.278: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Dec  9 08:05:56.282: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Dec  9 08:05:56.286: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Dec  9 08:05:56.297: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Dec  9 08:05:56.297: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Dec  9 08:05:56.300: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Dec  9 08:05:56.308: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Dec  9 08:05:56.308: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Dec  9 08:05:56.321: INFO: Updating deployment webserver-deployment
Dec  9 08:05:56.321: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Dec  9 08:05:56.352: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Dec  9 08:05:58.396: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Dec  9 08:05:58.410: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-1288  c83ae3c3-fcbb-412f-a89e-1f43e5bb8673 26680 3 2022-12-09 08:05:48 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2022-12-09 08:05:56 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-09 08:05:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003e44808 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2022-12-09 08:05:56 +0000 UTC,LastTransitionTime:2022-12-09 08:05:56 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-d9f79cb5" is progressing.,LastUpdateTime:2022-12-09 08:05:56 +0000 UTC,LastTransitionTime:2022-12-09 08:05:48 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

Dec  9 08:05:58.421: INFO: New ReplicaSet "webserver-deployment-d9f79cb5" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-d9f79cb5  deployment-1288  b17dfe59-7d8d-4e80-88b9-20409e587e4d 26677 3 2022-12-09 08:05:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment c83ae3c3-fcbb-412f-a89e-1f43e5bb8673 0xc003e44cf7 0xc003e44cf8}] [] [{kube-controller-manager Update apps/v1 2022-12-09 08:05:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c83ae3c3-fcbb-412f-a89e-1f43e5bb8673\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-09 08:05:56 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: d9f79cb5,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003e44d98 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Dec  9 08:05:58.421: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Dec  9 08:05:58.421: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-7f5969cbc7  deployment-1288  df4ab830-6b93-4164-9a26-1b865937a2a9 26668 3 2022-12-09 08:05:48 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment c83ae3c3-fcbb-412f-a89e-1f43e5bb8673 0xc003e44c07 0xc003e44c08}] [] [{kube-controller-manager Update apps/v1 2022-12-09 08:05:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c83ae3c3-fcbb-412f-a89e-1f43e5bb8673\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-09 08:05:56 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7f5969cbc7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003e44c98 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
Dec  9 08:05:58.446: INFO: Pod "webserver-deployment-7f5969cbc7-27tgc" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-27tgc webserver-deployment-7f5969cbc7- deployment-1288  39769d1e-c91a-4759-98b8-2540761dcede 26424 0 2022-12-09 08:05:48 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:53e66f78455f8c11b168ca4309d79d37fb400e835ff233f6ae6b9b3ddae1809e cni.projectcalico.org/podIP:10.2.166.157/32 cni.projectcalico.org/podIPs:10.2.166.157/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 df4ab830-6b93-4164-9a26-1b865937a2a9 0xc003f14827 0xc003f14828}] [] [{calico Update v1 2022-12-09 08:05:48 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2022-12-09 08:05:48 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"df4ab830-6b93-4164-9a26-1b865937a2a9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-09 08:05:50 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.2.166.157\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9l6xh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9l6xh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-17-108,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 08:05:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 08:05:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 08:05:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 08:05:48 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.17.108,PodIP:10.2.166.157,StartTime:2022-12-09 08:05:48 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-12-09 08:05:49 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://7513e9b3af0680cbf81004274c35df13f6b44c0b742fc1dea56be2ca48382bbd,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.2.166.157,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec  9 08:05:58.448: INFO: Pod "webserver-deployment-7f5969cbc7-5gf4z" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-5gf4z webserver-deployment-7f5969cbc7- deployment-1288  1e4b65d0-406f-48ca-910e-31fe52f24635 26762 0 2022-12-09 08:05:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:0de6dd2f780d3c9a108b0948b92bfc7a5a2a07b6debf6b6f07e812370594a3fb cni.projectcalico.org/podIP:10.2.166.168/32 cni.projectcalico.org/podIPs:10.2.166.168/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 df4ab830-6b93-4164-9a26-1b865937a2a9 0xc003f14a47 0xc003f14a48}] [] [{kube-controller-manager Update v1 2022-12-09 08:05:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"df4ab830-6b93-4164-9a26-1b865937a2a9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-12-09 08:05:58 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vjxpw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vjxpw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-17-108,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 08:05:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec  9 08:05:58.454: INFO: Pod "webserver-deployment-7f5969cbc7-6vbc2" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-6vbc2 webserver-deployment-7f5969cbc7- deployment-1288  43aac2cf-065e-4b32-86b2-b3b6e437da05 26672 0 2022-12-09 08:05:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 df4ab830-6b93-4164-9a26-1b865937a2a9 0xc003f14bd0 0xc003f14bd1}] [] [{kube-controller-manager Update v1 2022-12-09 08:05:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"df4ab830-6b93-4164-9a26-1b865937a2a9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-09 08:05:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-97kbl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-97kbl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-17-108,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 08:05:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 08:05:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 08:05:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 08:05:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.17.108,PodIP:,StartTime:2022-12-09 08:05:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec  9 08:05:58.454: INFO: Pod "webserver-deployment-7f5969cbc7-7fg5n" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-7fg5n webserver-deployment-7f5969cbc7- deployment-1288  acc283b5-9cb0-45f0-8ce0-4900d4bd4758 26739 0 2022-12-09 08:05:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:2aa7a5a1f4d1496d5b43fe939e2bb484e65f6d65b02f31ab21afdab2033079eb cni.projectcalico.org/podIP:10.2.136.97/32 cni.projectcalico.org/podIPs:10.2.136.97/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 df4ab830-6b93-4164-9a26-1b865937a2a9 0xc003f14db7 0xc003f14db8}] [] [{kube-controller-manager Update v1 2022-12-09 08:05:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"df4ab830-6b93-4164-9a26-1b865937a2a9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-12-09 08:05:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qtwkv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qtwkv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-10-179,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 08:05:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec  9 08:05:58.455: INFO: Pod "webserver-deployment-7f5969cbc7-7pm8k" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-7pm8k webserver-deployment-7f5969cbc7- deployment-1288  3d595ec4-b8a3-4af1-8e5b-cf28837481a0 26732 0 2022-12-09 08:05:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:df41a6952100fb58a16aaecbdeda458276e5b7254dd82419e5095d1b3ded17a4 cni.projectcalico.org/podIP:10.2.166.165/32 cni.projectcalico.org/podIPs:10.2.166.165/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 df4ab830-6b93-4164-9a26-1b865937a2a9 0xc003f14f60 0xc003f14f61}] [] [{kube-controller-manager Update v1 2022-12-09 08:05:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"df4ab830-6b93-4164-9a26-1b865937a2a9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-09 08:05:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2022-12-09 08:05:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bld7l,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bld7l,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-17-108,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 08:05:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 08:05:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 08:05:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 08:05:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.17.108,PodIP:,StartTime:2022-12-09 08:05:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec  9 08:05:58.455: INFO: Pod "webserver-deployment-7f5969cbc7-9c4t4" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-9c4t4 webserver-deployment-7f5969cbc7- deployment-1288  74f280b3-2e52-44f1-8581-c468bd0769f2 26627 0 2022-12-09 08:05:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 df4ab830-6b93-4164-9a26-1b865937a2a9 0xc003f15147 0xc003f15148}] [] [{kube-controller-manager Update v1 2022-12-09 08:05:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"df4ab830-6b93-4164-9a26-1b865937a2a9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mrbqv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mrbqv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-10-179,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 08:05:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec  9 08:05:58.458: INFO: Pod "webserver-deployment-7f5969cbc7-9kmjl" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-9kmjl webserver-deployment-7f5969cbc7- deployment-1288  92ce145d-f5e9-4ad5-ad94-5fabcf987881 26659 0 2022-12-09 08:05:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 df4ab830-6b93-4164-9a26-1b865937a2a9 0xc003f152b0 0xc003f152b1}] [] [{kube-controller-manager Update v1 2022-12-09 08:05:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"df4ab830-6b93-4164-9a26-1b865937a2a9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-x8tt2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-x8tt2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-10-179,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 08:05:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec  9 08:05:58.462: INFO: Pod "webserver-deployment-7f5969cbc7-bm87k" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-bm87k webserver-deployment-7f5969cbc7- deployment-1288  e4d6421f-d97e-473e-ba2c-8e52e4192356 26495 0 2022-12-09 08:05:48 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:6bf4b5219606da5753dca1220bee8d05bd7580dcd3412792a259c0fd4c6f116d cni.projectcalico.org/podIP:10.2.166.160/32 cni.projectcalico.org/podIPs:10.2.166.160/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 df4ab830-6b93-4164-9a26-1b865937a2a9 0xc003f15430 0xc003f15431}] [] [{kube-controller-manager Update v1 2022-12-09 08:05:48 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"df4ab830-6b93-4164-9a26-1b865937a2a9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-12-09 08:05:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-12-09 08:05:52 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.2.166.160\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lpg7p,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lpg7p,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-17-108,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 08:05:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 08:05:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 08:05:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 08:05:48 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.17.108,PodIP:10.2.166.160,StartTime:2022-12-09 08:05:48 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-12-09 08:05:50 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://a5bc97d8bf0fe818c4610e963e5a3b0a1fee06644d6c2df16c594e098d0d02b5,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.2.166.160,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec  9 08:05:58.463: INFO: Pod "webserver-deployment-7f5969cbc7-chckw" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-chckw webserver-deployment-7f5969cbc7- deployment-1288  d418d2cb-a5a6-4432-bfd4-588ae97987db 26723 0 2022-12-09 08:05:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:c0babfb432edb24b9d03be66195b89102a10acbf52f0c90245cefed34cff0e09 cni.projectcalico.org/podIP:10.2.136.96/32 cni.projectcalico.org/podIPs:10.2.136.96/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 df4ab830-6b93-4164-9a26-1b865937a2a9 0xc003f15657 0xc003f15658}] [] [{kube-controller-manager Update v1 2022-12-09 08:05:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"df4ab830-6b93-4164-9a26-1b865937a2a9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-12-09 08:05:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vt9q9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vt9q9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-10-179,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 08:05:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec  9 08:05:58.463: INFO: Pod "webserver-deployment-7f5969cbc7-crvb5" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-crvb5 webserver-deployment-7f5969cbc7- deployment-1288  edf57773-61a2-49bf-ab71-fcb007ad0b3b 26478 0 2022-12-09 08:05:48 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:b6fc7e26f94b17b579a5b727e5860c607ad350547283f276b2bf5e2b5314b0e9 cni.projectcalico.org/podIP:10.2.136.47/32 cni.projectcalico.org/podIPs:10.2.136.47/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 df4ab830-6b93-4164-9a26-1b865937a2a9 0xc003f15800 0xc003f15801}] [] [{kube-controller-manager Update v1 2022-12-09 08:05:48 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"df4ab830-6b93-4164-9a26-1b865937a2a9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-12-09 08:05:49 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-12-09 08:05:51 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.2.136.47\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9xtjt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9xtjt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-10-179,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 08:05:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 08:05:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 08:05:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 08:05:48 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.10.179,PodIP:10.2.136.47,StartTime:2022-12-09 08:05:48 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-12-09 08:05:49 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://0bd9e5c31e8c704a61da2d55165399b131594b2a0684e38c785f8415443068bd,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.2.136.47,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec  9 08:05:58.465: INFO: Pod "webserver-deployment-7f5969cbc7-g8wcm" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-g8wcm webserver-deployment-7f5969cbc7- deployment-1288  c824e5ec-739d-4b47-bf9c-f46da5e8f571 26460 0 2022-12-09 08:05:48 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:abdd1fac5f0adef5fc11bc9e14b6d27f9549a789c2298d31ce66cff2c1423cbd cni.projectcalico.org/podIP:10.2.166.159/32 cni.projectcalico.org/podIPs:10.2.166.159/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 df4ab830-6b93-4164-9a26-1b865937a2a9 0xc003f15a20 0xc003f15a21}] [] [{kube-controller-manager Update v1 2022-12-09 08:05:48 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"df4ab830-6b93-4164-9a26-1b865937a2a9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-12-09 08:05:49 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-12-09 08:05:50 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.2.166.159\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-gr864,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gr864,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-17-108,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 08:05:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 08:05:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 08:05:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 08:05:48 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.17.108,PodIP:10.2.166.159,StartTime:2022-12-09 08:05:48 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-12-09 08:05:50 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://2b7867ba60bdadcc54e96f1739a4d6110144d421edd96db4e20643e2156f9603,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.2.166.159,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec  9 08:05:58.466: INFO: Pod "webserver-deployment-7f5969cbc7-kwbn7" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-kwbn7 webserver-deployment-7f5969cbc7- deployment-1288  76d9ee60-db5f-45c3-8caf-b69a48a0703c 26483 0 2022-12-09 08:05:48 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:ec956856e570ec76959605de3c2c1e6b78eac04f4b5acb441382989fef046226 cni.projectcalico.org/podIP:10.2.166.161/32 cni.projectcalico.org/podIPs:10.2.166.161/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 df4ab830-6b93-4164-9a26-1b865937a2a9 0xc003f15c47 0xc003f15c48}] [] [{kube-controller-manager Update v1 2022-12-09 08:05:48 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"df4ab830-6b93-4164-9a26-1b865937a2a9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-12-09 08:05:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-12-09 08:05:51 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.2.166.161\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xvhfv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xvhfv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-17-108,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 08:05:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 08:05:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 08:05:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 08:05:48 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.17.108,PodIP:10.2.166.161,StartTime:2022-12-09 08:05:48 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-12-09 08:05:50 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://1e63ebb970695e6ce83f51759ba161bd0c33f87150ff4a3c1e15c8743434e271,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.2.166.161,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec  9 08:05:58.466: INFO: Pod "webserver-deployment-7f5969cbc7-l6vql" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-l6vql webserver-deployment-7f5969cbc7- deployment-1288  80ecf4a7-b8f2-4439-9692-306a14ed797a 26661 0 2022-12-09 08:05:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 df4ab830-6b93-4164-9a26-1b865937a2a9 0xc003f15e57 0xc003f15e58}] [] [{kube-controller-manager Update v1 2022-12-09 08:05:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"df4ab830-6b93-4164-9a26-1b865937a2a9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6fmhp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6fmhp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-10-179,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 08:05:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec  9 08:05:58.466: INFO: Pod "webserver-deployment-7f5969cbc7-lk9r4" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-lk9r4 webserver-deployment-7f5969cbc7- deployment-1288  cfa14eb6-873f-4860-af7d-1125e69a0619 26660 0 2022-12-09 08:05:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 df4ab830-6b93-4164-9a26-1b865937a2a9 0xc003f15fc0 0xc003f15fc1}] [] [{kube-controller-manager Update v1 2022-12-09 08:05:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"df4ab830-6b93-4164-9a26-1b865937a2a9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-m4krt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-m4krt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-17-108,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 08:05:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec  9 08:05:58.466: INFO: Pod "webserver-deployment-7f5969cbc7-lxljq" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-lxljq webserver-deployment-7f5969cbc7- deployment-1288  2fea6acd-75d1-4eb2-8fd8-9dcc2785ee6d 26766 0 2022-12-09 08:05:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:ef36271215909165546b18d19e73e2a3b6842094a07209deca614f77be67e5be cni.projectcalico.org/podIP:10.2.136.100/32 cni.projectcalico.org/podIPs:10.2.136.100/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 df4ab830-6b93-4164-9a26-1b865937a2a9 0xc004040140 0xc004040141}] [] [{kube-controller-manager Update v1 2022-12-09 08:05:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"df4ab830-6b93-4164-9a26-1b865937a2a9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-12-09 08:05:58 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4sg7b,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4sg7b,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-10-179,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 08:05:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec  9 08:05:58.467: INFO: Pod "webserver-deployment-7f5969cbc7-msl96" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-msl96 webserver-deployment-7f5969cbc7- deployment-1288  d6f28785-c4a5-4aef-810d-99ecccaaa990 26492 0 2022-12-09 08:05:48 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:2637bfe5e533b8a407aa5b5e4b6bde099e5303981bbef5095783b6004c6d0706 cni.projectcalico.org/podIP:10.2.136.89/32 cni.projectcalico.org/podIPs:10.2.136.89/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 df4ab830-6b93-4164-9a26-1b865937a2a9 0xc0040402e0 0xc0040402e1}] [] [{kube-controller-manager Update v1 2022-12-09 08:05:48 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"df4ab830-6b93-4164-9a26-1b865937a2a9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-12-09 08:05:49 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-12-09 08:05:52 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.2.136.89\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pnv8b,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pnv8b,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-10-179,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 08:05:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 08:05:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 08:05:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 08:05:48 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.10.179,PodIP:10.2.136.89,StartTime:2022-12-09 08:05:48 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-12-09 08:05:49 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://086d3ed61465cc958ccdd08a6034d3c2ff220db619128765004371ff2e9238f0,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.2.136.89,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec  9 08:05:58.467: INFO: Pod "webserver-deployment-7f5969cbc7-mtjcv" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-mtjcv webserver-deployment-7f5969cbc7- deployment-1288  b74c5079-6703-44ad-b235-f87888c692ce 26472 0 2022-12-09 08:05:48 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:bff6a43eea7ef0695bb8ab21c440680df2699d0ee2d93e713305d14c2489c7e6 cni.projectcalico.org/podIP:10.2.136.93/32 cni.projectcalico.org/podIPs:10.2.136.93/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 df4ab830-6b93-4164-9a26-1b865937a2a9 0xc004040500 0xc004040501}] [] [{kube-controller-manager Update v1 2022-12-09 08:05:48 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"df4ab830-6b93-4164-9a26-1b865937a2a9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-12-09 08:05:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-12-09 08:05:51 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.2.136.93\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-d8l6g,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-d8l6g,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-10-179,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 08:05:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 08:05:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 08:05:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 08:05:48 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.10.179,PodIP:10.2.136.93,StartTime:2022-12-09 08:05:48 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-12-09 08:05:50 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://6602eaedea84ba78e8d22184453f9a1fdc96d5b463698edcee51402b5e75b2d8,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.2.136.93,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec  9 08:05:58.467: INFO: Pod "webserver-deployment-7f5969cbc7-thsrc" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-thsrc webserver-deployment-7f5969cbc7- deployment-1288  a9f3eaa4-dda7-4c14-8963-cc881d2b5768 26746 0 2022-12-09 08:05:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:91ba62e2ce4605462126a161ff3544511a6721418bcc206f09ec9faf32fc99a3 cni.projectcalico.org/podIP:10.2.166.166/32 cni.projectcalico.org/podIPs:10.2.166.166/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 df4ab830-6b93-4164-9a26-1b865937a2a9 0xc004040720 0xc004040721}] [] [{kube-controller-manager Update v1 2022-12-09 08:05:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"df4ab830-6b93-4164-9a26-1b865937a2a9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-12-09 08:05:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7ksmq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7ksmq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-17-108,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 08:05:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec  9 08:05:58.468: INFO: Pod "webserver-deployment-7f5969cbc7-tnhgx" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-tnhgx webserver-deployment-7f5969cbc7- deployment-1288  1bf40608-3760-49d7-86ef-e02d2ca28382 26465 0 2022-12-09 08:05:48 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:597cb0ece234bf738fe1327139b09c9b2254edc3ce0d9c5a68669801696be75c cni.projectcalico.org/podIP:10.2.166.158/32 cni.projectcalico.org/podIPs:10.2.166.158/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 df4ab830-6b93-4164-9a26-1b865937a2a9 0xc0040408c0 0xc0040408c1}] [] [{kube-controller-manager Update v1 2022-12-09 08:05:48 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"df4ab830-6b93-4164-9a26-1b865937a2a9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-12-09 08:05:49 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-12-09 08:05:50 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.2.166.158\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-tqnc6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-tqnc6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-17-108,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 08:05:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 08:05:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 08:05:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 08:05:48 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.17.108,PodIP:10.2.166.158,StartTime:2022-12-09 08:05:48 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-12-09 08:05:49 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://ea167464fe710bed4a8f74efaa7c93d90446345be41142a2fc2fe95cd1c8cf50,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.2.166.158,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec  9 08:05:58.469: INFO: Pod "webserver-deployment-7f5969cbc7-vjnd6" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-vjnd6 webserver-deployment-7f5969cbc7- deployment-1288  bf301a92-640f-42c9-af89-9a673de1da92 26753 0 2022-12-09 08:05:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:62c5f966d3484cfca8e94f122f1081b886e6c91521cc0f4225498cb72d404c79 cni.projectcalico.org/podIP:10.2.136.98/32 cni.projectcalico.org/podIPs:10.2.136.98/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 df4ab830-6b93-4164-9a26-1b865937a2a9 0xc004040ae7 0xc004040ae8}] [] [{kube-controller-manager Update v1 2022-12-09 08:05:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"df4ab830-6b93-4164-9a26-1b865937a2a9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-12-09 08:05:58 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7fn9b,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7fn9b,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-10-179,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 08:05:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec  9 08:05:58.469: INFO: Pod "webserver-deployment-d9f79cb5-8gf97" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-8gf97 webserver-deployment-d9f79cb5- deployment-1288  32688c65-5ef7-4a1f-a0a5-0efb2e8fc797 26581 0 2022-12-09 08:05:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:e6c314ec2930a7edaaea6104d23267676e5f07642c9d1409c57dcf26f0f137f3 cni.projectcalico.org/podIP:10.2.136.92/32 cni.projectcalico.org/podIPs:10.2.136.92/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 b17dfe59-7d8d-4e80-88b9-20409e587e4d 0xc004040c5f 0xc004040c90}] [] [{kube-controller-manager Update v1 2022-12-09 08:05:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b17dfe59-7d8d-4e80-88b9-20409e587e4d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-09 08:05:54 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2022-12-09 08:05:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rx6qf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rx6qf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-10-179,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 08:05:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 08:05:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 08:05:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 08:05:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.10.179,PodIP:,StartTime:2022-12-09 08:05:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec  9 08:05:58.470: INFO: Pod "webserver-deployment-d9f79cb5-9qxpt" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-9qxpt webserver-deployment-d9f79cb5- deployment-1288  681b405f-1e3b-4f5d-8bda-fba274303897 26658 0 2022-12-09 08:05:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 b17dfe59-7d8d-4e80-88b9-20409e587e4d 0xc004040e7f 0xc004040e90}] [] [{kube-controller-manager Update v1 2022-12-09 08:05:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b17dfe59-7d8d-4e80-88b9-20409e587e4d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5qp9j,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5qp9j,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-10-179,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 08:05:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec  9 08:05:58.471: INFO: Pod "webserver-deployment-d9f79cb5-ffjd2" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-ffjd2 webserver-deployment-d9f79cb5- deployment-1288  f329919b-cb7d-4dae-84d9-086458174dba 26666 0 2022-12-09 08:05:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 b17dfe59-7d8d-4e80-88b9-20409e587e4d 0xc004040fef 0xc004041000}] [] [{kube-controller-manager Update v1 2022-12-09 08:05:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b17dfe59-7d8d-4e80-88b9-20409e587e4d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xf7g6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xf7g6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-10-179,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 08:05:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec  9 08:05:58.471: INFO: Pod "webserver-deployment-d9f79cb5-gplfs" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-gplfs webserver-deployment-d9f79cb5- deployment-1288  0d94e339-0ff1-46c9-97ce-fe568064f52a 26664 0 2022-12-09 08:05:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 b17dfe59-7d8d-4e80-88b9-20409e587e4d 0xc00404115f 0xc004041170}] [] [{kube-controller-manager Update v1 2022-12-09 08:05:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b17dfe59-7d8d-4e80-88b9-20409e587e4d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ll2g7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ll2g7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-10-179,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 08:05:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec  9 08:05:58.471: INFO: Pod "webserver-deployment-d9f79cb5-jgbxv" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-jgbxv webserver-deployment-d9f79cb5- deployment-1288  af1b96cd-2b50-4858-a6a7-1f9b69a81112 26712 0 2022-12-09 08:05:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:7ed001506e8590cf65d150ec473397f3840ff7d114d5874173cc33bf794404da cni.projectcalico.org/podIP:10.2.166.164/32 cni.projectcalico.org/podIPs:10.2.166.164/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 b17dfe59-7d8d-4e80-88b9-20409e587e4d 0xc0040412cf 0xc004041300}] [] [{calico Update v1 2022-12-09 08:05:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2022-12-09 08:05:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b17dfe59-7d8d-4e80-88b9-20409e587e4d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-09 08:05:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vp9fm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vp9fm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-17-108,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 08:05:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 08:05:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 08:05:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 08:05:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.17.108,PodIP:,StartTime:2022-12-09 08:05:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec  9 08:05:58.472: INFO: Pod "webserver-deployment-d9f79cb5-kjsfr" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-kjsfr webserver-deployment-d9f79cb5- deployment-1288  c61691f3-0a1d-4bd9-8541-f1e6eb92be1d 26750 0 2022-12-09 08:05:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:fea93174b652fb47a4d5cf734f9520ce531c01b93b92bbb55b4a455404241940 cni.projectcalico.org/podIP:10.2.166.167/32 cni.projectcalico.org/podIPs:10.2.166.167/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 b17dfe59-7d8d-4e80-88b9-20409e587e4d 0xc0040414ef 0xc004041520}] [] [{kube-controller-manager Update v1 2022-12-09 08:05:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b17dfe59-7d8d-4e80-88b9-20409e587e4d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-12-09 08:05:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-gmpl8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gmpl8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-17-108,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 08:05:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec  9 08:05:58.473: INFO: Pod "webserver-deployment-d9f79cb5-l9kmh" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-l9kmh webserver-deployment-d9f79cb5- deployment-1288  c76caaa0-200c-4244-9a29-8305368618e0 26599 0 2022-12-09 08:05:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:63149d538f059ef5f23252d9b50a9018212ddc51ae466d02f3727370c1f147af cni.projectcalico.org/podIP:10.2.136.95/32 cni.projectcalico.org/podIPs:10.2.136.95/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 b17dfe59-7d8d-4e80-88b9-20409e587e4d 0xc00404169f 0xc0040416d0}] [] [{kube-controller-manager Update v1 2022-12-09 08:05:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b17dfe59-7d8d-4e80-88b9-20409e587e4d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-12-09 08:05:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-12-09 08:05:55 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vp5s8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vp5s8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-10-179,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 08:05:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 08:05:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 08:05:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 08:05:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.10.179,PodIP:,StartTime:2022-12-09 08:05:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec  9 08:05:58.474: INFO: Pod "webserver-deployment-d9f79cb5-mg9zt" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-mg9zt webserver-deployment-d9f79cb5- deployment-1288  5b3eeed5-df4d-4d63-ac2f-872e9ba9c32b 26605 0 2022-12-09 08:05:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:298f13250572320fb7e6600e32146bd192ba28c0597d774cd4c4653363bb886e cni.projectcalico.org/podIP:10.2.166.163/32 cni.projectcalico.org/podIPs:10.2.166.163/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 b17dfe59-7d8d-4e80-88b9-20409e587e4d 0xc0040418bf 0xc0040418f0}] [] [{kube-controller-manager Update v1 2022-12-09 08:05:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b17dfe59-7d8d-4e80-88b9-20409e587e4d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-12-09 08:05:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-12-09 08:05:55 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.2.166.163\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-67zcq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-67zcq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-17-108,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 08:05:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 08:05:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 08:05:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 08:05:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.17.108,PodIP:10.2.166.163,StartTime:2022-12-09 08:05:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.2.166.163,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec  9 08:05:58.474: INFO: Pod "webserver-deployment-d9f79cb5-ngvfs" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-ngvfs webserver-deployment-d9f79cb5- deployment-1288  c3df4525-8bca-4b20-baa4-db807d53e6ab 26663 0 2022-12-09 08:05:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 b17dfe59-7d8d-4e80-88b9-20409e587e4d 0xc004041b0f 0xc004041b20}] [] [{kube-controller-manager Update v1 2022-12-09 08:05:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b17dfe59-7d8d-4e80-88b9-20409e587e4d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zzk8c,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zzk8c,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-17-108,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 08:05:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec  9 08:05:58.474: INFO: Pod "webserver-deployment-d9f79cb5-snslq" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-snslq webserver-deployment-d9f79cb5- deployment-1288  ce027dc1-a2da-4dbe-89a9-9d9101674a2a 26587 0 2022-12-09 08:05:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:bb78fe0bc0ca163bba9974aeec8276582e4c65bd2a93f177279c32da92131a13 cni.projectcalico.org/podIP:10.2.136.94/32 cni.projectcalico.org/podIPs:10.2.136.94/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 b17dfe59-7d8d-4e80-88b9-20409e587e4d 0xc004041c7f 0xc004041cb0}] [] [{kube-controller-manager Update v1 2022-12-09 08:05:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b17dfe59-7d8d-4e80-88b9-20409e587e4d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-09 08:05:54 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2022-12-09 08:05:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4hl66,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4hl66,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-10-179,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 08:05:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 08:05:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 08:05:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 08:05:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.10.179,PodIP:,StartTime:2022-12-09 08:05:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec  9 08:05:58.475: INFO: Pod "webserver-deployment-d9f79cb5-snw2s" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-snw2s webserver-deployment-d9f79cb5- deployment-1288  1e1c7ac9-cee1-407b-86f5-98a2195ed5bf 26601 0 2022-12-09 08:05:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:6b36d4f9d810d155864bf4b128207e54a619056177ef19bb5ef9afb9b0949c36 cni.projectcalico.org/podIP:10.2.166.162/32 cni.projectcalico.org/podIPs:10.2.166.162/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 b17dfe59-7d8d-4e80-88b9-20409e587e4d 0xc004041e9f 0xc004041ed0}] [] [{calico Update v1 2022-12-09 08:05:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2022-12-09 08:05:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b17dfe59-7d8d-4e80-88b9-20409e587e4d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-09 08:05:55 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.2.166.162\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-m4cmx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-m4cmx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-17-108,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 08:05:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 08:05:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 08:05:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 08:05:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.17.108,PodIP:10.2.166.162,StartTime:2022-12-09 08:05:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.2.166.162,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec  9 08:05:58.475: INFO: Pod "webserver-deployment-d9f79cb5-vq2qf" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-vq2qf webserver-deployment-d9f79cb5- deployment-1288  0d04b645-e86f-4ea1-8ef7-cf4838d88b12 26771 0 2022-12-09 08:05:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:07d3b6921c3715dec6db35cc8cce0832edd47bfdbb6c98e600782d159423a74c cni.projectcalico.org/podIP:10.2.166.169/32 cni.projectcalico.org/podIPs:10.2.166.169/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 b17dfe59-7d8d-4e80-88b9-20409e587e4d 0xc0040bc0ef 0xc0040bc120}] [] [{kube-controller-manager Update v1 2022-12-09 08:05:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b17dfe59-7d8d-4e80-88b9-20409e587e4d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-09 08:05:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2022-12-09 08:05:58 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6bgpm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6bgpm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-17-108,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 08:05:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 08:05:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 08:05:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 08:05:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.17.108,PodIP:,StartTime:2022-12-09 08:05:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec  9 08:05:58.475: INFO: Pod "webserver-deployment-d9f79cb5-wrdkq" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-wrdkq webserver-deployment-d9f79cb5- deployment-1288  b5b40151-bed7-48c9-8ba6-197b6b53db32 26757 0 2022-12-09 08:05:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:b9d5247cdb1434807d9d8bb478e8e593dd834cc87023f8e48a1d39f35f07ea92 cni.projectcalico.org/podIP:10.2.136.99/32 cni.projectcalico.org/podIPs:10.2.136.99/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 b17dfe59-7d8d-4e80-88b9-20409e587e4d 0xc0040bc30f 0xc0040bc340}] [] [{kube-controller-manager Update v1 2022-12-09 08:05:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b17dfe59-7d8d-4e80-88b9-20409e587e4d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-12-09 08:05:58 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-f65g8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-f65g8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-10-179,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 08:05:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Dec  9 08:05:58.476: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-1288" for this suite. 12/09/22 08:05:58.496
------------------------------
â€¢ [SLOW TEST] [10.452 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 08:05:48.052
    Dec  9 08:05:48.053: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename deployment 12/09/22 08:05:48.053
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:05:48.07
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:05:48.073
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should support proportional scaling [Conformance]
      test/e2e/apps/deployment.go:160
    Dec  9 08:05:48.078: INFO: Creating deployment "webserver-deployment"
    Dec  9 08:05:48.084: INFO: Waiting for observed generation 1
    Dec  9 08:05:50.105: INFO: Waiting for all required pods to come up
    Dec  9 08:05:50.164: INFO: Pod name httpd: Found 10 pods out of 10
    STEP: ensuring each pod is running 12/09/22 08:05:50.17
    Dec  9 08:05:50.172: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-tq2mk" in namespace "deployment-1288" to be "running"
    Dec  9 08:05:50.173: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-mtjcv" in namespace "deployment-1288" to be "running"
    Dec  9 08:05:50.173: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-tnhgx" in namespace "deployment-1288" to be "running"
    Dec  9 08:05:50.173: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-g8wcm" in namespace "deployment-1288" to be "running"
    Dec  9 08:05:50.173: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-27tgc" in namespace "deployment-1288" to be "running"
    Dec  9 08:05:50.173: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-bm87k" in namespace "deployment-1288" to be "running"
    Dec  9 08:05:50.173: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-crvb5" in namespace "deployment-1288" to be "running"
    Dec  9 08:05:50.173: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-fm89k" in namespace "deployment-1288" to be "running"
    Dec  9 08:05:50.173: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-msl96" in namespace "deployment-1288" to be "running"
    Dec  9 08:05:50.173: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-kwbn7" in namespace "deployment-1288" to be "running"
    Dec  9 08:05:50.248: INFO: Pod "webserver-deployment-7f5969cbc7-tnhgx": Phase="Pending", Reason="", readiness=false. Elapsed: 75.154449ms
    Dec  9 08:05:50.248: INFO: Pod "webserver-deployment-7f5969cbc7-kwbn7": Phase="Pending", Reason="", readiness=false. Elapsed: 72.662599ms
    Dec  9 08:05:50.249: INFO: Pod "webserver-deployment-7f5969cbc7-mtjcv": Phase="Pending", Reason="", readiness=false. Elapsed: 75.928298ms
    Dec  9 08:05:50.249: INFO: Pod "webserver-deployment-7f5969cbc7-msl96": Phase="Pending", Reason="", readiness=false. Elapsed: 73.315586ms
    Dec  9 08:05:50.249: INFO: Pod "webserver-deployment-7f5969cbc7-bm87k": Phase="Pending", Reason="", readiness=false. Elapsed: 74.672836ms
    Dec  9 08:05:50.249: INFO: Pod "webserver-deployment-7f5969cbc7-fm89k": Phase="Running", Reason="", readiness=true. Elapsed: 74.233274ms
    Dec  9 08:05:50.249: INFO: Pod "webserver-deployment-7f5969cbc7-27tgc": Phase="Running", Reason="", readiness=true. Elapsed: 75.503829ms
    Dec  9 08:05:50.249: INFO: Pod "webserver-deployment-7f5969cbc7-27tgc" satisfied condition "running"
    Dec  9 08:05:50.249: INFO: Pod "webserver-deployment-7f5969cbc7-fm89k" satisfied condition "running"
    Dec  9 08:05:50.249: INFO: Pod "webserver-deployment-7f5969cbc7-crvb5": Phase="Pending", Reason="", readiness=false. Elapsed: 74.57371ms
    Dec  9 08:05:50.249: INFO: Pod "webserver-deployment-7f5969cbc7-g8wcm": Phase="Pending", Reason="", readiness=false. Elapsed: 75.554284ms
    Dec  9 08:05:50.249: INFO: Pod "webserver-deployment-7f5969cbc7-tq2mk": Phase="Pending", Reason="", readiness=false. Elapsed: 76.355376ms
    Dec  9 08:05:52.262: INFO: Pod "webserver-deployment-7f5969cbc7-bm87k": Phase="Pending", Reason="", readiness=false. Elapsed: 2.087994465s
    Dec  9 08:05:52.263: INFO: Pod "webserver-deployment-7f5969cbc7-tq2mk": Phase="Pending", Reason="", readiness=false. Elapsed: 2.089538679s
    Dec  9 08:05:52.263: INFO: Pod "webserver-deployment-7f5969cbc7-tnhgx": Phase="Running", Reason="", readiness=true. Elapsed: 2.08983744s
    Dec  9 08:05:52.263: INFO: Pod "webserver-deployment-7f5969cbc7-tnhgx" satisfied condition "running"
    Dec  9 08:05:52.263: INFO: Pod "webserver-deployment-7f5969cbc7-crvb5": Phase="Running", Reason="", readiness=true. Elapsed: 2.088688524s
    Dec  9 08:05:52.263: INFO: Pod "webserver-deployment-7f5969cbc7-crvb5" satisfied condition "running"
    Dec  9 08:05:52.263: INFO: Pod "webserver-deployment-7f5969cbc7-mtjcv": Phase="Running", Reason="", readiness=true. Elapsed: 2.090336583s
    Dec  9 08:05:52.263: INFO: Pod "webserver-deployment-7f5969cbc7-mtjcv" satisfied condition "running"
    Dec  9 08:05:52.263: INFO: Pod "webserver-deployment-7f5969cbc7-kwbn7": Phase="Running", Reason="", readiness=true. Elapsed: 2.087295251s
    Dec  9 08:05:52.263: INFO: Pod "webserver-deployment-7f5969cbc7-kwbn7" satisfied condition "running"
    Dec  9 08:05:52.263: INFO: Pod "webserver-deployment-7f5969cbc7-g8wcm": Phase="Running", Reason="", readiness=true. Elapsed: 2.089587471s
    Dec  9 08:05:52.263: INFO: Pod "webserver-deployment-7f5969cbc7-g8wcm" satisfied condition "running"
    Dec  9 08:05:52.263: INFO: Pod "webserver-deployment-7f5969cbc7-msl96": Phase="Pending", Reason="", readiness=false. Elapsed: 2.087570438s
    Dec  9 08:05:54.254: INFO: Pod "webserver-deployment-7f5969cbc7-bm87k": Phase="Running", Reason="", readiness=true. Elapsed: 4.07954618s
    Dec  9 08:05:54.254: INFO: Pod "webserver-deployment-7f5969cbc7-bm87k" satisfied condition "running"
    Dec  9 08:05:54.254: INFO: Pod "webserver-deployment-7f5969cbc7-msl96": Phase="Running", Reason="", readiness=true. Elapsed: 4.07870787s
    Dec  9 08:05:54.254: INFO: Pod "webserver-deployment-7f5969cbc7-msl96" satisfied condition "running"
    Dec  9 08:05:54.255: INFO: Pod "webserver-deployment-7f5969cbc7-tq2mk": Phase="Running", Reason="", readiness=true. Elapsed: 4.081791097s
    Dec  9 08:05:54.255: INFO: Pod "webserver-deployment-7f5969cbc7-tq2mk" satisfied condition "running"
    Dec  9 08:05:54.255: INFO: Waiting for deployment "webserver-deployment" to complete
    Dec  9 08:05:54.260: INFO: Updating deployment "webserver-deployment" with a non-existent image
    Dec  9 08:05:54.270: INFO: Updating deployment webserver-deployment
    Dec  9 08:05:54.270: INFO: Waiting for observed generation 2
    Dec  9 08:05:56.278: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
    Dec  9 08:05:56.282: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
    Dec  9 08:05:56.286: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
    Dec  9 08:05:56.297: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
    Dec  9 08:05:56.297: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
    Dec  9 08:05:56.300: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
    Dec  9 08:05:56.308: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
    Dec  9 08:05:56.308: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
    Dec  9 08:05:56.321: INFO: Updating deployment webserver-deployment
    Dec  9 08:05:56.321: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
    Dec  9 08:05:56.352: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
    Dec  9 08:05:58.396: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Dec  9 08:05:58.410: INFO: Deployment "webserver-deployment":
    &Deployment{ObjectMeta:{webserver-deployment  deployment-1288  c83ae3c3-fcbb-412f-a89e-1f43e5bb8673 26680 3 2022-12-09 08:05:48 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2022-12-09 08:05:56 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-09 08:05:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003e44808 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2022-12-09 08:05:56 +0000 UTC,LastTransitionTime:2022-12-09 08:05:56 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-d9f79cb5" is progressing.,LastUpdateTime:2022-12-09 08:05:56 +0000 UTC,LastTransitionTime:2022-12-09 08:05:48 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

    Dec  9 08:05:58.421: INFO: New ReplicaSet "webserver-deployment-d9f79cb5" of Deployment "webserver-deployment":
    &ReplicaSet{ObjectMeta:{webserver-deployment-d9f79cb5  deployment-1288  b17dfe59-7d8d-4e80-88b9-20409e587e4d 26677 3 2022-12-09 08:05:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment c83ae3c3-fcbb-412f-a89e-1f43e5bb8673 0xc003e44cf7 0xc003e44cf8}] [] [{kube-controller-manager Update apps/v1 2022-12-09 08:05:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c83ae3c3-fcbb-412f-a89e-1f43e5bb8673\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-09 08:05:56 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: d9f79cb5,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003e44d98 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Dec  9 08:05:58.421: INFO: All old ReplicaSets of Deployment "webserver-deployment":
    Dec  9 08:05:58.421: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-7f5969cbc7  deployment-1288  df4ab830-6b93-4164-9a26-1b865937a2a9 26668 3 2022-12-09 08:05:48 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment c83ae3c3-fcbb-412f-a89e-1f43e5bb8673 0xc003e44c07 0xc003e44c08}] [] [{kube-controller-manager Update apps/v1 2022-12-09 08:05:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c83ae3c3-fcbb-412f-a89e-1f43e5bb8673\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-09 08:05:56 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7f5969cbc7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003e44c98 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
    Dec  9 08:05:58.446: INFO: Pod "webserver-deployment-7f5969cbc7-27tgc" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-27tgc webserver-deployment-7f5969cbc7- deployment-1288  39769d1e-c91a-4759-98b8-2540761dcede 26424 0 2022-12-09 08:05:48 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:53e66f78455f8c11b168ca4309d79d37fb400e835ff233f6ae6b9b3ddae1809e cni.projectcalico.org/podIP:10.2.166.157/32 cni.projectcalico.org/podIPs:10.2.166.157/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 df4ab830-6b93-4164-9a26-1b865937a2a9 0xc003f14827 0xc003f14828}] [] [{calico Update v1 2022-12-09 08:05:48 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2022-12-09 08:05:48 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"df4ab830-6b93-4164-9a26-1b865937a2a9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-09 08:05:50 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.2.166.157\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9l6xh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9l6xh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-17-108,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 08:05:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 08:05:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 08:05:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 08:05:48 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.17.108,PodIP:10.2.166.157,StartTime:2022-12-09 08:05:48 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-12-09 08:05:49 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://7513e9b3af0680cbf81004274c35df13f6b44c0b742fc1dea56be2ca48382bbd,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.2.166.157,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Dec  9 08:05:58.448: INFO: Pod "webserver-deployment-7f5969cbc7-5gf4z" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-5gf4z webserver-deployment-7f5969cbc7- deployment-1288  1e4b65d0-406f-48ca-910e-31fe52f24635 26762 0 2022-12-09 08:05:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:0de6dd2f780d3c9a108b0948b92bfc7a5a2a07b6debf6b6f07e812370594a3fb cni.projectcalico.org/podIP:10.2.166.168/32 cni.projectcalico.org/podIPs:10.2.166.168/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 df4ab830-6b93-4164-9a26-1b865937a2a9 0xc003f14a47 0xc003f14a48}] [] [{kube-controller-manager Update v1 2022-12-09 08:05:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"df4ab830-6b93-4164-9a26-1b865937a2a9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-12-09 08:05:58 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vjxpw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vjxpw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-17-108,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 08:05:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Dec  9 08:05:58.454: INFO: Pod "webserver-deployment-7f5969cbc7-6vbc2" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-6vbc2 webserver-deployment-7f5969cbc7- deployment-1288  43aac2cf-065e-4b32-86b2-b3b6e437da05 26672 0 2022-12-09 08:05:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 df4ab830-6b93-4164-9a26-1b865937a2a9 0xc003f14bd0 0xc003f14bd1}] [] [{kube-controller-manager Update v1 2022-12-09 08:05:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"df4ab830-6b93-4164-9a26-1b865937a2a9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-09 08:05:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-97kbl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-97kbl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-17-108,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 08:05:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 08:05:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 08:05:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 08:05:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.17.108,PodIP:,StartTime:2022-12-09 08:05:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Dec  9 08:05:58.454: INFO: Pod "webserver-deployment-7f5969cbc7-7fg5n" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-7fg5n webserver-deployment-7f5969cbc7- deployment-1288  acc283b5-9cb0-45f0-8ce0-4900d4bd4758 26739 0 2022-12-09 08:05:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:2aa7a5a1f4d1496d5b43fe939e2bb484e65f6d65b02f31ab21afdab2033079eb cni.projectcalico.org/podIP:10.2.136.97/32 cni.projectcalico.org/podIPs:10.2.136.97/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 df4ab830-6b93-4164-9a26-1b865937a2a9 0xc003f14db7 0xc003f14db8}] [] [{kube-controller-manager Update v1 2022-12-09 08:05:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"df4ab830-6b93-4164-9a26-1b865937a2a9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-12-09 08:05:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qtwkv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qtwkv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-10-179,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 08:05:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Dec  9 08:05:58.455: INFO: Pod "webserver-deployment-7f5969cbc7-7pm8k" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-7pm8k webserver-deployment-7f5969cbc7- deployment-1288  3d595ec4-b8a3-4af1-8e5b-cf28837481a0 26732 0 2022-12-09 08:05:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:df41a6952100fb58a16aaecbdeda458276e5b7254dd82419e5095d1b3ded17a4 cni.projectcalico.org/podIP:10.2.166.165/32 cni.projectcalico.org/podIPs:10.2.166.165/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 df4ab830-6b93-4164-9a26-1b865937a2a9 0xc003f14f60 0xc003f14f61}] [] [{kube-controller-manager Update v1 2022-12-09 08:05:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"df4ab830-6b93-4164-9a26-1b865937a2a9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-09 08:05:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2022-12-09 08:05:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bld7l,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bld7l,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-17-108,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 08:05:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 08:05:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 08:05:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 08:05:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.17.108,PodIP:,StartTime:2022-12-09 08:05:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Dec  9 08:05:58.455: INFO: Pod "webserver-deployment-7f5969cbc7-9c4t4" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-9c4t4 webserver-deployment-7f5969cbc7- deployment-1288  74f280b3-2e52-44f1-8581-c468bd0769f2 26627 0 2022-12-09 08:05:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 df4ab830-6b93-4164-9a26-1b865937a2a9 0xc003f15147 0xc003f15148}] [] [{kube-controller-manager Update v1 2022-12-09 08:05:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"df4ab830-6b93-4164-9a26-1b865937a2a9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mrbqv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mrbqv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-10-179,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 08:05:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Dec  9 08:05:58.458: INFO: Pod "webserver-deployment-7f5969cbc7-9kmjl" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-9kmjl webserver-deployment-7f5969cbc7- deployment-1288  92ce145d-f5e9-4ad5-ad94-5fabcf987881 26659 0 2022-12-09 08:05:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 df4ab830-6b93-4164-9a26-1b865937a2a9 0xc003f152b0 0xc003f152b1}] [] [{kube-controller-manager Update v1 2022-12-09 08:05:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"df4ab830-6b93-4164-9a26-1b865937a2a9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-x8tt2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-x8tt2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-10-179,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 08:05:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Dec  9 08:05:58.462: INFO: Pod "webserver-deployment-7f5969cbc7-bm87k" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-bm87k webserver-deployment-7f5969cbc7- deployment-1288  e4d6421f-d97e-473e-ba2c-8e52e4192356 26495 0 2022-12-09 08:05:48 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:6bf4b5219606da5753dca1220bee8d05bd7580dcd3412792a259c0fd4c6f116d cni.projectcalico.org/podIP:10.2.166.160/32 cni.projectcalico.org/podIPs:10.2.166.160/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 df4ab830-6b93-4164-9a26-1b865937a2a9 0xc003f15430 0xc003f15431}] [] [{kube-controller-manager Update v1 2022-12-09 08:05:48 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"df4ab830-6b93-4164-9a26-1b865937a2a9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-12-09 08:05:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-12-09 08:05:52 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.2.166.160\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lpg7p,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lpg7p,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-17-108,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 08:05:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 08:05:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 08:05:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 08:05:48 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.17.108,PodIP:10.2.166.160,StartTime:2022-12-09 08:05:48 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-12-09 08:05:50 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://a5bc97d8bf0fe818c4610e963e5a3b0a1fee06644d6c2df16c594e098d0d02b5,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.2.166.160,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Dec  9 08:05:58.463: INFO: Pod "webserver-deployment-7f5969cbc7-chckw" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-chckw webserver-deployment-7f5969cbc7- deployment-1288  d418d2cb-a5a6-4432-bfd4-588ae97987db 26723 0 2022-12-09 08:05:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:c0babfb432edb24b9d03be66195b89102a10acbf52f0c90245cefed34cff0e09 cni.projectcalico.org/podIP:10.2.136.96/32 cni.projectcalico.org/podIPs:10.2.136.96/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 df4ab830-6b93-4164-9a26-1b865937a2a9 0xc003f15657 0xc003f15658}] [] [{kube-controller-manager Update v1 2022-12-09 08:05:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"df4ab830-6b93-4164-9a26-1b865937a2a9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-12-09 08:05:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vt9q9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vt9q9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-10-179,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 08:05:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Dec  9 08:05:58.463: INFO: Pod "webserver-deployment-7f5969cbc7-crvb5" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-crvb5 webserver-deployment-7f5969cbc7- deployment-1288  edf57773-61a2-49bf-ab71-fcb007ad0b3b 26478 0 2022-12-09 08:05:48 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:b6fc7e26f94b17b579a5b727e5860c607ad350547283f276b2bf5e2b5314b0e9 cni.projectcalico.org/podIP:10.2.136.47/32 cni.projectcalico.org/podIPs:10.2.136.47/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 df4ab830-6b93-4164-9a26-1b865937a2a9 0xc003f15800 0xc003f15801}] [] [{kube-controller-manager Update v1 2022-12-09 08:05:48 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"df4ab830-6b93-4164-9a26-1b865937a2a9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-12-09 08:05:49 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-12-09 08:05:51 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.2.136.47\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9xtjt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9xtjt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-10-179,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 08:05:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 08:05:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 08:05:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 08:05:48 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.10.179,PodIP:10.2.136.47,StartTime:2022-12-09 08:05:48 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-12-09 08:05:49 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://0bd9e5c31e8c704a61da2d55165399b131594b2a0684e38c785f8415443068bd,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.2.136.47,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Dec  9 08:05:58.465: INFO: Pod "webserver-deployment-7f5969cbc7-g8wcm" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-g8wcm webserver-deployment-7f5969cbc7- deployment-1288  c824e5ec-739d-4b47-bf9c-f46da5e8f571 26460 0 2022-12-09 08:05:48 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:abdd1fac5f0adef5fc11bc9e14b6d27f9549a789c2298d31ce66cff2c1423cbd cni.projectcalico.org/podIP:10.2.166.159/32 cni.projectcalico.org/podIPs:10.2.166.159/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 df4ab830-6b93-4164-9a26-1b865937a2a9 0xc003f15a20 0xc003f15a21}] [] [{kube-controller-manager Update v1 2022-12-09 08:05:48 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"df4ab830-6b93-4164-9a26-1b865937a2a9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-12-09 08:05:49 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-12-09 08:05:50 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.2.166.159\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-gr864,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gr864,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-17-108,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 08:05:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 08:05:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 08:05:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 08:05:48 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.17.108,PodIP:10.2.166.159,StartTime:2022-12-09 08:05:48 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-12-09 08:05:50 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://2b7867ba60bdadcc54e96f1739a4d6110144d421edd96db4e20643e2156f9603,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.2.166.159,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Dec  9 08:05:58.466: INFO: Pod "webserver-deployment-7f5969cbc7-kwbn7" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-kwbn7 webserver-deployment-7f5969cbc7- deployment-1288  76d9ee60-db5f-45c3-8caf-b69a48a0703c 26483 0 2022-12-09 08:05:48 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:ec956856e570ec76959605de3c2c1e6b78eac04f4b5acb441382989fef046226 cni.projectcalico.org/podIP:10.2.166.161/32 cni.projectcalico.org/podIPs:10.2.166.161/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 df4ab830-6b93-4164-9a26-1b865937a2a9 0xc003f15c47 0xc003f15c48}] [] [{kube-controller-manager Update v1 2022-12-09 08:05:48 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"df4ab830-6b93-4164-9a26-1b865937a2a9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-12-09 08:05:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-12-09 08:05:51 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.2.166.161\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xvhfv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xvhfv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-17-108,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 08:05:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 08:05:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 08:05:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 08:05:48 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.17.108,PodIP:10.2.166.161,StartTime:2022-12-09 08:05:48 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-12-09 08:05:50 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://1e63ebb970695e6ce83f51759ba161bd0c33f87150ff4a3c1e15c8743434e271,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.2.166.161,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Dec  9 08:05:58.466: INFO: Pod "webserver-deployment-7f5969cbc7-l6vql" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-l6vql webserver-deployment-7f5969cbc7- deployment-1288  80ecf4a7-b8f2-4439-9692-306a14ed797a 26661 0 2022-12-09 08:05:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 df4ab830-6b93-4164-9a26-1b865937a2a9 0xc003f15e57 0xc003f15e58}] [] [{kube-controller-manager Update v1 2022-12-09 08:05:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"df4ab830-6b93-4164-9a26-1b865937a2a9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6fmhp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6fmhp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-10-179,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 08:05:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Dec  9 08:05:58.466: INFO: Pod "webserver-deployment-7f5969cbc7-lk9r4" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-lk9r4 webserver-deployment-7f5969cbc7- deployment-1288  cfa14eb6-873f-4860-af7d-1125e69a0619 26660 0 2022-12-09 08:05:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 df4ab830-6b93-4164-9a26-1b865937a2a9 0xc003f15fc0 0xc003f15fc1}] [] [{kube-controller-manager Update v1 2022-12-09 08:05:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"df4ab830-6b93-4164-9a26-1b865937a2a9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-m4krt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-m4krt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-17-108,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 08:05:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Dec  9 08:05:58.466: INFO: Pod "webserver-deployment-7f5969cbc7-lxljq" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-lxljq webserver-deployment-7f5969cbc7- deployment-1288  2fea6acd-75d1-4eb2-8fd8-9dcc2785ee6d 26766 0 2022-12-09 08:05:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:ef36271215909165546b18d19e73e2a3b6842094a07209deca614f77be67e5be cni.projectcalico.org/podIP:10.2.136.100/32 cni.projectcalico.org/podIPs:10.2.136.100/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 df4ab830-6b93-4164-9a26-1b865937a2a9 0xc004040140 0xc004040141}] [] [{kube-controller-manager Update v1 2022-12-09 08:05:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"df4ab830-6b93-4164-9a26-1b865937a2a9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-12-09 08:05:58 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4sg7b,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4sg7b,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-10-179,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 08:05:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Dec  9 08:05:58.467: INFO: Pod "webserver-deployment-7f5969cbc7-msl96" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-msl96 webserver-deployment-7f5969cbc7- deployment-1288  d6f28785-c4a5-4aef-810d-99ecccaaa990 26492 0 2022-12-09 08:05:48 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:2637bfe5e533b8a407aa5b5e4b6bde099e5303981bbef5095783b6004c6d0706 cni.projectcalico.org/podIP:10.2.136.89/32 cni.projectcalico.org/podIPs:10.2.136.89/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 df4ab830-6b93-4164-9a26-1b865937a2a9 0xc0040402e0 0xc0040402e1}] [] [{kube-controller-manager Update v1 2022-12-09 08:05:48 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"df4ab830-6b93-4164-9a26-1b865937a2a9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-12-09 08:05:49 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-12-09 08:05:52 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.2.136.89\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pnv8b,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pnv8b,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-10-179,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 08:05:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 08:05:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 08:05:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 08:05:48 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.10.179,PodIP:10.2.136.89,StartTime:2022-12-09 08:05:48 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-12-09 08:05:49 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://086d3ed61465cc958ccdd08a6034d3c2ff220db619128765004371ff2e9238f0,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.2.136.89,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Dec  9 08:05:58.467: INFO: Pod "webserver-deployment-7f5969cbc7-mtjcv" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-mtjcv webserver-deployment-7f5969cbc7- deployment-1288  b74c5079-6703-44ad-b235-f87888c692ce 26472 0 2022-12-09 08:05:48 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:bff6a43eea7ef0695bb8ab21c440680df2699d0ee2d93e713305d14c2489c7e6 cni.projectcalico.org/podIP:10.2.136.93/32 cni.projectcalico.org/podIPs:10.2.136.93/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 df4ab830-6b93-4164-9a26-1b865937a2a9 0xc004040500 0xc004040501}] [] [{kube-controller-manager Update v1 2022-12-09 08:05:48 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"df4ab830-6b93-4164-9a26-1b865937a2a9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-12-09 08:05:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-12-09 08:05:51 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.2.136.93\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-d8l6g,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-d8l6g,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-10-179,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 08:05:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 08:05:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 08:05:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 08:05:48 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.10.179,PodIP:10.2.136.93,StartTime:2022-12-09 08:05:48 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-12-09 08:05:50 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://6602eaedea84ba78e8d22184453f9a1fdc96d5b463698edcee51402b5e75b2d8,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.2.136.93,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Dec  9 08:05:58.467: INFO: Pod "webserver-deployment-7f5969cbc7-thsrc" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-thsrc webserver-deployment-7f5969cbc7- deployment-1288  a9f3eaa4-dda7-4c14-8963-cc881d2b5768 26746 0 2022-12-09 08:05:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:91ba62e2ce4605462126a161ff3544511a6721418bcc206f09ec9faf32fc99a3 cni.projectcalico.org/podIP:10.2.166.166/32 cni.projectcalico.org/podIPs:10.2.166.166/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 df4ab830-6b93-4164-9a26-1b865937a2a9 0xc004040720 0xc004040721}] [] [{kube-controller-manager Update v1 2022-12-09 08:05:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"df4ab830-6b93-4164-9a26-1b865937a2a9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-12-09 08:05:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7ksmq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7ksmq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-17-108,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 08:05:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Dec  9 08:05:58.468: INFO: Pod "webserver-deployment-7f5969cbc7-tnhgx" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-tnhgx webserver-deployment-7f5969cbc7- deployment-1288  1bf40608-3760-49d7-86ef-e02d2ca28382 26465 0 2022-12-09 08:05:48 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:597cb0ece234bf738fe1327139b09c9b2254edc3ce0d9c5a68669801696be75c cni.projectcalico.org/podIP:10.2.166.158/32 cni.projectcalico.org/podIPs:10.2.166.158/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 df4ab830-6b93-4164-9a26-1b865937a2a9 0xc0040408c0 0xc0040408c1}] [] [{kube-controller-manager Update v1 2022-12-09 08:05:48 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"df4ab830-6b93-4164-9a26-1b865937a2a9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-12-09 08:05:49 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-12-09 08:05:50 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.2.166.158\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-tqnc6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-tqnc6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-17-108,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 08:05:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 08:05:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 08:05:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 08:05:48 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.17.108,PodIP:10.2.166.158,StartTime:2022-12-09 08:05:48 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-12-09 08:05:49 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://ea167464fe710bed4a8f74efaa7c93d90446345be41142a2fc2fe95cd1c8cf50,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.2.166.158,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Dec  9 08:05:58.469: INFO: Pod "webserver-deployment-7f5969cbc7-vjnd6" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-vjnd6 webserver-deployment-7f5969cbc7- deployment-1288  bf301a92-640f-42c9-af89-9a673de1da92 26753 0 2022-12-09 08:05:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:62c5f966d3484cfca8e94f122f1081b886e6c91521cc0f4225498cb72d404c79 cni.projectcalico.org/podIP:10.2.136.98/32 cni.projectcalico.org/podIPs:10.2.136.98/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 df4ab830-6b93-4164-9a26-1b865937a2a9 0xc004040ae7 0xc004040ae8}] [] [{kube-controller-manager Update v1 2022-12-09 08:05:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"df4ab830-6b93-4164-9a26-1b865937a2a9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-12-09 08:05:58 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7fn9b,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7fn9b,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-10-179,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 08:05:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Dec  9 08:05:58.469: INFO: Pod "webserver-deployment-d9f79cb5-8gf97" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-8gf97 webserver-deployment-d9f79cb5- deployment-1288  32688c65-5ef7-4a1f-a0a5-0efb2e8fc797 26581 0 2022-12-09 08:05:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:e6c314ec2930a7edaaea6104d23267676e5f07642c9d1409c57dcf26f0f137f3 cni.projectcalico.org/podIP:10.2.136.92/32 cni.projectcalico.org/podIPs:10.2.136.92/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 b17dfe59-7d8d-4e80-88b9-20409e587e4d 0xc004040c5f 0xc004040c90}] [] [{kube-controller-manager Update v1 2022-12-09 08:05:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b17dfe59-7d8d-4e80-88b9-20409e587e4d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-09 08:05:54 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2022-12-09 08:05:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rx6qf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rx6qf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-10-179,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 08:05:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 08:05:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 08:05:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 08:05:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.10.179,PodIP:,StartTime:2022-12-09 08:05:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Dec  9 08:05:58.470: INFO: Pod "webserver-deployment-d9f79cb5-9qxpt" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-9qxpt webserver-deployment-d9f79cb5- deployment-1288  681b405f-1e3b-4f5d-8bda-fba274303897 26658 0 2022-12-09 08:05:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 b17dfe59-7d8d-4e80-88b9-20409e587e4d 0xc004040e7f 0xc004040e90}] [] [{kube-controller-manager Update v1 2022-12-09 08:05:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b17dfe59-7d8d-4e80-88b9-20409e587e4d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5qp9j,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5qp9j,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-10-179,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 08:05:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Dec  9 08:05:58.471: INFO: Pod "webserver-deployment-d9f79cb5-ffjd2" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-ffjd2 webserver-deployment-d9f79cb5- deployment-1288  f329919b-cb7d-4dae-84d9-086458174dba 26666 0 2022-12-09 08:05:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 b17dfe59-7d8d-4e80-88b9-20409e587e4d 0xc004040fef 0xc004041000}] [] [{kube-controller-manager Update v1 2022-12-09 08:05:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b17dfe59-7d8d-4e80-88b9-20409e587e4d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xf7g6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xf7g6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-10-179,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 08:05:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Dec  9 08:05:58.471: INFO: Pod "webserver-deployment-d9f79cb5-gplfs" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-gplfs webserver-deployment-d9f79cb5- deployment-1288  0d94e339-0ff1-46c9-97ce-fe568064f52a 26664 0 2022-12-09 08:05:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 b17dfe59-7d8d-4e80-88b9-20409e587e4d 0xc00404115f 0xc004041170}] [] [{kube-controller-manager Update v1 2022-12-09 08:05:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b17dfe59-7d8d-4e80-88b9-20409e587e4d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ll2g7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ll2g7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-10-179,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 08:05:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Dec  9 08:05:58.471: INFO: Pod "webserver-deployment-d9f79cb5-jgbxv" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-jgbxv webserver-deployment-d9f79cb5- deployment-1288  af1b96cd-2b50-4858-a6a7-1f9b69a81112 26712 0 2022-12-09 08:05:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:7ed001506e8590cf65d150ec473397f3840ff7d114d5874173cc33bf794404da cni.projectcalico.org/podIP:10.2.166.164/32 cni.projectcalico.org/podIPs:10.2.166.164/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 b17dfe59-7d8d-4e80-88b9-20409e587e4d 0xc0040412cf 0xc004041300}] [] [{calico Update v1 2022-12-09 08:05:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2022-12-09 08:05:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b17dfe59-7d8d-4e80-88b9-20409e587e4d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-09 08:05:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vp9fm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vp9fm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-17-108,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 08:05:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 08:05:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 08:05:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 08:05:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.17.108,PodIP:,StartTime:2022-12-09 08:05:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Dec  9 08:05:58.472: INFO: Pod "webserver-deployment-d9f79cb5-kjsfr" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-kjsfr webserver-deployment-d9f79cb5- deployment-1288  c61691f3-0a1d-4bd9-8541-f1e6eb92be1d 26750 0 2022-12-09 08:05:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:fea93174b652fb47a4d5cf734f9520ce531c01b93b92bbb55b4a455404241940 cni.projectcalico.org/podIP:10.2.166.167/32 cni.projectcalico.org/podIPs:10.2.166.167/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 b17dfe59-7d8d-4e80-88b9-20409e587e4d 0xc0040414ef 0xc004041520}] [] [{kube-controller-manager Update v1 2022-12-09 08:05:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b17dfe59-7d8d-4e80-88b9-20409e587e4d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-12-09 08:05:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-gmpl8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gmpl8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-17-108,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 08:05:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Dec  9 08:05:58.473: INFO: Pod "webserver-deployment-d9f79cb5-l9kmh" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-l9kmh webserver-deployment-d9f79cb5- deployment-1288  c76caaa0-200c-4244-9a29-8305368618e0 26599 0 2022-12-09 08:05:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:63149d538f059ef5f23252d9b50a9018212ddc51ae466d02f3727370c1f147af cni.projectcalico.org/podIP:10.2.136.95/32 cni.projectcalico.org/podIPs:10.2.136.95/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 b17dfe59-7d8d-4e80-88b9-20409e587e4d 0xc00404169f 0xc0040416d0}] [] [{kube-controller-manager Update v1 2022-12-09 08:05:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b17dfe59-7d8d-4e80-88b9-20409e587e4d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-12-09 08:05:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-12-09 08:05:55 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vp5s8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vp5s8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-10-179,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 08:05:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 08:05:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 08:05:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 08:05:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.10.179,PodIP:,StartTime:2022-12-09 08:05:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Dec  9 08:05:58.474: INFO: Pod "webserver-deployment-d9f79cb5-mg9zt" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-mg9zt webserver-deployment-d9f79cb5- deployment-1288  5b3eeed5-df4d-4d63-ac2f-872e9ba9c32b 26605 0 2022-12-09 08:05:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:298f13250572320fb7e6600e32146bd192ba28c0597d774cd4c4653363bb886e cni.projectcalico.org/podIP:10.2.166.163/32 cni.projectcalico.org/podIPs:10.2.166.163/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 b17dfe59-7d8d-4e80-88b9-20409e587e4d 0xc0040418bf 0xc0040418f0}] [] [{kube-controller-manager Update v1 2022-12-09 08:05:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b17dfe59-7d8d-4e80-88b9-20409e587e4d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-12-09 08:05:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-12-09 08:05:55 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.2.166.163\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-67zcq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-67zcq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-17-108,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 08:05:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 08:05:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 08:05:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 08:05:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.17.108,PodIP:10.2.166.163,StartTime:2022-12-09 08:05:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.2.166.163,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Dec  9 08:05:58.474: INFO: Pod "webserver-deployment-d9f79cb5-ngvfs" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-ngvfs webserver-deployment-d9f79cb5- deployment-1288  c3df4525-8bca-4b20-baa4-db807d53e6ab 26663 0 2022-12-09 08:05:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 b17dfe59-7d8d-4e80-88b9-20409e587e4d 0xc004041b0f 0xc004041b20}] [] [{kube-controller-manager Update v1 2022-12-09 08:05:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b17dfe59-7d8d-4e80-88b9-20409e587e4d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zzk8c,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zzk8c,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-17-108,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 08:05:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Dec  9 08:05:58.474: INFO: Pod "webserver-deployment-d9f79cb5-snslq" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-snslq webserver-deployment-d9f79cb5- deployment-1288  ce027dc1-a2da-4dbe-89a9-9d9101674a2a 26587 0 2022-12-09 08:05:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:bb78fe0bc0ca163bba9974aeec8276582e4c65bd2a93f177279c32da92131a13 cni.projectcalico.org/podIP:10.2.136.94/32 cni.projectcalico.org/podIPs:10.2.136.94/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 b17dfe59-7d8d-4e80-88b9-20409e587e4d 0xc004041c7f 0xc004041cb0}] [] [{kube-controller-manager Update v1 2022-12-09 08:05:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b17dfe59-7d8d-4e80-88b9-20409e587e4d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-09 08:05:54 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2022-12-09 08:05:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4hl66,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4hl66,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-10-179,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 08:05:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 08:05:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 08:05:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 08:05:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.10.179,PodIP:,StartTime:2022-12-09 08:05:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Dec  9 08:05:58.475: INFO: Pod "webserver-deployment-d9f79cb5-snw2s" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-snw2s webserver-deployment-d9f79cb5- deployment-1288  1e1c7ac9-cee1-407b-86f5-98a2195ed5bf 26601 0 2022-12-09 08:05:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:6b36d4f9d810d155864bf4b128207e54a619056177ef19bb5ef9afb9b0949c36 cni.projectcalico.org/podIP:10.2.166.162/32 cni.projectcalico.org/podIPs:10.2.166.162/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 b17dfe59-7d8d-4e80-88b9-20409e587e4d 0xc004041e9f 0xc004041ed0}] [] [{calico Update v1 2022-12-09 08:05:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2022-12-09 08:05:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b17dfe59-7d8d-4e80-88b9-20409e587e4d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-09 08:05:55 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.2.166.162\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-m4cmx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-m4cmx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-17-108,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 08:05:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 08:05:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 08:05:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 08:05:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.17.108,PodIP:10.2.166.162,StartTime:2022-12-09 08:05:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.2.166.162,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Dec  9 08:05:58.475: INFO: Pod "webserver-deployment-d9f79cb5-vq2qf" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-vq2qf webserver-deployment-d9f79cb5- deployment-1288  0d04b645-e86f-4ea1-8ef7-cf4838d88b12 26771 0 2022-12-09 08:05:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:07d3b6921c3715dec6db35cc8cce0832edd47bfdbb6c98e600782d159423a74c cni.projectcalico.org/podIP:10.2.166.169/32 cni.projectcalico.org/podIPs:10.2.166.169/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 b17dfe59-7d8d-4e80-88b9-20409e587e4d 0xc0040bc0ef 0xc0040bc120}] [] [{kube-controller-manager Update v1 2022-12-09 08:05:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b17dfe59-7d8d-4e80-88b9-20409e587e4d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-09 08:05:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2022-12-09 08:05:58 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6bgpm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6bgpm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-17-108,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 08:05:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 08:05:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 08:05:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 08:05:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.17.108,PodIP:,StartTime:2022-12-09 08:05:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Dec  9 08:05:58.475: INFO: Pod "webserver-deployment-d9f79cb5-wrdkq" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-wrdkq webserver-deployment-d9f79cb5- deployment-1288  b5b40151-bed7-48c9-8ba6-197b6b53db32 26757 0 2022-12-09 08:05:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:b9d5247cdb1434807d9d8bb478e8e593dd834cc87023f8e48a1d39f35f07ea92 cni.projectcalico.org/podIP:10.2.136.99/32 cni.projectcalico.org/podIPs:10.2.136.99/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 b17dfe59-7d8d-4e80-88b9-20409e587e4d 0xc0040bc30f 0xc0040bc340}] [] [{kube-controller-manager Update v1 2022-12-09 08:05:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b17dfe59-7d8d-4e80-88b9-20409e587e4d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-12-09 08:05:58 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-f65g8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-f65g8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-10-179,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 08:05:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Dec  9 08:05:58.476: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-1288" for this suite. 12/09/22 08:05:58.496
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:152
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 08:05:58.513
Dec  9 08:05:58.513: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename container-probe 12/09/22 08:05:58.515
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:05:58.538
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:05:58.548
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:152
STEP: Creating pod busybox-7d743031-f57b-4ca9-913b-e4e4294c14a5 in namespace container-probe-6584 12/09/22 08:05:58.554
Dec  9 08:05:58.563: INFO: Waiting up to 5m0s for pod "busybox-7d743031-f57b-4ca9-913b-e4e4294c14a5" in namespace "container-probe-6584" to be "not pending"
Dec  9 08:05:58.567: INFO: Pod "busybox-7d743031-f57b-4ca9-913b-e4e4294c14a5": Phase="Pending", Reason="", readiness=false. Elapsed: 3.955862ms
Dec  9 08:06:00.570: INFO: Pod "busybox-7d743031-f57b-4ca9-913b-e4e4294c14a5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007270594s
Dec  9 08:06:02.571: INFO: Pod "busybox-7d743031-f57b-4ca9-913b-e4e4294c14a5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008571754s
Dec  9 08:06:04.592: INFO: Pod "busybox-7d743031-f57b-4ca9-913b-e4e4294c14a5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.029814588s
Dec  9 08:06:06.577: INFO: Pod "busybox-7d743031-f57b-4ca9-913b-e4e4294c14a5": Phase="Running", Reason="", readiness=true. Elapsed: 8.014793003s
Dec  9 08:06:06.578: INFO: Pod "busybox-7d743031-f57b-4ca9-913b-e4e4294c14a5" satisfied condition "not pending"
Dec  9 08:06:06.578: INFO: Started pod busybox-7d743031-f57b-4ca9-913b-e4e4294c14a5 in namespace container-probe-6584
STEP: checking the pod's current state and verifying that restartCount is present 12/09/22 08:06:06.578
Dec  9 08:06:06.580: INFO: Initial restart count of pod busybox-7d743031-f57b-4ca9-913b-e4e4294c14a5 is 0
STEP: deleting the pod 12/09/22 08:10:07.148
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Dec  9 08:10:07.161: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-6584" for this suite. 12/09/22 08:10:07.166
------------------------------
â€¢ [SLOW TEST] [248.658 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:152

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 08:05:58.513
    Dec  9 08:05:58.513: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename container-probe 12/09/22 08:05:58.515
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:05:58.538
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:05:58.548
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:152
    STEP: Creating pod busybox-7d743031-f57b-4ca9-913b-e4e4294c14a5 in namespace container-probe-6584 12/09/22 08:05:58.554
    Dec  9 08:05:58.563: INFO: Waiting up to 5m0s for pod "busybox-7d743031-f57b-4ca9-913b-e4e4294c14a5" in namespace "container-probe-6584" to be "not pending"
    Dec  9 08:05:58.567: INFO: Pod "busybox-7d743031-f57b-4ca9-913b-e4e4294c14a5": Phase="Pending", Reason="", readiness=false. Elapsed: 3.955862ms
    Dec  9 08:06:00.570: INFO: Pod "busybox-7d743031-f57b-4ca9-913b-e4e4294c14a5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007270594s
    Dec  9 08:06:02.571: INFO: Pod "busybox-7d743031-f57b-4ca9-913b-e4e4294c14a5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008571754s
    Dec  9 08:06:04.592: INFO: Pod "busybox-7d743031-f57b-4ca9-913b-e4e4294c14a5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.029814588s
    Dec  9 08:06:06.577: INFO: Pod "busybox-7d743031-f57b-4ca9-913b-e4e4294c14a5": Phase="Running", Reason="", readiness=true. Elapsed: 8.014793003s
    Dec  9 08:06:06.578: INFO: Pod "busybox-7d743031-f57b-4ca9-913b-e4e4294c14a5" satisfied condition "not pending"
    Dec  9 08:06:06.578: INFO: Started pod busybox-7d743031-f57b-4ca9-913b-e4e4294c14a5 in namespace container-probe-6584
    STEP: checking the pod's current state and verifying that restartCount is present 12/09/22 08:06:06.578
    Dec  9 08:06:06.580: INFO: Initial restart count of pod busybox-7d743031-f57b-4ca9-913b-e4e4294c14a5 is 0
    STEP: deleting the pod 12/09/22 08:10:07.148
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Dec  9 08:10:07.161: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-6584" for this suite. 12/09/22 08:10:07.166
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 08:10:07.174
Dec  9 08:10:07.177: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename cronjob 12/09/22 08:10:07.178
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:10:07.199
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:10:07.205
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69
STEP: Creating a cronjob 12/09/22 08:10:07.209
STEP: Ensuring more than one job is running at a time 12/09/22 08:10:07.215
STEP: Ensuring at least two running jobs exists by listing jobs explicitly 12/09/22 08:12:01.22
STEP: Removing cronjob 12/09/22 08:12:01.23
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Dec  9 08:12:01.237: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-8585" for this suite. 12/09/22 08:12:01.252
------------------------------
â€¢ [SLOW TEST] [114.107 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 08:10:07.174
    Dec  9 08:10:07.177: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename cronjob 12/09/22 08:10:07.178
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:10:07.199
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:10:07.205
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should schedule multiple jobs concurrently [Conformance]
      test/e2e/apps/cronjob.go:69
    STEP: Creating a cronjob 12/09/22 08:10:07.209
    STEP: Ensuring more than one job is running at a time 12/09/22 08:10:07.215
    STEP: Ensuring at least two running jobs exists by listing jobs explicitly 12/09/22 08:12:01.22
    STEP: Removing cronjob 12/09/22 08:12:01.23
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Dec  9 08:12:01.237: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-8585" for this suite. 12/09/22 08:12:01.252
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:507
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 08:12:01.291
Dec  9 08:12:01.291: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename job 12/09/22 08:12:01.294
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:12:01.341
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:12:01.364
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:507
STEP: Creating a job 12/09/22 08:12:01.396
STEP: Ensuring active pods == parallelism 12/09/22 08:12:01.424
STEP: Orphaning one of the Job's Pods 12/09/22 08:12:03.429
Dec  9 08:12:03.969: INFO: Successfully updated pod "adopt-release-4nv7v"
STEP: Checking that the Job readopts the Pod 12/09/22 08:12:03.969
Dec  9 08:12:03.969: INFO: Waiting up to 15m0s for pod "adopt-release-4nv7v" in namespace "job-9995" to be "adopted"
Dec  9 08:12:04.001: INFO: Pod "adopt-release-4nv7v": Phase="Running", Reason="", readiness=true. Elapsed: 31.98975ms
Dec  9 08:12:06.007: INFO: Pod "adopt-release-4nv7v": Phase="Running", Reason="", readiness=true. Elapsed: 2.037687584s
Dec  9 08:12:06.007: INFO: Pod "adopt-release-4nv7v" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod 12/09/22 08:12:06.007
Dec  9 08:12:06.527: INFO: Successfully updated pod "adopt-release-4nv7v"
STEP: Checking that the Job releases the Pod 12/09/22 08:12:06.527
Dec  9 08:12:06.528: INFO: Waiting up to 15m0s for pod "adopt-release-4nv7v" in namespace "job-9995" to be "released"
Dec  9 08:12:06.532: INFO: Pod "adopt-release-4nv7v": Phase="Running", Reason="", readiness=true. Elapsed: 4.463656ms
Dec  9 08:12:08.537: INFO: Pod "adopt-release-4nv7v": Phase="Running", Reason="", readiness=true. Elapsed: 2.009022722s
Dec  9 08:12:08.537: INFO: Pod "adopt-release-4nv7v" satisfied condition "released"
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Dec  9 08:12:08.537: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-9995" for this suite. 12/09/22 08:12:08.547
------------------------------
â€¢ [SLOW TEST] [7.278 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:507

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 08:12:01.291
    Dec  9 08:12:01.291: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename job 12/09/22 08:12:01.294
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:12:01.341
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:12:01.364
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should adopt matching orphans and release non-matching pods [Conformance]
      test/e2e/apps/job.go:507
    STEP: Creating a job 12/09/22 08:12:01.396
    STEP: Ensuring active pods == parallelism 12/09/22 08:12:01.424
    STEP: Orphaning one of the Job's Pods 12/09/22 08:12:03.429
    Dec  9 08:12:03.969: INFO: Successfully updated pod "adopt-release-4nv7v"
    STEP: Checking that the Job readopts the Pod 12/09/22 08:12:03.969
    Dec  9 08:12:03.969: INFO: Waiting up to 15m0s for pod "adopt-release-4nv7v" in namespace "job-9995" to be "adopted"
    Dec  9 08:12:04.001: INFO: Pod "adopt-release-4nv7v": Phase="Running", Reason="", readiness=true. Elapsed: 31.98975ms
    Dec  9 08:12:06.007: INFO: Pod "adopt-release-4nv7v": Phase="Running", Reason="", readiness=true. Elapsed: 2.037687584s
    Dec  9 08:12:06.007: INFO: Pod "adopt-release-4nv7v" satisfied condition "adopted"
    STEP: Removing the labels from the Job's Pod 12/09/22 08:12:06.007
    Dec  9 08:12:06.527: INFO: Successfully updated pod "adopt-release-4nv7v"
    STEP: Checking that the Job releases the Pod 12/09/22 08:12:06.527
    Dec  9 08:12:06.528: INFO: Waiting up to 15m0s for pod "adopt-release-4nv7v" in namespace "job-9995" to be "released"
    Dec  9 08:12:06.532: INFO: Pod "adopt-release-4nv7v": Phase="Running", Reason="", readiness=true. Elapsed: 4.463656ms
    Dec  9 08:12:08.537: INFO: Pod "adopt-release-4nv7v": Phase="Running", Reason="", readiness=true. Elapsed: 2.009022722s
    Dec  9 08:12:08.537: INFO: Pod "adopt-release-4nv7v" satisfied condition "released"
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Dec  9 08:12:08.537: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-9995" for this suite. 12/09/22 08:12:08.547
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Downward API volume
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:207
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 08:12:08.573
Dec  9 08:12:08.574: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename downward-api 12/09/22 08:12:08.575
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:12:08.628
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:12:08.638
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:207
STEP: Creating a pod to test downward API volume plugin 12/09/22 08:12:08.641
Dec  9 08:12:08.652: INFO: Waiting up to 5m0s for pod "downwardapi-volume-26f30d58-ace3-4c5f-8357-e3361689dced" in namespace "downward-api-9029" to be "Succeeded or Failed"
Dec  9 08:12:08.656: INFO: Pod "downwardapi-volume-26f30d58-ace3-4c5f-8357-e3361689dced": Phase="Pending", Reason="", readiness=false. Elapsed: 3.624471ms
Dec  9 08:12:10.661: INFO: Pod "downwardapi-volume-26f30d58-ace3-4c5f-8357-e3361689dced": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008890797s
Dec  9 08:12:12.660: INFO: Pod "downwardapi-volume-26f30d58-ace3-4c5f-8357-e3361689dced": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007124293s
STEP: Saw pod success 12/09/22 08:12:12.66
Dec  9 08:12:12.660: INFO: Pod "downwardapi-volume-26f30d58-ace3-4c5f-8357-e3361689dced" satisfied condition "Succeeded or Failed"
Dec  9 08:12:12.663: INFO: Trying to get logs from node ip-10-0-10-179 pod downwardapi-volume-26f30d58-ace3-4c5f-8357-e3361689dced container client-container: <nil>
STEP: delete the pod 12/09/22 08:12:12.678
Dec  9 08:12:12.688: INFO: Waiting for pod downwardapi-volume-26f30d58-ace3-4c5f-8357-e3361689dced to disappear
Dec  9 08:12:12.691: INFO: Pod downwardapi-volume-26f30d58-ace3-4c5f-8357-e3361689dced no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Dec  9 08:12:12.691: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-9029" for this suite. 12/09/22 08:12:12.694
------------------------------
â€¢ [4.128 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 08:12:08.573
    Dec  9 08:12:08.574: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename downward-api 12/09/22 08:12:08.575
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:12:08.628
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:12:08.638
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide container's memory limit [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:207
    STEP: Creating a pod to test downward API volume plugin 12/09/22 08:12:08.641
    Dec  9 08:12:08.652: INFO: Waiting up to 5m0s for pod "downwardapi-volume-26f30d58-ace3-4c5f-8357-e3361689dced" in namespace "downward-api-9029" to be "Succeeded or Failed"
    Dec  9 08:12:08.656: INFO: Pod "downwardapi-volume-26f30d58-ace3-4c5f-8357-e3361689dced": Phase="Pending", Reason="", readiness=false. Elapsed: 3.624471ms
    Dec  9 08:12:10.661: INFO: Pod "downwardapi-volume-26f30d58-ace3-4c5f-8357-e3361689dced": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008890797s
    Dec  9 08:12:12.660: INFO: Pod "downwardapi-volume-26f30d58-ace3-4c5f-8357-e3361689dced": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007124293s
    STEP: Saw pod success 12/09/22 08:12:12.66
    Dec  9 08:12:12.660: INFO: Pod "downwardapi-volume-26f30d58-ace3-4c5f-8357-e3361689dced" satisfied condition "Succeeded or Failed"
    Dec  9 08:12:12.663: INFO: Trying to get logs from node ip-10-0-10-179 pod downwardapi-volume-26f30d58-ace3-4c5f-8357-e3361689dced container client-container: <nil>
    STEP: delete the pod 12/09/22 08:12:12.678
    Dec  9 08:12:12.688: INFO: Waiting for pod downwardapi-volume-26f30d58-ace3-4c5f-8357-e3361689dced to disappear
    Dec  9 08:12:12.691: INFO: Pod downwardapi-volume-26f30d58-ace3-4c5f-8357-e3361689dced no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Dec  9 08:12:12.691: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-9029" for this suite. 12/09/22 08:12:12.694
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
  should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 08:12:12.702
Dec  9 08:12:12.702: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename runtimeclass 12/09/22 08:12:12.703
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:12:12.721
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:12:12.726
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It] should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156
STEP: Deleting RuntimeClass runtimeclass-7957-delete-me 12/09/22 08:12:12.736
STEP: Waiting for the RuntimeClass to disappear 12/09/22 08:12:12.741
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Dec  9 08:12:12.757: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-7957" for this suite. 12/09/22 08:12:12.761
------------------------------
â€¢ [0.065 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 08:12:12.702
    Dec  9 08:12:12.702: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename runtimeclass 12/09/22 08:12:12.703
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:12:12.721
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:12:12.726
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It] should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:156
    STEP: Deleting RuntimeClass runtimeclass-7957-delete-me 12/09/22 08:12:12.736
    STEP: Waiting for the RuntimeClass to disappear 12/09/22 08:12:12.741
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Dec  9 08:12:12.757: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-7957" for this suite. 12/09/22 08:12:12.761
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:84
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 08:12:12.769
Dec  9 08:12:12.769: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename downward-api 12/09/22 08:12:12.77
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:12:12.796
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:12:12.799
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:84
STEP: Creating a pod to test downward API volume plugin 12/09/22 08:12:12.803
Dec  9 08:12:12.813: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0dc2e9fe-b308-429c-b114-4660da48fca3" in namespace "downward-api-7841" to be "Succeeded or Failed"
Dec  9 08:12:12.820: INFO: Pod "downwardapi-volume-0dc2e9fe-b308-429c-b114-4660da48fca3": Phase="Pending", Reason="", readiness=false. Elapsed: 6.833593ms
Dec  9 08:12:14.823: INFO: Pod "downwardapi-volume-0dc2e9fe-b308-429c-b114-4660da48fca3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010389494s
Dec  9 08:12:16.823: INFO: Pod "downwardapi-volume-0dc2e9fe-b308-429c-b114-4660da48fca3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010485143s
STEP: Saw pod success 12/09/22 08:12:16.824
Dec  9 08:12:16.824: INFO: Pod "downwardapi-volume-0dc2e9fe-b308-429c-b114-4660da48fca3" satisfied condition "Succeeded or Failed"
Dec  9 08:12:16.827: INFO: Trying to get logs from node ip-10-0-10-179 pod downwardapi-volume-0dc2e9fe-b308-429c-b114-4660da48fca3 container client-container: <nil>
STEP: delete the pod 12/09/22 08:12:16.832
Dec  9 08:12:16.844: INFO: Waiting for pod downwardapi-volume-0dc2e9fe-b308-429c-b114-4660da48fca3 to disappear
Dec  9 08:12:16.849: INFO: Pod downwardapi-volume-0dc2e9fe-b308-429c-b114-4660da48fca3 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Dec  9 08:12:16.849: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-7841" for this suite. 12/09/22 08:12:16.853
------------------------------
â€¢ [4.089 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:84

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 08:12:12.769
    Dec  9 08:12:12.769: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename downward-api 12/09/22 08:12:12.77
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:12:12.796
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:12:12.799
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:84
    STEP: Creating a pod to test downward API volume plugin 12/09/22 08:12:12.803
    Dec  9 08:12:12.813: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0dc2e9fe-b308-429c-b114-4660da48fca3" in namespace "downward-api-7841" to be "Succeeded or Failed"
    Dec  9 08:12:12.820: INFO: Pod "downwardapi-volume-0dc2e9fe-b308-429c-b114-4660da48fca3": Phase="Pending", Reason="", readiness=false. Elapsed: 6.833593ms
    Dec  9 08:12:14.823: INFO: Pod "downwardapi-volume-0dc2e9fe-b308-429c-b114-4660da48fca3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010389494s
    Dec  9 08:12:16.823: INFO: Pod "downwardapi-volume-0dc2e9fe-b308-429c-b114-4660da48fca3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010485143s
    STEP: Saw pod success 12/09/22 08:12:16.824
    Dec  9 08:12:16.824: INFO: Pod "downwardapi-volume-0dc2e9fe-b308-429c-b114-4660da48fca3" satisfied condition "Succeeded or Failed"
    Dec  9 08:12:16.827: INFO: Trying to get logs from node ip-10-0-10-179 pod downwardapi-volume-0dc2e9fe-b308-429c-b114-4660da48fca3 container client-container: <nil>
    STEP: delete the pod 12/09/22 08:12:16.832
    Dec  9 08:12:16.844: INFO: Waiting for pod downwardapi-volume-0dc2e9fe-b308-429c-b114-4660da48fca3 to disappear
    Dec  9 08:12:16.849: INFO: Pod downwardapi-volume-0dc2e9fe-b308-429c-b114-4660da48fca3 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Dec  9 08:12:16.849: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-7841" for this suite. 12/09/22 08:12:16.853
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Single Pod [Serial]
  removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:293
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 08:12:16.861
Dec  9 08:12:16.861: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename taint-single-pod 12/09/22 08:12:16.862
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:12:16.879
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:12:16.891
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/node/taints.go:170
Dec  9 08:12:16.895: INFO: Waiting up to 1m0s for all nodes to be ready
Dec  9 08:13:16.915: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:293
Dec  9 08:13:16.918: INFO: Starting informer...
STEP: Starting pod... 12/09/22 08:13:16.918
Dec  9 08:13:17.134: INFO: Pod is running on ip-10-0-10-179. Tainting Node
STEP: Trying to apply a taint on the Node 12/09/22 08:13:17.135
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 12/09/22 08:13:17.15
STEP: Waiting short time to make sure Pod is queued for deletion 12/09/22 08:13:17.154
Dec  9 08:13:17.154: INFO: Pod wasn't evicted. Proceeding
Dec  9 08:13:17.154: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 12/09/22 08:13:17.168
STEP: Waiting some time to make sure that toleration time passed. 12/09/22 08:13:17.172
Dec  9 08:14:32.173: INFO: Pod wasn't evicted. Test successful
[AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/node/init/init.go:32
Dec  9 08:14:32.173: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "taint-single-pod-8712" for this suite. 12/09/22 08:14:32.177
------------------------------
â€¢ [SLOW TEST] [135.323 seconds]
[sig-node] NoExecuteTaintManager Single Pod [Serial]
test/e2e/node/framework.go:23
  removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:293

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 08:12:16.861
    Dec  9 08:12:16.861: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename taint-single-pod 12/09/22 08:12:16.862
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:12:16.879
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:12:16.891
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/node/taints.go:170
    Dec  9 08:12:16.895: INFO: Waiting up to 1m0s for all nodes to be ready
    Dec  9 08:13:16.915: INFO: Waiting for terminating namespaces to be deleted...
    [It] removing taint cancels eviction [Disruptive] [Conformance]
      test/e2e/node/taints.go:293
    Dec  9 08:13:16.918: INFO: Starting informer...
    STEP: Starting pod... 12/09/22 08:13:16.918
    Dec  9 08:13:17.134: INFO: Pod is running on ip-10-0-10-179. Tainting Node
    STEP: Trying to apply a taint on the Node 12/09/22 08:13:17.135
    STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 12/09/22 08:13:17.15
    STEP: Waiting short time to make sure Pod is queued for deletion 12/09/22 08:13:17.154
    Dec  9 08:13:17.154: INFO: Pod wasn't evicted. Proceeding
    Dec  9 08:13:17.154: INFO: Removing taint from Node
    STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 12/09/22 08:13:17.168
    STEP: Waiting some time to make sure that toleration time passed. 12/09/22 08:13:17.172
    Dec  9 08:14:32.173: INFO: Pod wasn't evicted. Test successful
    [AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/node/init/init.go:32
    Dec  9 08:14:32.173: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "taint-single-pod-8712" for this suite. 12/09/22 08:14:32.177
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:742
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 08:14:32.185
Dec  9 08:14:32.185: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename svcaccounts 12/09/22 08:14:32.186
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:14:32.2
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:14:32.205
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:742
Dec  9 08:14:32.211: INFO: Got root ca configmap in namespace "svcaccounts-5615"
Dec  9 08:14:32.217: INFO: Deleted root ca configmap in namespace "svcaccounts-5615"
STEP: waiting for a new root ca configmap created 12/09/22 08:14:32.717
Dec  9 08:14:32.722: INFO: Recreated root ca configmap in namespace "svcaccounts-5615"
Dec  9 08:14:32.729: INFO: Updated root ca configmap in namespace "svcaccounts-5615"
STEP: waiting for the root ca configmap reconciled 12/09/22 08:14:33.23
Dec  9 08:14:33.234: INFO: Reconciled root ca configmap in namespace "svcaccounts-5615"
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Dec  9 08:14:33.234: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-5615" for this suite. 12/09/22 08:14:33.237
------------------------------
â€¢ [1.060 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:742

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 08:14:32.185
    Dec  9 08:14:32.185: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename svcaccounts 12/09/22 08:14:32.186
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:14:32.2
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:14:32.205
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
      test/e2e/auth/service_accounts.go:742
    Dec  9 08:14:32.211: INFO: Got root ca configmap in namespace "svcaccounts-5615"
    Dec  9 08:14:32.217: INFO: Deleted root ca configmap in namespace "svcaccounts-5615"
    STEP: waiting for a new root ca configmap created 12/09/22 08:14:32.717
    Dec  9 08:14:32.722: INFO: Recreated root ca configmap in namespace "svcaccounts-5615"
    Dec  9 08:14:32.729: INFO: Updated root ca configmap in namespace "svcaccounts-5615"
    STEP: waiting for the root ca configmap reconciled 12/09/22 08:14:33.23
    Dec  9 08:14:33.234: INFO: Reconciled root ca configmap in namespace "svcaccounts-5615"
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Dec  9 08:14:33.234: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-5615" for this suite. 12/09/22 08:14:33.237
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:100
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 08:14:33.248
Dec  9 08:14:33.248: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename resourcequota 12/09/22 08:14:33.249
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:14:33.279
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:14:33.284
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:100
STEP: Counting existing ResourceQuota 12/09/22 08:14:33.289
STEP: Creating a ResourceQuota 12/09/22 08:14:38.292
STEP: Ensuring resource quota status is calculated 12/09/22 08:14:38.299
STEP: Creating a Service 12/09/22 08:14:40.303
STEP: Creating a NodePort Service 12/09/22 08:14:40.321
STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota 12/09/22 08:14:40.347
STEP: Ensuring resource quota status captures service creation 12/09/22 08:14:40.365
STEP: Deleting Services 12/09/22 08:14:42.374
STEP: Ensuring resource quota status released usage 12/09/22 08:14:42.47
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Dec  9 08:14:44.475: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-244" for this suite. 12/09/22 08:14:44.478
------------------------------
â€¢ [SLOW TEST] [11.237 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:100

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 08:14:33.248
    Dec  9 08:14:33.248: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename resourcequota 12/09/22 08:14:33.249
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:14:33.279
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:14:33.284
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a service. [Conformance]
      test/e2e/apimachinery/resource_quota.go:100
    STEP: Counting existing ResourceQuota 12/09/22 08:14:33.289
    STEP: Creating a ResourceQuota 12/09/22 08:14:38.292
    STEP: Ensuring resource quota status is calculated 12/09/22 08:14:38.299
    STEP: Creating a Service 12/09/22 08:14:40.303
    STEP: Creating a NodePort Service 12/09/22 08:14:40.321
    STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota 12/09/22 08:14:40.347
    STEP: Ensuring resource quota status captures service creation 12/09/22 08:14:40.365
    STEP: Deleting Services 12/09/22 08:14:42.374
    STEP: Ensuring resource quota status released usage 12/09/22 08:14:42.47
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Dec  9 08:14:44.475: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-244" for this suite. 12/09/22 08:14:44.478
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:264
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 08:14:44.485
Dec  9 08:14:44.485: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename webhook 12/09/22 08:14:44.486
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:14:44.507
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:14:44.511
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 12/09/22 08:14:44.526
STEP: Create role binding to let webhook read extension-apiserver-authentication 12/09/22 08:14:44.951
STEP: Deploying the webhook pod 12/09/22 08:14:44.958
STEP: Wait for the deployment to be ready 12/09/22 08:14:44.971
Dec  9 08:14:44.978: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 12/09/22 08:14:46.987
STEP: Verifying the service has paired with the endpoint 12/09/22 08:14:46.997
Dec  9 08:14:47.998: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:264
STEP: Registering the mutating pod webhook via the AdmissionRegistration API 12/09/22 08:14:48.003
STEP: create a pod that should be updated by the webhook 12/09/22 08:14:48.032
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Dec  9 08:14:48.068: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-6007" for this suite. 12/09/22 08:14:48.137
STEP: Destroying namespace "webhook-6007-markers" for this suite. 12/09/22 08:14:48.145
------------------------------
â€¢ [3.673 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:264

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 08:14:44.485
    Dec  9 08:14:44.485: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename webhook 12/09/22 08:14:44.486
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:14:44.507
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:14:44.511
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 12/09/22 08:14:44.526
    STEP: Create role binding to let webhook read extension-apiserver-authentication 12/09/22 08:14:44.951
    STEP: Deploying the webhook pod 12/09/22 08:14:44.958
    STEP: Wait for the deployment to be ready 12/09/22 08:14:44.971
    Dec  9 08:14:44.978: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 12/09/22 08:14:46.987
    STEP: Verifying the service has paired with the endpoint 12/09/22 08:14:46.997
    Dec  9 08:14:47.998: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate pod and apply defaults after mutation [Conformance]
      test/e2e/apimachinery/webhook.go:264
    STEP: Registering the mutating pod webhook via the AdmissionRegistration API 12/09/22 08:14:48.003
    STEP: create a pod that should be updated by the webhook 12/09/22 08:14:48.032
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Dec  9 08:14:48.068: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-6007" for this suite. 12/09/22 08:14:48.137
    STEP: Destroying namespace "webhook-6007-markers" for this suite. 12/09/22 08:14:48.145
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  test/e2e/kubectl/kubectl.go:1276
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 08:14:48.166
Dec  9 08:14:48.166: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename kubectl 12/09/22 08:14:48.168
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:14:48.184
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:14:48.188
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  test/e2e/kubectl/kubectl.go:1276
Dec  9 08:14:48.195: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=kubectl-1377 create -f -'
Dec  9 08:14:48.612: INFO: stderr: ""
Dec  9 08:14:48.612: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
Dec  9 08:14:48.612: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=kubectl-1377 create -f -'
Dec  9 08:14:48.921: INFO: stderr: ""
Dec  9 08:14:48.921: INFO: stdout: "service/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 12/09/22 08:14:48.921
Dec  9 08:14:49.926: INFO: Selector matched 1 pods for map[app:agnhost]
Dec  9 08:14:49.926: INFO: Found 0 / 1
Dec  9 08:14:50.925: INFO: Selector matched 1 pods for map[app:agnhost]
Dec  9 08:14:50.926: INFO: Found 1 / 1
Dec  9 08:14:50.926: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Dec  9 08:14:50.928: INFO: Selector matched 1 pods for map[app:agnhost]
Dec  9 08:14:50.928: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Dec  9 08:14:50.928: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=kubectl-1377 describe pod agnhost-primary-hk7kd'
Dec  9 08:14:51.055: INFO: stderr: ""
Dec  9 08:14:51.055: INFO: stdout: "Name:             agnhost-primary-hk7kd\nNamespace:        kubectl-1377\nPriority:         0\nService Account:  default\nNode:             ip-10-0-10-179/10.0.10.179\nStart Time:       Fri, 09 Dec 2022 08:14:48 +0000\nLabels:           app=agnhost\n                  role=primary\nAnnotations:      cni.projectcalico.org/containerID: 0705dc7c1e1bddf23d8785557a7a3b5f1aa4500f03f37dd155d252b48428ca06\n                  cni.projectcalico.org/podIP: 10.2.136.114/32\n                  cni.projectcalico.org/podIPs: 10.2.136.114/32\nStatus:           Running\nIP:               10.2.136.114\nIPs:\n  IP:           10.2.136.114\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   containerd://d38716abb99843256228847b3d51f175f8a2b864e5f96ec810f952807dcd4a2f\n    Image:          registry.k8s.io/e2e-test-images/agnhost:2.43\n    Image ID:       registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Fri, 09 Dec 2022 08:14:49 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-drh24 (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-drh24:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  3s    default-scheduler  Successfully assigned kubectl-1377/agnhost-primary-hk7kd to ip-10-0-10-179\n  Normal  Pulled     2s    kubelet            Container image \"registry.k8s.io/e2e-test-images/agnhost:2.43\" already present on machine\n  Normal  Created    2s    kubelet            Created container agnhost-primary\n  Normal  Started    2s    kubelet            Started container agnhost-primary\n"
Dec  9 08:14:51.055: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=kubectl-1377 describe rc agnhost-primary'
Dec  9 08:14:51.150: INFO: stderr: ""
Dec  9 08:14:51.150: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-1377\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        registry.k8s.io/e2e-test-images/agnhost:2.43\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  3s    replication-controller  Created pod: agnhost-primary-hk7kd\n"
Dec  9 08:14:51.150: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=kubectl-1377 describe service agnhost-primary'
Dec  9 08:14:51.250: INFO: stderr: ""
Dec  9 08:14:51.250: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-1377\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                10.3.137.237\nIPs:               10.3.137.237\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         10.2.136.114:6379\nSession Affinity:  None\nEvents:            <none>\n"
Dec  9 08:14:51.254: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=kubectl-1377 describe node ip-10-0-10-179'
Dec  9 08:14:51.370: INFO: stderr: ""
Dec  9 08:14:51.370: INFO: stdout: "Name:               ip-10-0-10-179\nRoles:              <none>\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=ip-10-0-10-179\n                    kubernetes.io/os=linux\n                    node.kubernetes.io/node=\nAnnotations:        node.alpha.kubernetes.io/ttl: 0\n                    projectcalico.org/IPv4Address: 10.0.10.179/20\n                    projectcalico.org/IPv4IPIPTunnelAddr: 10.2.136.0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Fri, 09 Dec 2022 06:55:54 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  ip-10-0-10-179\n  AcquireTime:     <unset>\n  RenewTime:       Fri, 09 Dec 2022 08:14:42 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Fri, 09 Dec 2022 06:56:29 +0000   Fri, 09 Dec 2022 06:56:29 +0000   CalicoIsUp                   Calico is running on this node\n  MemoryPressure       False   Fri, 09 Dec 2022 08:10:41 +0000   Fri, 09 Dec 2022 06:55:54 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Fri, 09 Dec 2022 08:10:41 +0000   Fri, 09 Dec 2022 06:55:54 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Fri, 09 Dec 2022 08:10:41 +0000   Fri, 09 Dec 2022 06:55:54 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Fri, 09 Dec 2022 08:10:41 +0000   Fri, 09 Dec 2022 06:56:25 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:  10.0.10.179\n  Hostname:    ip-10-0-10-179\nCapacity:\n  cpu:                    2\n  ephemeral-storage:      30866412Ki\n  hugepages-1Gi:          0\n  hugepages-2Mi:          0\n  memory:                 1977944Ki\n  pods:                   110\n  scheduling.k8s.io/foo:  5\nAllocatable:\n  cpu:                    2\n  ephemeral-storage:      28446485253\n  hugepages-1Gi:          0\n  hugepages-2Mi:          0\n  memory:                 1875544Ki\n  pods:                   110\n  scheduling.k8s.io/foo:  5\nSystem Info:\n  Machine ID:                 ec2078b440da87bfde5bbc6382b65f6f\n  System UUID:                ec2078b4-40da-87bf-de5b-bc6382b65f6f\n  Boot ID:                    6023850d-da53-409b-b5bb-b27af9c6f7ae\n  Kernel Version:             6.0.9-300.fc37.x86_64\n  OS Image:                   Fedora CoreOS 37.20221127.2.0\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  containerd://1.6.10\n  Kubelet Version:            v1.26.0\n  Kube-Proxy Version:         v1.26.0\nPodCIDR:                      10.2.0.0/24\nPodCIDRs:                     10.2.0.0/24\nProviderID:                   aws:///us-east-2a/i-01f1899cba477e4c2\nNon-terminated Pods:          (5 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  kube-system                 calico-node-9zhz5                                          100m (5%)     0 (0%)      0 (0%)           0 (0%)         78m\n  kube-system                 kube-proxy-dv66w                                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         78m\n  kubectl-1377                agnhost-primary-hk7kd                                      0 (0%)        0 (0%)      0 (0%)           0 (0%)         3s\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-8dafb6c50b3e4797-scljx    0 (0%)        0 (0%)      0 (0%)           0 (0%)         76m\n  webhook-6007                webhook-to-be-mutated                                      0 (0%)        0 (0%)      0 (0%)           0 (0%)         3s\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource               Requests   Limits\n  --------               --------   ------\n  cpu                    100m (5%)  0 (0%)\n  memory                 0 (0%)     0 (0%)\n  ephemeral-storage      0 (0%)     0 (0%)\n  hugepages-1Gi          0 (0%)     0 (0%)\n  hugepages-2Mi          0 (0%)     0 (0%)\n  scheduling.k8s.io/foo  0          0\nEvents:                  <none>\n"
Dec  9 08:14:51.371: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=kubectl-1377 describe namespace kubectl-1377'
Dec  9 08:14:51.463: INFO: stderr: ""
Dec  9 08:14:51.463: INFO: stdout: "Name:         kubectl-1377\nLabels:       e2e-framework=kubectl\n              e2e-run=2287ad07-0247-4b9b-827b-66dd3791c4ed\n              kubernetes.io/metadata.name=kubectl-1377\n              pod-security.kubernetes.io/enforce=baseline\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Dec  9 08:14:51.463: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-1377" for this suite. 12/09/22 08:14:51.467
------------------------------
â€¢ [3.306 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl describe
  test/e2e/kubectl/kubectl.go:1270
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    test/e2e/kubectl/kubectl.go:1276

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 08:14:48.166
    Dec  9 08:14:48.166: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename kubectl 12/09/22 08:14:48.168
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:14:48.184
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:14:48.188
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
      test/e2e/kubectl/kubectl.go:1276
    Dec  9 08:14:48.195: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=kubectl-1377 create -f -'
    Dec  9 08:14:48.612: INFO: stderr: ""
    Dec  9 08:14:48.612: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    Dec  9 08:14:48.612: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=kubectl-1377 create -f -'
    Dec  9 08:14:48.921: INFO: stderr: ""
    Dec  9 08:14:48.921: INFO: stdout: "service/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 12/09/22 08:14:48.921
    Dec  9 08:14:49.926: INFO: Selector matched 1 pods for map[app:agnhost]
    Dec  9 08:14:49.926: INFO: Found 0 / 1
    Dec  9 08:14:50.925: INFO: Selector matched 1 pods for map[app:agnhost]
    Dec  9 08:14:50.926: INFO: Found 1 / 1
    Dec  9 08:14:50.926: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    Dec  9 08:14:50.928: INFO: Selector matched 1 pods for map[app:agnhost]
    Dec  9 08:14:50.928: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Dec  9 08:14:50.928: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=kubectl-1377 describe pod agnhost-primary-hk7kd'
    Dec  9 08:14:51.055: INFO: stderr: ""
    Dec  9 08:14:51.055: INFO: stdout: "Name:             agnhost-primary-hk7kd\nNamespace:        kubectl-1377\nPriority:         0\nService Account:  default\nNode:             ip-10-0-10-179/10.0.10.179\nStart Time:       Fri, 09 Dec 2022 08:14:48 +0000\nLabels:           app=agnhost\n                  role=primary\nAnnotations:      cni.projectcalico.org/containerID: 0705dc7c1e1bddf23d8785557a7a3b5f1aa4500f03f37dd155d252b48428ca06\n                  cni.projectcalico.org/podIP: 10.2.136.114/32\n                  cni.projectcalico.org/podIPs: 10.2.136.114/32\nStatus:           Running\nIP:               10.2.136.114\nIPs:\n  IP:           10.2.136.114\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   containerd://d38716abb99843256228847b3d51f175f8a2b864e5f96ec810f952807dcd4a2f\n    Image:          registry.k8s.io/e2e-test-images/agnhost:2.43\n    Image ID:       registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Fri, 09 Dec 2022 08:14:49 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-drh24 (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-drh24:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  3s    default-scheduler  Successfully assigned kubectl-1377/agnhost-primary-hk7kd to ip-10-0-10-179\n  Normal  Pulled     2s    kubelet            Container image \"registry.k8s.io/e2e-test-images/agnhost:2.43\" already present on machine\n  Normal  Created    2s    kubelet            Created container agnhost-primary\n  Normal  Started    2s    kubelet            Started container agnhost-primary\n"
    Dec  9 08:14:51.055: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=kubectl-1377 describe rc agnhost-primary'
    Dec  9 08:14:51.150: INFO: stderr: ""
    Dec  9 08:14:51.150: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-1377\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        registry.k8s.io/e2e-test-images/agnhost:2.43\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  3s    replication-controller  Created pod: agnhost-primary-hk7kd\n"
    Dec  9 08:14:51.150: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=kubectl-1377 describe service agnhost-primary'
    Dec  9 08:14:51.250: INFO: stderr: ""
    Dec  9 08:14:51.250: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-1377\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                10.3.137.237\nIPs:               10.3.137.237\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         10.2.136.114:6379\nSession Affinity:  None\nEvents:            <none>\n"
    Dec  9 08:14:51.254: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=kubectl-1377 describe node ip-10-0-10-179'
    Dec  9 08:14:51.370: INFO: stderr: ""
    Dec  9 08:14:51.370: INFO: stdout: "Name:               ip-10-0-10-179\nRoles:              <none>\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=ip-10-0-10-179\n                    kubernetes.io/os=linux\n                    node.kubernetes.io/node=\nAnnotations:        node.alpha.kubernetes.io/ttl: 0\n                    projectcalico.org/IPv4Address: 10.0.10.179/20\n                    projectcalico.org/IPv4IPIPTunnelAddr: 10.2.136.0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Fri, 09 Dec 2022 06:55:54 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  ip-10-0-10-179\n  AcquireTime:     <unset>\n  RenewTime:       Fri, 09 Dec 2022 08:14:42 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Fri, 09 Dec 2022 06:56:29 +0000   Fri, 09 Dec 2022 06:56:29 +0000   CalicoIsUp                   Calico is running on this node\n  MemoryPressure       False   Fri, 09 Dec 2022 08:10:41 +0000   Fri, 09 Dec 2022 06:55:54 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Fri, 09 Dec 2022 08:10:41 +0000   Fri, 09 Dec 2022 06:55:54 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Fri, 09 Dec 2022 08:10:41 +0000   Fri, 09 Dec 2022 06:55:54 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Fri, 09 Dec 2022 08:10:41 +0000   Fri, 09 Dec 2022 06:56:25 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:  10.0.10.179\n  Hostname:    ip-10-0-10-179\nCapacity:\n  cpu:                    2\n  ephemeral-storage:      30866412Ki\n  hugepages-1Gi:          0\n  hugepages-2Mi:          0\n  memory:                 1977944Ki\n  pods:                   110\n  scheduling.k8s.io/foo:  5\nAllocatable:\n  cpu:                    2\n  ephemeral-storage:      28446485253\n  hugepages-1Gi:          0\n  hugepages-2Mi:          0\n  memory:                 1875544Ki\n  pods:                   110\n  scheduling.k8s.io/foo:  5\nSystem Info:\n  Machine ID:                 ec2078b440da87bfde5bbc6382b65f6f\n  System UUID:                ec2078b4-40da-87bf-de5b-bc6382b65f6f\n  Boot ID:                    6023850d-da53-409b-b5bb-b27af9c6f7ae\n  Kernel Version:             6.0.9-300.fc37.x86_64\n  OS Image:                   Fedora CoreOS 37.20221127.2.0\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  containerd://1.6.10\n  Kubelet Version:            v1.26.0\n  Kube-Proxy Version:         v1.26.0\nPodCIDR:                      10.2.0.0/24\nPodCIDRs:                     10.2.0.0/24\nProviderID:                   aws:///us-east-2a/i-01f1899cba477e4c2\nNon-terminated Pods:          (5 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  kube-system                 calico-node-9zhz5                                          100m (5%)     0 (0%)      0 (0%)           0 (0%)         78m\n  kube-system                 kube-proxy-dv66w                                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         78m\n  kubectl-1377                agnhost-primary-hk7kd                                      0 (0%)        0 (0%)      0 (0%)           0 (0%)         3s\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-8dafb6c50b3e4797-scljx    0 (0%)        0 (0%)      0 (0%)           0 (0%)         76m\n  webhook-6007                webhook-to-be-mutated                                      0 (0%)        0 (0%)      0 (0%)           0 (0%)         3s\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource               Requests   Limits\n  --------               --------   ------\n  cpu                    100m (5%)  0 (0%)\n  memory                 0 (0%)     0 (0%)\n  ephemeral-storage      0 (0%)     0 (0%)\n  hugepages-1Gi          0 (0%)     0 (0%)\n  hugepages-2Mi          0 (0%)     0 (0%)\n  scheduling.k8s.io/foo  0          0\nEvents:                  <none>\n"
    Dec  9 08:14:51.371: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=kubectl-1377 describe namespace kubectl-1377'
    Dec  9 08:14:51.463: INFO: stderr: ""
    Dec  9 08:14:51.463: INFO: stdout: "Name:         kubectl-1377\nLabels:       e2e-framework=kubectl\n              e2e-run=2287ad07-0247-4b9b-827b-66dd3791c4ed\n              kubernetes.io/metadata.name=kubectl-1377\n              pod-security.kubernetes.io/enforce=baseline\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Dec  9 08:14:51.463: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-1377" for this suite. 12/09/22 08:14:51.467
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial]
  validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:222
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 08:14:51.472
Dec  9 08:14:51.473: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename sched-preemption 12/09/22 08:14:51.474
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:14:51.49
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:14:51.496
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:96
Dec  9 08:14:51.511: INFO: Waiting up to 1m0s for all nodes to be ready
Dec  9 08:15:51.534: INFO: Waiting for terminating namespaces to be deleted...
[It] validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:222
STEP: Create pods that use 4/5 of node resources. 12/09/22 08:15:51.536
Dec  9 08:15:51.553: INFO: Created pod: pod0-0-sched-preemption-low-priority
Dec  9 08:15:51.560: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Dec  9 08:15:51.593: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Dec  9 08:15:51.607: INFO: Created pod: pod1-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled. 12/09/22 08:15:51.607
Dec  9 08:15:51.607: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-6653" to be "running"
Dec  9 08:15:51.621: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 13.496766ms
Dec  9 08:15:53.627: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.019633168s
Dec  9 08:15:53.627: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
Dec  9 08:15:53.627: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-6653" to be "running"
Dec  9 08:15:53.630: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.471173ms
Dec  9 08:15:53.630: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
Dec  9 08:15:53.630: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-6653" to be "running"
Dec  9 08:15:53.632: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.444233ms
Dec  9 08:15:53.632: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
Dec  9 08:15:53.633: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-6653" to be "running"
Dec  9 08:15:53.635: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.29431ms
Dec  9 08:15:53.635: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
STEP: Run a critical pod that use same resources as that of a lower priority pod 12/09/22 08:15:53.635
Dec  9 08:15:53.643: INFO: Waiting up to 2m0s for pod "critical-pod" in namespace "kube-system" to be "running"
Dec  9 08:15:53.651: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 7.47323ms
Dec  9 08:15:55.655: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011755285s
Dec  9 08:15:57.656: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012319482s
Dec  9 08:15:59.656: INFO: Pod "critical-pod": Phase="Running", Reason="", readiness=true. Elapsed: 6.012568594s
Dec  9 08:15:59.656: INFO: Pod "critical-pod" satisfied condition "running"
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/node/init/init.go:32
Dec  9 08:15:59.701: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:84
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-6653" for this suite. 12/09/22 08:15:59.812
------------------------------
â€¢ [SLOW TEST] [68.350 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:222

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 08:14:51.472
    Dec  9 08:14:51.473: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename sched-preemption 12/09/22 08:14:51.474
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:14:51.49
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:14:51.496
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:96
    Dec  9 08:14:51.511: INFO: Waiting up to 1m0s for all nodes to be ready
    Dec  9 08:15:51.534: INFO: Waiting for terminating namespaces to be deleted...
    [It] validates lower priority pod preemption by critical pod [Conformance]
      test/e2e/scheduling/preemption.go:222
    STEP: Create pods that use 4/5 of node resources. 12/09/22 08:15:51.536
    Dec  9 08:15:51.553: INFO: Created pod: pod0-0-sched-preemption-low-priority
    Dec  9 08:15:51.560: INFO: Created pod: pod0-1-sched-preemption-medium-priority
    Dec  9 08:15:51.593: INFO: Created pod: pod1-0-sched-preemption-medium-priority
    Dec  9 08:15:51.607: INFO: Created pod: pod1-1-sched-preemption-medium-priority
    STEP: Wait for pods to be scheduled. 12/09/22 08:15:51.607
    Dec  9 08:15:51.607: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-6653" to be "running"
    Dec  9 08:15:51.621: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 13.496766ms
    Dec  9 08:15:53.627: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.019633168s
    Dec  9 08:15:53.627: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
    Dec  9 08:15:53.627: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-6653" to be "running"
    Dec  9 08:15:53.630: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.471173ms
    Dec  9 08:15:53.630: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
    Dec  9 08:15:53.630: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-6653" to be "running"
    Dec  9 08:15:53.632: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.444233ms
    Dec  9 08:15:53.632: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
    Dec  9 08:15:53.633: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-6653" to be "running"
    Dec  9 08:15:53.635: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.29431ms
    Dec  9 08:15:53.635: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
    STEP: Run a critical pod that use same resources as that of a lower priority pod 12/09/22 08:15:53.635
    Dec  9 08:15:53.643: INFO: Waiting up to 2m0s for pod "critical-pod" in namespace "kube-system" to be "running"
    Dec  9 08:15:53.651: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 7.47323ms
    Dec  9 08:15:55.655: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011755285s
    Dec  9 08:15:57.656: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012319482s
    Dec  9 08:15:59.656: INFO: Pod "critical-pod": Phase="Running", Reason="", readiness=true. Elapsed: 6.012568594s
    Dec  9 08:15:59.656: INFO: Pod "critical-pod" satisfied condition "running"
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/node/init/init.go:32
    Dec  9 08:15:59.701: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:84
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-6653" for this suite. 12/09/22 08:15:59.812
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 08:15:59.826
Dec  9 08:15:59.826: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename gc 12/09/22 08:15:59.827
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:15:59.873
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:15:59.878
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312
STEP: create the rc 12/09/22 08:15:59.882
STEP: delete the rc 12/09/22 08:16:04.896
STEP: wait for all pods to be garbage collected 12/09/22 08:16:04.905
STEP: Gathering metrics 12/09/22 08:16:09.911
Dec  9 08:16:09.933: INFO: Waiting up to 5m0s for pod "kube-controller-manager-ip-10-0-7-84" in namespace "kube-system" to be "running and ready"
Dec  9 08:16:09.936: INFO: Pod "kube-controller-manager-ip-10-0-7-84": Phase="Running", Reason="", readiness=true. Elapsed: 3.454202ms
Dec  9 08:16:09.936: INFO: The phase of Pod kube-controller-manager-ip-10-0-7-84 is Running (Ready = true)
Dec  9 08:16:09.936: INFO: Pod "kube-controller-manager-ip-10-0-7-84" satisfied condition "running and ready"
Dec  9 08:16:10.016: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Dec  9 08:16:10.017: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-6503" for this suite. 12/09/22 08:16:10.022
------------------------------
â€¢ [SLOW TEST] [10.202 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 08:15:59.826
    Dec  9 08:15:59.826: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename gc 12/09/22 08:15:59.827
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:15:59.873
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:15:59.878
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete pods created by rc when not orphaning [Conformance]
      test/e2e/apimachinery/garbage_collector.go:312
    STEP: create the rc 12/09/22 08:15:59.882
    STEP: delete the rc 12/09/22 08:16:04.896
    STEP: wait for all pods to be garbage collected 12/09/22 08:16:04.905
    STEP: Gathering metrics 12/09/22 08:16:09.911
    Dec  9 08:16:09.933: INFO: Waiting up to 5m0s for pod "kube-controller-manager-ip-10-0-7-84" in namespace "kube-system" to be "running and ready"
    Dec  9 08:16:09.936: INFO: Pod "kube-controller-manager-ip-10-0-7-84": Phase="Running", Reason="", readiness=true. Elapsed: 3.454202ms
    Dec  9 08:16:09.936: INFO: The phase of Pod kube-controller-manager-ip-10-0-7-84 is Running (Ready = true)
    Dec  9 08:16:09.936: INFO: Pod "kube-controller-manager-ip-10-0-7-84" satisfied condition "running and ready"
    Dec  9 08:16:10.016: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Dec  9 08:16:10.017: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-6503" for this suite. 12/09/22 08:16:10.022
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:261
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 08:16:10.032
Dec  9 08:16:10.032: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename downward-api 12/09/22 08:16:10.034
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:16:10.051
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:16:10.056
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:261
STEP: Creating a pod to test downward API volume plugin 12/09/22 08:16:10.06
Dec  9 08:16:10.070: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e759f659-06e4-4a4a-b2fa-7a3fa693e5f2" in namespace "downward-api-1548" to be "Succeeded or Failed"
Dec  9 08:16:10.073: INFO: Pod "downwardapi-volume-e759f659-06e4-4a4a-b2fa-7a3fa693e5f2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.854892ms
Dec  9 08:16:12.077: INFO: Pod "downwardapi-volume-e759f659-06e4-4a4a-b2fa-7a3fa693e5f2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007095716s
Dec  9 08:16:14.077: INFO: Pod "downwardapi-volume-e759f659-06e4-4a4a-b2fa-7a3fa693e5f2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007000903s
STEP: Saw pod success 12/09/22 08:16:14.077
Dec  9 08:16:14.078: INFO: Pod "downwardapi-volume-e759f659-06e4-4a4a-b2fa-7a3fa693e5f2" satisfied condition "Succeeded or Failed"
Dec  9 08:16:14.081: INFO: Trying to get logs from node ip-10-0-10-179 pod downwardapi-volume-e759f659-06e4-4a4a-b2fa-7a3fa693e5f2 container client-container: <nil>
STEP: delete the pod 12/09/22 08:16:14.094
Dec  9 08:16:14.108: INFO: Waiting for pod downwardapi-volume-e759f659-06e4-4a4a-b2fa-7a3fa693e5f2 to disappear
Dec  9 08:16:14.111: INFO: Pod downwardapi-volume-e759f659-06e4-4a4a-b2fa-7a3fa693e5f2 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Dec  9 08:16:14.111: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-1548" for this suite. 12/09/22 08:16:14.115
------------------------------
â€¢ [4.090 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:261

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 08:16:10.032
    Dec  9 08:16:10.032: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename downward-api 12/09/22 08:16:10.034
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:16:10.051
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:16:10.056
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:261
    STEP: Creating a pod to test downward API volume plugin 12/09/22 08:16:10.06
    Dec  9 08:16:10.070: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e759f659-06e4-4a4a-b2fa-7a3fa693e5f2" in namespace "downward-api-1548" to be "Succeeded or Failed"
    Dec  9 08:16:10.073: INFO: Pod "downwardapi-volume-e759f659-06e4-4a4a-b2fa-7a3fa693e5f2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.854892ms
    Dec  9 08:16:12.077: INFO: Pod "downwardapi-volume-e759f659-06e4-4a4a-b2fa-7a3fa693e5f2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007095716s
    Dec  9 08:16:14.077: INFO: Pod "downwardapi-volume-e759f659-06e4-4a4a-b2fa-7a3fa693e5f2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007000903s
    STEP: Saw pod success 12/09/22 08:16:14.077
    Dec  9 08:16:14.078: INFO: Pod "downwardapi-volume-e759f659-06e4-4a4a-b2fa-7a3fa693e5f2" satisfied condition "Succeeded or Failed"
    Dec  9 08:16:14.081: INFO: Trying to get logs from node ip-10-0-10-179 pod downwardapi-volume-e759f659-06e4-4a4a-b2fa-7a3fa693e5f2 container client-container: <nil>
    STEP: delete the pod 12/09/22 08:16:14.094
    Dec  9 08:16:14.108: INFO: Waiting for pod downwardapi-volume-e759f659-06e4-4a4a-b2fa-7a3fa693e5f2 to disappear
    Dec  9 08:16:14.111: INFO: Pod downwardapi-volume-e759f659-06e4-4a4a-b2fa-7a3fa693e5f2 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Dec  9 08:16:14.111: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-1548" for this suite. 12/09/22 08:16:14.115
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 08:16:14.127
Dec  9 08:16:14.128: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename deployment 12/09/22 08:16:14.128
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:16:14.15
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:16:14.156
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105
Dec  9 08:16:14.160: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Dec  9 08:16:14.169: INFO: Pod name sample-pod: Found 0 pods out of 1
Dec  9 08:16:19.175: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 12/09/22 08:16:19.175
Dec  9 08:16:19.175: INFO: Creating deployment "test-rolling-update-deployment"
Dec  9 08:16:19.184: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Dec  9 08:16:19.206: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Dec  9 08:16:21.214: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Dec  9 08:16:21.219: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Dec  9 08:16:21.238: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-9780  5789046c-478f-49a8-80cc-a25856d0e167 28862 1 2022-12-09 08:16:19 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] [] [{e2e.test Update apps/v1 2022-12-09 08:16:19 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-09 08:16:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0037ce4a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2022-12-09 08:16:19 +0000 UTC,LastTransitionTime:2022-12-09 08:16:19 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-7549d9f46d" has successfully progressed.,LastUpdateTime:2022-12-09 08:16:20 +0000 UTC,LastTransitionTime:2022-12-09 08:16:19 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Dec  9 08:16:21.246: INFO: New ReplicaSet "test-rolling-update-deployment-7549d9f46d" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-7549d9f46d  deployment-9780  8b7bdaab-6665-4d1c-ad2c-82b1b8a85707 28852 1 2022-12-09 08:16:19 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 5789046c-478f-49a8-80cc-a25856d0e167 0xc0037cf107 0xc0037cf108}] [] [{kube-controller-manager Update apps/v1 2022-12-09 08:16:19 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5789046c-478f-49a8-80cc-a25856d0e167\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-09 08:16:20 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 7549d9f46d,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0037cf1b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Dec  9 08:16:21.246: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Dec  9 08:16:21.247: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-9780  59e7b834-4122-4b2c-90c8-d5740ef9d71c 28861 2 2022-12-09 08:16:14 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 5789046c-478f-49a8-80cc-a25856d0e167 0xc0037cefd7 0xc0037cefd8}] [] [{e2e.test Update apps/v1 2022-12-09 08:16:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-09 08:16:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5789046c-478f-49a8-80cc-a25856d0e167\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2022-12-09 08:16:20 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0037cf098 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Dec  9 08:16:21.250: INFO: Pod "test-rolling-update-deployment-7549d9f46d-2tjsr" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-7549d9f46d-2tjsr test-rolling-update-deployment-7549d9f46d- deployment-9780  1757be7f-36ac-4ca5-94f7-10347e2a6786 28851 0 2022-12-09 08:16:19 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[cni.projectcalico.org/containerID:5a0753cfa53c2c400cd3fbd90b9f43e763cb430552f08239b234f7efa4d1f267 cni.projectcalico.org/podIP:10.2.136.120/32 cni.projectcalico.org/podIPs:10.2.136.120/32] [{apps/v1 ReplicaSet test-rolling-update-deployment-7549d9f46d 8b7bdaab-6665-4d1c-ad2c-82b1b8a85707 0xc00331c0a7 0xc00331c0a8}] [] [{calico Update v1 2022-12-09 08:16:19 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2022-12-09 08:16:19 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8b7bdaab-6665-4d1c-ad2c-82b1b8a85707\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-09 08:16:20 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.2.136.120\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-b5p6m,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-b5p6m,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-10-179,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 08:16:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 08:16:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 08:16:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 08:16:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.10.179,PodIP:10.2.136.120,StartTime:2022-12-09 08:16:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-12-09 08:16:20 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://0ee74d37f325cb07c25e05346419e4e9e5574e73bb7601fd3d39e414cee0f1ea,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.2.136.120,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Dec  9 08:16:21.251: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-9780" for this suite. 12/09/22 08:16:21.268
------------------------------
â€¢ [SLOW TEST] [7.169 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 08:16:14.127
    Dec  9 08:16:14.128: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename deployment 12/09/22 08:16:14.128
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:16:14.15
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:16:14.156
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
      test/e2e/apps/deployment.go:105
    Dec  9 08:16:14.160: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
    Dec  9 08:16:14.169: INFO: Pod name sample-pod: Found 0 pods out of 1
    Dec  9 08:16:19.175: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 12/09/22 08:16:19.175
    Dec  9 08:16:19.175: INFO: Creating deployment "test-rolling-update-deployment"
    Dec  9 08:16:19.184: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
    Dec  9 08:16:19.206: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
    Dec  9 08:16:21.214: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
    Dec  9 08:16:21.219: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Dec  9 08:16:21.238: INFO: Deployment "test-rolling-update-deployment":
    &Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-9780  5789046c-478f-49a8-80cc-a25856d0e167 28862 1 2022-12-09 08:16:19 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] [] [{e2e.test Update apps/v1 2022-12-09 08:16:19 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-09 08:16:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0037ce4a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2022-12-09 08:16:19 +0000 UTC,LastTransitionTime:2022-12-09 08:16:19 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-7549d9f46d" has successfully progressed.,LastUpdateTime:2022-12-09 08:16:20 +0000 UTC,LastTransitionTime:2022-12-09 08:16:19 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Dec  9 08:16:21.246: INFO: New ReplicaSet "test-rolling-update-deployment-7549d9f46d" of Deployment "test-rolling-update-deployment":
    &ReplicaSet{ObjectMeta:{test-rolling-update-deployment-7549d9f46d  deployment-9780  8b7bdaab-6665-4d1c-ad2c-82b1b8a85707 28852 1 2022-12-09 08:16:19 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 5789046c-478f-49a8-80cc-a25856d0e167 0xc0037cf107 0xc0037cf108}] [] [{kube-controller-manager Update apps/v1 2022-12-09 08:16:19 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5789046c-478f-49a8-80cc-a25856d0e167\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-09 08:16:20 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 7549d9f46d,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0037cf1b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Dec  9 08:16:21.246: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
    Dec  9 08:16:21.247: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-9780  59e7b834-4122-4b2c-90c8-d5740ef9d71c 28861 2 2022-12-09 08:16:14 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 5789046c-478f-49a8-80cc-a25856d0e167 0xc0037cefd7 0xc0037cefd8}] [] [{e2e.test Update apps/v1 2022-12-09 08:16:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-09 08:16:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5789046c-478f-49a8-80cc-a25856d0e167\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2022-12-09 08:16:20 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0037cf098 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Dec  9 08:16:21.250: INFO: Pod "test-rolling-update-deployment-7549d9f46d-2tjsr" is available:
    &Pod{ObjectMeta:{test-rolling-update-deployment-7549d9f46d-2tjsr test-rolling-update-deployment-7549d9f46d- deployment-9780  1757be7f-36ac-4ca5-94f7-10347e2a6786 28851 0 2022-12-09 08:16:19 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[cni.projectcalico.org/containerID:5a0753cfa53c2c400cd3fbd90b9f43e763cb430552f08239b234f7efa4d1f267 cni.projectcalico.org/podIP:10.2.136.120/32 cni.projectcalico.org/podIPs:10.2.136.120/32] [{apps/v1 ReplicaSet test-rolling-update-deployment-7549d9f46d 8b7bdaab-6665-4d1c-ad2c-82b1b8a85707 0xc00331c0a7 0xc00331c0a8}] [] [{calico Update v1 2022-12-09 08:16:19 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2022-12-09 08:16:19 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8b7bdaab-6665-4d1c-ad2c-82b1b8a85707\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-09 08:16:20 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.2.136.120\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-b5p6m,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-b5p6m,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-10-179,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 08:16:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 08:16:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 08:16:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-09 08:16:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.10.179,PodIP:10.2.136.120,StartTime:2022-12-09 08:16:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-12-09 08:16:20 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://0ee74d37f325cb07c25e05346419e4e9e5574e73bb7601fd3d39e414cee0f1ea,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.2.136.120,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Dec  9 08:16:21.251: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-9780" for this suite. 12/09/22 08:16:21.268
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:268
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 08:16:21.302
Dec  9 08:16:21.302: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename namespaces 12/09/22 08:16:21.303
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:16:21.319
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:16:21.323
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:268
STEP: creating a Namespace 12/09/22 08:16:21.327
STEP: patching the Namespace 12/09/22 08:16:21.34
STEP: get the Namespace and ensuring it has the label 12/09/22 08:16:21.348
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Dec  9 08:16:21.352: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-3452" for this suite. 12/09/22 08:16:21.356
STEP: Destroying namespace "nspatchtest-00ea1f3c-4a26-4c69-a20e-76f3e783680c-5273" for this suite. 12/09/22 08:16:21.361
------------------------------
â€¢ [0.069 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:268

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 08:16:21.302
    Dec  9 08:16:21.302: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename namespaces 12/09/22 08:16:21.303
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:16:21.319
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:16:21.323
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should patch a Namespace [Conformance]
      test/e2e/apimachinery/namespace.go:268
    STEP: creating a Namespace 12/09/22 08:16:21.327
    STEP: patching the Namespace 12/09/22 08:16:21.34
    STEP: get the Namespace and ensuring it has the label 12/09/22 08:16:21.348
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Dec  9 08:16:21.352: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-3452" for this suite. 12/09/22 08:16:21.356
    STEP: Destroying namespace "nspatchtest-00ea1f3c-4a26-4c69-a20e-76f3e783680c-5273" for this suite. 12/09/22 08:16:21.361
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-node] PodTemplates
  should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53
[BeforeEach] [sig-node] PodTemplates
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 08:16:21.369
Dec  9 08:16:21.369: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename podtemplate 12/09/22 08:16:21.37
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:16:21.386
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:16:21.39
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:31
[It] should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53
[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/node/init/init.go:32
Dec  9 08:16:21.418: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] PodTemplates
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] PodTemplates
  tear down framework | framework.go:193
STEP: Destroying namespace "podtemplate-4420" for this suite. 12/09/22 08:16:21.421
------------------------------
â€¢ [0.059 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 08:16:21.369
    Dec  9 08:16:21.369: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename podtemplate 12/09/22 08:16:21.37
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:16:21.386
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:16:21.39
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:31
    [It] should run the lifecycle of PodTemplates [Conformance]
      test/e2e/common/node/podtemplates.go:53
    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/node/init/init.go:32
    Dec  9 08:16:21.418: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] PodTemplates
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] PodTemplates
      tear down framework | framework.go:193
    STEP: Destroying namespace "podtemplate-4420" for this suite. 12/09/22 08:16:21.421
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] Projected downwardAPI
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:53
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 08:16:21.428
Dec  9 08:16:21.429: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename projected 12/09/22 08:16:21.43
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:16:21.445
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:16:21.449
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:53
STEP: Creating a pod to test downward API volume plugin 12/09/22 08:16:21.453
Dec  9 08:16:21.460: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c852056a-eee4-49cb-aa57-bf6225d226a8" in namespace "projected-195" to be "Succeeded or Failed"
Dec  9 08:16:21.464: INFO: Pod "downwardapi-volume-c852056a-eee4-49cb-aa57-bf6225d226a8": Phase="Pending", Reason="", readiness=false. Elapsed: 3.800156ms
Dec  9 08:16:23.468: INFO: Pod "downwardapi-volume-c852056a-eee4-49cb-aa57-bf6225d226a8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008048204s
Dec  9 08:16:25.468: INFO: Pod "downwardapi-volume-c852056a-eee4-49cb-aa57-bf6225d226a8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007435361s
STEP: Saw pod success 12/09/22 08:16:25.468
Dec  9 08:16:25.468: INFO: Pod "downwardapi-volume-c852056a-eee4-49cb-aa57-bf6225d226a8" satisfied condition "Succeeded or Failed"
Dec  9 08:16:25.473: INFO: Trying to get logs from node ip-10-0-10-179 pod downwardapi-volume-c852056a-eee4-49cb-aa57-bf6225d226a8 container client-container: <nil>
STEP: delete the pod 12/09/22 08:16:25.477
Dec  9 08:16:25.487: INFO: Waiting for pod downwardapi-volume-c852056a-eee4-49cb-aa57-bf6225d226a8 to disappear
Dec  9 08:16:25.490: INFO: Pod downwardapi-volume-c852056a-eee4-49cb-aa57-bf6225d226a8 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Dec  9 08:16:25.490: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-195" for this suite. 12/09/22 08:16:25.502
------------------------------
â€¢ [4.087 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 08:16:21.428
    Dec  9 08:16:21.429: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename projected 12/09/22 08:16:21.43
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:16:21.445
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:16:21.449
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide podname only [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:53
    STEP: Creating a pod to test downward API volume plugin 12/09/22 08:16:21.453
    Dec  9 08:16:21.460: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c852056a-eee4-49cb-aa57-bf6225d226a8" in namespace "projected-195" to be "Succeeded or Failed"
    Dec  9 08:16:21.464: INFO: Pod "downwardapi-volume-c852056a-eee4-49cb-aa57-bf6225d226a8": Phase="Pending", Reason="", readiness=false. Elapsed: 3.800156ms
    Dec  9 08:16:23.468: INFO: Pod "downwardapi-volume-c852056a-eee4-49cb-aa57-bf6225d226a8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008048204s
    Dec  9 08:16:25.468: INFO: Pod "downwardapi-volume-c852056a-eee4-49cb-aa57-bf6225d226a8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007435361s
    STEP: Saw pod success 12/09/22 08:16:25.468
    Dec  9 08:16:25.468: INFO: Pod "downwardapi-volume-c852056a-eee4-49cb-aa57-bf6225d226a8" satisfied condition "Succeeded or Failed"
    Dec  9 08:16:25.473: INFO: Trying to get logs from node ip-10-0-10-179 pod downwardapi-volume-c852056a-eee4-49cb-aa57-bf6225d226a8 container client-container: <nil>
    STEP: delete the pod 12/09/22 08:16:25.477
    Dec  9 08:16:25.487: INFO: Waiting for pod downwardapi-volume-c852056a-eee4-49cb-aa57-bf6225d226a8 to disappear
    Dec  9 08:16:25.490: INFO: Pod downwardapi-volume-c852056a-eee4-49cb-aa57-bf6225d226a8 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Dec  9 08:16:25.490: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-195" for this suite. 12/09/22 08:16:25.502
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:199
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 08:16:25.517
Dec  9 08:16:25.517: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename container-probe 12/09/22 08:16:25.518
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:16:25.534
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:16:25.538
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:199
STEP: Creating pod liveness-26e51669-4c1f-44f1-b5a0-cf0c312af87b in namespace container-probe-8757 12/09/22 08:16:25.541
Dec  9 08:16:25.552: INFO: Waiting up to 5m0s for pod "liveness-26e51669-4c1f-44f1-b5a0-cf0c312af87b" in namespace "container-probe-8757" to be "not pending"
Dec  9 08:16:25.557: INFO: Pod "liveness-26e51669-4c1f-44f1-b5a0-cf0c312af87b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.461517ms
Dec  9 08:16:27.562: INFO: Pod "liveness-26e51669-4c1f-44f1-b5a0-cf0c312af87b": Phase="Running", Reason="", readiness=true. Elapsed: 2.009233774s
Dec  9 08:16:27.562: INFO: Pod "liveness-26e51669-4c1f-44f1-b5a0-cf0c312af87b" satisfied condition "not pending"
Dec  9 08:16:27.562: INFO: Started pod liveness-26e51669-4c1f-44f1-b5a0-cf0c312af87b in namespace container-probe-8757
STEP: checking the pod's current state and verifying that restartCount is present 12/09/22 08:16:27.562
Dec  9 08:16:27.565: INFO: Initial restart count of pod liveness-26e51669-4c1f-44f1-b5a0-cf0c312af87b is 0
Dec  9 08:16:47.607: INFO: Restart count of pod container-probe-8757/liveness-26e51669-4c1f-44f1-b5a0-cf0c312af87b is now 1 (20.041541168s elapsed)
Dec  9 08:17:07.651: INFO: Restart count of pod container-probe-8757/liveness-26e51669-4c1f-44f1-b5a0-cf0c312af87b is now 2 (40.086270267s elapsed)
Dec  9 08:17:27.695: INFO: Restart count of pod container-probe-8757/liveness-26e51669-4c1f-44f1-b5a0-cf0c312af87b is now 3 (1m0.129887405s elapsed)
Dec  9 08:17:47.736: INFO: Restart count of pod container-probe-8757/liveness-26e51669-4c1f-44f1-b5a0-cf0c312af87b is now 4 (1m20.171344007s elapsed)
Dec  9 08:18:47.871: INFO: Restart count of pod container-probe-8757/liveness-26e51669-4c1f-44f1-b5a0-cf0c312af87b is now 5 (2m20.306114236s elapsed)
STEP: deleting the pod 12/09/22 08:18:47.871
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Dec  9 08:18:47.884: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-8757" for this suite. 12/09/22 08:18:47.889
------------------------------
â€¢ [SLOW TEST] [142.378 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:199

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 08:16:25.517
    Dec  9 08:16:25.517: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename container-probe 12/09/22 08:16:25.518
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:16:25.534
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:16:25.538
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should have monotonically increasing restart count [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:199
    STEP: Creating pod liveness-26e51669-4c1f-44f1-b5a0-cf0c312af87b in namespace container-probe-8757 12/09/22 08:16:25.541
    Dec  9 08:16:25.552: INFO: Waiting up to 5m0s for pod "liveness-26e51669-4c1f-44f1-b5a0-cf0c312af87b" in namespace "container-probe-8757" to be "not pending"
    Dec  9 08:16:25.557: INFO: Pod "liveness-26e51669-4c1f-44f1-b5a0-cf0c312af87b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.461517ms
    Dec  9 08:16:27.562: INFO: Pod "liveness-26e51669-4c1f-44f1-b5a0-cf0c312af87b": Phase="Running", Reason="", readiness=true. Elapsed: 2.009233774s
    Dec  9 08:16:27.562: INFO: Pod "liveness-26e51669-4c1f-44f1-b5a0-cf0c312af87b" satisfied condition "not pending"
    Dec  9 08:16:27.562: INFO: Started pod liveness-26e51669-4c1f-44f1-b5a0-cf0c312af87b in namespace container-probe-8757
    STEP: checking the pod's current state and verifying that restartCount is present 12/09/22 08:16:27.562
    Dec  9 08:16:27.565: INFO: Initial restart count of pod liveness-26e51669-4c1f-44f1-b5a0-cf0c312af87b is 0
    Dec  9 08:16:47.607: INFO: Restart count of pod container-probe-8757/liveness-26e51669-4c1f-44f1-b5a0-cf0c312af87b is now 1 (20.041541168s elapsed)
    Dec  9 08:17:07.651: INFO: Restart count of pod container-probe-8757/liveness-26e51669-4c1f-44f1-b5a0-cf0c312af87b is now 2 (40.086270267s elapsed)
    Dec  9 08:17:27.695: INFO: Restart count of pod container-probe-8757/liveness-26e51669-4c1f-44f1-b5a0-cf0c312af87b is now 3 (1m0.129887405s elapsed)
    Dec  9 08:17:47.736: INFO: Restart count of pod container-probe-8757/liveness-26e51669-4c1f-44f1-b5a0-cf0c312af87b is now 4 (1m20.171344007s elapsed)
    Dec  9 08:18:47.871: INFO: Restart count of pod container-probe-8757/liveness-26e51669-4c1f-44f1-b5a0-cf0c312af87b is now 5 (2m20.306114236s elapsed)
    STEP: deleting the pod 12/09/22 08:18:47.871
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Dec  9 08:18:47.884: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-8757" for this suite. 12/09/22 08:18:47.889
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:207
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 08:18:47.902
Dec  9 08:18:47.903: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename projected 12/09/22 08:18:47.904
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:18:47.926
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:18:47.931
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:207
STEP: Creating a pod to test downward API volume plugin 12/09/22 08:18:47.935
Dec  9 08:18:47.954: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9480e4d4-962a-4f0e-b3ae-c99486799ea0" in namespace "projected-7049" to be "Succeeded or Failed"
Dec  9 08:18:47.961: INFO: Pod "downwardapi-volume-9480e4d4-962a-4f0e-b3ae-c99486799ea0": Phase="Pending", Reason="", readiness=false. Elapsed: 6.685712ms
Dec  9 08:18:49.965: INFO: Pod "downwardapi-volume-9480e4d4-962a-4f0e-b3ae-c99486799ea0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010654972s
Dec  9 08:18:51.966: INFO: Pod "downwardapi-volume-9480e4d4-962a-4f0e-b3ae-c99486799ea0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011501975s
STEP: Saw pod success 12/09/22 08:18:51.966
Dec  9 08:18:51.966: INFO: Pod "downwardapi-volume-9480e4d4-962a-4f0e-b3ae-c99486799ea0" satisfied condition "Succeeded or Failed"
Dec  9 08:18:51.969: INFO: Trying to get logs from node ip-10-0-10-179 pod downwardapi-volume-9480e4d4-962a-4f0e-b3ae-c99486799ea0 container client-container: <nil>
STEP: delete the pod 12/09/22 08:18:51.983
Dec  9 08:18:51.994: INFO: Waiting for pod downwardapi-volume-9480e4d4-962a-4f0e-b3ae-c99486799ea0 to disappear
Dec  9 08:18:51.997: INFO: Pod downwardapi-volume-9480e4d4-962a-4f0e-b3ae-c99486799ea0 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Dec  9 08:18:51.997: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-7049" for this suite. 12/09/22 08:18:52.001
------------------------------
â€¢ [4.105 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 08:18:47.902
    Dec  9 08:18:47.903: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename projected 12/09/22 08:18:47.904
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:18:47.926
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:18:47.931
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide container's memory limit [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:207
    STEP: Creating a pod to test downward API volume plugin 12/09/22 08:18:47.935
    Dec  9 08:18:47.954: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9480e4d4-962a-4f0e-b3ae-c99486799ea0" in namespace "projected-7049" to be "Succeeded or Failed"
    Dec  9 08:18:47.961: INFO: Pod "downwardapi-volume-9480e4d4-962a-4f0e-b3ae-c99486799ea0": Phase="Pending", Reason="", readiness=false. Elapsed: 6.685712ms
    Dec  9 08:18:49.965: INFO: Pod "downwardapi-volume-9480e4d4-962a-4f0e-b3ae-c99486799ea0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010654972s
    Dec  9 08:18:51.966: INFO: Pod "downwardapi-volume-9480e4d4-962a-4f0e-b3ae-c99486799ea0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011501975s
    STEP: Saw pod success 12/09/22 08:18:51.966
    Dec  9 08:18:51.966: INFO: Pod "downwardapi-volume-9480e4d4-962a-4f0e-b3ae-c99486799ea0" satisfied condition "Succeeded or Failed"
    Dec  9 08:18:51.969: INFO: Trying to get logs from node ip-10-0-10-179 pod downwardapi-volume-9480e4d4-962a-4f0e-b3ae-c99486799ea0 container client-container: <nil>
    STEP: delete the pod 12/09/22 08:18:51.983
    Dec  9 08:18:51.994: INFO: Waiting for pod downwardapi-volume-9480e4d4-962a-4f0e-b3ae-c99486799ea0 to disappear
    Dec  9 08:18:51.997: INFO: Pod downwardapi-volume-9480e4d4-962a-4f0e-b3ae-c99486799ea0 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Dec  9 08:18:51.997: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-7049" for this suite. 12/09/22 08:18:52.001
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:89
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 08:18:52.009
Dec  9 08:18:52.009: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename projected 12/09/22 08:18:52.01
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:18:52.028
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:18:52.038
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:89
STEP: Creating configMap with name projected-configmap-test-volume-map-ac31146c-2737-4eb2-9508-b8fd3971e39a 12/09/22 08:18:52.042
STEP: Creating a pod to test consume configMaps 12/09/22 08:18:52.046
Dec  9 08:18:52.054: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-42fdf92e-6285-43cb-8432-ca4ac58da778" in namespace "projected-9722" to be "Succeeded or Failed"
Dec  9 08:18:52.060: INFO: Pod "pod-projected-configmaps-42fdf92e-6285-43cb-8432-ca4ac58da778": Phase="Pending", Reason="", readiness=false. Elapsed: 5.226233ms
Dec  9 08:18:54.063: INFO: Pod "pod-projected-configmaps-42fdf92e-6285-43cb-8432-ca4ac58da778": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009063278s
Dec  9 08:18:56.063: INFO: Pod "pod-projected-configmaps-42fdf92e-6285-43cb-8432-ca4ac58da778": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008612893s
STEP: Saw pod success 12/09/22 08:18:56.063
Dec  9 08:18:56.063: INFO: Pod "pod-projected-configmaps-42fdf92e-6285-43cb-8432-ca4ac58da778" satisfied condition "Succeeded or Failed"
Dec  9 08:18:56.066: INFO: Trying to get logs from node ip-10-0-10-179 pod pod-projected-configmaps-42fdf92e-6285-43cb-8432-ca4ac58da778 container agnhost-container: <nil>
STEP: delete the pod 12/09/22 08:18:56.072
Dec  9 08:18:56.087: INFO: Waiting for pod pod-projected-configmaps-42fdf92e-6285-43cb-8432-ca4ac58da778 to disappear
Dec  9 08:18:56.090: INFO: Pod pod-projected-configmaps-42fdf92e-6285-43cb-8432-ca4ac58da778 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Dec  9 08:18:56.091: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-9722" for this suite. 12/09/22 08:18:56.101
------------------------------
â€¢ [4.102 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:89

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 08:18:52.009
    Dec  9 08:18:52.009: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename projected 12/09/22 08:18:52.01
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:18:52.028
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:18:52.038
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:89
    STEP: Creating configMap with name projected-configmap-test-volume-map-ac31146c-2737-4eb2-9508-b8fd3971e39a 12/09/22 08:18:52.042
    STEP: Creating a pod to test consume configMaps 12/09/22 08:18:52.046
    Dec  9 08:18:52.054: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-42fdf92e-6285-43cb-8432-ca4ac58da778" in namespace "projected-9722" to be "Succeeded or Failed"
    Dec  9 08:18:52.060: INFO: Pod "pod-projected-configmaps-42fdf92e-6285-43cb-8432-ca4ac58da778": Phase="Pending", Reason="", readiness=false. Elapsed: 5.226233ms
    Dec  9 08:18:54.063: INFO: Pod "pod-projected-configmaps-42fdf92e-6285-43cb-8432-ca4ac58da778": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009063278s
    Dec  9 08:18:56.063: INFO: Pod "pod-projected-configmaps-42fdf92e-6285-43cb-8432-ca4ac58da778": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008612893s
    STEP: Saw pod success 12/09/22 08:18:56.063
    Dec  9 08:18:56.063: INFO: Pod "pod-projected-configmaps-42fdf92e-6285-43cb-8432-ca4ac58da778" satisfied condition "Succeeded or Failed"
    Dec  9 08:18:56.066: INFO: Trying to get logs from node ip-10-0-10-179 pod pod-projected-configmaps-42fdf92e-6285-43cb-8432-ca4ac58da778 container agnhost-container: <nil>
    STEP: delete the pod 12/09/22 08:18:56.072
    Dec  9 08:18:56.087: INFO: Waiting for pod pod-projected-configmaps-42fdf92e-6285-43cb-8432-ca4ac58da778 to disappear
    Dec  9 08:18:56.090: INFO: Pod pod-projected-configmaps-42fdf92e-6285-43cb-8432-ca4ac58da778 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Dec  9 08:18:56.091: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-9722" for this suite. 12/09/22 08:18:56.101
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:153
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 08:18:56.114
Dec  9 08:18:56.114: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename crd-publish-openapi 12/09/22 08:18:56.116
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:18:56.136
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:18:56.142
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:153
Dec  9 08:18:56.147: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 12/09/22 08:18:57.651
Dec  9 08:18:57.651: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=crd-publish-openapi-3053 --namespace=crd-publish-openapi-3053 create -f -'
Dec  9 08:18:58.526: INFO: stderr: ""
Dec  9 08:18:58.526: INFO: stdout: "e2e-test-crd-publish-openapi-4275-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Dec  9 08:18:58.526: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=crd-publish-openapi-3053 --namespace=crd-publish-openapi-3053 delete e2e-test-crd-publish-openapi-4275-crds test-cr'
Dec  9 08:18:58.632: INFO: stderr: ""
Dec  9 08:18:58.632: INFO: stdout: "e2e-test-crd-publish-openapi-4275-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Dec  9 08:18:58.632: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=crd-publish-openapi-3053 --namespace=crd-publish-openapi-3053 apply -f -'
Dec  9 08:18:58.902: INFO: stderr: ""
Dec  9 08:18:58.902: INFO: stdout: "e2e-test-crd-publish-openapi-4275-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Dec  9 08:18:58.902: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=crd-publish-openapi-3053 --namespace=crd-publish-openapi-3053 delete e2e-test-crd-publish-openapi-4275-crds test-cr'
Dec  9 08:18:58.971: INFO: stderr: ""
Dec  9 08:18:58.971: INFO: stdout: "e2e-test-crd-publish-openapi-4275-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema 12/09/22 08:18:58.971
Dec  9 08:18:58.972: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=crd-publish-openapi-3053 explain e2e-test-crd-publish-openapi-4275-crds'
Dec  9 08:18:59.166: INFO: stderr: ""
Dec  9 08:18:59.166: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-4275-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Dec  9 08:19:00.757: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-3053" for this suite. 12/09/22 08:19:00.765
------------------------------
â€¢ [4.658 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:153

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 08:18:56.114
    Dec  9 08:18:56.114: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename crd-publish-openapi 12/09/22 08:18:56.116
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:18:56.136
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:18:56.142
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for CRD without validation schema [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:153
    Dec  9 08:18:56.147: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 12/09/22 08:18:57.651
    Dec  9 08:18:57.651: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=crd-publish-openapi-3053 --namespace=crd-publish-openapi-3053 create -f -'
    Dec  9 08:18:58.526: INFO: stderr: ""
    Dec  9 08:18:58.526: INFO: stdout: "e2e-test-crd-publish-openapi-4275-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
    Dec  9 08:18:58.526: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=crd-publish-openapi-3053 --namespace=crd-publish-openapi-3053 delete e2e-test-crd-publish-openapi-4275-crds test-cr'
    Dec  9 08:18:58.632: INFO: stderr: ""
    Dec  9 08:18:58.632: INFO: stdout: "e2e-test-crd-publish-openapi-4275-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
    Dec  9 08:18:58.632: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=crd-publish-openapi-3053 --namespace=crd-publish-openapi-3053 apply -f -'
    Dec  9 08:18:58.902: INFO: stderr: ""
    Dec  9 08:18:58.902: INFO: stdout: "e2e-test-crd-publish-openapi-4275-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
    Dec  9 08:18:58.902: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=crd-publish-openapi-3053 --namespace=crd-publish-openapi-3053 delete e2e-test-crd-publish-openapi-4275-crds test-cr'
    Dec  9 08:18:58.971: INFO: stderr: ""
    Dec  9 08:18:58.971: INFO: stdout: "e2e-test-crd-publish-openapi-4275-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR without validation schema 12/09/22 08:18:58.971
    Dec  9 08:18:58.972: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=crd-publish-openapi-3053 explain e2e-test-crd-publish-openapi-4275-crds'
    Dec  9 08:18:59.166: INFO: stderr: ""
    Dec  9 08:18:59.166: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-4275-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Dec  9 08:19:00.757: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-3053" for this suite. 12/09/22 08:19:00.765
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] CSIInlineVolumes
  should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
  test/e2e/storage/csi_inline.go:46
[BeforeEach] [sig-storage] CSIInlineVolumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 08:19:00.772
Dec  9 08:19:00.772: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename csiinlinevolumes 12/09/22 08:19:00.773
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:19:00.787
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:19:00.791
[BeforeEach] [sig-storage] CSIInlineVolumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
  test/e2e/storage/csi_inline.go:46
STEP: creating 12/09/22 08:19:00.795
STEP: getting 12/09/22 08:19:00.808
STEP: listing 12/09/22 08:19:00.813
STEP: deleting 12/09/22 08:19:00.816
[AfterEach] [sig-storage] CSIInlineVolumes
  test/e2e/framework/node/init/init.go:32
Dec  9 08:19:00.831: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  tear down framework | framework.go:193
STEP: Destroying namespace "csiinlinevolumes-4440" for this suite. 12/09/22 08:19:00.834
------------------------------
â€¢ [0.068 seconds]
[sig-storage] CSIInlineVolumes
test/e2e/storage/utils/framework.go:23
  should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
  test/e2e/storage/csi_inline.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] CSIInlineVolumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 08:19:00.772
    Dec  9 08:19:00.772: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename csiinlinevolumes 12/09/22 08:19:00.773
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:19:00.787
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:19:00.791
    [BeforeEach] [sig-storage] CSIInlineVolumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
      test/e2e/storage/csi_inline.go:46
    STEP: creating 12/09/22 08:19:00.795
    STEP: getting 12/09/22 08:19:00.808
    STEP: listing 12/09/22 08:19:00.813
    STEP: deleting 12/09/22 08:19:00.816
    [AfterEach] [sig-storage] CSIInlineVolumes
      test/e2e/framework/node/init/init.go:32
    Dec  9 08:19:00.831: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "csiinlinevolumes-4440" for this suite. 12/09/22 08:19:00.834
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should delete a job [Conformance]
  test/e2e/apps/job.go:481
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 08:19:00.843
Dec  9 08:19:00.844: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename job 12/09/22 08:19:00.845
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:19:00.859
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:19:00.864
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should delete a job [Conformance]
  test/e2e/apps/job.go:481
STEP: Creating a job 12/09/22 08:19:00.868
STEP: Ensuring active pods == parallelism 12/09/22 08:19:00.874
STEP: delete a job 12/09/22 08:19:04.88
STEP: deleting Job.batch foo in namespace job-2443, will wait for the garbage collector to delete the pods 12/09/22 08:19:04.881
Dec  9 08:19:04.940: INFO: Deleting Job.batch foo took: 5.792335ms
Dec  9 08:19:05.141: INFO: Terminating Job.batch foo pods took: 200.915479ms
STEP: Ensuring job was deleted 12/09/22 08:19:36.842
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Dec  9 08:19:36.848: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-2443" for this suite. 12/09/22 08:19:36.854
------------------------------
â€¢ [SLOW TEST] [36.016 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  test/e2e/apps/job.go:481

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 08:19:00.843
    Dec  9 08:19:00.844: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename job 12/09/22 08:19:00.845
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:19:00.859
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:19:00.864
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete a job [Conformance]
      test/e2e/apps/job.go:481
    STEP: Creating a job 12/09/22 08:19:00.868
    STEP: Ensuring active pods == parallelism 12/09/22 08:19:00.874
    STEP: delete a job 12/09/22 08:19:04.88
    STEP: deleting Job.batch foo in namespace job-2443, will wait for the garbage collector to delete the pods 12/09/22 08:19:04.881
    Dec  9 08:19:04.940: INFO: Deleting Job.batch foo took: 5.792335ms
    Dec  9 08:19:05.141: INFO: Terminating Job.batch foo pods took: 200.915479ms
    STEP: Ensuring job was deleted 12/09/22 08:19:36.842
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Dec  9 08:19:36.848: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-2443" for this suite. 12/09/22 08:19:36.854
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath
  runs ReplicaSets to verify preemption running path [Conformance]
  test/e2e/scheduling/preemption.go:616
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 08:19:36.862
Dec  9 08:19:36.862: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename sched-preemption 12/09/22 08:19:36.863
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:19:36.892
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:19:36.897
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:96
Dec  9 08:19:36.918: INFO: Waiting up to 1m0s for all nodes to be ready
Dec  9 08:20:36.951: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PreemptionExecutionPath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 08:20:36.954
Dec  9 08:20:36.954: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename sched-preemption-path 12/09/22 08:20:36.955
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:20:36.974
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:20:36.979
[BeforeEach] PreemptionExecutionPath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:569
STEP: Finding an available node 12/09/22 08:20:36.983
STEP: Trying to launch a pod without a label to get a node which can launch it. 12/09/22 08:20:36.983
Dec  9 08:20:36.991: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-preemption-path-5864" to be "running"
Dec  9 08:20:36.996: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 5.637844ms
Dec  9 08:20:39.001: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.009931515s
Dec  9 08:20:39.001: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 12/09/22 08:20:39.003
Dec  9 08:20:39.011: INFO: found a healthy node: ip-10-0-10-179
[It] runs ReplicaSets to verify preemption running path [Conformance]
  test/e2e/scheduling/preemption.go:616
Dec  9 08:20:49.078: INFO: pods created so far: [1 1 1]
Dec  9 08:20:49.078: INFO: length of pods created so far: 3
Dec  9 08:20:51.087: INFO: pods created so far: [2 2 1]
[AfterEach] PreemptionExecutionPath
  test/e2e/framework/node/init/init.go:32
Dec  9 08:20:58.091: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:543
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/node/init/init.go:32
Dec  9 08:20:58.121: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:84
[DeferCleanup (Each)] PreemptionExecutionPath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] PreemptionExecutionPath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] PreemptionExecutionPath
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-path-5864" for this suite. 12/09/22 08:20:58.162
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-5007" for this suite. 12/09/22 08:20:58.175
------------------------------
â€¢ [SLOW TEST] [81.324 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:531
    runs ReplicaSets to verify preemption running path [Conformance]
    test/e2e/scheduling/preemption.go:616

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 08:19:36.862
    Dec  9 08:19:36.862: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename sched-preemption 12/09/22 08:19:36.863
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:19:36.892
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:19:36.897
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:96
    Dec  9 08:19:36.918: INFO: Waiting up to 1m0s for all nodes to be ready
    Dec  9 08:20:36.951: INFO: Waiting for terminating namespaces to be deleted...
    [BeforeEach] PreemptionExecutionPath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 08:20:36.954
    Dec  9 08:20:36.954: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename sched-preemption-path 12/09/22 08:20:36.955
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:20:36.974
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:20:36.979
    [BeforeEach] PreemptionExecutionPath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] PreemptionExecutionPath
      test/e2e/scheduling/preemption.go:569
    STEP: Finding an available node 12/09/22 08:20:36.983
    STEP: Trying to launch a pod without a label to get a node which can launch it. 12/09/22 08:20:36.983
    Dec  9 08:20:36.991: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-preemption-path-5864" to be "running"
    Dec  9 08:20:36.996: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 5.637844ms
    Dec  9 08:20:39.001: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.009931515s
    Dec  9 08:20:39.001: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 12/09/22 08:20:39.003
    Dec  9 08:20:39.011: INFO: found a healthy node: ip-10-0-10-179
    [It] runs ReplicaSets to verify preemption running path [Conformance]
      test/e2e/scheduling/preemption.go:616
    Dec  9 08:20:49.078: INFO: pods created so far: [1 1 1]
    Dec  9 08:20:49.078: INFO: length of pods created so far: 3
    Dec  9 08:20:51.087: INFO: pods created so far: [2 2 1]
    [AfterEach] PreemptionExecutionPath
      test/e2e/framework/node/init/init.go:32
    Dec  9 08:20:58.091: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] PreemptionExecutionPath
      test/e2e/scheduling/preemption.go:543
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/node/init/init.go:32
    Dec  9 08:20:58.121: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:84
    [DeferCleanup (Each)] PreemptionExecutionPath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] PreemptionExecutionPath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] PreemptionExecutionPath
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-path-5864" for this suite. 12/09/22 08:20:58.162
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-5007" for this suite. 12/09/22 08:20:58.175
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Containers
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:39
[BeforeEach] [sig-node] Containers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 08:20:58.186
Dec  9 08:20:58.186: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename containers 12/09/22 08:20:58.188
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:20:58.215
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:20:58.225
[BeforeEach] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:31
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:39
Dec  9 08:20:58.237: INFO: Waiting up to 5m0s for pod "client-containers-c69a5398-b993-4751-aa0e-4bbceb2be886" in namespace "containers-1624" to be "running"
Dec  9 08:20:58.241: INFO: Pod "client-containers-c69a5398-b993-4751-aa0e-4bbceb2be886": Phase="Pending", Reason="", readiness=false. Elapsed: 4.269111ms
Dec  9 08:21:00.246: INFO: Pod "client-containers-c69a5398-b993-4751-aa0e-4bbceb2be886": Phase="Running", Reason="", readiness=true. Elapsed: 2.008638018s
Dec  9 08:21:00.246: INFO: Pod "client-containers-c69a5398-b993-4751-aa0e-4bbceb2be886" satisfied condition "running"
[AfterEach] [sig-node] Containers
  test/e2e/framework/node/init/init.go:32
Dec  9 08:21:00.258: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Containers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Containers
  tear down framework | framework.go:193
STEP: Destroying namespace "containers-1624" for this suite. 12/09/22 08:21:00.262
------------------------------
â€¢ [2.082 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:39

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 08:20:58.186
    Dec  9 08:20:58.186: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename containers 12/09/22 08:20:58.188
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:20:58.215
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:20:58.225
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:31
    [It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:39
    Dec  9 08:20:58.237: INFO: Waiting up to 5m0s for pod "client-containers-c69a5398-b993-4751-aa0e-4bbceb2be886" in namespace "containers-1624" to be "running"
    Dec  9 08:20:58.241: INFO: Pod "client-containers-c69a5398-b993-4751-aa0e-4bbceb2be886": Phase="Pending", Reason="", readiness=false. Elapsed: 4.269111ms
    Dec  9 08:21:00.246: INFO: Pod "client-containers-c69a5398-b993-4751-aa0e-4bbceb2be886": Phase="Running", Reason="", readiness=true. Elapsed: 2.008638018s
    Dec  9 08:21:00.246: INFO: Pod "client-containers-c69a5398-b993-4751-aa0e-4bbceb2be886" satisfied condition "running"
    [AfterEach] [sig-node] Containers
      test/e2e/framework/node/init/init.go:32
    Dec  9 08:21:00.258: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Containers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Containers
      tear down framework | framework.go:193
    STEP: Destroying namespace "containers-1624" for this suite. 12/09/22 08:21:00.262
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 08:21:00.27
Dec  9 08:21:00.270: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename gc 12/09/22 08:21:00.271
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:21:00.294
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:21:00.299
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735
STEP: create the rc1 12/09/22 08:21:00.306
STEP: create the rc2 12/09/22 08:21:00.312
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well 12/09/22 08:21:05.334
STEP: delete the rc simpletest-rc-to-be-deleted 12/09/22 08:21:06.5
STEP: wait for the rc to be deleted 12/09/22 08:21:06.586
Dec  9 08:21:11.737: INFO: 75 pods remaining
Dec  9 08:21:11.737: INFO: 75 pods has nil DeletionTimestamp
Dec  9 08:21:11.737: INFO: 
STEP: Gathering metrics 12/09/22 08:21:16.713
Dec  9 08:21:16.898: INFO: Waiting up to 5m0s for pod "kube-controller-manager-ip-10-0-7-84" in namespace "kube-system" to be "running and ready"
Dec  9 08:21:16.904: INFO: Pod "kube-controller-manager-ip-10-0-7-84": Phase="Running", Reason="", readiness=true. Elapsed: 5.541748ms
Dec  9 08:21:16.904: INFO: The phase of Pod kube-controller-manager-ip-10-0-7-84 is Running (Ready = true)
Dec  9 08:21:16.904: INFO: Pod "kube-controller-manager-ip-10-0-7-84" satisfied condition "running and ready"
Dec  9 08:21:17.357: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Dec  9 08:21:17.358: INFO: Deleting pod "simpletest-rc-to-be-deleted-2k64z" in namespace "gc-3045"
Dec  9 08:21:17.375: INFO: Deleting pod "simpletest-rc-to-be-deleted-2lwpj" in namespace "gc-3045"
Dec  9 08:21:17.390: INFO: Deleting pod "simpletest-rc-to-be-deleted-2qkmg" in namespace "gc-3045"
Dec  9 08:21:17.425: INFO: Deleting pod "simpletest-rc-to-be-deleted-4jlbb" in namespace "gc-3045"
Dec  9 08:21:17.438: INFO: Deleting pod "simpletest-rc-to-be-deleted-4q5cx" in namespace "gc-3045"
Dec  9 08:21:17.468: INFO: Deleting pod "simpletest-rc-to-be-deleted-592jh" in namespace "gc-3045"
Dec  9 08:21:17.504: INFO: Deleting pod "simpletest-rc-to-be-deleted-62gh7" in namespace "gc-3045"
Dec  9 08:21:17.522: INFO: Deleting pod "simpletest-rc-to-be-deleted-6gmqz" in namespace "gc-3045"
Dec  9 08:21:17.553: INFO: Deleting pod "simpletest-rc-to-be-deleted-6jj49" in namespace "gc-3045"
Dec  9 08:21:17.594: INFO: Deleting pod "simpletest-rc-to-be-deleted-6kjfz" in namespace "gc-3045"
Dec  9 08:21:17.644: INFO: Deleting pod "simpletest-rc-to-be-deleted-6z4bd" in namespace "gc-3045"
Dec  9 08:21:17.654: INFO: Deleting pod "simpletest-rc-to-be-deleted-7bql4" in namespace "gc-3045"
Dec  9 08:21:17.671: INFO: Deleting pod "simpletest-rc-to-be-deleted-7ctpg" in namespace "gc-3045"
Dec  9 08:21:17.681: INFO: Deleting pod "simpletest-rc-to-be-deleted-7ltnw" in namespace "gc-3045"
Dec  9 08:21:17.701: INFO: Deleting pod "simpletest-rc-to-be-deleted-7n5pw" in namespace "gc-3045"
Dec  9 08:21:17.718: INFO: Deleting pod "simpletest-rc-to-be-deleted-7ngkv" in namespace "gc-3045"
Dec  9 08:21:17.741: INFO: Deleting pod "simpletest-rc-to-be-deleted-7st2c" in namespace "gc-3045"
Dec  9 08:21:17.773: INFO: Deleting pod "simpletest-rc-to-be-deleted-8jq7c" in namespace "gc-3045"
Dec  9 08:21:17.798: INFO: Deleting pod "simpletest-rc-to-be-deleted-8kxss" in namespace "gc-3045"
Dec  9 08:21:17.822: INFO: Deleting pod "simpletest-rc-to-be-deleted-8nmk7" in namespace "gc-3045"
Dec  9 08:21:17.841: INFO: Deleting pod "simpletest-rc-to-be-deleted-8vbmk" in namespace "gc-3045"
Dec  9 08:21:17.856: INFO: Deleting pod "simpletest-rc-to-be-deleted-8z6w8" in namespace "gc-3045"
Dec  9 08:21:17.954: INFO: Deleting pod "simpletest-rc-to-be-deleted-8zk8j" in namespace "gc-3045"
Dec  9 08:21:17.992: INFO: Deleting pod "simpletest-rc-to-be-deleted-8zkmc" in namespace "gc-3045"
Dec  9 08:21:18.040: INFO: Deleting pod "simpletest-rc-to-be-deleted-9s5hn" in namespace "gc-3045"
Dec  9 08:21:18.064: INFO: Deleting pod "simpletest-rc-to-be-deleted-9sspx" in namespace "gc-3045"
Dec  9 08:21:18.172: INFO: Deleting pod "simpletest-rc-to-be-deleted-9tvzq" in namespace "gc-3045"
Dec  9 08:21:18.284: INFO: Deleting pod "simpletest-rc-to-be-deleted-9vncf" in namespace "gc-3045"
Dec  9 08:21:18.338: INFO: Deleting pod "simpletest-rc-to-be-deleted-b4wjn" in namespace "gc-3045"
Dec  9 08:21:18.383: INFO: Deleting pod "simpletest-rc-to-be-deleted-b5jbn" in namespace "gc-3045"
Dec  9 08:21:18.427: INFO: Deleting pod "simpletest-rc-to-be-deleted-bqrjk" in namespace "gc-3045"
Dec  9 08:21:18.485: INFO: Deleting pod "simpletest-rc-to-be-deleted-ccntw" in namespace "gc-3045"
Dec  9 08:21:18.537: INFO: Deleting pod "simpletest-rc-to-be-deleted-cgk65" in namespace "gc-3045"
Dec  9 08:21:18.642: INFO: Deleting pod "simpletest-rc-to-be-deleted-d2mm2" in namespace "gc-3045"
Dec  9 08:21:18.670: INFO: Deleting pod "simpletest-rc-to-be-deleted-d5dvt" in namespace "gc-3045"
Dec  9 08:21:18.688: INFO: Deleting pod "simpletest-rc-to-be-deleted-dnkbp" in namespace "gc-3045"
Dec  9 08:21:18.717: INFO: Deleting pod "simpletest-rc-to-be-deleted-dpdvv" in namespace "gc-3045"
Dec  9 08:21:18.750: INFO: Deleting pod "simpletest-rc-to-be-deleted-dtw92" in namespace "gc-3045"
Dec  9 08:21:18.765: INFO: Deleting pod "simpletest-rc-to-be-deleted-fkxmp" in namespace "gc-3045"
Dec  9 08:21:18.849: INFO: Deleting pod "simpletest-rc-to-be-deleted-gb82w" in namespace "gc-3045"
Dec  9 08:21:18.894: INFO: Deleting pod "simpletest-rc-to-be-deleted-gwd6t" in namespace "gc-3045"
Dec  9 08:21:18.927: INFO: Deleting pod "simpletest-rc-to-be-deleted-h6fdf" in namespace "gc-3045"
Dec  9 08:21:18.963: INFO: Deleting pod "simpletest-rc-to-be-deleted-hc6ss" in namespace "gc-3045"
Dec  9 08:21:18.978: INFO: Deleting pod "simpletest-rc-to-be-deleted-hpggn" in namespace "gc-3045"
Dec  9 08:21:18.988: INFO: Deleting pod "simpletest-rc-to-be-deleted-j95g9" in namespace "gc-3045"
Dec  9 08:21:19.002: INFO: Deleting pod "simpletest-rc-to-be-deleted-jcrgh" in namespace "gc-3045"
Dec  9 08:21:19.015: INFO: Deleting pod "simpletest-rc-to-be-deleted-jk94k" in namespace "gc-3045"
Dec  9 08:21:19.059: INFO: Deleting pod "simpletest-rc-to-be-deleted-jwrfm" in namespace "gc-3045"
Dec  9 08:21:19.096: INFO: Deleting pod "simpletest-rc-to-be-deleted-jzmvb" in namespace "gc-3045"
Dec  9 08:21:19.125: INFO: Deleting pod "simpletest-rc-to-be-deleted-jzss2" in namespace "gc-3045"
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Dec  9 08:21:19.154: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-3045" for this suite. 12/09/22 08:21:19.2
------------------------------
â€¢ [SLOW TEST] [18.947 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 08:21:00.27
    Dec  9 08:21:00.270: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename gc 12/09/22 08:21:00.271
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:21:00.294
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:21:00.299
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
      test/e2e/apimachinery/garbage_collector.go:735
    STEP: create the rc1 12/09/22 08:21:00.306
    STEP: create the rc2 12/09/22 08:21:00.312
    STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well 12/09/22 08:21:05.334
    STEP: delete the rc simpletest-rc-to-be-deleted 12/09/22 08:21:06.5
    STEP: wait for the rc to be deleted 12/09/22 08:21:06.586
    Dec  9 08:21:11.737: INFO: 75 pods remaining
    Dec  9 08:21:11.737: INFO: 75 pods has nil DeletionTimestamp
    Dec  9 08:21:11.737: INFO: 
    STEP: Gathering metrics 12/09/22 08:21:16.713
    Dec  9 08:21:16.898: INFO: Waiting up to 5m0s for pod "kube-controller-manager-ip-10-0-7-84" in namespace "kube-system" to be "running and ready"
    Dec  9 08:21:16.904: INFO: Pod "kube-controller-manager-ip-10-0-7-84": Phase="Running", Reason="", readiness=true. Elapsed: 5.541748ms
    Dec  9 08:21:16.904: INFO: The phase of Pod kube-controller-manager-ip-10-0-7-84 is Running (Ready = true)
    Dec  9 08:21:16.904: INFO: Pod "kube-controller-manager-ip-10-0-7-84" satisfied condition "running and ready"
    Dec  9 08:21:17.357: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    Dec  9 08:21:17.358: INFO: Deleting pod "simpletest-rc-to-be-deleted-2k64z" in namespace "gc-3045"
    Dec  9 08:21:17.375: INFO: Deleting pod "simpletest-rc-to-be-deleted-2lwpj" in namespace "gc-3045"
    Dec  9 08:21:17.390: INFO: Deleting pod "simpletest-rc-to-be-deleted-2qkmg" in namespace "gc-3045"
    Dec  9 08:21:17.425: INFO: Deleting pod "simpletest-rc-to-be-deleted-4jlbb" in namespace "gc-3045"
    Dec  9 08:21:17.438: INFO: Deleting pod "simpletest-rc-to-be-deleted-4q5cx" in namespace "gc-3045"
    Dec  9 08:21:17.468: INFO: Deleting pod "simpletest-rc-to-be-deleted-592jh" in namespace "gc-3045"
    Dec  9 08:21:17.504: INFO: Deleting pod "simpletest-rc-to-be-deleted-62gh7" in namespace "gc-3045"
    Dec  9 08:21:17.522: INFO: Deleting pod "simpletest-rc-to-be-deleted-6gmqz" in namespace "gc-3045"
    Dec  9 08:21:17.553: INFO: Deleting pod "simpletest-rc-to-be-deleted-6jj49" in namespace "gc-3045"
    Dec  9 08:21:17.594: INFO: Deleting pod "simpletest-rc-to-be-deleted-6kjfz" in namespace "gc-3045"
    Dec  9 08:21:17.644: INFO: Deleting pod "simpletest-rc-to-be-deleted-6z4bd" in namespace "gc-3045"
    Dec  9 08:21:17.654: INFO: Deleting pod "simpletest-rc-to-be-deleted-7bql4" in namespace "gc-3045"
    Dec  9 08:21:17.671: INFO: Deleting pod "simpletest-rc-to-be-deleted-7ctpg" in namespace "gc-3045"
    Dec  9 08:21:17.681: INFO: Deleting pod "simpletest-rc-to-be-deleted-7ltnw" in namespace "gc-3045"
    Dec  9 08:21:17.701: INFO: Deleting pod "simpletest-rc-to-be-deleted-7n5pw" in namespace "gc-3045"
    Dec  9 08:21:17.718: INFO: Deleting pod "simpletest-rc-to-be-deleted-7ngkv" in namespace "gc-3045"
    Dec  9 08:21:17.741: INFO: Deleting pod "simpletest-rc-to-be-deleted-7st2c" in namespace "gc-3045"
    Dec  9 08:21:17.773: INFO: Deleting pod "simpletest-rc-to-be-deleted-8jq7c" in namespace "gc-3045"
    Dec  9 08:21:17.798: INFO: Deleting pod "simpletest-rc-to-be-deleted-8kxss" in namespace "gc-3045"
    Dec  9 08:21:17.822: INFO: Deleting pod "simpletest-rc-to-be-deleted-8nmk7" in namespace "gc-3045"
    Dec  9 08:21:17.841: INFO: Deleting pod "simpletest-rc-to-be-deleted-8vbmk" in namespace "gc-3045"
    Dec  9 08:21:17.856: INFO: Deleting pod "simpletest-rc-to-be-deleted-8z6w8" in namespace "gc-3045"
    Dec  9 08:21:17.954: INFO: Deleting pod "simpletest-rc-to-be-deleted-8zk8j" in namespace "gc-3045"
    Dec  9 08:21:17.992: INFO: Deleting pod "simpletest-rc-to-be-deleted-8zkmc" in namespace "gc-3045"
    Dec  9 08:21:18.040: INFO: Deleting pod "simpletest-rc-to-be-deleted-9s5hn" in namespace "gc-3045"
    Dec  9 08:21:18.064: INFO: Deleting pod "simpletest-rc-to-be-deleted-9sspx" in namespace "gc-3045"
    Dec  9 08:21:18.172: INFO: Deleting pod "simpletest-rc-to-be-deleted-9tvzq" in namespace "gc-3045"
    Dec  9 08:21:18.284: INFO: Deleting pod "simpletest-rc-to-be-deleted-9vncf" in namespace "gc-3045"
    Dec  9 08:21:18.338: INFO: Deleting pod "simpletest-rc-to-be-deleted-b4wjn" in namespace "gc-3045"
    Dec  9 08:21:18.383: INFO: Deleting pod "simpletest-rc-to-be-deleted-b5jbn" in namespace "gc-3045"
    Dec  9 08:21:18.427: INFO: Deleting pod "simpletest-rc-to-be-deleted-bqrjk" in namespace "gc-3045"
    Dec  9 08:21:18.485: INFO: Deleting pod "simpletest-rc-to-be-deleted-ccntw" in namespace "gc-3045"
    Dec  9 08:21:18.537: INFO: Deleting pod "simpletest-rc-to-be-deleted-cgk65" in namespace "gc-3045"
    Dec  9 08:21:18.642: INFO: Deleting pod "simpletest-rc-to-be-deleted-d2mm2" in namespace "gc-3045"
    Dec  9 08:21:18.670: INFO: Deleting pod "simpletest-rc-to-be-deleted-d5dvt" in namespace "gc-3045"
    Dec  9 08:21:18.688: INFO: Deleting pod "simpletest-rc-to-be-deleted-dnkbp" in namespace "gc-3045"
    Dec  9 08:21:18.717: INFO: Deleting pod "simpletest-rc-to-be-deleted-dpdvv" in namespace "gc-3045"
    Dec  9 08:21:18.750: INFO: Deleting pod "simpletest-rc-to-be-deleted-dtw92" in namespace "gc-3045"
    Dec  9 08:21:18.765: INFO: Deleting pod "simpletest-rc-to-be-deleted-fkxmp" in namespace "gc-3045"
    Dec  9 08:21:18.849: INFO: Deleting pod "simpletest-rc-to-be-deleted-gb82w" in namespace "gc-3045"
    Dec  9 08:21:18.894: INFO: Deleting pod "simpletest-rc-to-be-deleted-gwd6t" in namespace "gc-3045"
    Dec  9 08:21:18.927: INFO: Deleting pod "simpletest-rc-to-be-deleted-h6fdf" in namespace "gc-3045"
    Dec  9 08:21:18.963: INFO: Deleting pod "simpletest-rc-to-be-deleted-hc6ss" in namespace "gc-3045"
    Dec  9 08:21:18.978: INFO: Deleting pod "simpletest-rc-to-be-deleted-hpggn" in namespace "gc-3045"
    Dec  9 08:21:18.988: INFO: Deleting pod "simpletest-rc-to-be-deleted-j95g9" in namespace "gc-3045"
    Dec  9 08:21:19.002: INFO: Deleting pod "simpletest-rc-to-be-deleted-jcrgh" in namespace "gc-3045"
    Dec  9 08:21:19.015: INFO: Deleting pod "simpletest-rc-to-be-deleted-jk94k" in namespace "gc-3045"
    Dec  9 08:21:19.059: INFO: Deleting pod "simpletest-rc-to-be-deleted-jwrfm" in namespace "gc-3045"
    Dec  9 08:21:19.096: INFO: Deleting pod "simpletest-rc-to-be-deleted-jzmvb" in namespace "gc-3045"
    Dec  9 08:21:19.125: INFO: Deleting pod "simpletest-rc-to-be-deleted-jzss2" in namespace "gc-3045"
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Dec  9 08:21:19.154: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-3045" for this suite. 12/09/22 08:21:19.2
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:53
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 08:21:19.222
Dec  9 08:21:19.222: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename downward-api 12/09/22 08:21:19.237
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:21:19.258
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:21:19.262
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:53
STEP: Creating a pod to test downward API volume plugin 12/09/22 08:21:19.266
Dec  9 08:21:19.314: INFO: Waiting up to 5m0s for pod "downwardapi-volume-497b8f7a-abf6-4327-8517-da95948b9eac" in namespace "downward-api-3948" to be "Succeeded or Failed"
Dec  9 08:21:19.317: INFO: Pod "downwardapi-volume-497b8f7a-abf6-4327-8517-da95948b9eac": Phase="Pending", Reason="", readiness=false. Elapsed: 2.596925ms
Dec  9 08:21:21.322: INFO: Pod "downwardapi-volume-497b8f7a-abf6-4327-8517-da95948b9eac": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007158897s
Dec  9 08:21:23.331: INFO: Pod "downwardapi-volume-497b8f7a-abf6-4327-8517-da95948b9eac": Phase="Pending", Reason="", readiness=false. Elapsed: 4.016550298s
Dec  9 08:21:25.321: INFO: Pod "downwardapi-volume-497b8f7a-abf6-4327-8517-da95948b9eac": Phase="Pending", Reason="", readiness=false. Elapsed: 6.006390133s
Dec  9 08:21:27.348: INFO: Pod "downwardapi-volume-497b8f7a-abf6-4327-8517-da95948b9eac": Phase="Pending", Reason="", readiness=false. Elapsed: 8.033455972s
Dec  9 08:21:29.322: INFO: Pod "downwardapi-volume-497b8f7a-abf6-4327-8517-da95948b9eac": Phase="Pending", Reason="", readiness=false. Elapsed: 10.007593268s
Dec  9 08:21:31.320: INFO: Pod "downwardapi-volume-497b8f7a-abf6-4327-8517-da95948b9eac": Phase="Succeeded", Reason="", readiness=false. Elapsed: 12.005940715s
STEP: Saw pod success 12/09/22 08:21:31.321
Dec  9 08:21:31.321: INFO: Pod "downwardapi-volume-497b8f7a-abf6-4327-8517-da95948b9eac" satisfied condition "Succeeded or Failed"
Dec  9 08:21:31.325: INFO: Trying to get logs from node ip-10-0-10-179 pod downwardapi-volume-497b8f7a-abf6-4327-8517-da95948b9eac container client-container: <nil>
STEP: delete the pod 12/09/22 08:21:31.331
Dec  9 08:21:31.343: INFO: Waiting for pod downwardapi-volume-497b8f7a-abf6-4327-8517-da95948b9eac to disappear
Dec  9 08:21:31.345: INFO: Pod downwardapi-volume-497b8f7a-abf6-4327-8517-da95948b9eac no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Dec  9 08:21:31.345: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-3948" for this suite. 12/09/22 08:21:31.349
------------------------------
â€¢ [SLOW TEST] [12.136 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 08:21:19.222
    Dec  9 08:21:19.222: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename downward-api 12/09/22 08:21:19.237
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:21:19.258
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:21:19.262
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide podname only [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:53
    STEP: Creating a pod to test downward API volume plugin 12/09/22 08:21:19.266
    Dec  9 08:21:19.314: INFO: Waiting up to 5m0s for pod "downwardapi-volume-497b8f7a-abf6-4327-8517-da95948b9eac" in namespace "downward-api-3948" to be "Succeeded or Failed"
    Dec  9 08:21:19.317: INFO: Pod "downwardapi-volume-497b8f7a-abf6-4327-8517-da95948b9eac": Phase="Pending", Reason="", readiness=false. Elapsed: 2.596925ms
    Dec  9 08:21:21.322: INFO: Pod "downwardapi-volume-497b8f7a-abf6-4327-8517-da95948b9eac": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007158897s
    Dec  9 08:21:23.331: INFO: Pod "downwardapi-volume-497b8f7a-abf6-4327-8517-da95948b9eac": Phase="Pending", Reason="", readiness=false. Elapsed: 4.016550298s
    Dec  9 08:21:25.321: INFO: Pod "downwardapi-volume-497b8f7a-abf6-4327-8517-da95948b9eac": Phase="Pending", Reason="", readiness=false. Elapsed: 6.006390133s
    Dec  9 08:21:27.348: INFO: Pod "downwardapi-volume-497b8f7a-abf6-4327-8517-da95948b9eac": Phase="Pending", Reason="", readiness=false. Elapsed: 8.033455972s
    Dec  9 08:21:29.322: INFO: Pod "downwardapi-volume-497b8f7a-abf6-4327-8517-da95948b9eac": Phase="Pending", Reason="", readiness=false. Elapsed: 10.007593268s
    Dec  9 08:21:31.320: INFO: Pod "downwardapi-volume-497b8f7a-abf6-4327-8517-da95948b9eac": Phase="Succeeded", Reason="", readiness=false. Elapsed: 12.005940715s
    STEP: Saw pod success 12/09/22 08:21:31.321
    Dec  9 08:21:31.321: INFO: Pod "downwardapi-volume-497b8f7a-abf6-4327-8517-da95948b9eac" satisfied condition "Succeeded or Failed"
    Dec  9 08:21:31.325: INFO: Trying to get logs from node ip-10-0-10-179 pod downwardapi-volume-497b8f7a-abf6-4327-8517-da95948b9eac container client-container: <nil>
    STEP: delete the pod 12/09/22 08:21:31.331
    Dec  9 08:21:31.343: INFO: Waiting for pod downwardapi-volume-497b8f7a-abf6-4327-8517-da95948b9eac to disappear
    Dec  9 08:21:31.345: INFO: Pod downwardapi-volume-497b8f7a-abf6-4327-8517-da95948b9eac no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Dec  9 08:21:31.345: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-3948" for this suite. 12/09/22 08:21:31.349
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:92
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 08:21:31.361
Dec  9 08:21:31.361: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename replication-controller 12/09/22 08:21:31.362
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:21:31.379
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:21:31.385
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:92
STEP: Given a Pod with a 'name' label pod-adoption is created 12/09/22 08:21:31.395
Dec  9 08:21:31.402: INFO: Waiting up to 5m0s for pod "pod-adoption" in namespace "replication-controller-2072" to be "running and ready"
Dec  9 08:21:31.407: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 5.235523ms
Dec  9 08:21:31.407: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
Dec  9 08:21:33.411: INFO: Pod "pod-adoption": Phase="Running", Reason="", readiness=true. Elapsed: 2.008826933s
Dec  9 08:21:33.411: INFO: The phase of Pod pod-adoption is Running (Ready = true)
Dec  9 08:21:33.411: INFO: Pod "pod-adoption" satisfied condition "running and ready"
STEP: When a replication controller with a matching selector is created 12/09/22 08:21:33.417
STEP: Then the orphan pod is adopted 12/09/22 08:21:33.422
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Dec  9 08:21:34.433: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-2072" for this suite. 12/09/22 08:21:34.437
------------------------------
â€¢ [3.081 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 08:21:31.361
    Dec  9 08:21:31.361: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename replication-controller 12/09/22 08:21:31.362
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:21:31.379
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:21:31.385
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should adopt matching pods on creation [Conformance]
      test/e2e/apps/rc.go:92
    STEP: Given a Pod with a 'name' label pod-adoption is created 12/09/22 08:21:31.395
    Dec  9 08:21:31.402: INFO: Waiting up to 5m0s for pod "pod-adoption" in namespace "replication-controller-2072" to be "running and ready"
    Dec  9 08:21:31.407: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 5.235523ms
    Dec  9 08:21:31.407: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
    Dec  9 08:21:33.411: INFO: Pod "pod-adoption": Phase="Running", Reason="", readiness=true. Elapsed: 2.008826933s
    Dec  9 08:21:33.411: INFO: The phase of Pod pod-adoption is Running (Ready = true)
    Dec  9 08:21:33.411: INFO: Pod "pod-adoption" satisfied condition "running and ready"
    STEP: When a replication controller with a matching selector is created 12/09/22 08:21:33.417
    STEP: Then the orphan pod is adopted 12/09/22 08:21:33.422
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Dec  9 08:21:34.433: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-2072" for this suite. 12/09/22 08:21:34.437
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:177
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 08:21:34.443
Dec  9 08:21:34.443: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename emptydir 12/09/22 08:21:34.444
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:21:34.458
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:21:34.462
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:177
STEP: Creating a pod to test emptydir 0666 on node default medium 12/09/22 08:21:34.467
Dec  9 08:21:34.483: INFO: Waiting up to 5m0s for pod "pod-46b5ca59-d65b-431d-a308-a19fd419c027" in namespace "emptydir-8528" to be "Succeeded or Failed"
Dec  9 08:21:34.489: INFO: Pod "pod-46b5ca59-d65b-431d-a308-a19fd419c027": Phase="Pending", Reason="", readiness=false. Elapsed: 6.080705ms
Dec  9 08:21:36.492: INFO: Pod "pod-46b5ca59-d65b-431d-a308-a19fd419c027": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009418945s
Dec  9 08:21:38.494: INFO: Pod "pod-46b5ca59-d65b-431d-a308-a19fd419c027": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010867995s
STEP: Saw pod success 12/09/22 08:21:38.494
Dec  9 08:21:38.494: INFO: Pod "pod-46b5ca59-d65b-431d-a308-a19fd419c027" satisfied condition "Succeeded or Failed"
Dec  9 08:21:38.497: INFO: Trying to get logs from node ip-10-0-10-179 pod pod-46b5ca59-d65b-431d-a308-a19fd419c027 container test-container: <nil>
STEP: delete the pod 12/09/22 08:21:38.503
Dec  9 08:21:38.513: INFO: Waiting for pod pod-46b5ca59-d65b-431d-a308-a19fd419c027 to disappear
Dec  9 08:21:38.516: INFO: Pod pod-46b5ca59-d65b-431d-a308-a19fd419c027 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Dec  9 08:21:38.516: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-8528" for this suite. 12/09/22 08:21:38.521
------------------------------
â€¢ [4.086 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:177

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 08:21:34.443
    Dec  9 08:21:34.443: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename emptydir 12/09/22 08:21:34.444
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:21:34.458
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:21:34.462
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:177
    STEP: Creating a pod to test emptydir 0666 on node default medium 12/09/22 08:21:34.467
    Dec  9 08:21:34.483: INFO: Waiting up to 5m0s for pod "pod-46b5ca59-d65b-431d-a308-a19fd419c027" in namespace "emptydir-8528" to be "Succeeded or Failed"
    Dec  9 08:21:34.489: INFO: Pod "pod-46b5ca59-d65b-431d-a308-a19fd419c027": Phase="Pending", Reason="", readiness=false. Elapsed: 6.080705ms
    Dec  9 08:21:36.492: INFO: Pod "pod-46b5ca59-d65b-431d-a308-a19fd419c027": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009418945s
    Dec  9 08:21:38.494: INFO: Pod "pod-46b5ca59-d65b-431d-a308-a19fd419c027": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010867995s
    STEP: Saw pod success 12/09/22 08:21:38.494
    Dec  9 08:21:38.494: INFO: Pod "pod-46b5ca59-d65b-431d-a308-a19fd419c027" satisfied condition "Succeeded or Failed"
    Dec  9 08:21:38.497: INFO: Trying to get logs from node ip-10-0-10-179 pod pod-46b5ca59-d65b-431d-a308-a19fd419c027 container test-container: <nil>
    STEP: delete the pod 12/09/22 08:21:38.503
    Dec  9 08:21:38.513: INFO: Waiting for pod pod-46b5ca59-d65b-431d-a308-a19fd419c027 to disappear
    Dec  9 08:21:38.516: INFO: Pod pod-46b5ca59-d65b-431d-a308-a19fd419c027 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Dec  9 08:21:38.516: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-8528" for this suite. 12/09/22 08:21:38.521
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] Watchers
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 08:21:38.531
Dec  9 08:21:38.531: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename watch 12/09/22 08:21:38.532
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:21:38.551
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:21:38.556
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60
STEP: creating a watch on configmaps with label A 12/09/22 08:21:38.56
STEP: creating a watch on configmaps with label B 12/09/22 08:21:38.562
STEP: creating a watch on configmaps with label A or B 12/09/22 08:21:38.564
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification 12/09/22 08:21:38.566
Dec  9 08:21:38.570: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-196  1e337f99-cf9c-48e0-b9ce-227d3793a401 31675 0 2022-12-09 08:21:38 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2022-12-09 08:21:38 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Dec  9 08:21:38.571: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-196  1e337f99-cf9c-48e0-b9ce-227d3793a401 31675 0 2022-12-09 08:21:38 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2022-12-09 08:21:38 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A and ensuring the correct watchers observe the notification 12/09/22 08:21:38.571
Dec  9 08:21:38.579: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-196  1e337f99-cf9c-48e0-b9ce-227d3793a401 31676 0 2022-12-09 08:21:38 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2022-12-09 08:21:38 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Dec  9 08:21:38.580: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-196  1e337f99-cf9c-48e0-b9ce-227d3793a401 31676 0 2022-12-09 08:21:38 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2022-12-09 08:21:38 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification 12/09/22 08:21:38.58
Dec  9 08:21:38.588: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-196  1e337f99-cf9c-48e0-b9ce-227d3793a401 31677 0 2022-12-09 08:21:38 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2022-12-09 08:21:38 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Dec  9 08:21:38.588: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-196  1e337f99-cf9c-48e0-b9ce-227d3793a401 31677 0 2022-12-09 08:21:38 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2022-12-09 08:21:38 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap A and ensuring the correct watchers observe the notification 12/09/22 08:21:38.588
Dec  9 08:21:38.594: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-196  1e337f99-cf9c-48e0-b9ce-227d3793a401 31678 0 2022-12-09 08:21:38 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2022-12-09 08:21:38 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Dec  9 08:21:38.594: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-196  1e337f99-cf9c-48e0-b9ce-227d3793a401 31678 0 2022-12-09 08:21:38 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2022-12-09 08:21:38 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification 12/09/22 08:21:38.594
Dec  9 08:21:38.599: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-196  5db35992-c394-424e-b835-f3464ef0e06c 31679 0 2022-12-09 08:21:38 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2022-12-09 08:21:38 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Dec  9 08:21:38.599: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-196  5db35992-c394-424e-b835-f3464ef0e06c 31679 0 2022-12-09 08:21:38 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2022-12-09 08:21:38 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap B and ensuring the correct watchers observe the notification 12/09/22 08:21:48.6
Dec  9 08:21:48.606: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-196  5db35992-c394-424e-b835-f3464ef0e06c 31719 0 2022-12-09 08:21:38 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2022-12-09 08:21:38 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Dec  9 08:21:48.606: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-196  5db35992-c394-424e-b835-f3464ef0e06c 31719 0 2022-12-09 08:21:38 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2022-12-09 08:21:38 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Dec  9 08:21:58.607: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-196" for this suite. 12/09/22 08:21:58.611
------------------------------
â€¢ [SLOW TEST] [20.088 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 08:21:38.531
    Dec  9 08:21:38.531: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename watch 12/09/22 08:21:38.532
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:21:38.551
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:21:38.556
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should observe add, update, and delete watch notifications on configmaps [Conformance]
      test/e2e/apimachinery/watch.go:60
    STEP: creating a watch on configmaps with label A 12/09/22 08:21:38.56
    STEP: creating a watch on configmaps with label B 12/09/22 08:21:38.562
    STEP: creating a watch on configmaps with label A or B 12/09/22 08:21:38.564
    STEP: creating a configmap with label A and ensuring the correct watchers observe the notification 12/09/22 08:21:38.566
    Dec  9 08:21:38.570: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-196  1e337f99-cf9c-48e0-b9ce-227d3793a401 31675 0 2022-12-09 08:21:38 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2022-12-09 08:21:38 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Dec  9 08:21:38.571: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-196  1e337f99-cf9c-48e0-b9ce-227d3793a401 31675 0 2022-12-09 08:21:38 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2022-12-09 08:21:38 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying configmap A and ensuring the correct watchers observe the notification 12/09/22 08:21:38.571
    Dec  9 08:21:38.579: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-196  1e337f99-cf9c-48e0-b9ce-227d3793a401 31676 0 2022-12-09 08:21:38 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2022-12-09 08:21:38 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    Dec  9 08:21:38.580: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-196  1e337f99-cf9c-48e0-b9ce-227d3793a401 31676 0 2022-12-09 08:21:38 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2022-12-09 08:21:38 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying configmap A again and ensuring the correct watchers observe the notification 12/09/22 08:21:38.58
    Dec  9 08:21:38.588: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-196  1e337f99-cf9c-48e0-b9ce-227d3793a401 31677 0 2022-12-09 08:21:38 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2022-12-09 08:21:38 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Dec  9 08:21:38.588: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-196  1e337f99-cf9c-48e0-b9ce-227d3793a401 31677 0 2022-12-09 08:21:38 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2022-12-09 08:21:38 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: deleting configmap A and ensuring the correct watchers observe the notification 12/09/22 08:21:38.588
    Dec  9 08:21:38.594: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-196  1e337f99-cf9c-48e0-b9ce-227d3793a401 31678 0 2022-12-09 08:21:38 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2022-12-09 08:21:38 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Dec  9 08:21:38.594: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-196  1e337f99-cf9c-48e0-b9ce-227d3793a401 31678 0 2022-12-09 08:21:38 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2022-12-09 08:21:38 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: creating a configmap with label B and ensuring the correct watchers observe the notification 12/09/22 08:21:38.594
    Dec  9 08:21:38.599: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-196  5db35992-c394-424e-b835-f3464ef0e06c 31679 0 2022-12-09 08:21:38 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2022-12-09 08:21:38 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Dec  9 08:21:38.599: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-196  5db35992-c394-424e-b835-f3464ef0e06c 31679 0 2022-12-09 08:21:38 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2022-12-09 08:21:38 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: deleting configmap B and ensuring the correct watchers observe the notification 12/09/22 08:21:48.6
    Dec  9 08:21:48.606: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-196  5db35992-c394-424e-b835-f3464ef0e06c 31719 0 2022-12-09 08:21:38 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2022-12-09 08:21:38 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Dec  9 08:21:48.606: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-196  5db35992-c394-424e-b835-f3464ef0e06c 31719 0 2022-12-09 08:21:38 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2022-12-09 08:21:38 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Dec  9 08:21:58.607: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-196" for this suite. 12/09/22 08:21:58.611
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:217
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 08:21:58.62
Dec  9 08:21:58.620: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename emptydir 12/09/22 08:21:58.621
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:21:58.676
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:21:58.694
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:217
STEP: Creating a pod to test emptydir 0777 on node default medium 12/09/22 08:21:58.703
Dec  9 08:21:58.714: INFO: Waiting up to 5m0s for pod "pod-e4cbbcac-e752-420b-a591-cd4c26071abe" in namespace "emptydir-6511" to be "Succeeded or Failed"
Dec  9 08:21:58.718: INFO: Pod "pod-e4cbbcac-e752-420b-a591-cd4c26071abe": Phase="Pending", Reason="", readiness=false. Elapsed: 3.300879ms
Dec  9 08:22:00.722: INFO: Pod "pod-e4cbbcac-e752-420b-a591-cd4c26071abe": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007278688s
Dec  9 08:22:02.722: INFO: Pod "pod-e4cbbcac-e752-420b-a591-cd4c26071abe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007365041s
STEP: Saw pod success 12/09/22 08:22:02.722
Dec  9 08:22:02.722: INFO: Pod "pod-e4cbbcac-e752-420b-a591-cd4c26071abe" satisfied condition "Succeeded or Failed"
Dec  9 08:22:02.725: INFO: Trying to get logs from node ip-10-0-10-179 pod pod-e4cbbcac-e752-420b-a591-cd4c26071abe container test-container: <nil>
STEP: delete the pod 12/09/22 08:22:02.737
Dec  9 08:22:02.754: INFO: Waiting for pod pod-e4cbbcac-e752-420b-a591-cd4c26071abe to disappear
Dec  9 08:22:02.760: INFO: Pod pod-e4cbbcac-e752-420b-a591-cd4c26071abe no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Dec  9 08:22:02.761: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-6511" for this suite. 12/09/22 08:22:02.766
------------------------------
â€¢ [4.159 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:217

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 08:21:58.62
    Dec  9 08:21:58.620: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename emptydir 12/09/22 08:21:58.621
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:21:58.676
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:21:58.694
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:217
    STEP: Creating a pod to test emptydir 0777 on node default medium 12/09/22 08:21:58.703
    Dec  9 08:21:58.714: INFO: Waiting up to 5m0s for pod "pod-e4cbbcac-e752-420b-a591-cd4c26071abe" in namespace "emptydir-6511" to be "Succeeded or Failed"
    Dec  9 08:21:58.718: INFO: Pod "pod-e4cbbcac-e752-420b-a591-cd4c26071abe": Phase="Pending", Reason="", readiness=false. Elapsed: 3.300879ms
    Dec  9 08:22:00.722: INFO: Pod "pod-e4cbbcac-e752-420b-a591-cd4c26071abe": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007278688s
    Dec  9 08:22:02.722: INFO: Pod "pod-e4cbbcac-e752-420b-a591-cd4c26071abe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007365041s
    STEP: Saw pod success 12/09/22 08:22:02.722
    Dec  9 08:22:02.722: INFO: Pod "pod-e4cbbcac-e752-420b-a591-cd4c26071abe" satisfied condition "Succeeded or Failed"
    Dec  9 08:22:02.725: INFO: Trying to get logs from node ip-10-0-10-179 pod pod-e4cbbcac-e752-420b-a591-cd4c26071abe container test-container: <nil>
    STEP: delete the pod 12/09/22 08:22:02.737
    Dec  9 08:22:02.754: INFO: Waiting for pod pod-e4cbbcac-e752-420b-a591-cd4c26071abe to disappear
    Dec  9 08:22:02.760: INFO: Pod pod-e4cbbcac-e752-420b-a591-cd4c26071abe no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Dec  9 08:22:02.761: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-6511" for this suite. 12/09/22 08:22:02.766
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:261
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 08:22:02.781
Dec  9 08:22:02.781: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename projected 12/09/22 08:22:02.782
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:22:02.815
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:22:02.82
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:261
STEP: Creating a pod to test downward API volume plugin 12/09/22 08:22:02.828
Dec  9 08:22:02.840: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b31f3655-759b-4d9d-b88b-1a5077a189b0" in namespace "projected-7032" to be "Succeeded or Failed"
Dec  9 08:22:02.852: INFO: Pod "downwardapi-volume-b31f3655-759b-4d9d-b88b-1a5077a189b0": Phase="Pending", Reason="", readiness=false. Elapsed: 11.610958ms
Dec  9 08:22:04.857: INFO: Pod "downwardapi-volume-b31f3655-759b-4d9d-b88b-1a5077a189b0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016602939s
Dec  9 08:22:06.857: INFO: Pod "downwardapi-volume-b31f3655-759b-4d9d-b88b-1a5077a189b0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016791922s
STEP: Saw pod success 12/09/22 08:22:06.857
Dec  9 08:22:06.857: INFO: Pod "downwardapi-volume-b31f3655-759b-4d9d-b88b-1a5077a189b0" satisfied condition "Succeeded or Failed"
Dec  9 08:22:06.860: INFO: Trying to get logs from node ip-10-0-10-179 pod downwardapi-volume-b31f3655-759b-4d9d-b88b-1a5077a189b0 container client-container: <nil>
STEP: delete the pod 12/09/22 08:22:06.865
Dec  9 08:22:06.879: INFO: Waiting for pod downwardapi-volume-b31f3655-759b-4d9d-b88b-1a5077a189b0 to disappear
Dec  9 08:22:06.884: INFO: Pod downwardapi-volume-b31f3655-759b-4d9d-b88b-1a5077a189b0 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Dec  9 08:22:06.884: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-7032" for this suite. 12/09/22 08:22:06.898
------------------------------
â€¢ [4.124 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:261

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 08:22:02.781
    Dec  9 08:22:02.781: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename projected 12/09/22 08:22:02.782
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:22:02.815
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:22:02.82
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:261
    STEP: Creating a pod to test downward API volume plugin 12/09/22 08:22:02.828
    Dec  9 08:22:02.840: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b31f3655-759b-4d9d-b88b-1a5077a189b0" in namespace "projected-7032" to be "Succeeded or Failed"
    Dec  9 08:22:02.852: INFO: Pod "downwardapi-volume-b31f3655-759b-4d9d-b88b-1a5077a189b0": Phase="Pending", Reason="", readiness=false. Elapsed: 11.610958ms
    Dec  9 08:22:04.857: INFO: Pod "downwardapi-volume-b31f3655-759b-4d9d-b88b-1a5077a189b0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016602939s
    Dec  9 08:22:06.857: INFO: Pod "downwardapi-volume-b31f3655-759b-4d9d-b88b-1a5077a189b0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016791922s
    STEP: Saw pod success 12/09/22 08:22:06.857
    Dec  9 08:22:06.857: INFO: Pod "downwardapi-volume-b31f3655-759b-4d9d-b88b-1a5077a189b0" satisfied condition "Succeeded or Failed"
    Dec  9 08:22:06.860: INFO: Trying to get logs from node ip-10-0-10-179 pod downwardapi-volume-b31f3655-759b-4d9d-b88b-1a5077a189b0 container client-container: <nil>
    STEP: delete the pod 12/09/22 08:22:06.865
    Dec  9 08:22:06.879: INFO: Waiting for pod downwardapi-volume-b31f3655-759b-4d9d-b88b-1a5077a189b0 to disappear
    Dec  9 08:22:06.884: INFO: Pod downwardapi-volume-b31f3655-759b-4d9d-b88b-1a5077a189b0 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Dec  9 08:22:06.884: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-7032" for this suite. 12/09/22 08:22:06.898
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute prestop http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:212
[BeforeEach] [sig-node] Container Lifecycle Hook
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 08:22:06.906
Dec  9 08:22:06.906: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename container-lifecycle-hook 12/09/22 08:22:06.907
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:22:06.921
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:22:06.926
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:77
STEP: create the container to handle the HTTPGet hook request. 12/09/22 08:22:06.933
Dec  9 08:22:06.941: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-7289" to be "running and ready"
Dec  9 08:22:06.955: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 10.224297ms
Dec  9 08:22:06.955: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Dec  9 08:22:08.960: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.015119645s
Dec  9 08:22:08.961: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Dec  9 08:22:08.961: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:212
STEP: create the pod with lifecycle hook 12/09/22 08:22:08.964
Dec  9 08:22:08.971: INFO: Waiting up to 5m0s for pod "pod-with-prestop-http-hook" in namespace "container-lifecycle-hook-7289" to be "running and ready"
Dec  9 08:22:08.977: INFO: Pod "pod-with-prestop-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 6.74802ms
Dec  9 08:22:08.978: INFO: The phase of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
Dec  9 08:22:10.986: INFO: Pod "pod-with-prestop-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.015402313s
Dec  9 08:22:10.986: INFO: The phase of Pod pod-with-prestop-http-hook is Running (Ready = true)
Dec  9 08:22:10.987: INFO: Pod "pod-with-prestop-http-hook" satisfied condition "running and ready"
STEP: delete the pod with lifecycle hook 12/09/22 08:22:10.991
Dec  9 08:22:10.998: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Dec  9 08:22:11.007: INFO: Pod pod-with-prestop-http-hook still exists
Dec  9 08:22:13.008: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Dec  9 08:22:13.014: INFO: Pod pod-with-prestop-http-hook still exists
Dec  9 08:22:15.008: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Dec  9 08:22:15.013: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook 12/09/22 08:22:15.013
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/node/init/init.go:32
Dec  9 08:22:15.031: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  tear down framework | framework.go:193
STEP: Destroying namespace "container-lifecycle-hook-7289" for this suite. 12/09/22 08:22:15.034
------------------------------
â€¢ [SLOW TEST] [8.134 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute prestop http hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:212

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 08:22:06.906
    Dec  9 08:22:06.906: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename container-lifecycle-hook 12/09/22 08:22:06.907
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:22:06.921
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:22:06.926
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:77
    STEP: create the container to handle the HTTPGet hook request. 12/09/22 08:22:06.933
    Dec  9 08:22:06.941: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-7289" to be "running and ready"
    Dec  9 08:22:06.955: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 10.224297ms
    Dec  9 08:22:06.955: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Dec  9 08:22:08.960: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.015119645s
    Dec  9 08:22:08.961: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Dec  9 08:22:08.961: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute prestop http hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:212
    STEP: create the pod with lifecycle hook 12/09/22 08:22:08.964
    Dec  9 08:22:08.971: INFO: Waiting up to 5m0s for pod "pod-with-prestop-http-hook" in namespace "container-lifecycle-hook-7289" to be "running and ready"
    Dec  9 08:22:08.977: INFO: Pod "pod-with-prestop-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 6.74802ms
    Dec  9 08:22:08.978: INFO: The phase of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
    Dec  9 08:22:10.986: INFO: Pod "pod-with-prestop-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.015402313s
    Dec  9 08:22:10.986: INFO: The phase of Pod pod-with-prestop-http-hook is Running (Ready = true)
    Dec  9 08:22:10.987: INFO: Pod "pod-with-prestop-http-hook" satisfied condition "running and ready"
    STEP: delete the pod with lifecycle hook 12/09/22 08:22:10.991
    Dec  9 08:22:10.998: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Dec  9 08:22:11.007: INFO: Pod pod-with-prestop-http-hook still exists
    Dec  9 08:22:13.008: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Dec  9 08:22:13.014: INFO: Pod pod-with-prestop-http-hook still exists
    Dec  9 08:22:15.008: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Dec  9 08:22:15.013: INFO: Pod pod-with-prestop-http-hook no longer exists
    STEP: check prestop hook 12/09/22 08:22:15.013
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/node/init/init.go:32
    Dec  9 08:22:15.031: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-lifecycle-hook-7289" for this suite. 12/09/22 08:22:15.034
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:239
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 08:22:15.042
Dec  9 08:22:15.042: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename webhook 12/09/22 08:22:15.043
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:22:15.057
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:22:15.062
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 12/09/22 08:22:15.084
STEP: Create role binding to let webhook read extension-apiserver-authentication 12/09/22 08:22:15.825
STEP: Deploying the webhook pod 12/09/22 08:22:15.836
STEP: Wait for the deployment to be ready 12/09/22 08:22:15.847
Dec  9 08:22:15.865: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 12/09/22 08:22:17.874
STEP: Verifying the service has paired with the endpoint 12/09/22 08:22:17.889
Dec  9 08:22:18.890: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:239
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API 12/09/22 08:22:18.893
STEP: create a namespace for the webhook 12/09/22 08:22:18.906
STEP: create a configmap should be unconditionally rejected by the webhook 12/09/22 08:22:18.912
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Dec  9 08:22:18.937: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-8506" for this suite. 12/09/22 08:22:18.995
STEP: Destroying namespace "webhook-8506-markers" for this suite. 12/09/22 08:22:19.004
------------------------------
â€¢ [3.972 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:239

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 08:22:15.042
    Dec  9 08:22:15.042: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename webhook 12/09/22 08:22:15.043
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:22:15.057
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:22:15.062
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 12/09/22 08:22:15.084
    STEP: Create role binding to let webhook read extension-apiserver-authentication 12/09/22 08:22:15.825
    STEP: Deploying the webhook pod 12/09/22 08:22:15.836
    STEP: Wait for the deployment to be ready 12/09/22 08:22:15.847
    Dec  9 08:22:15.865: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 12/09/22 08:22:17.874
    STEP: Verifying the service has paired with the endpoint 12/09/22 08:22:17.889
    Dec  9 08:22:18.890: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should unconditionally reject operations on fail closed webhook [Conformance]
      test/e2e/apimachinery/webhook.go:239
    STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API 12/09/22 08:22:18.893
    STEP: create a namespace for the webhook 12/09/22 08:22:18.906
    STEP: create a configmap should be unconditionally rejected by the webhook 12/09/22 08:22:18.912
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Dec  9 08:22:18.937: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-8506" for this suite. 12/09/22 08:22:18.995
    STEP: Destroying namespace "webhook-8506-markers" for this suite. 12/09/22 08:22:19.004
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a read only busybox container
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:184
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 08:22:19.018
Dec  9 08:22:19.018: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename kubelet-test 12/09/22 08:22:19.019
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:22:19.033
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:22:19.038
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:184
Dec  9 08:22:19.051: INFO: Waiting up to 5m0s for pod "busybox-readonly-fsbcf13df7-b18d-45f9-bcbc-787de27beb10" in namespace "kubelet-test-1334" to be "running and ready"
Dec  9 08:22:19.057: INFO: Pod "busybox-readonly-fsbcf13df7-b18d-45f9-bcbc-787de27beb10": Phase="Pending", Reason="", readiness=false. Elapsed: 5.800638ms
Dec  9 08:22:19.057: INFO: The phase of Pod busybox-readonly-fsbcf13df7-b18d-45f9-bcbc-787de27beb10 is Pending, waiting for it to be Running (with Ready = true)
Dec  9 08:22:21.061: INFO: Pod "busybox-readonly-fsbcf13df7-b18d-45f9-bcbc-787de27beb10": Phase="Running", Reason="", readiness=true. Elapsed: 2.009635792s
Dec  9 08:22:21.061: INFO: The phase of Pod busybox-readonly-fsbcf13df7-b18d-45f9-bcbc-787de27beb10 is Running (Ready = true)
Dec  9 08:22:21.061: INFO: Pod "busybox-readonly-fsbcf13df7-b18d-45f9-bcbc-787de27beb10" satisfied condition "running and ready"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Dec  9 08:22:21.069: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-1334" for this suite. 12/09/22 08:22:21.073
------------------------------
â€¢ [2.061 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a read only busybox container
  test/e2e/common/node/kubelet.go:175
    should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 08:22:19.018
    Dec  9 08:22:19.018: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename kubelet-test 12/09/22 08:22:19.019
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:22:19.033
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:22:19.038
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:184
    Dec  9 08:22:19.051: INFO: Waiting up to 5m0s for pod "busybox-readonly-fsbcf13df7-b18d-45f9-bcbc-787de27beb10" in namespace "kubelet-test-1334" to be "running and ready"
    Dec  9 08:22:19.057: INFO: Pod "busybox-readonly-fsbcf13df7-b18d-45f9-bcbc-787de27beb10": Phase="Pending", Reason="", readiness=false. Elapsed: 5.800638ms
    Dec  9 08:22:19.057: INFO: The phase of Pod busybox-readonly-fsbcf13df7-b18d-45f9-bcbc-787de27beb10 is Pending, waiting for it to be Running (with Ready = true)
    Dec  9 08:22:21.061: INFO: Pod "busybox-readonly-fsbcf13df7-b18d-45f9-bcbc-787de27beb10": Phase="Running", Reason="", readiness=true. Elapsed: 2.009635792s
    Dec  9 08:22:21.061: INFO: The phase of Pod busybox-readonly-fsbcf13df7-b18d-45f9-bcbc-787de27beb10 is Running (Ready = true)
    Dec  9 08:22:21.061: INFO: Pod "busybox-readonly-fsbcf13df7-b18d-45f9-bcbc-787de27beb10" satisfied condition "running and ready"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Dec  9 08:22:21.069: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-1334" for this suite. 12/09/22 08:22:21.073
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] Pods
  should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:226
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 08:22:21.08
Dec  9 08:22:21.080: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename pods 12/09/22 08:22:21.08
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:22:21.098
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:22:21.102
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:226
STEP: creating the pod 12/09/22 08:22:21.106
STEP: setting up watch 12/09/22 08:22:21.107
STEP: submitting the pod to kubernetes 12/09/22 08:22:21.21
STEP: verifying the pod is in kubernetes 12/09/22 08:22:21.218
STEP: verifying pod creation was observed 12/09/22 08:22:21.223
Dec  9 08:22:21.223: INFO: Waiting up to 5m0s for pod "pod-submit-remove-d06a631b-d43d-4e9c-885e-6964e9b1b8ec" in namespace "pods-938" to be "running"
Dec  9 08:22:21.228: INFO: Pod "pod-submit-remove-d06a631b-d43d-4e9c-885e-6964e9b1b8ec": Phase="Pending", Reason="", readiness=false. Elapsed: 5.01237ms
Dec  9 08:22:23.232: INFO: Pod "pod-submit-remove-d06a631b-d43d-4e9c-885e-6964e9b1b8ec": Phase="Running", Reason="", readiness=true. Elapsed: 2.009458726s
Dec  9 08:22:23.233: INFO: Pod "pod-submit-remove-d06a631b-d43d-4e9c-885e-6964e9b1b8ec" satisfied condition "running"
STEP: deleting the pod gracefully 12/09/22 08:22:23.236
STEP: verifying pod deletion was observed 12/09/22 08:22:23.247
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Dec  9 08:22:25.770: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-938" for this suite. 12/09/22 08:22:25.774
------------------------------
â€¢ [4.700 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:226

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 08:22:21.08
    Dec  9 08:22:21.080: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename pods 12/09/22 08:22:21.08
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:22:21.098
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:22:21.102
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should be submitted and removed [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:226
    STEP: creating the pod 12/09/22 08:22:21.106
    STEP: setting up watch 12/09/22 08:22:21.107
    STEP: submitting the pod to kubernetes 12/09/22 08:22:21.21
    STEP: verifying the pod is in kubernetes 12/09/22 08:22:21.218
    STEP: verifying pod creation was observed 12/09/22 08:22:21.223
    Dec  9 08:22:21.223: INFO: Waiting up to 5m0s for pod "pod-submit-remove-d06a631b-d43d-4e9c-885e-6964e9b1b8ec" in namespace "pods-938" to be "running"
    Dec  9 08:22:21.228: INFO: Pod "pod-submit-remove-d06a631b-d43d-4e9c-885e-6964e9b1b8ec": Phase="Pending", Reason="", readiness=false. Elapsed: 5.01237ms
    Dec  9 08:22:23.232: INFO: Pod "pod-submit-remove-d06a631b-d43d-4e9c-885e-6964e9b1b8ec": Phase="Running", Reason="", readiness=true. Elapsed: 2.009458726s
    Dec  9 08:22:23.233: INFO: Pod "pod-submit-remove-d06a631b-d43d-4e9c-885e-6964e9b1b8ec" satisfied condition "running"
    STEP: deleting the pod gracefully 12/09/22 08:22:23.236
    STEP: verifying pod deletion was observed 12/09/22 08:22:23.247
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Dec  9 08:22:25.770: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-938" for this suite. 12/09/22 08:22:25.774
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 08:22:25.781
Dec  9 08:22:25.781: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename dns 12/09/22 08:22:25.784
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:22:25.798
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:22:25.802
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-6176.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-6176.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done
 12/09/22 08:22:25.806
STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-6176.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-6176.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done
 12/09/22 08:22:25.806
STEP: creating a pod to probe /etc/hosts 12/09/22 08:22:25.807
STEP: submitting the pod to kubernetes 12/09/22 08:22:25.807
Dec  9 08:22:25.815: INFO: Waiting up to 15m0s for pod "dns-test-c64c5e04-cbb0-4d33-a88b-dc51ff3cffa3" in namespace "dns-6176" to be "running"
Dec  9 08:22:25.821: INFO: Pod "dns-test-c64c5e04-cbb0-4d33-a88b-dc51ff3cffa3": Phase="Pending", Reason="", readiness=false. Elapsed: 5.313708ms
Dec  9 08:22:27.828: INFO: Pod "dns-test-c64c5e04-cbb0-4d33-a88b-dc51ff3cffa3": Phase="Running", Reason="", readiness=true. Elapsed: 2.012646011s
Dec  9 08:22:27.828: INFO: Pod "dns-test-c64c5e04-cbb0-4d33-a88b-dc51ff3cffa3" satisfied condition "running"
STEP: retrieving the pod 12/09/22 08:22:27.828
STEP: looking for the results for each expected name from probers 12/09/22 08:22:27.831
Dec  9 08:22:27.846: INFO: DNS probes using dns-6176/dns-test-c64c5e04-cbb0-4d33-a88b-dc51ff3cffa3 succeeded

STEP: deleting the pod 12/09/22 08:22:27.846
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Dec  9 08:22:27.856: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-6176" for this suite. 12/09/22 08:22:27.86
------------------------------
â€¢ [2.084 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 08:22:25.781
    Dec  9 08:22:25.781: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename dns 12/09/22 08:22:25.784
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:22:25.798
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:22:25.802
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide /etc/hosts entries for the cluster [Conformance]
      test/e2e/network/dns.go:117
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-6176.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-6176.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done
     12/09/22 08:22:25.806
    STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-6176.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-6176.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done
     12/09/22 08:22:25.806
    STEP: creating a pod to probe /etc/hosts 12/09/22 08:22:25.807
    STEP: submitting the pod to kubernetes 12/09/22 08:22:25.807
    Dec  9 08:22:25.815: INFO: Waiting up to 15m0s for pod "dns-test-c64c5e04-cbb0-4d33-a88b-dc51ff3cffa3" in namespace "dns-6176" to be "running"
    Dec  9 08:22:25.821: INFO: Pod "dns-test-c64c5e04-cbb0-4d33-a88b-dc51ff3cffa3": Phase="Pending", Reason="", readiness=false. Elapsed: 5.313708ms
    Dec  9 08:22:27.828: INFO: Pod "dns-test-c64c5e04-cbb0-4d33-a88b-dc51ff3cffa3": Phase="Running", Reason="", readiness=true. Elapsed: 2.012646011s
    Dec  9 08:22:27.828: INFO: Pod "dns-test-c64c5e04-cbb0-4d33-a88b-dc51ff3cffa3" satisfied condition "running"
    STEP: retrieving the pod 12/09/22 08:22:27.828
    STEP: looking for the results for each expected name from probers 12/09/22 08:22:27.831
    Dec  9 08:22:27.846: INFO: DNS probes using dns-6176/dns-test-c64c5e04-cbb0-4d33-a88b-dc51ff3cffa3 succeeded

    STEP: deleting the pod 12/09/22 08:22:27.846
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Dec  9 08:22:27.856: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-6176" for this suite. 12/09/22 08:22:27.86
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Secrets
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:99
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 08:22:27.866
Dec  9 08:22:27.867: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename secrets 12/09/22 08:22:27.867
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:22:27.88
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:22:27.885
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:99
STEP: Creating secret with name secret-test-a1a010aa-0d56-457a-bffd-1011d8135e76 12/09/22 08:22:27.908
STEP: Creating a pod to test consume secrets 12/09/22 08:22:27.911
Dec  9 08:22:27.917: INFO: Waiting up to 5m0s for pod "pod-secrets-614f3ee1-7318-4962-832e-9f10ef1feb6a" in namespace "secrets-3867" to be "Succeeded or Failed"
Dec  9 08:22:27.920: INFO: Pod "pod-secrets-614f3ee1-7318-4962-832e-9f10ef1feb6a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.787186ms
Dec  9 08:22:29.924: INFO: Pod "pod-secrets-614f3ee1-7318-4962-832e-9f10ef1feb6a": Phase="Running", Reason="", readiness=false. Elapsed: 2.006221763s
Dec  9 08:22:31.924: INFO: Pod "pod-secrets-614f3ee1-7318-4962-832e-9f10ef1feb6a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006235335s
STEP: Saw pod success 12/09/22 08:22:31.924
Dec  9 08:22:31.924: INFO: Pod "pod-secrets-614f3ee1-7318-4962-832e-9f10ef1feb6a" satisfied condition "Succeeded or Failed"
Dec  9 08:22:31.927: INFO: Trying to get logs from node ip-10-0-10-179 pod pod-secrets-614f3ee1-7318-4962-832e-9f10ef1feb6a container secret-volume-test: <nil>
STEP: delete the pod 12/09/22 08:22:31.933
Dec  9 08:22:31.946: INFO: Waiting for pod pod-secrets-614f3ee1-7318-4962-832e-9f10ef1feb6a to disappear
Dec  9 08:22:31.950: INFO: Pod pod-secrets-614f3ee1-7318-4962-832e-9f10ef1feb6a no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Dec  9 08:22:31.950: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-3867" for this suite. 12/09/22 08:22:31.953
STEP: Destroying namespace "secret-namespace-4038" for this suite. 12/09/22 08:22:31.959
------------------------------
â€¢ [4.098 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:99

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 08:22:27.866
    Dec  9 08:22:27.867: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename secrets 12/09/22 08:22:27.867
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:22:27.88
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:22:27.885
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:99
    STEP: Creating secret with name secret-test-a1a010aa-0d56-457a-bffd-1011d8135e76 12/09/22 08:22:27.908
    STEP: Creating a pod to test consume secrets 12/09/22 08:22:27.911
    Dec  9 08:22:27.917: INFO: Waiting up to 5m0s for pod "pod-secrets-614f3ee1-7318-4962-832e-9f10ef1feb6a" in namespace "secrets-3867" to be "Succeeded or Failed"
    Dec  9 08:22:27.920: INFO: Pod "pod-secrets-614f3ee1-7318-4962-832e-9f10ef1feb6a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.787186ms
    Dec  9 08:22:29.924: INFO: Pod "pod-secrets-614f3ee1-7318-4962-832e-9f10ef1feb6a": Phase="Running", Reason="", readiness=false. Elapsed: 2.006221763s
    Dec  9 08:22:31.924: INFO: Pod "pod-secrets-614f3ee1-7318-4962-832e-9f10ef1feb6a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006235335s
    STEP: Saw pod success 12/09/22 08:22:31.924
    Dec  9 08:22:31.924: INFO: Pod "pod-secrets-614f3ee1-7318-4962-832e-9f10ef1feb6a" satisfied condition "Succeeded or Failed"
    Dec  9 08:22:31.927: INFO: Trying to get logs from node ip-10-0-10-179 pod pod-secrets-614f3ee1-7318-4962-832e-9f10ef1feb6a container secret-volume-test: <nil>
    STEP: delete the pod 12/09/22 08:22:31.933
    Dec  9 08:22:31.946: INFO: Waiting for pod pod-secrets-614f3ee1-7318-4962-832e-9f10ef1feb6a to disappear
    Dec  9 08:22:31.950: INFO: Pod pod-secrets-614f3ee1-7318-4962-832e-9f10ef1feb6a no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Dec  9 08:22:31.950: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-3867" for this suite. 12/09/22 08:22:31.953
    STEP: Destroying namespace "secret-namespace-4038" for this suite. 12/09/22 08:22:31.959
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  test/e2e/apps/statefulset.go:317
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 08:22:31.969
Dec  9 08:22:31.969: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename statefulset 12/09/22 08:22:31.971
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:22:31.985
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:22:31.992
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-590 12/09/22 08:22:31.996
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  test/e2e/apps/statefulset.go:317
STEP: Creating a new StatefulSet 12/09/22 08:22:32.002
Dec  9 08:22:32.016: INFO: Found 0 stateful pods, waiting for 3
Dec  9 08:22:42.021: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Dec  9 08:22:42.021: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Dec  9 08:22:42.021: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 12/09/22 08:22:42.033
Dec  9 08:22:42.052: INFO: Updating stateful set ss2
STEP: Creating a new revision 12/09/22 08:22:42.052
STEP: Not applying an update when the partition is greater than the number of replicas 12/09/22 08:22:52.067
STEP: Performing a canary update 12/09/22 08:22:52.067
Dec  9 08:22:52.085: INFO: Updating stateful set ss2
Dec  9 08:22:52.098: INFO: Waiting for Pod statefulset-590/ss2-2 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
STEP: Restoring Pods to the correct revision when they are deleted 12/09/22 08:23:02.121
Dec  9 08:23:02.430: INFO: Found 1 stateful pods, waiting for 3
Dec  9 08:23:12.437: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Dec  9 08:23:12.437: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Dec  9 08:23:12.437: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update 12/09/22 08:23:12.443
Dec  9 08:23:12.464: INFO: Updating stateful set ss2
Dec  9 08:23:12.477: INFO: Waiting for Pod statefulset-590/ss2-1 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
Dec  9 08:23:22.514: INFO: Updating stateful set ss2
Dec  9 08:23:22.521: INFO: Waiting for StatefulSet statefulset-590/ss2 to complete update
Dec  9 08:23:22.521: INFO: Waiting for Pod statefulset-590/ss2-0 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Dec  9 08:23:32.529: INFO: Deleting all statefulset in ns statefulset-590
Dec  9 08:23:32.532: INFO: Scaling statefulset ss2 to 0
Dec  9 08:23:42.552: INFO: Waiting for statefulset status.replicas updated to 0
Dec  9 08:23:42.555: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Dec  9 08:23:42.577: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-590" for this suite. 12/09/22 08:23:42.593
------------------------------
â€¢ [SLOW TEST] [70.631 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    test/e2e/apps/statefulset.go:317

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 08:22:31.969
    Dec  9 08:22:31.969: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename statefulset 12/09/22 08:22:31.971
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:22:31.985
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:22:31.992
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-590 12/09/22 08:22:31.996
    [It] should perform canary updates and phased rolling updates of template modifications [Conformance]
      test/e2e/apps/statefulset.go:317
    STEP: Creating a new StatefulSet 12/09/22 08:22:32.002
    Dec  9 08:22:32.016: INFO: Found 0 stateful pods, waiting for 3
    Dec  9 08:22:42.021: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Dec  9 08:22:42.021: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Dec  9 08:22:42.021: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Updating stateful set template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 12/09/22 08:22:42.033
    Dec  9 08:22:42.052: INFO: Updating stateful set ss2
    STEP: Creating a new revision 12/09/22 08:22:42.052
    STEP: Not applying an update when the partition is greater than the number of replicas 12/09/22 08:22:52.067
    STEP: Performing a canary update 12/09/22 08:22:52.067
    Dec  9 08:22:52.085: INFO: Updating stateful set ss2
    Dec  9 08:22:52.098: INFO: Waiting for Pod statefulset-590/ss2-2 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
    STEP: Restoring Pods to the correct revision when they are deleted 12/09/22 08:23:02.121
    Dec  9 08:23:02.430: INFO: Found 1 stateful pods, waiting for 3
    Dec  9 08:23:12.437: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Dec  9 08:23:12.437: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Dec  9 08:23:12.437: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Performing a phased rolling update 12/09/22 08:23:12.443
    Dec  9 08:23:12.464: INFO: Updating stateful set ss2
    Dec  9 08:23:12.477: INFO: Waiting for Pod statefulset-590/ss2-1 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
    Dec  9 08:23:22.514: INFO: Updating stateful set ss2
    Dec  9 08:23:22.521: INFO: Waiting for StatefulSet statefulset-590/ss2 to complete update
    Dec  9 08:23:22.521: INFO: Waiting for Pod statefulset-590/ss2-0 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Dec  9 08:23:32.529: INFO: Deleting all statefulset in ns statefulset-590
    Dec  9 08:23:32.532: INFO: Scaling statefulset ss2 to 0
    Dec  9 08:23:42.552: INFO: Waiting for statefulset status.replicas updated to 0
    Dec  9 08:23:42.555: INFO: Deleting statefulset ss2
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Dec  9 08:23:42.577: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-590" for this suite. 12/09/22 08:23:42.593
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:291
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 08:23:42.601
Dec  9 08:23:42.601: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename webhook 12/09/22 08:23:42.602
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:23:42.622
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:23:42.626
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 12/09/22 08:23:42.644
STEP: Create role binding to let webhook read extension-apiserver-authentication 12/09/22 08:23:42.807
STEP: Deploying the webhook pod 12/09/22 08:23:42.814
STEP: Wait for the deployment to be ready 12/09/22 08:23:42.824
Dec  9 08:23:42.834: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 12/09/22 08:23:44.845
STEP: Verifying the service has paired with the endpoint 12/09/22 08:23:44.858
Dec  9 08:23:45.859: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:291
Dec  9 08:23:45.862: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-175-crds.webhook.example.com via the AdmissionRegistration API 12/09/22 08:23:46.38
STEP: Creating a custom resource that should be mutated by the webhook 12/09/22 08:23:46.403
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Dec  9 08:23:48.986: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-6518" for this suite. 12/09/22 08:23:49.026
STEP: Destroying namespace "webhook-6518-markers" for this suite. 12/09/22 08:23:49.033
------------------------------
â€¢ [SLOW TEST] [6.573 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:291

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 08:23:42.601
    Dec  9 08:23:42.601: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename webhook 12/09/22 08:23:42.602
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:23:42.622
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:23:42.626
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 12/09/22 08:23:42.644
    STEP: Create role binding to let webhook read extension-apiserver-authentication 12/09/22 08:23:42.807
    STEP: Deploying the webhook pod 12/09/22 08:23:42.814
    STEP: Wait for the deployment to be ready 12/09/22 08:23:42.824
    Dec  9 08:23:42.834: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 12/09/22 08:23:44.845
    STEP: Verifying the service has paired with the endpoint 12/09/22 08:23:44.858
    Dec  9 08:23:45.859: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource [Conformance]
      test/e2e/apimachinery/webhook.go:291
    Dec  9 08:23:45.862: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-175-crds.webhook.example.com via the AdmissionRegistration API 12/09/22 08:23:46.38
    STEP: Creating a custom resource that should be mutated by the webhook 12/09/22 08:23:46.403
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Dec  9 08:23:48.986: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-6518" for this suite. 12/09/22 08:23:49.026
    STEP: Destroying namespace "webhook-6518-markers" for this suite. 12/09/22 08:23:49.033
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:152
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 08:23:49.177
Dec  9 08:23:49.177: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename var-expansion 12/09/22 08:23:49.178
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:23:49.327
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:23:49.337
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:152
Dec  9 08:23:49.352: INFO: Waiting up to 2m0s for pod "var-expansion-b8807301-8b4a-488e-b585-0c77f8dd759c" in namespace "var-expansion-2527" to be "container 0 failed with reason CreateContainerConfigError"
Dec  9 08:23:49.359: INFO: Pod "var-expansion-b8807301-8b4a-488e-b585-0c77f8dd759c": Phase="Pending", Reason="", readiness=false. Elapsed: 7.065127ms
Dec  9 08:23:51.364: INFO: Pod "var-expansion-b8807301-8b4a-488e-b585-0c77f8dd759c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011862734s
Dec  9 08:23:51.364: INFO: Pod "var-expansion-b8807301-8b4a-488e-b585-0c77f8dd759c" satisfied condition "container 0 failed with reason CreateContainerConfigError"
Dec  9 08:23:51.364: INFO: Deleting pod "var-expansion-b8807301-8b4a-488e-b585-0c77f8dd759c" in namespace "var-expansion-2527"
Dec  9 08:23:51.377: INFO: Wait up to 5m0s for pod "var-expansion-b8807301-8b4a-488e-b585-0c77f8dd759c" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Dec  9 08:23:55.384: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-2527" for this suite. 12/09/22 08:23:55.387
------------------------------
â€¢ [SLOW TEST] [6.215 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:152

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 08:23:49.177
    Dec  9 08:23:49.177: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename var-expansion 12/09/22 08:23:49.178
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:23:49.327
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:23:49.337
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
      test/e2e/common/node/expansion.go:152
    Dec  9 08:23:49.352: INFO: Waiting up to 2m0s for pod "var-expansion-b8807301-8b4a-488e-b585-0c77f8dd759c" in namespace "var-expansion-2527" to be "container 0 failed with reason CreateContainerConfigError"
    Dec  9 08:23:49.359: INFO: Pod "var-expansion-b8807301-8b4a-488e-b585-0c77f8dd759c": Phase="Pending", Reason="", readiness=false. Elapsed: 7.065127ms
    Dec  9 08:23:51.364: INFO: Pod "var-expansion-b8807301-8b4a-488e-b585-0c77f8dd759c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011862734s
    Dec  9 08:23:51.364: INFO: Pod "var-expansion-b8807301-8b4a-488e-b585-0c77f8dd759c" satisfied condition "container 0 failed with reason CreateContainerConfigError"
    Dec  9 08:23:51.364: INFO: Deleting pod "var-expansion-b8807301-8b4a-488e-b585-0c77f8dd759c" in namespace "var-expansion-2527"
    Dec  9 08:23:51.377: INFO: Wait up to 5m0s for pod "var-expansion-b8807301-8b4a-488e-b585-0c77f8dd759c" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Dec  9 08:23:55.384: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-2527" for this suite. 12/09/22 08:23:55.387
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:73
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 08:23:55.395
Dec  9 08:23:55.395: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename var-expansion 12/09/22 08:23:55.396
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:23:55.412
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:23:55.416
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:73
STEP: Creating a pod to test substitution in container's command 12/09/22 08:23:55.419
Dec  9 08:23:55.428: INFO: Waiting up to 5m0s for pod "var-expansion-9a8aa8f7-040c-40f0-b632-3d5b34dd14e2" in namespace "var-expansion-3862" to be "Succeeded or Failed"
Dec  9 08:23:55.434: INFO: Pod "var-expansion-9a8aa8f7-040c-40f0-b632-3d5b34dd14e2": Phase="Pending", Reason="", readiness=false. Elapsed: 6.111428ms
Dec  9 08:23:57.437: INFO: Pod "var-expansion-9a8aa8f7-040c-40f0-b632-3d5b34dd14e2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009535572s
Dec  9 08:23:59.438: INFO: Pod "var-expansion-9a8aa8f7-040c-40f0-b632-3d5b34dd14e2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010521307s
STEP: Saw pod success 12/09/22 08:23:59.438
Dec  9 08:23:59.439: INFO: Pod "var-expansion-9a8aa8f7-040c-40f0-b632-3d5b34dd14e2" satisfied condition "Succeeded or Failed"
Dec  9 08:23:59.442: INFO: Trying to get logs from node ip-10-0-10-179 pod var-expansion-9a8aa8f7-040c-40f0-b632-3d5b34dd14e2 container dapi-container: <nil>
STEP: delete the pod 12/09/22 08:23:59.449
Dec  9 08:23:59.460: INFO: Waiting for pod var-expansion-9a8aa8f7-040c-40f0-b632-3d5b34dd14e2 to disappear
Dec  9 08:23:59.463: INFO: Pod var-expansion-9a8aa8f7-040c-40f0-b632-3d5b34dd14e2 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Dec  9 08:23:59.463: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-3862" for this suite. 12/09/22 08:23:59.467
------------------------------
â€¢ [4.078 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:73

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 08:23:55.395
    Dec  9 08:23:55.395: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename var-expansion 12/09/22 08:23:55.396
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:23:55.412
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:23:55.416
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow substituting values in a container's command [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:73
    STEP: Creating a pod to test substitution in container's command 12/09/22 08:23:55.419
    Dec  9 08:23:55.428: INFO: Waiting up to 5m0s for pod "var-expansion-9a8aa8f7-040c-40f0-b632-3d5b34dd14e2" in namespace "var-expansion-3862" to be "Succeeded or Failed"
    Dec  9 08:23:55.434: INFO: Pod "var-expansion-9a8aa8f7-040c-40f0-b632-3d5b34dd14e2": Phase="Pending", Reason="", readiness=false. Elapsed: 6.111428ms
    Dec  9 08:23:57.437: INFO: Pod "var-expansion-9a8aa8f7-040c-40f0-b632-3d5b34dd14e2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009535572s
    Dec  9 08:23:59.438: INFO: Pod "var-expansion-9a8aa8f7-040c-40f0-b632-3d5b34dd14e2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010521307s
    STEP: Saw pod success 12/09/22 08:23:59.438
    Dec  9 08:23:59.439: INFO: Pod "var-expansion-9a8aa8f7-040c-40f0-b632-3d5b34dd14e2" satisfied condition "Succeeded or Failed"
    Dec  9 08:23:59.442: INFO: Trying to get logs from node ip-10-0-10-179 pod var-expansion-9a8aa8f7-040c-40f0-b632-3d5b34dd14e2 container dapi-container: <nil>
    STEP: delete the pod 12/09/22 08:23:59.449
    Dec  9 08:23:59.460: INFO: Waiting for pod var-expansion-9a8aa8f7-040c-40f0-b632-3d5b34dd14e2 to disappear
    Dec  9 08:23:59.463: INFO: Pod var-expansion-9a8aa8f7-040c-40f0-b632-3d5b34dd14e2 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Dec  9 08:23:59.463: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-3862" for this suite. 12/09/22 08:23:59.467
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-network] Services
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2213
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 08:23:59.475
Dec  9 08:23:59.475: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename services 12/09/22 08:23:59.477
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:23:59.498
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:23:59.502
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2213
STEP: creating service in namespace services-5860 12/09/22 08:23:59.505
STEP: creating service affinity-clusterip-transition in namespace services-5860 12/09/22 08:23:59.506
STEP: creating replication controller affinity-clusterip-transition in namespace services-5860 12/09/22 08:23:59.52
I1209 08:23:59.540859      20 runners.go:193] Created replication controller with name: affinity-clusterip-transition, namespace: services-5860, replica count: 3
I1209 08:24:02.591869      20 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Dec  9 08:24:02.605: INFO: Creating new exec pod
Dec  9 08:24:02.612: INFO: Waiting up to 5m0s for pod "execpod-affinityqvsb7" in namespace "services-5860" to be "running"
Dec  9 08:24:02.627: INFO: Pod "execpod-affinityqvsb7": Phase="Pending", Reason="", readiness=false. Elapsed: 15.0352ms
Dec  9 08:24:04.632: INFO: Pod "execpod-affinityqvsb7": Phase="Running", Reason="", readiness=true. Elapsed: 2.01970774s
Dec  9 08:24:04.632: INFO: Pod "execpod-affinityqvsb7" satisfied condition "running"
Dec  9 08:24:05.636: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=services-5860 exec execpod-affinityqvsb7 -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip-transition 80'
Dec  9 08:24:05.906: INFO: stderr: "+ nc -v -z -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
Dec  9 08:24:05.906: INFO: stdout: ""
Dec  9 08:24:05.906: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=services-5860 exec execpod-affinityqvsb7 -- /bin/sh -x -c nc -v -z -w 2 10.3.221.68 80'
Dec  9 08:24:06.108: INFO: stderr: "+ nc -v -z -w 2 10.3.221.68 80\nConnection to 10.3.221.68 80 port [tcp/http] succeeded!\n"
Dec  9 08:24:06.108: INFO: stdout: ""
Dec  9 08:24:06.117: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=services-5860 exec execpod-affinityqvsb7 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.3.221.68:80/ ; done'
Dec  9 08:24:06.521: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.221.68:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.221.68:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.221.68:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.221.68:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.221.68:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.221.68:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.221.68:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.221.68:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.221.68:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.221.68:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.221.68:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.221.68:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.221.68:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.221.68:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.221.68:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.221.68:80/\n"
Dec  9 08:24:06.521: INFO: stdout: "\naffinity-clusterip-transition-jjhxc\naffinity-clusterip-transition-h85nx\naffinity-clusterip-transition-bgxpl\naffinity-clusterip-transition-jjhxc\naffinity-clusterip-transition-h85nx\naffinity-clusterip-transition-bgxpl\naffinity-clusterip-transition-jjhxc\naffinity-clusterip-transition-h85nx\naffinity-clusterip-transition-bgxpl\naffinity-clusterip-transition-jjhxc\naffinity-clusterip-transition-h85nx\naffinity-clusterip-transition-bgxpl\naffinity-clusterip-transition-jjhxc\naffinity-clusterip-transition-h85nx\naffinity-clusterip-transition-bgxpl\naffinity-clusterip-transition-jjhxc"
Dec  9 08:24:06.521: INFO: Received response from host: affinity-clusterip-transition-jjhxc
Dec  9 08:24:06.521: INFO: Received response from host: affinity-clusterip-transition-h85nx
Dec  9 08:24:06.521: INFO: Received response from host: affinity-clusterip-transition-bgxpl
Dec  9 08:24:06.521: INFO: Received response from host: affinity-clusterip-transition-jjhxc
Dec  9 08:24:06.521: INFO: Received response from host: affinity-clusterip-transition-h85nx
Dec  9 08:24:06.521: INFO: Received response from host: affinity-clusterip-transition-bgxpl
Dec  9 08:24:06.521: INFO: Received response from host: affinity-clusterip-transition-jjhxc
Dec  9 08:24:06.521: INFO: Received response from host: affinity-clusterip-transition-h85nx
Dec  9 08:24:06.521: INFO: Received response from host: affinity-clusterip-transition-bgxpl
Dec  9 08:24:06.521: INFO: Received response from host: affinity-clusterip-transition-jjhxc
Dec  9 08:24:06.521: INFO: Received response from host: affinity-clusterip-transition-h85nx
Dec  9 08:24:06.521: INFO: Received response from host: affinity-clusterip-transition-bgxpl
Dec  9 08:24:06.521: INFO: Received response from host: affinity-clusterip-transition-jjhxc
Dec  9 08:24:06.521: INFO: Received response from host: affinity-clusterip-transition-h85nx
Dec  9 08:24:06.521: INFO: Received response from host: affinity-clusterip-transition-bgxpl
Dec  9 08:24:06.521: INFO: Received response from host: affinity-clusterip-transition-jjhxc
Dec  9 08:24:06.531: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=services-5860 exec execpod-affinityqvsb7 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.3.221.68:80/ ; done'
Dec  9 08:24:06.799: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.221.68:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.221.68:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.221.68:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.221.68:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.221.68:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.221.68:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.221.68:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.221.68:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.221.68:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.221.68:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.221.68:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.221.68:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.221.68:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.221.68:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.221.68:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.221.68:80/\n"
Dec  9 08:24:06.799: INFO: stdout: "\naffinity-clusterip-transition-bgxpl\naffinity-clusterip-transition-bgxpl\naffinity-clusterip-transition-bgxpl\naffinity-clusterip-transition-bgxpl\naffinity-clusterip-transition-bgxpl\naffinity-clusterip-transition-bgxpl\naffinity-clusterip-transition-bgxpl\naffinity-clusterip-transition-bgxpl\naffinity-clusterip-transition-bgxpl\naffinity-clusterip-transition-bgxpl\naffinity-clusterip-transition-bgxpl\naffinity-clusterip-transition-bgxpl\naffinity-clusterip-transition-bgxpl\naffinity-clusterip-transition-bgxpl\naffinity-clusterip-transition-bgxpl\naffinity-clusterip-transition-bgxpl"
Dec  9 08:24:06.799: INFO: Received response from host: affinity-clusterip-transition-bgxpl
Dec  9 08:24:06.799: INFO: Received response from host: affinity-clusterip-transition-bgxpl
Dec  9 08:24:06.799: INFO: Received response from host: affinity-clusterip-transition-bgxpl
Dec  9 08:24:06.799: INFO: Received response from host: affinity-clusterip-transition-bgxpl
Dec  9 08:24:06.799: INFO: Received response from host: affinity-clusterip-transition-bgxpl
Dec  9 08:24:06.799: INFO: Received response from host: affinity-clusterip-transition-bgxpl
Dec  9 08:24:06.799: INFO: Received response from host: affinity-clusterip-transition-bgxpl
Dec  9 08:24:06.799: INFO: Received response from host: affinity-clusterip-transition-bgxpl
Dec  9 08:24:06.799: INFO: Received response from host: affinity-clusterip-transition-bgxpl
Dec  9 08:24:06.799: INFO: Received response from host: affinity-clusterip-transition-bgxpl
Dec  9 08:24:06.799: INFO: Received response from host: affinity-clusterip-transition-bgxpl
Dec  9 08:24:06.799: INFO: Received response from host: affinity-clusterip-transition-bgxpl
Dec  9 08:24:06.799: INFO: Received response from host: affinity-clusterip-transition-bgxpl
Dec  9 08:24:06.799: INFO: Received response from host: affinity-clusterip-transition-bgxpl
Dec  9 08:24:06.799: INFO: Received response from host: affinity-clusterip-transition-bgxpl
Dec  9 08:24:06.799: INFO: Received response from host: affinity-clusterip-transition-bgxpl
Dec  9 08:24:06.799: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-5860, will wait for the garbage collector to delete the pods 12/09/22 08:24:06.812
Dec  9 08:24:06.873: INFO: Deleting ReplicationController affinity-clusterip-transition took: 5.552869ms
Dec  9 08:24:06.974: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 100.690945ms
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Dec  9 08:24:09.392: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-5860" for this suite. 12/09/22 08:24:09.4
------------------------------
â€¢ [SLOW TEST] [9.942 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2213

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 08:23:59.475
    Dec  9 08:23:59.475: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename services 12/09/22 08:23:59.477
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:23:59.498
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:23:59.502
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2213
    STEP: creating service in namespace services-5860 12/09/22 08:23:59.505
    STEP: creating service affinity-clusterip-transition in namespace services-5860 12/09/22 08:23:59.506
    STEP: creating replication controller affinity-clusterip-transition in namespace services-5860 12/09/22 08:23:59.52
    I1209 08:23:59.540859      20 runners.go:193] Created replication controller with name: affinity-clusterip-transition, namespace: services-5860, replica count: 3
    I1209 08:24:02.591869      20 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Dec  9 08:24:02.605: INFO: Creating new exec pod
    Dec  9 08:24:02.612: INFO: Waiting up to 5m0s for pod "execpod-affinityqvsb7" in namespace "services-5860" to be "running"
    Dec  9 08:24:02.627: INFO: Pod "execpod-affinityqvsb7": Phase="Pending", Reason="", readiness=false. Elapsed: 15.0352ms
    Dec  9 08:24:04.632: INFO: Pod "execpod-affinityqvsb7": Phase="Running", Reason="", readiness=true. Elapsed: 2.01970774s
    Dec  9 08:24:04.632: INFO: Pod "execpod-affinityqvsb7" satisfied condition "running"
    Dec  9 08:24:05.636: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=services-5860 exec execpod-affinityqvsb7 -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip-transition 80'
    Dec  9 08:24:05.906: INFO: stderr: "+ nc -v -z -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
    Dec  9 08:24:05.906: INFO: stdout: ""
    Dec  9 08:24:05.906: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=services-5860 exec execpod-affinityqvsb7 -- /bin/sh -x -c nc -v -z -w 2 10.3.221.68 80'
    Dec  9 08:24:06.108: INFO: stderr: "+ nc -v -z -w 2 10.3.221.68 80\nConnection to 10.3.221.68 80 port [tcp/http] succeeded!\n"
    Dec  9 08:24:06.108: INFO: stdout: ""
    Dec  9 08:24:06.117: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=services-5860 exec execpod-affinityqvsb7 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.3.221.68:80/ ; done'
    Dec  9 08:24:06.521: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.221.68:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.221.68:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.221.68:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.221.68:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.221.68:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.221.68:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.221.68:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.221.68:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.221.68:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.221.68:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.221.68:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.221.68:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.221.68:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.221.68:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.221.68:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.221.68:80/\n"
    Dec  9 08:24:06.521: INFO: stdout: "\naffinity-clusterip-transition-jjhxc\naffinity-clusterip-transition-h85nx\naffinity-clusterip-transition-bgxpl\naffinity-clusterip-transition-jjhxc\naffinity-clusterip-transition-h85nx\naffinity-clusterip-transition-bgxpl\naffinity-clusterip-transition-jjhxc\naffinity-clusterip-transition-h85nx\naffinity-clusterip-transition-bgxpl\naffinity-clusterip-transition-jjhxc\naffinity-clusterip-transition-h85nx\naffinity-clusterip-transition-bgxpl\naffinity-clusterip-transition-jjhxc\naffinity-clusterip-transition-h85nx\naffinity-clusterip-transition-bgxpl\naffinity-clusterip-transition-jjhxc"
    Dec  9 08:24:06.521: INFO: Received response from host: affinity-clusterip-transition-jjhxc
    Dec  9 08:24:06.521: INFO: Received response from host: affinity-clusterip-transition-h85nx
    Dec  9 08:24:06.521: INFO: Received response from host: affinity-clusterip-transition-bgxpl
    Dec  9 08:24:06.521: INFO: Received response from host: affinity-clusterip-transition-jjhxc
    Dec  9 08:24:06.521: INFO: Received response from host: affinity-clusterip-transition-h85nx
    Dec  9 08:24:06.521: INFO: Received response from host: affinity-clusterip-transition-bgxpl
    Dec  9 08:24:06.521: INFO: Received response from host: affinity-clusterip-transition-jjhxc
    Dec  9 08:24:06.521: INFO: Received response from host: affinity-clusterip-transition-h85nx
    Dec  9 08:24:06.521: INFO: Received response from host: affinity-clusterip-transition-bgxpl
    Dec  9 08:24:06.521: INFO: Received response from host: affinity-clusterip-transition-jjhxc
    Dec  9 08:24:06.521: INFO: Received response from host: affinity-clusterip-transition-h85nx
    Dec  9 08:24:06.521: INFO: Received response from host: affinity-clusterip-transition-bgxpl
    Dec  9 08:24:06.521: INFO: Received response from host: affinity-clusterip-transition-jjhxc
    Dec  9 08:24:06.521: INFO: Received response from host: affinity-clusterip-transition-h85nx
    Dec  9 08:24:06.521: INFO: Received response from host: affinity-clusterip-transition-bgxpl
    Dec  9 08:24:06.521: INFO: Received response from host: affinity-clusterip-transition-jjhxc
    Dec  9 08:24:06.531: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=services-5860 exec execpod-affinityqvsb7 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.3.221.68:80/ ; done'
    Dec  9 08:24:06.799: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.221.68:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.221.68:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.221.68:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.221.68:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.221.68:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.221.68:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.221.68:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.221.68:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.221.68:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.221.68:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.221.68:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.221.68:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.221.68:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.221.68:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.221.68:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.221.68:80/\n"
    Dec  9 08:24:06.799: INFO: stdout: "\naffinity-clusterip-transition-bgxpl\naffinity-clusterip-transition-bgxpl\naffinity-clusterip-transition-bgxpl\naffinity-clusterip-transition-bgxpl\naffinity-clusterip-transition-bgxpl\naffinity-clusterip-transition-bgxpl\naffinity-clusterip-transition-bgxpl\naffinity-clusterip-transition-bgxpl\naffinity-clusterip-transition-bgxpl\naffinity-clusterip-transition-bgxpl\naffinity-clusterip-transition-bgxpl\naffinity-clusterip-transition-bgxpl\naffinity-clusterip-transition-bgxpl\naffinity-clusterip-transition-bgxpl\naffinity-clusterip-transition-bgxpl\naffinity-clusterip-transition-bgxpl"
    Dec  9 08:24:06.799: INFO: Received response from host: affinity-clusterip-transition-bgxpl
    Dec  9 08:24:06.799: INFO: Received response from host: affinity-clusterip-transition-bgxpl
    Dec  9 08:24:06.799: INFO: Received response from host: affinity-clusterip-transition-bgxpl
    Dec  9 08:24:06.799: INFO: Received response from host: affinity-clusterip-transition-bgxpl
    Dec  9 08:24:06.799: INFO: Received response from host: affinity-clusterip-transition-bgxpl
    Dec  9 08:24:06.799: INFO: Received response from host: affinity-clusterip-transition-bgxpl
    Dec  9 08:24:06.799: INFO: Received response from host: affinity-clusterip-transition-bgxpl
    Dec  9 08:24:06.799: INFO: Received response from host: affinity-clusterip-transition-bgxpl
    Dec  9 08:24:06.799: INFO: Received response from host: affinity-clusterip-transition-bgxpl
    Dec  9 08:24:06.799: INFO: Received response from host: affinity-clusterip-transition-bgxpl
    Dec  9 08:24:06.799: INFO: Received response from host: affinity-clusterip-transition-bgxpl
    Dec  9 08:24:06.799: INFO: Received response from host: affinity-clusterip-transition-bgxpl
    Dec  9 08:24:06.799: INFO: Received response from host: affinity-clusterip-transition-bgxpl
    Dec  9 08:24:06.799: INFO: Received response from host: affinity-clusterip-transition-bgxpl
    Dec  9 08:24:06.799: INFO: Received response from host: affinity-clusterip-transition-bgxpl
    Dec  9 08:24:06.799: INFO: Received response from host: affinity-clusterip-transition-bgxpl
    Dec  9 08:24:06.799: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-5860, will wait for the garbage collector to delete the pods 12/09/22 08:24:06.812
    Dec  9 08:24:06.873: INFO: Deleting ReplicationController affinity-clusterip-transition took: 5.552869ms
    Dec  9 08:24:06.974: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 100.690945ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Dec  9 08:24:09.392: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-5860" for this suite. 12/09/22 08:24:09.4
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:508
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 08:24:09.439
Dec  9 08:24:09.440: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename webhook 12/09/22 08:24:09.441
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:24:09.456
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:24:09.461
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 12/09/22 08:24:09.484
STEP: Create role binding to let webhook read extension-apiserver-authentication 12/09/22 08:24:09.804
STEP: Deploying the webhook pod 12/09/22 08:24:09.808
STEP: Wait for the deployment to be ready 12/09/22 08:24:09.818
Dec  9 08:24:09.826: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 12/09/22 08:24:11.837
STEP: Verifying the service has paired with the endpoint 12/09/22 08:24:11.851
Dec  9 08:24:12.851: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:508
STEP: Creating a mutating webhook configuration 12/09/22 08:24:12.854
STEP: Updating a mutating webhook configuration's rules to not include the create operation 12/09/22 08:24:12.871
STEP: Creating a configMap that should not be mutated 12/09/22 08:24:12.878
STEP: Patching a mutating webhook configuration's rules to include the create operation 12/09/22 08:24:12.89
STEP: Creating a configMap that should be mutated 12/09/22 08:24:12.896
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Dec  9 08:24:12.915: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-6880" for this suite. 12/09/22 08:24:12.955
STEP: Destroying namespace "webhook-6880-markers" for this suite. 12/09/22 08:24:12.963
------------------------------
â€¢ [3.531 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:508

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 08:24:09.439
    Dec  9 08:24:09.440: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename webhook 12/09/22 08:24:09.441
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:24:09.456
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:24:09.461
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 12/09/22 08:24:09.484
    STEP: Create role binding to let webhook read extension-apiserver-authentication 12/09/22 08:24:09.804
    STEP: Deploying the webhook pod 12/09/22 08:24:09.808
    STEP: Wait for the deployment to be ready 12/09/22 08:24:09.818
    Dec  9 08:24:09.826: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 12/09/22 08:24:11.837
    STEP: Verifying the service has paired with the endpoint 12/09/22 08:24:11.851
    Dec  9 08:24:12.851: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] patching/updating a mutating webhook should work [Conformance]
      test/e2e/apimachinery/webhook.go:508
    STEP: Creating a mutating webhook configuration 12/09/22 08:24:12.854
    STEP: Updating a mutating webhook configuration's rules to not include the create operation 12/09/22 08:24:12.871
    STEP: Creating a configMap that should not be mutated 12/09/22 08:24:12.878
    STEP: Patching a mutating webhook configuration's rules to include the create operation 12/09/22 08:24:12.89
    STEP: Creating a configMap that should be mutated 12/09/22 08:24:12.896
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Dec  9 08:24:12.915: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-6880" for this suite. 12/09/22 08:24:12.955
    STEP: Destroying namespace "webhook-6880-markers" for this suite. 12/09/22 08:24:12.963
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with configmap pod [Conformance]
  test/e2e/storage/subpath.go:70
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 08:24:12.971
Dec  9 08:24:12.971: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename subpath 12/09/22 08:24:12.973
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:24:12.996
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:24:13
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 12/09/22 08:24:13.006
[It] should support subpaths with configmap pod [Conformance]
  test/e2e/storage/subpath.go:70
STEP: Creating pod pod-subpath-test-configmap-whwx 12/09/22 08:24:13.023
STEP: Creating a pod to test atomic-volume-subpath 12/09/22 08:24:13.023
Dec  9 08:24:13.032: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-whwx" in namespace "subpath-5655" to be "Succeeded or Failed"
Dec  9 08:24:13.039: INFO: Pod "pod-subpath-test-configmap-whwx": Phase="Pending", Reason="", readiness=false. Elapsed: 6.685239ms
Dec  9 08:24:15.044: INFO: Pod "pod-subpath-test-configmap-whwx": Phase="Running", Reason="", readiness=true. Elapsed: 2.011590754s
Dec  9 08:24:17.043: INFO: Pod "pod-subpath-test-configmap-whwx": Phase="Running", Reason="", readiness=true. Elapsed: 4.01084019s
Dec  9 08:24:19.043: INFO: Pod "pod-subpath-test-configmap-whwx": Phase="Running", Reason="", readiness=true. Elapsed: 6.011370502s
Dec  9 08:24:21.052: INFO: Pod "pod-subpath-test-configmap-whwx": Phase="Running", Reason="", readiness=true. Elapsed: 8.019911398s
Dec  9 08:24:23.042: INFO: Pod "pod-subpath-test-configmap-whwx": Phase="Running", Reason="", readiness=true. Elapsed: 10.010085913s
Dec  9 08:24:25.044: INFO: Pod "pod-subpath-test-configmap-whwx": Phase="Running", Reason="", readiness=true. Elapsed: 12.011708171s
Dec  9 08:24:27.043: INFO: Pod "pod-subpath-test-configmap-whwx": Phase="Running", Reason="", readiness=true. Elapsed: 14.010566308s
Dec  9 08:24:29.044: INFO: Pod "pod-subpath-test-configmap-whwx": Phase="Running", Reason="", readiness=true. Elapsed: 16.012342667s
Dec  9 08:24:31.043: INFO: Pod "pod-subpath-test-configmap-whwx": Phase="Running", Reason="", readiness=true. Elapsed: 18.010593078s
Dec  9 08:24:33.045: INFO: Pod "pod-subpath-test-configmap-whwx": Phase="Running", Reason="", readiness=true. Elapsed: 20.012780839s
Dec  9 08:24:35.043: INFO: Pod "pod-subpath-test-configmap-whwx": Phase="Running", Reason="", readiness=false. Elapsed: 22.011423215s
Dec  9 08:24:37.043: INFO: Pod "pod-subpath-test-configmap-whwx": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.010710333s
STEP: Saw pod success 12/09/22 08:24:37.043
Dec  9 08:24:37.043: INFO: Pod "pod-subpath-test-configmap-whwx" satisfied condition "Succeeded or Failed"
Dec  9 08:24:37.051: INFO: Trying to get logs from node ip-10-0-10-179 pod pod-subpath-test-configmap-whwx container test-container-subpath-configmap-whwx: <nil>
STEP: delete the pod 12/09/22 08:24:37.057
Dec  9 08:24:37.069: INFO: Waiting for pod pod-subpath-test-configmap-whwx to disappear
Dec  9 08:24:37.072: INFO: Pod pod-subpath-test-configmap-whwx no longer exists
STEP: Deleting pod pod-subpath-test-configmap-whwx 12/09/22 08:24:37.072
Dec  9 08:24:37.073: INFO: Deleting pod "pod-subpath-test-configmap-whwx" in namespace "subpath-5655"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Dec  9 08:24:37.075: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-5655" for this suite. 12/09/22 08:24:37.081
------------------------------
â€¢ [SLOW TEST] [24.118 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with configmap pod [Conformance]
    test/e2e/storage/subpath.go:70

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 08:24:12.971
    Dec  9 08:24:12.971: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename subpath 12/09/22 08:24:12.973
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:24:12.996
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:24:13
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 12/09/22 08:24:13.006
    [It] should support subpaths with configmap pod [Conformance]
      test/e2e/storage/subpath.go:70
    STEP: Creating pod pod-subpath-test-configmap-whwx 12/09/22 08:24:13.023
    STEP: Creating a pod to test atomic-volume-subpath 12/09/22 08:24:13.023
    Dec  9 08:24:13.032: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-whwx" in namespace "subpath-5655" to be "Succeeded or Failed"
    Dec  9 08:24:13.039: INFO: Pod "pod-subpath-test-configmap-whwx": Phase="Pending", Reason="", readiness=false. Elapsed: 6.685239ms
    Dec  9 08:24:15.044: INFO: Pod "pod-subpath-test-configmap-whwx": Phase="Running", Reason="", readiness=true. Elapsed: 2.011590754s
    Dec  9 08:24:17.043: INFO: Pod "pod-subpath-test-configmap-whwx": Phase="Running", Reason="", readiness=true. Elapsed: 4.01084019s
    Dec  9 08:24:19.043: INFO: Pod "pod-subpath-test-configmap-whwx": Phase="Running", Reason="", readiness=true. Elapsed: 6.011370502s
    Dec  9 08:24:21.052: INFO: Pod "pod-subpath-test-configmap-whwx": Phase="Running", Reason="", readiness=true. Elapsed: 8.019911398s
    Dec  9 08:24:23.042: INFO: Pod "pod-subpath-test-configmap-whwx": Phase="Running", Reason="", readiness=true. Elapsed: 10.010085913s
    Dec  9 08:24:25.044: INFO: Pod "pod-subpath-test-configmap-whwx": Phase="Running", Reason="", readiness=true. Elapsed: 12.011708171s
    Dec  9 08:24:27.043: INFO: Pod "pod-subpath-test-configmap-whwx": Phase="Running", Reason="", readiness=true. Elapsed: 14.010566308s
    Dec  9 08:24:29.044: INFO: Pod "pod-subpath-test-configmap-whwx": Phase="Running", Reason="", readiness=true. Elapsed: 16.012342667s
    Dec  9 08:24:31.043: INFO: Pod "pod-subpath-test-configmap-whwx": Phase="Running", Reason="", readiness=true. Elapsed: 18.010593078s
    Dec  9 08:24:33.045: INFO: Pod "pod-subpath-test-configmap-whwx": Phase="Running", Reason="", readiness=true. Elapsed: 20.012780839s
    Dec  9 08:24:35.043: INFO: Pod "pod-subpath-test-configmap-whwx": Phase="Running", Reason="", readiness=false. Elapsed: 22.011423215s
    Dec  9 08:24:37.043: INFO: Pod "pod-subpath-test-configmap-whwx": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.010710333s
    STEP: Saw pod success 12/09/22 08:24:37.043
    Dec  9 08:24:37.043: INFO: Pod "pod-subpath-test-configmap-whwx" satisfied condition "Succeeded or Failed"
    Dec  9 08:24:37.051: INFO: Trying to get logs from node ip-10-0-10-179 pod pod-subpath-test-configmap-whwx container test-container-subpath-configmap-whwx: <nil>
    STEP: delete the pod 12/09/22 08:24:37.057
    Dec  9 08:24:37.069: INFO: Waiting for pod pod-subpath-test-configmap-whwx to disappear
    Dec  9 08:24:37.072: INFO: Pod pod-subpath-test-configmap-whwx no longer exists
    STEP: Deleting pod pod-subpath-test-configmap-whwx 12/09/22 08:24:37.072
    Dec  9 08:24:37.073: INFO: Deleting pod "pod-subpath-test-configmap-whwx" in namespace "subpath-5655"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Dec  9 08:24:37.075: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-5655" for this suite. 12/09/22 08:24:37.081
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a container with runAsUser
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:347
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 08:24:37.091
Dec  9 08:24:37.091: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename security-context-test 12/09/22 08:24:37.092
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:24:37.11
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:24:37.119
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:50
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:347
Dec  9 08:24:37.137: INFO: Waiting up to 5m0s for pod "busybox-user-65534-1760ffeb-b843-4906-98a0-7464190da7e0" in namespace "security-context-test-3948" to be "Succeeded or Failed"
Dec  9 08:24:37.147: INFO: Pod "busybox-user-65534-1760ffeb-b843-4906-98a0-7464190da7e0": Phase="Pending", Reason="", readiness=false. Elapsed: 9.629349ms
Dec  9 08:24:39.151: INFO: Pod "busybox-user-65534-1760ffeb-b843-4906-98a0-7464190da7e0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013068672s
Dec  9 08:24:41.151: INFO: Pod "busybox-user-65534-1760ffeb-b843-4906-98a0-7464190da7e0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013543952s
Dec  9 08:24:41.151: INFO: Pod "busybox-user-65534-1760ffeb-b843-4906-98a0-7464190da7e0" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Dec  9 08:24:41.151: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-test-3948" for this suite. 12/09/22 08:24:41.155
------------------------------
â€¢ [4.070 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a container with runAsUser
  test/e2e/common/node/security_context.go:309
    should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:347

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 08:24:37.091
    Dec  9 08:24:37.091: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename security-context-test 12/09/22 08:24:37.092
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:24:37.11
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:24:37.119
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:50
    [It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:347
    Dec  9 08:24:37.137: INFO: Waiting up to 5m0s for pod "busybox-user-65534-1760ffeb-b843-4906-98a0-7464190da7e0" in namespace "security-context-test-3948" to be "Succeeded or Failed"
    Dec  9 08:24:37.147: INFO: Pod "busybox-user-65534-1760ffeb-b843-4906-98a0-7464190da7e0": Phase="Pending", Reason="", readiness=false. Elapsed: 9.629349ms
    Dec  9 08:24:39.151: INFO: Pod "busybox-user-65534-1760ffeb-b843-4906-98a0-7464190da7e0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013068672s
    Dec  9 08:24:41.151: INFO: Pod "busybox-user-65534-1760ffeb-b843-4906-98a0-7464190da7e0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013543952s
    Dec  9 08:24:41.151: INFO: Pod "busybox-user-65534-1760ffeb-b843-4906-98a0-7464190da7e0" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Dec  9 08:24:41.151: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-test-3948" for this suite. 12/09/22 08:24:41.155
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] RuntimeClass
  should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 08:24:41.162
Dec  9 08:24:41.162: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename runtimeclass 12/09/22 08:24:41.163
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:24:41.181
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:24:41.184
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It] should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104
Dec  9 08:24:41.202: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-1044 to be scheduled
Dec  9 08:24:41.207: INFO: 1 pods are not scheduled: [runtimeclass-1044/test-runtimeclass-runtimeclass-1044-preconfigured-handler-dvk4t(f0583c81-ebe1-4bcf-84d8-054a5bc2907a)]
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Dec  9 08:24:43.217: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-1044" for this suite. 12/09/22 08:24:43.22
------------------------------
â€¢ [2.064 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 08:24:41.162
    Dec  9 08:24:41.162: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename runtimeclass 12/09/22 08:24:41.163
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:24:41.181
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:24:41.184
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It] should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:104
    Dec  9 08:24:41.202: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-1044 to be scheduled
    Dec  9 08:24:41.207: INFO: 1 pods are not scheduled: [runtimeclass-1044/test-runtimeclass-runtimeclass-1044-preconfigured-handler-dvk4t(f0583c81-ebe1-4bcf-84d8-054a5bc2907a)]
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Dec  9 08:24:43.217: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-1044" for this suite. 12/09/22 08:24:43.22
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:164
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 08:24:43.229
Dec  9 08:24:43.229: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename security-context 12/09/22 08:24:43.231
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:24:43.245
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:24:43.25
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:164
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 12/09/22 08:24:43.255
Dec  9 08:24:43.262: INFO: Waiting up to 5m0s for pod "security-context-1ca4bc48-faae-4e96-9401-446d930fadf7" in namespace "security-context-2041" to be "Succeeded or Failed"
Dec  9 08:24:43.268: INFO: Pod "security-context-1ca4bc48-faae-4e96-9401-446d930fadf7": Phase="Pending", Reason="", readiness=false. Elapsed: 5.968552ms
Dec  9 08:24:45.272: INFO: Pod "security-context-1ca4bc48-faae-4e96-9401-446d930fadf7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010194796s
Dec  9 08:24:47.272: INFO: Pod "security-context-1ca4bc48-faae-4e96-9401-446d930fadf7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010058616s
STEP: Saw pod success 12/09/22 08:24:47.272
Dec  9 08:24:47.272: INFO: Pod "security-context-1ca4bc48-faae-4e96-9401-446d930fadf7" satisfied condition "Succeeded or Failed"
Dec  9 08:24:47.275: INFO: Trying to get logs from node ip-10-0-10-179 pod security-context-1ca4bc48-faae-4e96-9401-446d930fadf7 container test-container: <nil>
STEP: delete the pod 12/09/22 08:24:47.28
Dec  9 08:24:47.290: INFO: Waiting for pod security-context-1ca4bc48-faae-4e96-9401-446d930fadf7 to disappear
Dec  9 08:24:47.293: INFO: Pod security-context-1ca4bc48-faae-4e96-9401-446d930fadf7 no longer exists
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Dec  9 08:24:47.293: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-2041" for this suite. 12/09/22 08:24:47.296
------------------------------
â€¢ [4.072 seconds]
[sig-node] Security Context
test/e2e/node/framework.go:23
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:164

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 08:24:43.229
    Dec  9 08:24:43.229: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename security-context 12/09/22 08:24:43.231
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:24:43.245
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:24:43.25
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
      test/e2e/node/security_context.go:164
    STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 12/09/22 08:24:43.255
    Dec  9 08:24:43.262: INFO: Waiting up to 5m0s for pod "security-context-1ca4bc48-faae-4e96-9401-446d930fadf7" in namespace "security-context-2041" to be "Succeeded or Failed"
    Dec  9 08:24:43.268: INFO: Pod "security-context-1ca4bc48-faae-4e96-9401-446d930fadf7": Phase="Pending", Reason="", readiness=false. Elapsed: 5.968552ms
    Dec  9 08:24:45.272: INFO: Pod "security-context-1ca4bc48-faae-4e96-9401-446d930fadf7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010194796s
    Dec  9 08:24:47.272: INFO: Pod "security-context-1ca4bc48-faae-4e96-9401-446d930fadf7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010058616s
    STEP: Saw pod success 12/09/22 08:24:47.272
    Dec  9 08:24:47.272: INFO: Pod "security-context-1ca4bc48-faae-4e96-9401-446d930fadf7" satisfied condition "Succeeded or Failed"
    Dec  9 08:24:47.275: INFO: Trying to get logs from node ip-10-0-10-179 pod security-context-1ca4bc48-faae-4e96-9401-446d930fadf7 container test-container: <nil>
    STEP: delete the pod 12/09/22 08:24:47.28
    Dec  9 08:24:47.290: INFO: Waiting for pod security-context-1ca4bc48-faae-4e96-9401-446d930fadf7 to disappear
    Dec  9 08:24:47.293: INFO: Pod security-context-1ca4bc48-faae-4e96-9401-446d930fadf7 no longer exists
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Dec  9 08:24:47.293: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-2041" for this suite. 12/09/22 08:24:47.296
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] LimitRange
  should list, patch and delete a LimitRange by collection [Conformance]
  test/e2e/scheduling/limit_range.go:239
[BeforeEach] [sig-scheduling] LimitRange
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 08:24:47.303
Dec  9 08:24:47.303: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename limitrange 12/09/22 08:24:47.304
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:24:47.319
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:24:47.323
[BeforeEach] [sig-scheduling] LimitRange
  test/e2e/framework/metrics/init/init.go:31
[It] should list, patch and delete a LimitRange by collection [Conformance]
  test/e2e/scheduling/limit_range.go:239
STEP: Creating LimitRange "e2e-limitrange-tkbs5" in namespace "limitrange-887" 12/09/22 08:24:47.327
STEP: Creating another limitRange in another namespace 12/09/22 08:24:47.332
Dec  9 08:24:47.354: INFO: Namespace "e2e-limitrange-tkbs5-6473" created
Dec  9 08:24:47.355: INFO: Creating LimitRange "e2e-limitrange-tkbs5" in namespace "e2e-limitrange-tkbs5-6473"
STEP: Listing all LimitRanges with label "e2e-test=e2e-limitrange-tkbs5" 12/09/22 08:24:47.36
Dec  9 08:24:47.363: INFO: Found 2 limitRanges
STEP: Patching LimitRange "e2e-limitrange-tkbs5" in "limitrange-887" namespace 12/09/22 08:24:47.363
Dec  9 08:24:47.370: INFO: LimitRange "e2e-limitrange-tkbs5" has been patched
STEP: Delete LimitRange "e2e-limitrange-tkbs5" by Collection with labelSelector: "e2e-limitrange-tkbs5=patched" 12/09/22 08:24:47.37
STEP: Confirm that the limitRange "e2e-limitrange-tkbs5" has been deleted 12/09/22 08:24:47.375
Dec  9 08:24:47.375: INFO: Requesting list of LimitRange to confirm quantity
Dec  9 08:24:47.377: INFO: Found 0 LimitRange with label "e2e-limitrange-tkbs5=patched"
Dec  9 08:24:47.377: INFO: LimitRange "e2e-limitrange-tkbs5" has been deleted.
STEP: Confirm that a single LimitRange still exists with label "e2e-test=e2e-limitrange-tkbs5" 12/09/22 08:24:47.377
Dec  9 08:24:47.379: INFO: Found 1 limitRange
[AfterEach] [sig-scheduling] LimitRange
  test/e2e/framework/node/init/init.go:32
Dec  9 08:24:47.380: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  tear down framework | framework.go:193
STEP: Destroying namespace "limitrange-887" for this suite. 12/09/22 08:24:47.383
STEP: Destroying namespace "e2e-limitrange-tkbs5-6473" for this suite. 12/09/22 08:24:47.388
------------------------------
â€¢ [0.091 seconds]
[sig-scheduling] LimitRange
test/e2e/scheduling/framework.go:40
  should list, patch and delete a LimitRange by collection [Conformance]
  test/e2e/scheduling/limit_range.go:239

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] LimitRange
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 08:24:47.303
    Dec  9 08:24:47.303: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename limitrange 12/09/22 08:24:47.304
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:24:47.319
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:24:47.323
    [BeforeEach] [sig-scheduling] LimitRange
      test/e2e/framework/metrics/init/init.go:31
    [It] should list, patch and delete a LimitRange by collection [Conformance]
      test/e2e/scheduling/limit_range.go:239
    STEP: Creating LimitRange "e2e-limitrange-tkbs5" in namespace "limitrange-887" 12/09/22 08:24:47.327
    STEP: Creating another limitRange in another namespace 12/09/22 08:24:47.332
    Dec  9 08:24:47.354: INFO: Namespace "e2e-limitrange-tkbs5-6473" created
    Dec  9 08:24:47.355: INFO: Creating LimitRange "e2e-limitrange-tkbs5" in namespace "e2e-limitrange-tkbs5-6473"
    STEP: Listing all LimitRanges with label "e2e-test=e2e-limitrange-tkbs5" 12/09/22 08:24:47.36
    Dec  9 08:24:47.363: INFO: Found 2 limitRanges
    STEP: Patching LimitRange "e2e-limitrange-tkbs5" in "limitrange-887" namespace 12/09/22 08:24:47.363
    Dec  9 08:24:47.370: INFO: LimitRange "e2e-limitrange-tkbs5" has been patched
    STEP: Delete LimitRange "e2e-limitrange-tkbs5" by Collection with labelSelector: "e2e-limitrange-tkbs5=patched" 12/09/22 08:24:47.37
    STEP: Confirm that the limitRange "e2e-limitrange-tkbs5" has been deleted 12/09/22 08:24:47.375
    Dec  9 08:24:47.375: INFO: Requesting list of LimitRange to confirm quantity
    Dec  9 08:24:47.377: INFO: Found 0 LimitRange with label "e2e-limitrange-tkbs5=patched"
    Dec  9 08:24:47.377: INFO: LimitRange "e2e-limitrange-tkbs5" has been deleted.
    STEP: Confirm that a single LimitRange still exists with label "e2e-test=e2e-limitrange-tkbs5" 12/09/22 08:24:47.377
    Dec  9 08:24:47.379: INFO: Found 1 limitRange
    [AfterEach] [sig-scheduling] LimitRange
      test/e2e/framework/node/init/init.go:32
    Dec  9 08:24:47.380: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      tear down framework | framework.go:193
    STEP: Destroying namespace "limitrange-887" for this suite. 12/09/22 08:24:47.383
    STEP: Destroying namespace "e2e-limitrange-tkbs5-6473" for this suite. 12/09/22 08:24:47.388
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:197
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 08:24:47.394
Dec  9 08:24:47.395: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename webhook 12/09/22 08:24:47.396
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:24:47.41
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:24:47.413
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 12/09/22 08:24:47.43
STEP: Create role binding to let webhook read extension-apiserver-authentication 12/09/22 08:24:47.99
STEP: Deploying the webhook pod 12/09/22 08:24:47.996
STEP: Wait for the deployment to be ready 12/09/22 08:24:48.008
Dec  9 08:24:48.016: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 12/09/22 08:24:50.026
STEP: Verifying the service has paired with the endpoint 12/09/22 08:24:50.036
Dec  9 08:24:51.038: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:197
STEP: Registering the webhook via the AdmissionRegistration API 12/09/22 08:24:51.042
STEP: create a pod that should be denied by the webhook 12/09/22 08:24:51.062
STEP: create a pod that causes the webhook to hang 12/09/22 08:24:51.084
STEP: create a configmap that should be denied by the webhook 12/09/22 08:25:01.091
STEP: create a configmap that should be admitted by the webhook 12/09/22 08:25:01.105
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook 12/09/22 08:25:01.117
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook 12/09/22 08:25:01.126
STEP: create a namespace that bypass the webhook 12/09/22 08:25:01.142
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace 12/09/22 08:25:01.152
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Dec  9 08:25:01.227: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-1468" for this suite. 12/09/22 08:25:01.308
STEP: Destroying namespace "webhook-1468-markers" for this suite. 12/09/22 08:25:01.348
------------------------------
â€¢ [SLOW TEST] [13.959 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:197

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 08:24:47.394
    Dec  9 08:24:47.395: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename webhook 12/09/22 08:24:47.396
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:24:47.41
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:24:47.413
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 12/09/22 08:24:47.43
    STEP: Create role binding to let webhook read extension-apiserver-authentication 12/09/22 08:24:47.99
    STEP: Deploying the webhook pod 12/09/22 08:24:47.996
    STEP: Wait for the deployment to be ready 12/09/22 08:24:48.008
    Dec  9 08:24:48.016: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 12/09/22 08:24:50.026
    STEP: Verifying the service has paired with the endpoint 12/09/22 08:24:50.036
    Dec  9 08:24:51.038: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny pod and configmap creation [Conformance]
      test/e2e/apimachinery/webhook.go:197
    STEP: Registering the webhook via the AdmissionRegistration API 12/09/22 08:24:51.042
    STEP: create a pod that should be denied by the webhook 12/09/22 08:24:51.062
    STEP: create a pod that causes the webhook to hang 12/09/22 08:24:51.084
    STEP: create a configmap that should be denied by the webhook 12/09/22 08:25:01.091
    STEP: create a configmap that should be admitted by the webhook 12/09/22 08:25:01.105
    STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook 12/09/22 08:25:01.117
    STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook 12/09/22 08:25:01.126
    STEP: create a namespace that bypass the webhook 12/09/22 08:25:01.142
    STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace 12/09/22 08:25:01.152
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Dec  9 08:25:01.227: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-1468" for this suite. 12/09/22 08:25:01.308
    STEP: Destroying namespace "webhook-1468-markers" for this suite. 12/09/22 08:25:01.348
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation
  should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 08:25:01.355
Dec  9 08:25:01.355: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename tables 12/09/22 08:25:01.355
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:25:01.394
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:25:01.399
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/apimachinery/table_conversion.go:49
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/node/init/init.go:32
Dec  9 08:25:01.426: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
  tear down framework | framework.go:193
STEP: Destroying namespace "tables-6327" for this suite. 12/09/22 08:25:01.43
------------------------------
â€¢ [0.094 seconds]
[sig-api-machinery] Servers with support for Table transformation
test/e2e/apimachinery/framework.go:23
  should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 08:25:01.355
    Dec  9 08:25:01.355: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename tables 12/09/22 08:25:01.355
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:25:01.394
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:25:01.399
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/apimachinery/table_conversion.go:49
    [It] should return a 406 for a backend which does not implement metadata [Conformance]
      test/e2e/apimachinery/table_conversion.go:154
    [AfterEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/node/init/init.go:32
    Dec  9 08:25:01.426: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
      tear down framework | framework.go:193
    STEP: Destroying namespace "tables-6327" for this suite. 12/09/22 08:25:01.43
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:432
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 08:25:01.452
Dec  9 08:25:01.452: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename daemonsets 12/09/22 08:25:01.466
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:25:01.51
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:25:01.518
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:432
Dec  9 08:25:01.638: INFO: Create a RollingUpdate DaemonSet
Dec  9 08:25:01.667: INFO: Check that daemon pods launch on every node of the cluster
Dec  9 08:25:01.695: INFO: DaemonSet pods can't tolerate node ip-10-0-7-84 with taints [{Key:node-role.kubernetes.io/controller Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 08:25:01.711: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Dec  9 08:25:01.711: INFO: Node ip-10-0-10-179 is running 0 daemon pod, expected 1
Dec  9 08:25:02.716: INFO: DaemonSet pods can't tolerate node ip-10-0-7-84 with taints [{Key:node-role.kubernetes.io/controller Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 08:25:02.724: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Dec  9 08:25:02.724: INFO: Node ip-10-0-10-179 is running 0 daemon pod, expected 1
Dec  9 08:25:03.715: INFO: DaemonSet pods can't tolerate node ip-10-0-7-84 with taints [{Key:node-role.kubernetes.io/controller Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 08:25:03.718: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Dec  9 08:25:03.718: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
Dec  9 08:25:03.719: INFO: Update the DaemonSet to trigger a rollout
Dec  9 08:25:03.728: INFO: Updating DaemonSet daemon-set
Dec  9 08:25:06.885: INFO: Roll back the DaemonSet before rollout is complete
Dec  9 08:25:06.969: INFO: Updating DaemonSet daemon-set
Dec  9 08:25:06.970: INFO: Make sure DaemonSet rollback is complete
Dec  9 08:25:06.979: INFO: Wrong image for pod: daemon-set-ghqgx. Expected: registry.k8s.io/e2e-test-images/httpd:2.4.38-4, got: foo:non-existent.
Dec  9 08:25:06.979: INFO: Pod daemon-set-ghqgx is not available
Dec  9 08:25:06.987: INFO: DaemonSet pods can't tolerate node ip-10-0-7-84 with taints [{Key:node-role.kubernetes.io/controller Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 08:25:07.994: INFO: DaemonSet pods can't tolerate node ip-10-0-7-84 with taints [{Key:node-role.kubernetes.io/controller Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 08:25:08.998: INFO: DaemonSet pods can't tolerate node ip-10-0-7-84 with taints [{Key:node-role.kubernetes.io/controller Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 08:25:09.995: INFO: DaemonSet pods can't tolerate node ip-10-0-7-84 with taints [{Key:node-role.kubernetes.io/controller Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  9 08:25:10.995: INFO: Pod daemon-set-nd2f7 is not available
Dec  9 08:25:10.999: INFO: DaemonSet pods can't tolerate node ip-10-0-7-84 with taints [{Key:node-role.kubernetes.io/controller Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 12/09/22 08:25:11.012
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6088, will wait for the garbage collector to delete the pods 12/09/22 08:25:11.017
Dec  9 08:25:11.087: INFO: Deleting DaemonSet.extensions daemon-set took: 11.278327ms
Dec  9 08:25:11.188: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.690139ms
Dec  9 08:25:12.493: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Dec  9 08:25:12.493: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Dec  9 08:25:12.496: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"33428"},"items":null}

Dec  9 08:25:12.503: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"33428"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Dec  9 08:25:12.510: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-6088" for this suite. 12/09/22 08:25:12.513
------------------------------
â€¢ [SLOW TEST] [11.066 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:432

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 08:25:01.452
    Dec  9 08:25:01.452: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename daemonsets 12/09/22 08:25:01.466
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:25:01.51
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:25:01.518
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should rollback without unnecessary restarts [Conformance]
      test/e2e/apps/daemon_set.go:432
    Dec  9 08:25:01.638: INFO: Create a RollingUpdate DaemonSet
    Dec  9 08:25:01.667: INFO: Check that daemon pods launch on every node of the cluster
    Dec  9 08:25:01.695: INFO: DaemonSet pods can't tolerate node ip-10-0-7-84 with taints [{Key:node-role.kubernetes.io/controller Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Dec  9 08:25:01.711: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Dec  9 08:25:01.711: INFO: Node ip-10-0-10-179 is running 0 daemon pod, expected 1
    Dec  9 08:25:02.716: INFO: DaemonSet pods can't tolerate node ip-10-0-7-84 with taints [{Key:node-role.kubernetes.io/controller Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Dec  9 08:25:02.724: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Dec  9 08:25:02.724: INFO: Node ip-10-0-10-179 is running 0 daemon pod, expected 1
    Dec  9 08:25:03.715: INFO: DaemonSet pods can't tolerate node ip-10-0-7-84 with taints [{Key:node-role.kubernetes.io/controller Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Dec  9 08:25:03.718: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Dec  9 08:25:03.718: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    Dec  9 08:25:03.719: INFO: Update the DaemonSet to trigger a rollout
    Dec  9 08:25:03.728: INFO: Updating DaemonSet daemon-set
    Dec  9 08:25:06.885: INFO: Roll back the DaemonSet before rollout is complete
    Dec  9 08:25:06.969: INFO: Updating DaemonSet daemon-set
    Dec  9 08:25:06.970: INFO: Make sure DaemonSet rollback is complete
    Dec  9 08:25:06.979: INFO: Wrong image for pod: daemon-set-ghqgx. Expected: registry.k8s.io/e2e-test-images/httpd:2.4.38-4, got: foo:non-existent.
    Dec  9 08:25:06.979: INFO: Pod daemon-set-ghqgx is not available
    Dec  9 08:25:06.987: INFO: DaemonSet pods can't tolerate node ip-10-0-7-84 with taints [{Key:node-role.kubernetes.io/controller Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Dec  9 08:25:07.994: INFO: DaemonSet pods can't tolerate node ip-10-0-7-84 with taints [{Key:node-role.kubernetes.io/controller Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Dec  9 08:25:08.998: INFO: DaemonSet pods can't tolerate node ip-10-0-7-84 with taints [{Key:node-role.kubernetes.io/controller Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Dec  9 08:25:09.995: INFO: DaemonSet pods can't tolerate node ip-10-0-7-84 with taints [{Key:node-role.kubernetes.io/controller Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Dec  9 08:25:10.995: INFO: Pod daemon-set-nd2f7 is not available
    Dec  9 08:25:10.999: INFO: DaemonSet pods can't tolerate node ip-10-0-7-84 with taints [{Key:node-role.kubernetes.io/controller Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 12/09/22 08:25:11.012
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6088, will wait for the garbage collector to delete the pods 12/09/22 08:25:11.017
    Dec  9 08:25:11.087: INFO: Deleting DaemonSet.extensions daemon-set took: 11.278327ms
    Dec  9 08:25:11.188: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.690139ms
    Dec  9 08:25:12.493: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Dec  9 08:25:12.493: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Dec  9 08:25:12.496: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"33428"},"items":null}

    Dec  9 08:25:12.503: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"33428"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Dec  9 08:25:12.510: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-6088" for this suite. 12/09/22 08:25:12.513
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:69
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 08:25:12.519
Dec  9 08:25:12.519: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename crd-publish-openapi 12/09/22 08:25:12.52
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:25:12.537
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:25:12.54
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:69
Dec  9 08:25:12.544: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: kubectl validation (kubectl create and apply) allows request with known and required properties 12/09/22 08:25:14.279
Dec  9 08:25:14.279: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=crd-publish-openapi-3641 --namespace=crd-publish-openapi-3641 create -f -'
Dec  9 08:25:14.951: INFO: stderr: ""
Dec  9 08:25:14.951: INFO: stdout: "e2e-test-crd-publish-openapi-3531-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Dec  9 08:25:14.952: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=crd-publish-openapi-3641 --namespace=crd-publish-openapi-3641 delete e2e-test-crd-publish-openapi-3531-crds test-foo'
Dec  9 08:25:15.068: INFO: stderr: ""
Dec  9 08:25:15.069: INFO: stdout: "e2e-test-crd-publish-openapi-3531-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Dec  9 08:25:15.069: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=crd-publish-openapi-3641 --namespace=crd-publish-openapi-3641 apply -f -'
Dec  9 08:25:15.367: INFO: stderr: ""
Dec  9 08:25:15.368: INFO: stdout: "e2e-test-crd-publish-openapi-3531-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Dec  9 08:25:15.368: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=crd-publish-openapi-3641 --namespace=crd-publish-openapi-3641 delete e2e-test-crd-publish-openapi-3531-crds test-foo'
Dec  9 08:25:15.455: INFO: stderr: ""
Dec  9 08:25:15.455: INFO: stdout: "e2e-test-crd-publish-openapi-3531-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values 12/09/22 08:25:15.455
Dec  9 08:25:15.456: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=crd-publish-openapi-3641 --namespace=crd-publish-openapi-3641 create -f -'
Dec  9 08:25:15.693: INFO: rc: 1
STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema 12/09/22 08:25:15.693
Dec  9 08:25:15.694: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=crd-publish-openapi-3641 --namespace=crd-publish-openapi-3641 create -f -'
Dec  9 08:25:15.881: INFO: rc: 1
Dec  9 08:25:15.881: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=crd-publish-openapi-3641 --namespace=crd-publish-openapi-3641 apply -f -'
Dec  9 08:25:16.091: INFO: rc: 1
STEP: kubectl validation (kubectl create and apply) rejects request without required properties 12/09/22 08:25:16.091
Dec  9 08:25:16.091: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=crd-publish-openapi-3641 --namespace=crd-publish-openapi-3641 create -f -'
Dec  9 08:25:16.331: INFO: rc: 1
Dec  9 08:25:16.331: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=crd-publish-openapi-3641 --namespace=crd-publish-openapi-3641 apply -f -'
Dec  9 08:25:16.576: INFO: rc: 1
STEP: kubectl explain works to explain CR properties 12/09/22 08:25:16.576
Dec  9 08:25:16.576: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=crd-publish-openapi-3641 explain e2e-test-crd-publish-openapi-3531-crds'
Dec  9 08:25:16.781: INFO: stderr: ""
Dec  9 08:25:16.781: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-3531-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively 12/09/22 08:25:16.782
Dec  9 08:25:16.782: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=crd-publish-openapi-3641 explain e2e-test-crd-publish-openapi-3531-crds.metadata'
Dec  9 08:25:16.981: INFO: stderr: ""
Dec  9 08:25:16.981: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-3531-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     return a 409.\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     Deprecated: selfLink is a legacy read-only field that is no longer\n     populated by the system.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Dec  9 08:25:16.982: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=crd-publish-openapi-3641 explain e2e-test-crd-publish-openapi-3531-crds.spec'
Dec  9 08:25:17.187: INFO: stderr: ""
Dec  9 08:25:17.187: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-3531-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Dec  9 08:25:17.187: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=crd-publish-openapi-3641 explain e2e-test-crd-publish-openapi-3531-crds.spec.bars'
Dec  9 08:25:17.378: INFO: stderr: ""
Dec  9 08:25:17.378: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-3531-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   feeling\t<string>\n     Whether Bar is feeling great.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist 12/09/22 08:25:17.378
Dec  9 08:25:17.379: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=crd-publish-openapi-3641 explain e2e-test-crd-publish-openapi-3531-crds.spec.bars2'
Dec  9 08:25:17.576: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Dec  9 08:25:19.359: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-3641" for this suite. 12/09/22 08:25:19.369
------------------------------
â€¢ [SLOW TEST] [6.855 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:69

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 08:25:12.519
    Dec  9 08:25:12.519: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename crd-publish-openapi 12/09/22 08:25:12.52
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:25:12.537
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:25:12.54
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for CRD with validation schema [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:69
    Dec  9 08:25:12.544: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: kubectl validation (kubectl create and apply) allows request with known and required properties 12/09/22 08:25:14.279
    Dec  9 08:25:14.279: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=crd-publish-openapi-3641 --namespace=crd-publish-openapi-3641 create -f -'
    Dec  9 08:25:14.951: INFO: stderr: ""
    Dec  9 08:25:14.951: INFO: stdout: "e2e-test-crd-publish-openapi-3531-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
    Dec  9 08:25:14.952: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=crd-publish-openapi-3641 --namespace=crd-publish-openapi-3641 delete e2e-test-crd-publish-openapi-3531-crds test-foo'
    Dec  9 08:25:15.068: INFO: stderr: ""
    Dec  9 08:25:15.069: INFO: stdout: "e2e-test-crd-publish-openapi-3531-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
    Dec  9 08:25:15.069: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=crd-publish-openapi-3641 --namespace=crd-publish-openapi-3641 apply -f -'
    Dec  9 08:25:15.367: INFO: stderr: ""
    Dec  9 08:25:15.368: INFO: stdout: "e2e-test-crd-publish-openapi-3531-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
    Dec  9 08:25:15.368: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=crd-publish-openapi-3641 --namespace=crd-publish-openapi-3641 delete e2e-test-crd-publish-openapi-3531-crds test-foo'
    Dec  9 08:25:15.455: INFO: stderr: ""
    Dec  9 08:25:15.455: INFO: stdout: "e2e-test-crd-publish-openapi-3531-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
    STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values 12/09/22 08:25:15.455
    Dec  9 08:25:15.456: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=crd-publish-openapi-3641 --namespace=crd-publish-openapi-3641 create -f -'
    Dec  9 08:25:15.693: INFO: rc: 1
    STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema 12/09/22 08:25:15.693
    Dec  9 08:25:15.694: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=crd-publish-openapi-3641 --namespace=crd-publish-openapi-3641 create -f -'
    Dec  9 08:25:15.881: INFO: rc: 1
    Dec  9 08:25:15.881: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=crd-publish-openapi-3641 --namespace=crd-publish-openapi-3641 apply -f -'
    Dec  9 08:25:16.091: INFO: rc: 1
    STEP: kubectl validation (kubectl create and apply) rejects request without required properties 12/09/22 08:25:16.091
    Dec  9 08:25:16.091: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=crd-publish-openapi-3641 --namespace=crd-publish-openapi-3641 create -f -'
    Dec  9 08:25:16.331: INFO: rc: 1
    Dec  9 08:25:16.331: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=crd-publish-openapi-3641 --namespace=crd-publish-openapi-3641 apply -f -'
    Dec  9 08:25:16.576: INFO: rc: 1
    STEP: kubectl explain works to explain CR properties 12/09/22 08:25:16.576
    Dec  9 08:25:16.576: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=crd-publish-openapi-3641 explain e2e-test-crd-publish-openapi-3531-crds'
    Dec  9 08:25:16.781: INFO: stderr: ""
    Dec  9 08:25:16.781: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-3531-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
    STEP: kubectl explain works to explain CR properties recursively 12/09/22 08:25:16.782
    Dec  9 08:25:16.782: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=crd-publish-openapi-3641 explain e2e-test-crd-publish-openapi-3531-crds.metadata'
    Dec  9 08:25:16.981: INFO: stderr: ""
    Dec  9 08:25:16.981: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-3531-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     return a 409.\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     Deprecated: selfLink is a legacy read-only field that is no longer\n     populated by the system.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
    Dec  9 08:25:16.982: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=crd-publish-openapi-3641 explain e2e-test-crd-publish-openapi-3531-crds.spec'
    Dec  9 08:25:17.187: INFO: stderr: ""
    Dec  9 08:25:17.187: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-3531-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
    Dec  9 08:25:17.187: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=crd-publish-openapi-3641 explain e2e-test-crd-publish-openapi-3531-crds.spec.bars'
    Dec  9 08:25:17.378: INFO: stderr: ""
    Dec  9 08:25:17.378: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-3531-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   feeling\t<string>\n     Whether Bar is feeling great.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
    STEP: kubectl explain works to return error when explain is called on property that doesn't exist 12/09/22 08:25:17.378
    Dec  9 08:25:17.379: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=crd-publish-openapi-3641 explain e2e-test-crd-publish-openapi-3531-crds.spec.bars2'
    Dec  9 08:25:17.576: INFO: rc: 1
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Dec  9 08:25:19.359: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-3641" for this suite. 12/09/22 08:25:19.369
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 08:25:19.374
Dec  9 08:25:19.375: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename replicaset 12/09/22 08:25:19.376
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:25:19.393
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:25:19.408
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154
Dec  9 08:25:19.426: INFO: Pod name sample-pod: Found 0 pods out of 1
Dec  9 08:25:24.433: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 12/09/22 08:25:24.433
STEP: Scaling up "test-rs" replicaset  12/09/22 08:25:24.433
Dec  9 08:25:24.444: INFO: Updating replica set "test-rs"
STEP: patching the ReplicaSet 12/09/22 08:25:24.444
W1209 08:25:24.457292      20 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Dec  9 08:25:24.463: INFO: observed ReplicaSet test-rs in namespace replicaset-4600 with ReadyReplicas 1, AvailableReplicas 1
Dec  9 08:25:24.493: INFO: observed ReplicaSet test-rs in namespace replicaset-4600 with ReadyReplicas 1, AvailableReplicas 1
Dec  9 08:25:24.542: INFO: observed ReplicaSet test-rs in namespace replicaset-4600 with ReadyReplicas 1, AvailableReplicas 1
Dec  9 08:25:24.558: INFO: observed ReplicaSet test-rs in namespace replicaset-4600 with ReadyReplicas 1, AvailableReplicas 1
Dec  9 08:25:25.468: INFO: observed ReplicaSet test-rs in namespace replicaset-4600 with ReadyReplicas 2, AvailableReplicas 2
Dec  9 08:25:26.368: INFO: observed Replicaset test-rs in namespace replicaset-4600 with ReadyReplicas 3 found true
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Dec  9 08:25:26.368: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-4600" for this suite. 12/09/22 08:25:26.374
------------------------------
â€¢ [SLOW TEST] [7.005 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 08:25:19.374
    Dec  9 08:25:19.375: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename replicaset 12/09/22 08:25:19.376
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:25:19.393
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:25:19.408
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] Replace and Patch tests [Conformance]
      test/e2e/apps/replica_set.go:154
    Dec  9 08:25:19.426: INFO: Pod name sample-pod: Found 0 pods out of 1
    Dec  9 08:25:24.433: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 12/09/22 08:25:24.433
    STEP: Scaling up "test-rs" replicaset  12/09/22 08:25:24.433
    Dec  9 08:25:24.444: INFO: Updating replica set "test-rs"
    STEP: patching the ReplicaSet 12/09/22 08:25:24.444
    W1209 08:25:24.457292      20 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Dec  9 08:25:24.463: INFO: observed ReplicaSet test-rs in namespace replicaset-4600 with ReadyReplicas 1, AvailableReplicas 1
    Dec  9 08:25:24.493: INFO: observed ReplicaSet test-rs in namespace replicaset-4600 with ReadyReplicas 1, AvailableReplicas 1
    Dec  9 08:25:24.542: INFO: observed ReplicaSet test-rs in namespace replicaset-4600 with ReadyReplicas 1, AvailableReplicas 1
    Dec  9 08:25:24.558: INFO: observed ReplicaSet test-rs in namespace replicaset-4600 with ReadyReplicas 1, AvailableReplicas 1
    Dec  9 08:25:25.468: INFO: observed ReplicaSet test-rs in namespace replicaset-4600 with ReadyReplicas 2, AvailableReplicas 2
    Dec  9 08:25:26.368: INFO: observed Replicaset test-rs in namespace replicaset-4600 with ReadyReplicas 3 found true
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Dec  9 08:25:26.368: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-4600" for this suite. 12/09/22 08:25:26.374
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should provide secure master service  [Conformance]
  test/e2e/network/service.go:777
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 08:25:26.383
Dec  9 08:25:26.383: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename services 12/09/22 08:25:26.384
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:25:26.4
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:25:26.404
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should provide secure master service  [Conformance]
  test/e2e/network/service.go:777
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Dec  9 08:25:26.410: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-5380" for this suite. 12/09/22 08:25:26.413
------------------------------
â€¢ [0.035 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should provide secure master service  [Conformance]
  test/e2e/network/service.go:777

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 08:25:26.383
    Dec  9 08:25:26.383: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename services 12/09/22 08:25:26.384
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:25:26.4
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:25:26.404
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should provide secure master service  [Conformance]
      test/e2e/network/service.go:777
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Dec  9 08:25:26.410: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-5380" for this suite. 12/09/22 08:25:26.413
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:74
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 08:25:26.419
Dec  9 08:25:26.419: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename projected 12/09/22 08:25:26.42
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:25:26.437
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:25:26.44
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:74
STEP: Creating configMap with name projected-configmap-test-volume-baa4d8c1-aafc-452d-992b-736423829d7a 12/09/22 08:25:26.443
STEP: Creating a pod to test consume configMaps 12/09/22 08:25:26.449
Dec  9 08:25:26.457: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-bab1404f-53e8-47ff-bf3f-452b7f20174f" in namespace "projected-884" to be "Succeeded or Failed"
Dec  9 08:25:26.461: INFO: Pod "pod-projected-configmaps-bab1404f-53e8-47ff-bf3f-452b7f20174f": Phase="Pending", Reason="", readiness=false. Elapsed: 3.581288ms
Dec  9 08:25:28.479: INFO: Pod "pod-projected-configmaps-bab1404f-53e8-47ff-bf3f-452b7f20174f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021408404s
Dec  9 08:25:30.466: INFO: Pod "pod-projected-configmaps-bab1404f-53e8-47ff-bf3f-452b7f20174f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009007344s
STEP: Saw pod success 12/09/22 08:25:30.467
Dec  9 08:25:30.467: INFO: Pod "pod-projected-configmaps-bab1404f-53e8-47ff-bf3f-452b7f20174f" satisfied condition "Succeeded or Failed"
Dec  9 08:25:30.469: INFO: Trying to get logs from node ip-10-0-10-179 pod pod-projected-configmaps-bab1404f-53e8-47ff-bf3f-452b7f20174f container agnhost-container: <nil>
STEP: delete the pod 12/09/22 08:25:30.478
Dec  9 08:25:30.493: INFO: Waiting for pod pod-projected-configmaps-bab1404f-53e8-47ff-bf3f-452b7f20174f to disappear
Dec  9 08:25:30.499: INFO: Pod pod-projected-configmaps-bab1404f-53e8-47ff-bf3f-452b7f20174f no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Dec  9 08:25:30.499: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-884" for this suite. 12/09/22 08:25:30.508
------------------------------
â€¢ [4.098 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:74

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 08:25:26.419
    Dec  9 08:25:26.419: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename projected 12/09/22 08:25:26.42
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:25:26.437
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:25:26.44
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:74
    STEP: Creating configMap with name projected-configmap-test-volume-baa4d8c1-aafc-452d-992b-736423829d7a 12/09/22 08:25:26.443
    STEP: Creating a pod to test consume configMaps 12/09/22 08:25:26.449
    Dec  9 08:25:26.457: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-bab1404f-53e8-47ff-bf3f-452b7f20174f" in namespace "projected-884" to be "Succeeded or Failed"
    Dec  9 08:25:26.461: INFO: Pod "pod-projected-configmaps-bab1404f-53e8-47ff-bf3f-452b7f20174f": Phase="Pending", Reason="", readiness=false. Elapsed: 3.581288ms
    Dec  9 08:25:28.479: INFO: Pod "pod-projected-configmaps-bab1404f-53e8-47ff-bf3f-452b7f20174f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021408404s
    Dec  9 08:25:30.466: INFO: Pod "pod-projected-configmaps-bab1404f-53e8-47ff-bf3f-452b7f20174f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009007344s
    STEP: Saw pod success 12/09/22 08:25:30.467
    Dec  9 08:25:30.467: INFO: Pod "pod-projected-configmaps-bab1404f-53e8-47ff-bf3f-452b7f20174f" satisfied condition "Succeeded or Failed"
    Dec  9 08:25:30.469: INFO: Trying to get logs from node ip-10-0-10-179 pod pod-projected-configmaps-bab1404f-53e8-47ff-bf3f-452b7f20174f container agnhost-container: <nil>
    STEP: delete the pod 12/09/22 08:25:30.478
    Dec  9 08:25:30.493: INFO: Waiting for pod pod-projected-configmaps-bab1404f-53e8-47ff-bf3f-452b7f20174f to disappear
    Dec  9 08:25:30.499: INFO: Pod pod-projected-configmaps-bab1404f-53e8-47ff-bf3f-452b7f20174f no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Dec  9 08:25:30.499: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-884" for this suite. 12/09/22 08:25:30.508
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:341
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 08:25:30.526
Dec  9 08:25:30.527: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename webhook 12/09/22 08:25:30.535
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:25:30.554
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:25:30.568
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 12/09/22 08:25:30.597
STEP: Create role binding to let webhook read extension-apiserver-authentication 12/09/22 08:25:31.248
STEP: Deploying the webhook pod 12/09/22 08:25:31.256
STEP: Wait for the deployment to be ready 12/09/22 08:25:31.268
Dec  9 08:25:31.278: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Dec  9 08:25:33.288: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.December, 9, 8, 25, 31, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 9, 8, 25, 31, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 9, 8, 25, 31, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 9, 8, 25, 31, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 12/09/22 08:25:35.292
STEP: Verifying the service has paired with the endpoint 12/09/22 08:25:35.302
Dec  9 08:25:36.305: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:341
Dec  9 08:25:36.310: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-84-crds.webhook.example.com via the AdmissionRegistration API 12/09/22 08:25:36.835
STEP: Creating a custom resource that should be mutated by the webhook 12/09/22 08:25:36.864
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Dec  9 08:25:39.471: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-2828" for this suite. 12/09/22 08:25:39.536
STEP: Destroying namespace "webhook-2828-markers" for this suite. 12/09/22 08:25:39.545
------------------------------
â€¢ [SLOW TEST] [9.035 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:341

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 08:25:30.526
    Dec  9 08:25:30.527: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename webhook 12/09/22 08:25:30.535
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:25:30.554
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:25:30.568
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 12/09/22 08:25:30.597
    STEP: Create role binding to let webhook read extension-apiserver-authentication 12/09/22 08:25:31.248
    STEP: Deploying the webhook pod 12/09/22 08:25:31.256
    STEP: Wait for the deployment to be ready 12/09/22 08:25:31.268
    Dec  9 08:25:31.278: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Dec  9 08:25:33.288: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.December, 9, 8, 25, 31, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 9, 8, 25, 31, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 9, 8, 25, 31, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 9, 8, 25, 31, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 12/09/22 08:25:35.292
    STEP: Verifying the service has paired with the endpoint 12/09/22 08:25:35.302
    Dec  9 08:25:36.305: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource with pruning [Conformance]
      test/e2e/apimachinery/webhook.go:341
    Dec  9 08:25:36.310: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-84-crds.webhook.example.com via the AdmissionRegistration API 12/09/22 08:25:36.835
    STEP: Creating a custom resource that should be mutated by the webhook 12/09/22 08:25:36.864
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Dec  9 08:25:39.471: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-2828" for this suite. 12/09/22 08:25:39.536
    STEP: Destroying namespace "webhook-2828-markers" for this suite. 12/09/22 08:25:39.545
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:108
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 08:25:39.567
Dec  9 08:25:39.567: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename container-probe 12/09/22 08:25:39.568
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:25:39.591
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:25:39.598
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:108
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Dec  9 08:26:39.615: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-1793" for this suite. 12/09/22 08:26:39.618
------------------------------
â€¢ [SLOW TEST] [60.057 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:108

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 08:25:39.567
    Dec  9 08:25:39.567: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename container-probe 12/09/22 08:25:39.568
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:25:39.591
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:25:39.598
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:108
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Dec  9 08:26:39.615: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-1793" for this suite. 12/09/22 08:26:39.618
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch
  watch on custom resource definition objects [Conformance]
  test/e2e/apimachinery/crd_watch.go:51
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 08:26:39.63
Dec  9 08:26:39.630: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename crd-watch 12/09/22 08:26:39.631
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:26:39.65
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:26:39.654
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] watch on custom resource definition objects [Conformance]
  test/e2e/apimachinery/crd_watch.go:51
Dec  9 08:26:39.657: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Creating first CR  12/09/22 08:26:42.211
Dec  9 08:26:42.217: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-12-09T08:26:42Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-12-09T08:26:42Z]] name:name1 resourceVersion:33856 uid:981b21ed-5963-4bb6-bdf5-a0b303e46aa7] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR 12/09/22 08:26:52.218
Dec  9 08:26:52.228: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-12-09T08:26:52Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-12-09T08:26:52Z]] name:name2 resourceVersion:33890 uid:24f4afe9-5caf-464b-a686-573207d8933a] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR 12/09/22 08:27:02.229
Dec  9 08:27:02.245: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-12-09T08:26:42Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-12-09T08:27:02Z]] name:name1 resourceVersion:33906 uid:981b21ed-5963-4bb6-bdf5-a0b303e46aa7] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR 12/09/22 08:27:12.246
Dec  9 08:27:12.255: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-12-09T08:26:52Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-12-09T08:27:12Z]] name:name2 resourceVersion:33922 uid:24f4afe9-5caf-464b-a686-573207d8933a] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR 12/09/22 08:27:22.255
Dec  9 08:27:22.264: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-12-09T08:26:42Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-12-09T08:27:02Z]] name:name1 resourceVersion:33938 uid:981b21ed-5963-4bb6-bdf5-a0b303e46aa7] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR 12/09/22 08:27:32.265
Dec  9 08:27:32.275: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-12-09T08:26:52Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-12-09T08:27:12Z]] name:name2 resourceVersion:33954 uid:24f4afe9-5caf-464b-a686-573207d8933a] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Dec  9 08:27:42.795: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-watch-6474" for this suite. 12/09/22 08:27:42.799
------------------------------
â€¢ [SLOW TEST] [63.176 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  test/e2e/apimachinery/crd_watch.go:44
    watch on custom resource definition objects [Conformance]
    test/e2e/apimachinery/crd_watch.go:51

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 08:26:39.63
    Dec  9 08:26:39.630: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename crd-watch 12/09/22 08:26:39.631
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:26:39.65
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:26:39.654
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] watch on custom resource definition objects [Conformance]
      test/e2e/apimachinery/crd_watch.go:51
    Dec  9 08:26:39.657: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Creating first CR  12/09/22 08:26:42.211
    Dec  9 08:26:42.217: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-12-09T08:26:42Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-12-09T08:26:42Z]] name:name1 resourceVersion:33856 uid:981b21ed-5963-4bb6-bdf5-a0b303e46aa7] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Creating second CR 12/09/22 08:26:52.218
    Dec  9 08:26:52.228: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-12-09T08:26:52Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-12-09T08:26:52Z]] name:name2 resourceVersion:33890 uid:24f4afe9-5caf-464b-a686-573207d8933a] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Modifying first CR 12/09/22 08:27:02.229
    Dec  9 08:27:02.245: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-12-09T08:26:42Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-12-09T08:27:02Z]] name:name1 resourceVersion:33906 uid:981b21ed-5963-4bb6-bdf5-a0b303e46aa7] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Modifying second CR 12/09/22 08:27:12.246
    Dec  9 08:27:12.255: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-12-09T08:26:52Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-12-09T08:27:12Z]] name:name2 resourceVersion:33922 uid:24f4afe9-5caf-464b-a686-573207d8933a] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Deleting first CR 12/09/22 08:27:22.255
    Dec  9 08:27:22.264: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-12-09T08:26:42Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-12-09T08:27:02Z]] name:name1 resourceVersion:33938 uid:981b21ed-5963-4bb6-bdf5-a0b303e46aa7] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Deleting second CR 12/09/22 08:27:32.265
    Dec  9 08:27:32.275: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-12-09T08:26:52Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-12-09T08:27:12Z]] name:name2 resourceVersion:33954 uid:24f4afe9-5caf-464b-a686-573207d8933a] num:map[num1:9223372036854775807 num2:1000000]]}
    [AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Dec  9 08:27:42.795: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-watch-6474" for this suite. 12/09/22 08:27:42.799
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo
  should scale a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:352
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 08:27:42.811
Dec  9 08:27:42.812: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename kubectl 12/09/22 08:27:42.812
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:27:42.834
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:27:42.846
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Update Demo
  test/e2e/kubectl/kubectl.go:326
[It] should scale a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:352
STEP: creating a replication controller 12/09/22 08:27:42.852
Dec  9 08:27:42.852: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=kubectl-5803 create -f -'
Dec  9 08:27:44.240: INFO: stderr: ""
Dec  9 08:27:44.240: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up. 12/09/22 08:27:44.24
Dec  9 08:27:44.240: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=kubectl-5803 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Dec  9 08:27:44.413: INFO: stderr: ""
Dec  9 08:27:44.413: INFO: stdout: "update-demo-nautilus-2zwpk update-demo-nautilus-c9hdj "
Dec  9 08:27:44.414: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=kubectl-5803 get pods update-demo-nautilus-2zwpk -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Dec  9 08:27:44.493: INFO: stderr: ""
Dec  9 08:27:44.493: INFO: stdout: ""
Dec  9 08:27:44.493: INFO: update-demo-nautilus-2zwpk is created but not running
Dec  9 08:27:49.493: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=kubectl-5803 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Dec  9 08:27:49.701: INFO: stderr: ""
Dec  9 08:27:49.701: INFO: stdout: "update-demo-nautilus-2zwpk update-demo-nautilus-c9hdj "
Dec  9 08:27:49.701: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=kubectl-5803 get pods update-demo-nautilus-2zwpk -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Dec  9 08:27:49.908: INFO: stderr: ""
Dec  9 08:27:49.908: INFO: stdout: "true"
Dec  9 08:27:49.908: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=kubectl-5803 get pods update-demo-nautilus-2zwpk -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Dec  9 08:27:50.111: INFO: stderr: ""
Dec  9 08:27:50.112: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Dec  9 08:27:50.112: INFO: validating pod update-demo-nautilus-2zwpk
Dec  9 08:27:50.130: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec  9 08:27:50.130: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec  9 08:27:50.130: INFO: update-demo-nautilus-2zwpk is verified up and running
Dec  9 08:27:50.130: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=kubectl-5803 get pods update-demo-nautilus-c9hdj -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Dec  9 08:27:50.219: INFO: stderr: ""
Dec  9 08:27:50.219: INFO: stdout: "true"
Dec  9 08:27:50.219: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=kubectl-5803 get pods update-demo-nautilus-c9hdj -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Dec  9 08:27:50.288: INFO: stderr: ""
Dec  9 08:27:50.288: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Dec  9 08:27:50.288: INFO: validating pod update-demo-nautilus-c9hdj
Dec  9 08:27:50.301: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec  9 08:27:50.301: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec  9 08:27:50.301: INFO: update-demo-nautilus-c9hdj is verified up and running
STEP: scaling down the replication controller 12/09/22 08:27:50.301
Dec  9 08:27:50.307: INFO: scanned /root for discovery docs: <nil>
Dec  9 08:27:50.307: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=kubectl-5803 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
Dec  9 08:27:51.390: INFO: stderr: ""
Dec  9 08:27:51.390: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up. 12/09/22 08:27:51.39
Dec  9 08:27:51.391: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=kubectl-5803 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Dec  9 08:27:51.463: INFO: stderr: ""
Dec  9 08:27:51.463: INFO: stdout: "update-demo-nautilus-2zwpk update-demo-nautilus-c9hdj "
STEP: Replicas for name=update-demo: expected=1 actual=2 12/09/22 08:27:51.463
Dec  9 08:27:56.465: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=kubectl-5803 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Dec  9 08:27:56.589: INFO: stderr: ""
Dec  9 08:27:56.589: INFO: stdout: "update-demo-nautilus-2zwpk "
Dec  9 08:27:56.589: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=kubectl-5803 get pods update-demo-nautilus-2zwpk -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Dec  9 08:27:56.665: INFO: stderr: ""
Dec  9 08:27:56.665: INFO: stdout: "true"
Dec  9 08:27:56.665: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=kubectl-5803 get pods update-demo-nautilus-2zwpk -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Dec  9 08:27:56.739: INFO: stderr: ""
Dec  9 08:27:56.739: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Dec  9 08:27:56.739: INFO: validating pod update-demo-nautilus-2zwpk
Dec  9 08:27:56.744: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec  9 08:27:56.744: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec  9 08:27:56.744: INFO: update-demo-nautilus-2zwpk is verified up and running
STEP: scaling up the replication controller 12/09/22 08:27:56.744
Dec  9 08:27:56.745: INFO: scanned /root for discovery docs: <nil>
Dec  9 08:27:56.745: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=kubectl-5803 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
Dec  9 08:27:57.849: INFO: stderr: ""
Dec  9 08:27:57.849: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up. 12/09/22 08:27:57.849
Dec  9 08:27:57.849: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=kubectl-5803 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Dec  9 08:27:57.928: INFO: stderr: ""
Dec  9 08:27:57.928: INFO: stdout: "update-demo-nautilus-2zwpk update-demo-nautilus-tblhf "
Dec  9 08:27:57.928: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=kubectl-5803 get pods update-demo-nautilus-2zwpk -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Dec  9 08:27:58.007: INFO: stderr: ""
Dec  9 08:27:58.007: INFO: stdout: "true"
Dec  9 08:27:58.007: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=kubectl-5803 get pods update-demo-nautilus-2zwpk -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Dec  9 08:27:58.074: INFO: stderr: ""
Dec  9 08:27:58.074: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Dec  9 08:27:58.074: INFO: validating pod update-demo-nautilus-2zwpk
Dec  9 08:27:58.078: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec  9 08:27:58.078: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec  9 08:27:58.078: INFO: update-demo-nautilus-2zwpk is verified up and running
Dec  9 08:27:58.078: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=kubectl-5803 get pods update-demo-nautilus-tblhf -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Dec  9 08:27:58.146: INFO: stderr: ""
Dec  9 08:27:58.146: INFO: stdout: ""
Dec  9 08:27:58.146: INFO: update-demo-nautilus-tblhf is created but not running
Dec  9 08:28:03.146: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=kubectl-5803 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Dec  9 08:28:03.285: INFO: stderr: ""
Dec  9 08:28:03.285: INFO: stdout: "update-demo-nautilus-2zwpk update-demo-nautilus-tblhf "
Dec  9 08:28:03.285: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=kubectl-5803 get pods update-demo-nautilus-2zwpk -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Dec  9 08:28:03.416: INFO: stderr: ""
Dec  9 08:28:03.416: INFO: stdout: "true"
Dec  9 08:28:03.416: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=kubectl-5803 get pods update-demo-nautilus-2zwpk -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Dec  9 08:28:03.525: INFO: stderr: ""
Dec  9 08:28:03.525: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Dec  9 08:28:03.525: INFO: validating pod update-demo-nautilus-2zwpk
Dec  9 08:28:03.529: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec  9 08:28:03.529: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec  9 08:28:03.529: INFO: update-demo-nautilus-2zwpk is verified up and running
Dec  9 08:28:03.529: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=kubectl-5803 get pods update-demo-nautilus-tblhf -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Dec  9 08:28:03.641: INFO: stderr: ""
Dec  9 08:28:03.641: INFO: stdout: "true"
Dec  9 08:28:03.641: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=kubectl-5803 get pods update-demo-nautilus-tblhf -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Dec  9 08:28:03.746: INFO: stderr: ""
Dec  9 08:28:03.746: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Dec  9 08:28:03.746: INFO: validating pod update-demo-nautilus-tblhf
Dec  9 08:28:03.752: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec  9 08:28:03.752: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec  9 08:28:03.752: INFO: update-demo-nautilus-tblhf is verified up and running
STEP: using delete to clean up resources 12/09/22 08:28:03.752
Dec  9 08:28:03.752: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=kubectl-5803 delete --grace-period=0 --force -f -'
Dec  9 08:28:03.859: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec  9 08:28:03.859: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Dec  9 08:28:03.860: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=kubectl-5803 get rc,svc -l name=update-demo --no-headers'
Dec  9 08:28:04.069: INFO: stderr: "No resources found in kubectl-5803 namespace.\n"
Dec  9 08:28:04.069: INFO: stdout: ""
Dec  9 08:28:04.069: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=kubectl-5803 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Dec  9 08:28:04.219: INFO: stderr: ""
Dec  9 08:28:04.219: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Dec  9 08:28:04.219: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-5803" for this suite. 12/09/22 08:28:04.224
------------------------------
â€¢ [SLOW TEST] [21.437 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Update Demo
  test/e2e/kubectl/kubectl.go:324
    should scale a replication controller  [Conformance]
    test/e2e/kubectl/kubectl.go:352

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 08:27:42.811
    Dec  9 08:27:42.812: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename kubectl 12/09/22 08:27:42.812
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:27:42.834
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:27:42.846
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Update Demo
      test/e2e/kubectl/kubectl.go:326
    [It] should scale a replication controller  [Conformance]
      test/e2e/kubectl/kubectl.go:352
    STEP: creating a replication controller 12/09/22 08:27:42.852
    Dec  9 08:27:42.852: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=kubectl-5803 create -f -'
    Dec  9 08:27:44.240: INFO: stderr: ""
    Dec  9 08:27:44.240: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 12/09/22 08:27:44.24
    Dec  9 08:27:44.240: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=kubectl-5803 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Dec  9 08:27:44.413: INFO: stderr: ""
    Dec  9 08:27:44.413: INFO: stdout: "update-demo-nautilus-2zwpk update-demo-nautilus-c9hdj "
    Dec  9 08:27:44.414: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=kubectl-5803 get pods update-demo-nautilus-2zwpk -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Dec  9 08:27:44.493: INFO: stderr: ""
    Dec  9 08:27:44.493: INFO: stdout: ""
    Dec  9 08:27:44.493: INFO: update-demo-nautilus-2zwpk is created but not running
    Dec  9 08:27:49.493: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=kubectl-5803 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Dec  9 08:27:49.701: INFO: stderr: ""
    Dec  9 08:27:49.701: INFO: stdout: "update-demo-nautilus-2zwpk update-demo-nautilus-c9hdj "
    Dec  9 08:27:49.701: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=kubectl-5803 get pods update-demo-nautilus-2zwpk -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Dec  9 08:27:49.908: INFO: stderr: ""
    Dec  9 08:27:49.908: INFO: stdout: "true"
    Dec  9 08:27:49.908: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=kubectl-5803 get pods update-demo-nautilus-2zwpk -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Dec  9 08:27:50.111: INFO: stderr: ""
    Dec  9 08:27:50.112: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Dec  9 08:27:50.112: INFO: validating pod update-demo-nautilus-2zwpk
    Dec  9 08:27:50.130: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Dec  9 08:27:50.130: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Dec  9 08:27:50.130: INFO: update-demo-nautilus-2zwpk is verified up and running
    Dec  9 08:27:50.130: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=kubectl-5803 get pods update-demo-nautilus-c9hdj -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Dec  9 08:27:50.219: INFO: stderr: ""
    Dec  9 08:27:50.219: INFO: stdout: "true"
    Dec  9 08:27:50.219: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=kubectl-5803 get pods update-demo-nautilus-c9hdj -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Dec  9 08:27:50.288: INFO: stderr: ""
    Dec  9 08:27:50.288: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Dec  9 08:27:50.288: INFO: validating pod update-demo-nautilus-c9hdj
    Dec  9 08:27:50.301: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Dec  9 08:27:50.301: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Dec  9 08:27:50.301: INFO: update-demo-nautilus-c9hdj is verified up and running
    STEP: scaling down the replication controller 12/09/22 08:27:50.301
    Dec  9 08:27:50.307: INFO: scanned /root for discovery docs: <nil>
    Dec  9 08:27:50.307: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=kubectl-5803 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
    Dec  9 08:27:51.390: INFO: stderr: ""
    Dec  9 08:27:51.390: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 12/09/22 08:27:51.39
    Dec  9 08:27:51.391: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=kubectl-5803 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Dec  9 08:27:51.463: INFO: stderr: ""
    Dec  9 08:27:51.463: INFO: stdout: "update-demo-nautilus-2zwpk update-demo-nautilus-c9hdj "
    STEP: Replicas for name=update-demo: expected=1 actual=2 12/09/22 08:27:51.463
    Dec  9 08:27:56.465: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=kubectl-5803 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Dec  9 08:27:56.589: INFO: stderr: ""
    Dec  9 08:27:56.589: INFO: stdout: "update-demo-nautilus-2zwpk "
    Dec  9 08:27:56.589: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=kubectl-5803 get pods update-demo-nautilus-2zwpk -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Dec  9 08:27:56.665: INFO: stderr: ""
    Dec  9 08:27:56.665: INFO: stdout: "true"
    Dec  9 08:27:56.665: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=kubectl-5803 get pods update-demo-nautilus-2zwpk -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Dec  9 08:27:56.739: INFO: stderr: ""
    Dec  9 08:27:56.739: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Dec  9 08:27:56.739: INFO: validating pod update-demo-nautilus-2zwpk
    Dec  9 08:27:56.744: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Dec  9 08:27:56.744: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Dec  9 08:27:56.744: INFO: update-demo-nautilus-2zwpk is verified up and running
    STEP: scaling up the replication controller 12/09/22 08:27:56.744
    Dec  9 08:27:56.745: INFO: scanned /root for discovery docs: <nil>
    Dec  9 08:27:56.745: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=kubectl-5803 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
    Dec  9 08:27:57.849: INFO: stderr: ""
    Dec  9 08:27:57.849: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 12/09/22 08:27:57.849
    Dec  9 08:27:57.849: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=kubectl-5803 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Dec  9 08:27:57.928: INFO: stderr: ""
    Dec  9 08:27:57.928: INFO: stdout: "update-demo-nautilus-2zwpk update-demo-nautilus-tblhf "
    Dec  9 08:27:57.928: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=kubectl-5803 get pods update-demo-nautilus-2zwpk -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Dec  9 08:27:58.007: INFO: stderr: ""
    Dec  9 08:27:58.007: INFO: stdout: "true"
    Dec  9 08:27:58.007: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=kubectl-5803 get pods update-demo-nautilus-2zwpk -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Dec  9 08:27:58.074: INFO: stderr: ""
    Dec  9 08:27:58.074: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Dec  9 08:27:58.074: INFO: validating pod update-demo-nautilus-2zwpk
    Dec  9 08:27:58.078: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Dec  9 08:27:58.078: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Dec  9 08:27:58.078: INFO: update-demo-nautilus-2zwpk is verified up and running
    Dec  9 08:27:58.078: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=kubectl-5803 get pods update-demo-nautilus-tblhf -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Dec  9 08:27:58.146: INFO: stderr: ""
    Dec  9 08:27:58.146: INFO: stdout: ""
    Dec  9 08:27:58.146: INFO: update-demo-nautilus-tblhf is created but not running
    Dec  9 08:28:03.146: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=kubectl-5803 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Dec  9 08:28:03.285: INFO: stderr: ""
    Dec  9 08:28:03.285: INFO: stdout: "update-demo-nautilus-2zwpk update-demo-nautilus-tblhf "
    Dec  9 08:28:03.285: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=kubectl-5803 get pods update-demo-nautilus-2zwpk -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Dec  9 08:28:03.416: INFO: stderr: ""
    Dec  9 08:28:03.416: INFO: stdout: "true"
    Dec  9 08:28:03.416: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=kubectl-5803 get pods update-demo-nautilus-2zwpk -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Dec  9 08:28:03.525: INFO: stderr: ""
    Dec  9 08:28:03.525: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Dec  9 08:28:03.525: INFO: validating pod update-demo-nautilus-2zwpk
    Dec  9 08:28:03.529: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Dec  9 08:28:03.529: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Dec  9 08:28:03.529: INFO: update-demo-nautilus-2zwpk is verified up and running
    Dec  9 08:28:03.529: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=kubectl-5803 get pods update-demo-nautilus-tblhf -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Dec  9 08:28:03.641: INFO: stderr: ""
    Dec  9 08:28:03.641: INFO: stdout: "true"
    Dec  9 08:28:03.641: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=kubectl-5803 get pods update-demo-nautilus-tblhf -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Dec  9 08:28:03.746: INFO: stderr: ""
    Dec  9 08:28:03.746: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Dec  9 08:28:03.746: INFO: validating pod update-demo-nautilus-tblhf
    Dec  9 08:28:03.752: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Dec  9 08:28:03.752: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Dec  9 08:28:03.752: INFO: update-demo-nautilus-tblhf is verified up and running
    STEP: using delete to clean up resources 12/09/22 08:28:03.752
    Dec  9 08:28:03.752: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=kubectl-5803 delete --grace-period=0 --force -f -'
    Dec  9 08:28:03.859: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Dec  9 08:28:03.859: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
    Dec  9 08:28:03.860: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=kubectl-5803 get rc,svc -l name=update-demo --no-headers'
    Dec  9 08:28:04.069: INFO: stderr: "No resources found in kubectl-5803 namespace.\n"
    Dec  9 08:28:04.069: INFO: stdout: ""
    Dec  9 08:28:04.069: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=kubectl-5803 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Dec  9 08:28:04.219: INFO: stderr: ""
    Dec  9 08:28:04.219: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Dec  9 08:28:04.219: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-5803" for this suite. 12/09/22 08:28:04.224
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:99
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 08:28:04.25
Dec  9 08:28:04.250: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename configmap 12/09/22 08:28:04.252
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:28:04.278
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:28:04.283
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:99
STEP: Creating configMap with name configmap-test-volume-map-ffd99b52-58bf-4fa6-8c10-3c53d6838ea8 12/09/22 08:28:04.287
STEP: Creating a pod to test consume configMaps 12/09/22 08:28:04.307
Dec  9 08:28:04.332: INFO: Waiting up to 5m0s for pod "pod-configmaps-fc452e81-1685-4466-bec2-ec1bba5d331e" in namespace "configmap-2622" to be "Succeeded or Failed"
Dec  9 08:28:04.353: INFO: Pod "pod-configmaps-fc452e81-1685-4466-bec2-ec1bba5d331e": Phase="Pending", Reason="", readiness=false. Elapsed: 21.33398ms
Dec  9 08:28:06.389: INFO: Pod "pod-configmaps-fc452e81-1685-4466-bec2-ec1bba5d331e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.056917985s
Dec  9 08:28:08.357: INFO: Pod "pod-configmaps-fc452e81-1685-4466-bec2-ec1bba5d331e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025170052s
STEP: Saw pod success 12/09/22 08:28:08.357
Dec  9 08:28:08.357: INFO: Pod "pod-configmaps-fc452e81-1685-4466-bec2-ec1bba5d331e" satisfied condition "Succeeded or Failed"
Dec  9 08:28:08.361: INFO: Trying to get logs from node ip-10-0-10-179 pod pod-configmaps-fc452e81-1685-4466-bec2-ec1bba5d331e container agnhost-container: <nil>
STEP: delete the pod 12/09/22 08:28:08.374
Dec  9 08:28:08.386: INFO: Waiting for pod pod-configmaps-fc452e81-1685-4466-bec2-ec1bba5d331e to disappear
Dec  9 08:28:08.389: INFO: Pod pod-configmaps-fc452e81-1685-4466-bec2-ec1bba5d331e no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Dec  9 08:28:08.389: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-2622" for this suite. 12/09/22 08:28:08.393
------------------------------
â€¢ [4.149 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:99

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 08:28:04.25
    Dec  9 08:28:04.250: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename configmap 12/09/22 08:28:04.252
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:28:04.278
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:28:04.283
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:99
    STEP: Creating configMap with name configmap-test-volume-map-ffd99b52-58bf-4fa6-8c10-3c53d6838ea8 12/09/22 08:28:04.287
    STEP: Creating a pod to test consume configMaps 12/09/22 08:28:04.307
    Dec  9 08:28:04.332: INFO: Waiting up to 5m0s for pod "pod-configmaps-fc452e81-1685-4466-bec2-ec1bba5d331e" in namespace "configmap-2622" to be "Succeeded or Failed"
    Dec  9 08:28:04.353: INFO: Pod "pod-configmaps-fc452e81-1685-4466-bec2-ec1bba5d331e": Phase="Pending", Reason="", readiness=false. Elapsed: 21.33398ms
    Dec  9 08:28:06.389: INFO: Pod "pod-configmaps-fc452e81-1685-4466-bec2-ec1bba5d331e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.056917985s
    Dec  9 08:28:08.357: INFO: Pod "pod-configmaps-fc452e81-1685-4466-bec2-ec1bba5d331e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025170052s
    STEP: Saw pod success 12/09/22 08:28:08.357
    Dec  9 08:28:08.357: INFO: Pod "pod-configmaps-fc452e81-1685-4466-bec2-ec1bba5d331e" satisfied condition "Succeeded or Failed"
    Dec  9 08:28:08.361: INFO: Trying to get logs from node ip-10-0-10-179 pod pod-configmaps-fc452e81-1685-4466-bec2-ec1bba5d331e container agnhost-container: <nil>
    STEP: delete the pod 12/09/22 08:28:08.374
    Dec  9 08:28:08.386: INFO: Waiting for pod pod-configmaps-fc452e81-1685-4466-bec2-ec1bba5d331e to disappear
    Dec  9 08:28:08.389: INFO: Pod pod-configmaps-fc452e81-1685-4466-bec2-ec1bba5d331e no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Dec  9 08:28:08.389: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-2622" for this suite. 12/09/22 08:28:08.393
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 08:28:08.405
Dec  9 08:28:08.405: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename watch 12/09/22 08:28:08.406
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:28:08.425
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:28:08.429
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142
STEP: creating a new configmap 12/09/22 08:28:08.433
STEP: modifying the configmap once 12/09/22 08:28:08.437
STEP: modifying the configmap a second time 12/09/22 08:28:08.446
STEP: deleting the configmap 12/09/22 08:28:08.455
STEP: creating a watch on configmaps from the resource version returned by the first update 12/09/22 08:28:08.46
STEP: Expecting to observe notifications for all changes to the configmap after the first update 12/09/22 08:28:08.462
Dec  9 08:28:08.463: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-6455  0bd88f76-0605-4e6c-a4d6-8ed79674756f 34129 0 2022-12-09 08:28:08 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2022-12-09 08:28:08 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Dec  9 08:28:08.463: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-6455  0bd88f76-0605-4e6c-a4d6-8ed79674756f 34130 0 2022-12-09 08:28:08 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2022-12-09 08:28:08 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Dec  9 08:28:08.463: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-6455" for this suite. 12/09/22 08:28:08.467
------------------------------
â€¢ [0.068 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 08:28:08.405
    Dec  9 08:28:08.405: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename watch 12/09/22 08:28:08.406
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:28:08.425
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:28:08.429
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to start watching from a specific resource version [Conformance]
      test/e2e/apimachinery/watch.go:142
    STEP: creating a new configmap 12/09/22 08:28:08.433
    STEP: modifying the configmap once 12/09/22 08:28:08.437
    STEP: modifying the configmap a second time 12/09/22 08:28:08.446
    STEP: deleting the configmap 12/09/22 08:28:08.455
    STEP: creating a watch on configmaps from the resource version returned by the first update 12/09/22 08:28:08.46
    STEP: Expecting to observe notifications for all changes to the configmap after the first update 12/09/22 08:28:08.462
    Dec  9 08:28:08.463: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-6455  0bd88f76-0605-4e6c-a4d6-8ed79674756f 34129 0 2022-12-09 08:28:08 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2022-12-09 08:28:08 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Dec  9 08:28:08.463: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-6455  0bd88f76-0605-4e6c-a4d6-8ed79674756f 34130 0 2022-12-09 08:28:08 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2022-12-09 08:28:08 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Dec  9 08:28:08.463: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-6455" for this suite. 12/09/22 08:28:08.467
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:57
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 08:28:08.474
Dec  9 08:28:08.474: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename secrets 12/09/22 08:28:08.475
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:28:08.493
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:28:08.501
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:57
STEP: Creating secret with name secret-test-568b1c06-ef66-4576-b1bf-ca986c9bd242 12/09/22 08:28:08.506
STEP: Creating a pod to test consume secrets 12/09/22 08:28:08.513
Dec  9 08:28:08.524: INFO: Waiting up to 5m0s for pod "pod-secrets-8ebfe942-1f89-42f5-9264-6ba6fb799a59" in namespace "secrets-9338" to be "Succeeded or Failed"
Dec  9 08:28:08.534: INFO: Pod "pod-secrets-8ebfe942-1f89-42f5-9264-6ba6fb799a59": Phase="Pending", Reason="", readiness=false. Elapsed: 9.682613ms
Dec  9 08:28:10.537: INFO: Pod "pod-secrets-8ebfe942-1f89-42f5-9264-6ba6fb799a59": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012960158s
Dec  9 08:28:12.539: INFO: Pod "pod-secrets-8ebfe942-1f89-42f5-9264-6ba6fb799a59": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014106942s
STEP: Saw pod success 12/09/22 08:28:12.539
Dec  9 08:28:12.539: INFO: Pod "pod-secrets-8ebfe942-1f89-42f5-9264-6ba6fb799a59" satisfied condition "Succeeded or Failed"
Dec  9 08:28:12.542: INFO: Trying to get logs from node ip-10-0-10-179 pod pod-secrets-8ebfe942-1f89-42f5-9264-6ba6fb799a59 container secret-volume-test: <nil>
STEP: delete the pod 12/09/22 08:28:12.547
Dec  9 08:28:12.557: INFO: Waiting for pod pod-secrets-8ebfe942-1f89-42f5-9264-6ba6fb799a59 to disappear
Dec  9 08:28:12.560: INFO: Pod pod-secrets-8ebfe942-1f89-42f5-9264-6ba6fb799a59 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Dec  9 08:28:12.560: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-9338" for this suite. 12/09/22 08:28:12.563
------------------------------
â€¢ [4.094 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 08:28:08.474
    Dec  9 08:28:08.474: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename secrets 12/09/22 08:28:08.475
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:28:08.493
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:28:08.501
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:57
    STEP: Creating secret with name secret-test-568b1c06-ef66-4576-b1bf-ca986c9bd242 12/09/22 08:28:08.506
    STEP: Creating a pod to test consume secrets 12/09/22 08:28:08.513
    Dec  9 08:28:08.524: INFO: Waiting up to 5m0s for pod "pod-secrets-8ebfe942-1f89-42f5-9264-6ba6fb799a59" in namespace "secrets-9338" to be "Succeeded or Failed"
    Dec  9 08:28:08.534: INFO: Pod "pod-secrets-8ebfe942-1f89-42f5-9264-6ba6fb799a59": Phase="Pending", Reason="", readiness=false. Elapsed: 9.682613ms
    Dec  9 08:28:10.537: INFO: Pod "pod-secrets-8ebfe942-1f89-42f5-9264-6ba6fb799a59": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012960158s
    Dec  9 08:28:12.539: INFO: Pod "pod-secrets-8ebfe942-1f89-42f5-9264-6ba6fb799a59": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014106942s
    STEP: Saw pod success 12/09/22 08:28:12.539
    Dec  9 08:28:12.539: INFO: Pod "pod-secrets-8ebfe942-1f89-42f5-9264-6ba6fb799a59" satisfied condition "Succeeded or Failed"
    Dec  9 08:28:12.542: INFO: Trying to get logs from node ip-10-0-10-179 pod pod-secrets-8ebfe942-1f89-42f5-9264-6ba6fb799a59 container secret-volume-test: <nil>
    STEP: delete the pod 12/09/22 08:28:12.547
    Dec  9 08:28:12.557: INFO: Waiting for pod pod-secrets-8ebfe942-1f89-42f5-9264-6ba6fb799a59 to disappear
    Dec  9 08:28:12.560: INFO: Pod pod-secrets-8ebfe942-1f89-42f5-9264-6ba6fb799a59 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Dec  9 08:28:12.560: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-9338" for this suite. 12/09/22 08:28:12.563
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] Projected combined
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:44
[BeforeEach] [sig-storage] Projected combined
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 08:28:12.569
Dec  9 08:28:12.569: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename projected 12/09/22 08:28:12.57
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:28:12.587
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:28:12.597
[BeforeEach] [sig-storage] Projected combined
  test/e2e/framework/metrics/init/init.go:31
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:44
STEP: Creating configMap with name configmap-projected-all-test-volume-03cf1e5e-6fba-4bc5-bf41-ffbba2cc4408 12/09/22 08:28:12.603
STEP: Creating secret with name secret-projected-all-test-volume-5ce796f5-a3e4-415e-8b00-d048d9339bba 12/09/22 08:28:12.608
STEP: Creating a pod to test Check all projections for projected volume plugin 12/09/22 08:28:12.613
Dec  9 08:28:12.620: INFO: Waiting up to 5m0s for pod "projected-volume-eb16a5c4-f458-4828-8e08-fd5918a08c38" in namespace "projected-6674" to be "Succeeded or Failed"
Dec  9 08:28:12.625: INFO: Pod "projected-volume-eb16a5c4-f458-4828-8e08-fd5918a08c38": Phase="Pending", Reason="", readiness=false. Elapsed: 4.737396ms
Dec  9 08:28:14.628: INFO: Pod "projected-volume-eb16a5c4-f458-4828-8e08-fd5918a08c38": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008075299s
Dec  9 08:28:16.629: INFO: Pod "projected-volume-eb16a5c4-f458-4828-8e08-fd5918a08c38": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008643713s
STEP: Saw pod success 12/09/22 08:28:16.629
Dec  9 08:28:16.629: INFO: Pod "projected-volume-eb16a5c4-f458-4828-8e08-fd5918a08c38" satisfied condition "Succeeded or Failed"
Dec  9 08:28:16.632: INFO: Trying to get logs from node ip-10-0-10-179 pod projected-volume-eb16a5c4-f458-4828-8e08-fd5918a08c38 container projected-all-volume-test: <nil>
STEP: delete the pod 12/09/22 08:28:16.637
Dec  9 08:28:16.650: INFO: Waiting for pod projected-volume-eb16a5c4-f458-4828-8e08-fd5918a08c38 to disappear
Dec  9 08:28:16.653: INFO: Pod projected-volume-eb16a5c4-f458-4828-8e08-fd5918a08c38 no longer exists
[AfterEach] [sig-storage] Projected combined
  test/e2e/framework/node/init/init.go:32
Dec  9 08:28:16.653: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected combined
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected combined
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected combined
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-6674" for this suite. 12/09/22 08:28:16.66
------------------------------
â€¢ [4.097 seconds]
[sig-storage] Projected combined
test/e2e/common/storage/framework.go:23
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:44

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected combined
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 08:28:12.569
    Dec  9 08:28:12.569: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename projected 12/09/22 08:28:12.57
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:28:12.587
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:28:12.597
    [BeforeEach] [sig-storage] Projected combined
      test/e2e/framework/metrics/init/init.go:31
    [It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
      test/e2e/common/storage/projected_combined.go:44
    STEP: Creating configMap with name configmap-projected-all-test-volume-03cf1e5e-6fba-4bc5-bf41-ffbba2cc4408 12/09/22 08:28:12.603
    STEP: Creating secret with name secret-projected-all-test-volume-5ce796f5-a3e4-415e-8b00-d048d9339bba 12/09/22 08:28:12.608
    STEP: Creating a pod to test Check all projections for projected volume plugin 12/09/22 08:28:12.613
    Dec  9 08:28:12.620: INFO: Waiting up to 5m0s for pod "projected-volume-eb16a5c4-f458-4828-8e08-fd5918a08c38" in namespace "projected-6674" to be "Succeeded or Failed"
    Dec  9 08:28:12.625: INFO: Pod "projected-volume-eb16a5c4-f458-4828-8e08-fd5918a08c38": Phase="Pending", Reason="", readiness=false. Elapsed: 4.737396ms
    Dec  9 08:28:14.628: INFO: Pod "projected-volume-eb16a5c4-f458-4828-8e08-fd5918a08c38": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008075299s
    Dec  9 08:28:16.629: INFO: Pod "projected-volume-eb16a5c4-f458-4828-8e08-fd5918a08c38": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008643713s
    STEP: Saw pod success 12/09/22 08:28:16.629
    Dec  9 08:28:16.629: INFO: Pod "projected-volume-eb16a5c4-f458-4828-8e08-fd5918a08c38" satisfied condition "Succeeded or Failed"
    Dec  9 08:28:16.632: INFO: Trying to get logs from node ip-10-0-10-179 pod projected-volume-eb16a5c4-f458-4828-8e08-fd5918a08c38 container projected-all-volume-test: <nil>
    STEP: delete the pod 12/09/22 08:28:16.637
    Dec  9 08:28:16.650: INFO: Waiting for pod projected-volume-eb16a5c4-f458-4828-8e08-fd5918a08c38 to disappear
    Dec  9 08:28:16.653: INFO: Pod projected-volume-eb16a5c4-f458-4828-8e08-fd5918a08c38 no longer exists
    [AfterEach] [sig-storage] Projected combined
      test/e2e/framework/node/init/init.go:32
    Dec  9 08:28:16.653: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected combined
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected combined
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected combined
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-6674" for this suite. 12/09/22 08:28:16.66
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:157
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 08:28:16.668
Dec  9 08:28:16.668: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename emptydir 12/09/22 08:28:16.67
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:28:16.689
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:28:16.693
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:157
STEP: Creating a pod to test emptydir volume type on node default medium 12/09/22 08:28:16.697
Dec  9 08:28:16.705: INFO: Waiting up to 5m0s for pod "pod-ca0be042-3243-4768-bca9-e755d3eb088c" in namespace "emptydir-6036" to be "Succeeded or Failed"
Dec  9 08:28:16.709: INFO: Pod "pod-ca0be042-3243-4768-bca9-e755d3eb088c": Phase="Pending", Reason="", readiness=false. Elapsed: 3.625003ms
Dec  9 08:28:18.717: INFO: Pod "pod-ca0be042-3243-4768-bca9-e755d3eb088c": Phase="Running", Reason="", readiness=false. Elapsed: 2.011307778s
Dec  9 08:28:20.713: INFO: Pod "pod-ca0be042-3243-4768-bca9-e755d3eb088c": Phase="Running", Reason="", readiness=false. Elapsed: 4.007954299s
Dec  9 08:28:22.712: INFO: Pod "pod-ca0be042-3243-4768-bca9-e755d3eb088c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.007002842s
STEP: Saw pod success 12/09/22 08:28:22.712
Dec  9 08:28:22.713: INFO: Pod "pod-ca0be042-3243-4768-bca9-e755d3eb088c" satisfied condition "Succeeded or Failed"
Dec  9 08:28:22.716: INFO: Trying to get logs from node ip-10-0-10-179 pod pod-ca0be042-3243-4768-bca9-e755d3eb088c container test-container: <nil>
STEP: delete the pod 12/09/22 08:28:22.722
Dec  9 08:28:22.731: INFO: Waiting for pod pod-ca0be042-3243-4768-bca9-e755d3eb088c to disappear
Dec  9 08:28:22.733: INFO: Pod pod-ca0be042-3243-4768-bca9-e755d3eb088c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Dec  9 08:28:22.733: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-6036" for this suite. 12/09/22 08:28:22.736
------------------------------
â€¢ [SLOW TEST] [6.073 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:157

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 08:28:16.668
    Dec  9 08:28:16.668: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename emptydir 12/09/22 08:28:16.67
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:28:16.689
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:28:16.693
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:157
    STEP: Creating a pod to test emptydir volume type on node default medium 12/09/22 08:28:16.697
    Dec  9 08:28:16.705: INFO: Waiting up to 5m0s for pod "pod-ca0be042-3243-4768-bca9-e755d3eb088c" in namespace "emptydir-6036" to be "Succeeded or Failed"
    Dec  9 08:28:16.709: INFO: Pod "pod-ca0be042-3243-4768-bca9-e755d3eb088c": Phase="Pending", Reason="", readiness=false. Elapsed: 3.625003ms
    Dec  9 08:28:18.717: INFO: Pod "pod-ca0be042-3243-4768-bca9-e755d3eb088c": Phase="Running", Reason="", readiness=false. Elapsed: 2.011307778s
    Dec  9 08:28:20.713: INFO: Pod "pod-ca0be042-3243-4768-bca9-e755d3eb088c": Phase="Running", Reason="", readiness=false. Elapsed: 4.007954299s
    Dec  9 08:28:22.712: INFO: Pod "pod-ca0be042-3243-4768-bca9-e755d3eb088c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.007002842s
    STEP: Saw pod success 12/09/22 08:28:22.712
    Dec  9 08:28:22.713: INFO: Pod "pod-ca0be042-3243-4768-bca9-e755d3eb088c" satisfied condition "Succeeded or Failed"
    Dec  9 08:28:22.716: INFO: Trying to get logs from node ip-10-0-10-179 pod pod-ca0be042-3243-4768-bca9-e755d3eb088c container test-container: <nil>
    STEP: delete the pod 12/09/22 08:28:22.722
    Dec  9 08:28:22.731: INFO: Waiting for pod pod-ca0be042-3243-4768-bca9-e755d3eb088c to disappear
    Dec  9 08:28:22.733: INFO: Pod pod-ca0be042-3243-4768-bca9-e755d3eb088c no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Dec  9 08:28:22.733: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-6036" for this suite. 12/09/22 08:28:22.736
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:57
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 08:28:22.743
Dec  9 08:28:22.743: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename configmap 12/09/22 08:28:22.744
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:28:22.758
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:28:22.763
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:57
STEP: Creating configMap with name configmap-test-volume-aabd64c4-63e4-4df2-b2ca-770681446aef 12/09/22 08:28:22.766
STEP: Creating a pod to test consume configMaps 12/09/22 08:28:22.77
Dec  9 08:28:22.778: INFO: Waiting up to 5m0s for pod "pod-configmaps-c840943b-5f58-4856-b2ce-cef36fa65be5" in namespace "configmap-8348" to be "Succeeded or Failed"
Dec  9 08:28:22.786: INFO: Pod "pod-configmaps-c840943b-5f58-4856-b2ce-cef36fa65be5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.336859ms
Dec  9 08:28:24.790: INFO: Pod "pod-configmaps-c840943b-5f58-4856-b2ce-cef36fa65be5": Phase="Running", Reason="", readiness=false. Elapsed: 2.012180327s
Dec  9 08:28:26.792: INFO: Pod "pod-configmaps-c840943b-5f58-4856-b2ce-cef36fa65be5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013938651s
STEP: Saw pod success 12/09/22 08:28:26.792
Dec  9 08:28:26.792: INFO: Pod "pod-configmaps-c840943b-5f58-4856-b2ce-cef36fa65be5" satisfied condition "Succeeded or Failed"
Dec  9 08:28:26.795: INFO: Trying to get logs from node ip-10-0-10-179 pod pod-configmaps-c840943b-5f58-4856-b2ce-cef36fa65be5 container agnhost-container: <nil>
STEP: delete the pod 12/09/22 08:28:26.8
Dec  9 08:28:26.812: INFO: Waiting for pod pod-configmaps-c840943b-5f58-4856-b2ce-cef36fa65be5 to disappear
Dec  9 08:28:26.814: INFO: Pod pod-configmaps-c840943b-5f58-4856-b2ce-cef36fa65be5 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Dec  9 08:28:26.815: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-8348" for this suite. 12/09/22 08:28:26.818
------------------------------
â€¢ [4.087 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 08:28:22.743
    Dec  9 08:28:22.743: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename configmap 12/09/22 08:28:22.744
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:28:22.758
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:28:22.763
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:57
    STEP: Creating configMap with name configmap-test-volume-aabd64c4-63e4-4df2-b2ca-770681446aef 12/09/22 08:28:22.766
    STEP: Creating a pod to test consume configMaps 12/09/22 08:28:22.77
    Dec  9 08:28:22.778: INFO: Waiting up to 5m0s for pod "pod-configmaps-c840943b-5f58-4856-b2ce-cef36fa65be5" in namespace "configmap-8348" to be "Succeeded or Failed"
    Dec  9 08:28:22.786: INFO: Pod "pod-configmaps-c840943b-5f58-4856-b2ce-cef36fa65be5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.336859ms
    Dec  9 08:28:24.790: INFO: Pod "pod-configmaps-c840943b-5f58-4856-b2ce-cef36fa65be5": Phase="Running", Reason="", readiness=false. Elapsed: 2.012180327s
    Dec  9 08:28:26.792: INFO: Pod "pod-configmaps-c840943b-5f58-4856-b2ce-cef36fa65be5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013938651s
    STEP: Saw pod success 12/09/22 08:28:26.792
    Dec  9 08:28:26.792: INFO: Pod "pod-configmaps-c840943b-5f58-4856-b2ce-cef36fa65be5" satisfied condition "Succeeded or Failed"
    Dec  9 08:28:26.795: INFO: Trying to get logs from node ip-10-0-10-179 pod pod-configmaps-c840943b-5f58-4856-b2ce-cef36fa65be5 container agnhost-container: <nil>
    STEP: delete the pod 12/09/22 08:28:26.8
    Dec  9 08:28:26.812: INFO: Waiting for pod pod-configmaps-c840943b-5f58-4856-b2ce-cef36fa65be5 to disappear
    Dec  9 08:28:26.814: INFO: Pod pod-configmaps-c840943b-5f58-4856-b2ce-cef36fa65be5 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Dec  9 08:28:26.815: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-8348" for this suite. 12/09/22 08:28:26.818
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run
  should check if kubectl can dry-run update Pods [Conformance]
  test/e2e/kubectl/kubectl.go:962
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 08:28:26.831
Dec  9 08:28:26.831: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename kubectl 12/09/22 08:28:26.834
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:28:26.853
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:28:26.857
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if kubectl can dry-run update Pods [Conformance]
  test/e2e/kubectl/kubectl.go:962
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 12/09/22 08:28:26.861
Dec  9 08:28:26.861: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=kubectl-1429 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Dec  9 08:28:26.940: INFO: stderr: ""
Dec  9 08:28:26.940: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: replace the image in the pod with server-side dry-run 12/09/22 08:28:26.94
Dec  9 08:28:26.941: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=kubectl-1429 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "registry.k8s.io/e2e-test-images/busybox:1.29-4"}]}} --dry-run=server'
Dec  9 08:28:27.982: INFO: stderr: ""
Dec  9 08:28:27.982: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 12/09/22 08:28:27.982
Dec  9 08:28:27.985: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=kubectl-1429 delete pods e2e-test-httpd-pod'
Dec  9 08:28:29.774: INFO: stderr: ""
Dec  9 08:28:29.774: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Dec  9 08:28:29.774: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-1429" for this suite. 12/09/22 08:28:29.779
------------------------------
â€¢ [2.955 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl server-side dry-run
  test/e2e/kubectl/kubectl.go:956
    should check if kubectl can dry-run update Pods [Conformance]
    test/e2e/kubectl/kubectl.go:962

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 08:28:26.831
    Dec  9 08:28:26.831: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename kubectl 12/09/22 08:28:26.834
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:28:26.853
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:28:26.857
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if kubectl can dry-run update Pods [Conformance]
      test/e2e/kubectl/kubectl.go:962
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 12/09/22 08:28:26.861
    Dec  9 08:28:26.861: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=kubectl-1429 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
    Dec  9 08:28:26.940: INFO: stderr: ""
    Dec  9 08:28:26.940: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: replace the image in the pod with server-side dry-run 12/09/22 08:28:26.94
    Dec  9 08:28:26.941: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=kubectl-1429 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "registry.k8s.io/e2e-test-images/busybox:1.29-4"}]}} --dry-run=server'
    Dec  9 08:28:27.982: INFO: stderr: ""
    Dec  9 08:28:27.982: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
    STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 12/09/22 08:28:27.982
    Dec  9 08:28:27.985: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=kubectl-1429 delete pods e2e-test-httpd-pod'
    Dec  9 08:28:29.774: INFO: stderr: ""
    Dec  9 08:28:29.774: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Dec  9 08:28:29.774: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-1429" for this suite. 12/09/22 08:28:29.779
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Downward API volume
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:235
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 08:28:29.786
Dec  9 08:28:29.786: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename downward-api 12/09/22 08:28:29.787
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:28:29.807
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:28:29.811
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:235
STEP: Creating a pod to test downward API volume plugin 12/09/22 08:28:29.815
Dec  9 08:28:29.826: INFO: Waiting up to 5m0s for pod "downwardapi-volume-60f22a28-60ad-4ca2-abdf-a9549e89407d" in namespace "downward-api-8686" to be "Succeeded or Failed"
Dec  9 08:28:29.834: INFO: Pod "downwardapi-volume-60f22a28-60ad-4ca2-abdf-a9549e89407d": Phase="Pending", Reason="", readiness=false. Elapsed: 8.500359ms
Dec  9 08:28:31.838: INFO: Pod "downwardapi-volume-60f22a28-60ad-4ca2-abdf-a9549e89407d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012829907s
Dec  9 08:28:33.838: INFO: Pod "downwardapi-volume-60f22a28-60ad-4ca2-abdf-a9549e89407d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012062582s
STEP: Saw pod success 12/09/22 08:28:33.838
Dec  9 08:28:33.838: INFO: Pod "downwardapi-volume-60f22a28-60ad-4ca2-abdf-a9549e89407d" satisfied condition "Succeeded or Failed"
Dec  9 08:28:33.843: INFO: Trying to get logs from node ip-10-0-10-179 pod downwardapi-volume-60f22a28-60ad-4ca2-abdf-a9549e89407d container client-container: <nil>
STEP: delete the pod 12/09/22 08:28:33.852
Dec  9 08:28:33.863: INFO: Waiting for pod downwardapi-volume-60f22a28-60ad-4ca2-abdf-a9549e89407d to disappear
Dec  9 08:28:33.866: INFO: Pod downwardapi-volume-60f22a28-60ad-4ca2-abdf-a9549e89407d no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Dec  9 08:28:33.867: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-8686" for this suite. 12/09/22 08:28:33.871
------------------------------
â€¢ [4.092 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:235

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 08:28:29.786
    Dec  9 08:28:29.786: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename downward-api 12/09/22 08:28:29.787
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:28:29.807
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:28:29.811
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide container's memory request [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:235
    STEP: Creating a pod to test downward API volume plugin 12/09/22 08:28:29.815
    Dec  9 08:28:29.826: INFO: Waiting up to 5m0s for pod "downwardapi-volume-60f22a28-60ad-4ca2-abdf-a9549e89407d" in namespace "downward-api-8686" to be "Succeeded or Failed"
    Dec  9 08:28:29.834: INFO: Pod "downwardapi-volume-60f22a28-60ad-4ca2-abdf-a9549e89407d": Phase="Pending", Reason="", readiness=false. Elapsed: 8.500359ms
    Dec  9 08:28:31.838: INFO: Pod "downwardapi-volume-60f22a28-60ad-4ca2-abdf-a9549e89407d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012829907s
    Dec  9 08:28:33.838: INFO: Pod "downwardapi-volume-60f22a28-60ad-4ca2-abdf-a9549e89407d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012062582s
    STEP: Saw pod success 12/09/22 08:28:33.838
    Dec  9 08:28:33.838: INFO: Pod "downwardapi-volume-60f22a28-60ad-4ca2-abdf-a9549e89407d" satisfied condition "Succeeded or Failed"
    Dec  9 08:28:33.843: INFO: Trying to get logs from node ip-10-0-10-179 pod downwardapi-volume-60f22a28-60ad-4ca2-abdf-a9549e89407d container client-container: <nil>
    STEP: delete the pod 12/09/22 08:28:33.852
    Dec  9 08:28:33.863: INFO: Waiting for pod downwardapi-volume-60f22a28-60ad-4ca2-abdf-a9549e89407d to disappear
    Dec  9 08:28:33.866: INFO: Pod downwardapi-volume-60f22a28-60ad-4ca2-abdf-a9549e89407d no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Dec  9 08:28:33.867: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-8686" for this suite. 12/09/22 08:28:33.871
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207
[BeforeEach] [sig-instrumentation] Events API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 08:28:33.881
Dec  9 08:28:33.881: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename events 12/09/22 08:28:33.887
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:28:33.911
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:28:33.915
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/instrumentation/events.go:84
[It] should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207
STEP: Create set of events 12/09/22 08:28:33.919
STEP: get a list of Events with a label in the current namespace 12/09/22 08:28:33.933
STEP: delete a list of events 12/09/22 08:28:33.936
Dec  9 08:28:33.937: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity 12/09/22 08:28:33.948
[AfterEach] [sig-instrumentation] Events API
  test/e2e/framework/node/init/init.go:32
Dec  9 08:28:33.951: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-instrumentation] Events API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-instrumentation] Events API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-instrumentation] Events API
  tear down framework | framework.go:193
STEP: Destroying namespace "events-4208" for this suite. 12/09/22 08:28:33.955
------------------------------
â€¢ [0.079 seconds]
[sig-instrumentation] Events API
test/e2e/instrumentation/common/framework.go:23
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 08:28:33.881
    Dec  9 08:28:33.881: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename events 12/09/22 08:28:33.887
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:28:33.911
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:28:33.915
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/instrumentation/events.go:84
    [It] should delete a collection of events [Conformance]
      test/e2e/instrumentation/events.go:207
    STEP: Create set of events 12/09/22 08:28:33.919
    STEP: get a list of Events with a label in the current namespace 12/09/22 08:28:33.933
    STEP: delete a list of events 12/09/22 08:28:33.936
    Dec  9 08:28:33.937: INFO: requesting DeleteCollection of events
    STEP: check that the list of events matches the requested quantity 12/09/22 08:28:33.948
    [AfterEach] [sig-instrumentation] Events API
      test/e2e/framework/node/init/init.go:32
    Dec  9 08:28:33.951: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      tear down framework | framework.go:193
    STEP: Destroying namespace "events-4208" for this suite. 12/09/22 08:28:33.955
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should perform rolling updates and roll backs of template modifications [Conformance]
  test/e2e/apps/statefulset.go:306
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 08:28:33.964
Dec  9 08:28:33.964: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename statefulset 12/09/22 08:28:33.965
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:28:33.983
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:28:33.987
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-8647 12/09/22 08:28:33.991
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  test/e2e/apps/statefulset.go:306
STEP: Creating a new StatefulSet 12/09/22 08:28:33.998
Dec  9 08:28:34.015: INFO: Found 0 stateful pods, waiting for 3
Dec  9 08:28:44.025: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Dec  9 08:28:44.025: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Dec  9 08:28:44.025: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Dec  9 08:28:44.036: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=statefulset-8647 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Dec  9 08:28:44.207: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Dec  9 08:28:44.207: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Dec  9 08:28:44.207: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 12/09/22 08:28:54.227
Dec  9 08:28:54.248: INFO: Updating stateful set ss2
STEP: Creating a new revision 12/09/22 08:28:54.248
STEP: Updating Pods in reverse ordinal order 12/09/22 08:29:04.265
Dec  9 08:29:04.269: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=statefulset-8647 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec  9 08:29:04.456: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Dec  9 08:29:04.456: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Dec  9 08:29:04.456: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

STEP: Rolling back to a previous revision 12/09/22 08:29:14.476
Dec  9 08:29:14.477: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=statefulset-8647 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Dec  9 08:29:14.681: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Dec  9 08:29:14.681: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Dec  9 08:29:14.681: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Dec  9 08:29:24.729: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order 12/09/22 08:29:34.768
Dec  9 08:29:34.776: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=statefulset-8647 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec  9 08:29:34.939: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Dec  9 08:29:34.939: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Dec  9 08:29:34.939: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Dec  9 08:29:44.965: INFO: Deleting all statefulset in ns statefulset-8647
Dec  9 08:29:44.967: INFO: Scaling statefulset ss2 to 0
Dec  9 08:29:54.986: INFO: Waiting for statefulset status.replicas updated to 0
Dec  9 08:29:54.989: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Dec  9 08:29:55.003: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-8647" for this suite. 12/09/22 08:29:55.017
------------------------------
â€¢ [SLOW TEST] [81.063 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should perform rolling updates and roll backs of template modifications [Conformance]
    test/e2e/apps/statefulset.go:306

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 08:28:33.964
    Dec  9 08:28:33.964: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename statefulset 12/09/22 08:28:33.965
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:28:33.983
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:28:33.987
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-8647 12/09/22 08:28:33.991
    [It] should perform rolling updates and roll backs of template modifications [Conformance]
      test/e2e/apps/statefulset.go:306
    STEP: Creating a new StatefulSet 12/09/22 08:28:33.998
    Dec  9 08:28:34.015: INFO: Found 0 stateful pods, waiting for 3
    Dec  9 08:28:44.025: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Dec  9 08:28:44.025: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Dec  9 08:28:44.025: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    Dec  9 08:28:44.036: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=statefulset-8647 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Dec  9 08:28:44.207: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Dec  9 08:28:44.207: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Dec  9 08:28:44.207: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    STEP: Updating StatefulSet template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 12/09/22 08:28:54.227
    Dec  9 08:28:54.248: INFO: Updating stateful set ss2
    STEP: Creating a new revision 12/09/22 08:28:54.248
    STEP: Updating Pods in reverse ordinal order 12/09/22 08:29:04.265
    Dec  9 08:29:04.269: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=statefulset-8647 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Dec  9 08:29:04.456: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Dec  9 08:29:04.456: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Dec  9 08:29:04.456: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    STEP: Rolling back to a previous revision 12/09/22 08:29:14.476
    Dec  9 08:29:14.477: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=statefulset-8647 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Dec  9 08:29:14.681: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Dec  9 08:29:14.681: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Dec  9 08:29:14.681: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Dec  9 08:29:24.729: INFO: Updating stateful set ss2
    STEP: Rolling back update in reverse ordinal order 12/09/22 08:29:34.768
    Dec  9 08:29:34.776: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=statefulset-8647 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Dec  9 08:29:34.939: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Dec  9 08:29:34.939: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Dec  9 08:29:34.939: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Dec  9 08:29:44.965: INFO: Deleting all statefulset in ns statefulset-8647
    Dec  9 08:29:44.967: INFO: Scaling statefulset ss2 to 0
    Dec  9 08:29:54.986: INFO: Waiting for statefulset status.replicas updated to 0
    Dec  9 08:29:54.989: INFO: Deleting statefulset ss2
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Dec  9 08:29:55.003: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-8647" for this suite. 12/09/22 08:29:55.017
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl expose
  should create services for rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1415
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 08:29:55.034
Dec  9 08:29:55.034: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename kubectl 12/09/22 08:29:55.036
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:29:55.062
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:29:55.065
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should create services for rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1415
STEP: creating Agnhost RC 12/09/22 08:29:55.069
Dec  9 08:29:55.069: INFO: namespace kubectl-9227
Dec  9 08:29:55.069: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=kubectl-9227 create -f -'
Dec  9 08:29:55.346: INFO: stderr: ""
Dec  9 08:29:55.346: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 12/09/22 08:29:55.346
Dec  9 08:29:56.385: INFO: Selector matched 1 pods for map[app:agnhost]
Dec  9 08:29:56.393: INFO: Found 0 / 1
Dec  9 08:29:57.351: INFO: Selector matched 1 pods for map[app:agnhost]
Dec  9 08:29:57.351: INFO: Found 1 / 1
Dec  9 08:29:57.351: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Dec  9 08:29:57.355: INFO: Selector matched 1 pods for map[app:agnhost]
Dec  9 08:29:57.355: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Dec  9 08:29:57.355: INFO: wait on agnhost-primary startup in kubectl-9227 
Dec  9 08:29:57.355: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=kubectl-9227 logs agnhost-primary-nfdqw agnhost-primary'
Dec  9 08:29:57.473: INFO: stderr: ""
Dec  9 08:29:57.473: INFO: stdout: "Paused\n"
STEP: exposing RC 12/09/22 08:29:57.473
Dec  9 08:29:57.473: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=kubectl-9227 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
Dec  9 08:29:57.668: INFO: stderr: ""
Dec  9 08:29:57.668: INFO: stdout: "service/rm2 exposed\n"
Dec  9 08:29:57.674: INFO: Service rm2 in namespace kubectl-9227 found.
STEP: exposing service 12/09/22 08:29:59.681
Dec  9 08:29:59.681: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=kubectl-9227 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
Dec  9 08:29:59.783: INFO: stderr: ""
Dec  9 08:29:59.783: INFO: stdout: "service/rm3 exposed\n"
Dec  9 08:29:59.791: INFO: Service rm3 in namespace kubectl-9227 found.
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Dec  9 08:30:01.799: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-9227" for this suite. 12/09/22 08:30:01.813
------------------------------
â€¢ [SLOW TEST] [6.800 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl expose
  test/e2e/kubectl/kubectl.go:1409
    should create services for rc  [Conformance]
    test/e2e/kubectl/kubectl.go:1415

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 08:29:55.034
    Dec  9 08:29:55.034: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename kubectl 12/09/22 08:29:55.036
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:29:55.062
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:29:55.065
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should create services for rc  [Conformance]
      test/e2e/kubectl/kubectl.go:1415
    STEP: creating Agnhost RC 12/09/22 08:29:55.069
    Dec  9 08:29:55.069: INFO: namespace kubectl-9227
    Dec  9 08:29:55.069: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=kubectl-9227 create -f -'
    Dec  9 08:29:55.346: INFO: stderr: ""
    Dec  9 08:29:55.346: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 12/09/22 08:29:55.346
    Dec  9 08:29:56.385: INFO: Selector matched 1 pods for map[app:agnhost]
    Dec  9 08:29:56.393: INFO: Found 0 / 1
    Dec  9 08:29:57.351: INFO: Selector matched 1 pods for map[app:agnhost]
    Dec  9 08:29:57.351: INFO: Found 1 / 1
    Dec  9 08:29:57.351: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    Dec  9 08:29:57.355: INFO: Selector matched 1 pods for map[app:agnhost]
    Dec  9 08:29:57.355: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Dec  9 08:29:57.355: INFO: wait on agnhost-primary startup in kubectl-9227 
    Dec  9 08:29:57.355: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=kubectl-9227 logs agnhost-primary-nfdqw agnhost-primary'
    Dec  9 08:29:57.473: INFO: stderr: ""
    Dec  9 08:29:57.473: INFO: stdout: "Paused\n"
    STEP: exposing RC 12/09/22 08:29:57.473
    Dec  9 08:29:57.473: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=kubectl-9227 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
    Dec  9 08:29:57.668: INFO: stderr: ""
    Dec  9 08:29:57.668: INFO: stdout: "service/rm2 exposed\n"
    Dec  9 08:29:57.674: INFO: Service rm2 in namespace kubectl-9227 found.
    STEP: exposing service 12/09/22 08:29:59.681
    Dec  9 08:29:59.681: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=kubectl-9227 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
    Dec  9 08:29:59.783: INFO: stderr: ""
    Dec  9 08:29:59.783: INFO: stdout: "service/rm3 exposed\n"
    Dec  9 08:29:59.791: INFO: Service rm3 in namespace kubectl-9227 found.
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Dec  9 08:30:01.799: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-9227" for this suite. 12/09/22 08:30:01.813
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:249
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 08:30:01.834
Dec  9 08:30:01.834: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename projected 12/09/22 08:30:01.835
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:30:01.917
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:30:01.932
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:249
STEP: Creating a pod to test downward API volume plugin 12/09/22 08:30:01.945
Dec  9 08:30:01.961: INFO: Waiting up to 5m0s for pod "downwardapi-volume-22783fa3-35eb-4781-baca-8c47780b60f9" in namespace "projected-9214" to be "Succeeded or Failed"
Dec  9 08:30:01.992: INFO: Pod "downwardapi-volume-22783fa3-35eb-4781-baca-8c47780b60f9": Phase="Pending", Reason="", readiness=false. Elapsed: 30.91624ms
Dec  9 08:30:03.995: INFO: Pod "downwardapi-volume-22783fa3-35eb-4781-baca-8c47780b60f9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034276236s
Dec  9 08:30:05.996: INFO: Pod "downwardapi-volume-22783fa3-35eb-4781-baca-8c47780b60f9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.035166056s
STEP: Saw pod success 12/09/22 08:30:05.996
Dec  9 08:30:05.996: INFO: Pod "downwardapi-volume-22783fa3-35eb-4781-baca-8c47780b60f9" satisfied condition "Succeeded or Failed"
Dec  9 08:30:05.999: INFO: Trying to get logs from node ip-10-0-10-179 pod downwardapi-volume-22783fa3-35eb-4781-baca-8c47780b60f9 container client-container: <nil>
STEP: delete the pod 12/09/22 08:30:06.017
Dec  9 08:30:06.028: INFO: Waiting for pod downwardapi-volume-22783fa3-35eb-4781-baca-8c47780b60f9 to disappear
Dec  9 08:30:06.034: INFO: Pod downwardapi-volume-22783fa3-35eb-4781-baca-8c47780b60f9 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Dec  9 08:30:06.034: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-9214" for this suite. 12/09/22 08:30:06.038
------------------------------
â€¢ [4.211 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:249

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 08:30:01.834
    Dec  9 08:30:01.834: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename projected 12/09/22 08:30:01.835
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:30:01.917
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:30:01.932
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:249
    STEP: Creating a pod to test downward API volume plugin 12/09/22 08:30:01.945
    Dec  9 08:30:01.961: INFO: Waiting up to 5m0s for pod "downwardapi-volume-22783fa3-35eb-4781-baca-8c47780b60f9" in namespace "projected-9214" to be "Succeeded or Failed"
    Dec  9 08:30:01.992: INFO: Pod "downwardapi-volume-22783fa3-35eb-4781-baca-8c47780b60f9": Phase="Pending", Reason="", readiness=false. Elapsed: 30.91624ms
    Dec  9 08:30:03.995: INFO: Pod "downwardapi-volume-22783fa3-35eb-4781-baca-8c47780b60f9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034276236s
    Dec  9 08:30:05.996: INFO: Pod "downwardapi-volume-22783fa3-35eb-4781-baca-8c47780b60f9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.035166056s
    STEP: Saw pod success 12/09/22 08:30:05.996
    Dec  9 08:30:05.996: INFO: Pod "downwardapi-volume-22783fa3-35eb-4781-baca-8c47780b60f9" satisfied condition "Succeeded or Failed"
    Dec  9 08:30:05.999: INFO: Trying to get logs from node ip-10-0-10-179 pod downwardapi-volume-22783fa3-35eb-4781-baca-8c47780b60f9 container client-container: <nil>
    STEP: delete the pod 12/09/22 08:30:06.017
    Dec  9 08:30:06.028: INFO: Waiting for pod downwardapi-volume-22783fa3-35eb-4781-baca-8c47780b60f9 to disappear
    Dec  9 08:30:06.034: INFO: Pod downwardapi-volume-22783fa3-35eb-4781-baca-8c47780b60f9 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Dec  9 08:30:06.034: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-9214" for this suite. 12/09/22 08:30:06.038
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with secret pod [Conformance]
  test/e2e/storage/subpath.go:60
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 08:30:06.047
Dec  9 08:30:06.047: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename subpath 12/09/22 08:30:06.048
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:30:06.074
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:30:06.078
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 12/09/22 08:30:06.082
[It] should support subpaths with secret pod [Conformance]
  test/e2e/storage/subpath.go:60
STEP: Creating pod pod-subpath-test-secret-s465 12/09/22 08:30:06.091
STEP: Creating a pod to test atomic-volume-subpath 12/09/22 08:30:06.091
Dec  9 08:30:06.099: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-s465" in namespace "subpath-4440" to be "Succeeded or Failed"
Dec  9 08:30:06.103: INFO: Pod "pod-subpath-test-secret-s465": Phase="Pending", Reason="", readiness=false. Elapsed: 3.826493ms
Dec  9 08:30:08.106: INFO: Pod "pod-subpath-test-secret-s465": Phase="Running", Reason="", readiness=true. Elapsed: 2.007252271s
Dec  9 08:30:10.107: INFO: Pod "pod-subpath-test-secret-s465": Phase="Running", Reason="", readiness=true. Elapsed: 4.0079217s
Dec  9 08:30:12.106: INFO: Pod "pod-subpath-test-secret-s465": Phase="Running", Reason="", readiness=true. Elapsed: 6.007205236s
Dec  9 08:30:14.107: INFO: Pod "pod-subpath-test-secret-s465": Phase="Running", Reason="", readiness=true. Elapsed: 8.007754829s
Dec  9 08:30:16.108: INFO: Pod "pod-subpath-test-secret-s465": Phase="Running", Reason="", readiness=true. Elapsed: 10.009250499s
Dec  9 08:30:18.107: INFO: Pod "pod-subpath-test-secret-s465": Phase="Running", Reason="", readiness=true. Elapsed: 12.007975171s
Dec  9 08:30:20.107: INFO: Pod "pod-subpath-test-secret-s465": Phase="Running", Reason="", readiness=true. Elapsed: 14.008030913s
Dec  9 08:30:22.110: INFO: Pod "pod-subpath-test-secret-s465": Phase="Running", Reason="", readiness=true. Elapsed: 16.011126378s
Dec  9 08:30:24.108: INFO: Pod "pod-subpath-test-secret-s465": Phase="Running", Reason="", readiness=true. Elapsed: 18.009196799s
Dec  9 08:30:26.106: INFO: Pod "pod-subpath-test-secret-s465": Phase="Running", Reason="", readiness=true. Elapsed: 20.007220806s
Dec  9 08:30:28.107: INFO: Pod "pod-subpath-test-secret-s465": Phase="Running", Reason="", readiness=false. Elapsed: 22.008372816s
Dec  9 08:30:30.108: INFO: Pod "pod-subpath-test-secret-s465": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.009222049s
STEP: Saw pod success 12/09/22 08:30:30.108
Dec  9 08:30:30.109: INFO: Pod "pod-subpath-test-secret-s465" satisfied condition "Succeeded or Failed"
Dec  9 08:30:30.112: INFO: Trying to get logs from node ip-10-0-10-179 pod pod-subpath-test-secret-s465 container test-container-subpath-secret-s465: <nil>
STEP: delete the pod 12/09/22 08:30:30.118
Dec  9 08:30:30.130: INFO: Waiting for pod pod-subpath-test-secret-s465 to disappear
Dec  9 08:30:30.133: INFO: Pod pod-subpath-test-secret-s465 no longer exists
STEP: Deleting pod pod-subpath-test-secret-s465 12/09/22 08:30:30.133
Dec  9 08:30:30.134: INFO: Deleting pod "pod-subpath-test-secret-s465" in namespace "subpath-4440"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Dec  9 08:30:30.139: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-4440" for this suite. 12/09/22 08:30:30.146
------------------------------
â€¢ [SLOW TEST] [24.106 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with secret pod [Conformance]
    test/e2e/storage/subpath.go:60

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 08:30:06.047
    Dec  9 08:30:06.047: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename subpath 12/09/22 08:30:06.048
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:30:06.074
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:30:06.078
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 12/09/22 08:30:06.082
    [It] should support subpaths with secret pod [Conformance]
      test/e2e/storage/subpath.go:60
    STEP: Creating pod pod-subpath-test-secret-s465 12/09/22 08:30:06.091
    STEP: Creating a pod to test atomic-volume-subpath 12/09/22 08:30:06.091
    Dec  9 08:30:06.099: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-s465" in namespace "subpath-4440" to be "Succeeded or Failed"
    Dec  9 08:30:06.103: INFO: Pod "pod-subpath-test-secret-s465": Phase="Pending", Reason="", readiness=false. Elapsed: 3.826493ms
    Dec  9 08:30:08.106: INFO: Pod "pod-subpath-test-secret-s465": Phase="Running", Reason="", readiness=true. Elapsed: 2.007252271s
    Dec  9 08:30:10.107: INFO: Pod "pod-subpath-test-secret-s465": Phase="Running", Reason="", readiness=true. Elapsed: 4.0079217s
    Dec  9 08:30:12.106: INFO: Pod "pod-subpath-test-secret-s465": Phase="Running", Reason="", readiness=true. Elapsed: 6.007205236s
    Dec  9 08:30:14.107: INFO: Pod "pod-subpath-test-secret-s465": Phase="Running", Reason="", readiness=true. Elapsed: 8.007754829s
    Dec  9 08:30:16.108: INFO: Pod "pod-subpath-test-secret-s465": Phase="Running", Reason="", readiness=true. Elapsed: 10.009250499s
    Dec  9 08:30:18.107: INFO: Pod "pod-subpath-test-secret-s465": Phase="Running", Reason="", readiness=true. Elapsed: 12.007975171s
    Dec  9 08:30:20.107: INFO: Pod "pod-subpath-test-secret-s465": Phase="Running", Reason="", readiness=true. Elapsed: 14.008030913s
    Dec  9 08:30:22.110: INFO: Pod "pod-subpath-test-secret-s465": Phase="Running", Reason="", readiness=true. Elapsed: 16.011126378s
    Dec  9 08:30:24.108: INFO: Pod "pod-subpath-test-secret-s465": Phase="Running", Reason="", readiness=true. Elapsed: 18.009196799s
    Dec  9 08:30:26.106: INFO: Pod "pod-subpath-test-secret-s465": Phase="Running", Reason="", readiness=true. Elapsed: 20.007220806s
    Dec  9 08:30:28.107: INFO: Pod "pod-subpath-test-secret-s465": Phase="Running", Reason="", readiness=false. Elapsed: 22.008372816s
    Dec  9 08:30:30.108: INFO: Pod "pod-subpath-test-secret-s465": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.009222049s
    STEP: Saw pod success 12/09/22 08:30:30.108
    Dec  9 08:30:30.109: INFO: Pod "pod-subpath-test-secret-s465" satisfied condition "Succeeded or Failed"
    Dec  9 08:30:30.112: INFO: Trying to get logs from node ip-10-0-10-179 pod pod-subpath-test-secret-s465 container test-container-subpath-secret-s465: <nil>
    STEP: delete the pod 12/09/22 08:30:30.118
    Dec  9 08:30:30.130: INFO: Waiting for pod pod-subpath-test-secret-s465 to disappear
    Dec  9 08:30:30.133: INFO: Pod pod-subpath-test-secret-s465 no longer exists
    STEP: Deleting pod pod-subpath-test-secret-s465 12/09/22 08:30:30.133
    Dec  9 08:30:30.134: INFO: Deleting pod "pod-subpath-test-secret-s465" in namespace "subpath-4440"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Dec  9 08:30:30.139: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-4440" for this suite. 12/09/22 08:30:30.146
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:236
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 08:30:30.155
Dec  9 08:30:30.155: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename crd-publish-openapi 12/09/22 08:30:30.156
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:30:30.179
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:30:30.183
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:236
Dec  9 08:30:30.188: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 12/09/22 08:30:32.369
Dec  9 08:30:32.369: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=crd-publish-openapi-1136 --namespace=crd-publish-openapi-1136 create -f -'
Dec  9 08:30:33.044: INFO: stderr: ""
Dec  9 08:30:33.044: INFO: stdout: "e2e-test-crd-publish-openapi-1159-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Dec  9 08:30:33.044: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=crd-publish-openapi-1136 --namespace=crd-publish-openapi-1136 delete e2e-test-crd-publish-openapi-1159-crds test-cr'
Dec  9 08:30:33.118: INFO: stderr: ""
Dec  9 08:30:33.118: INFO: stdout: "e2e-test-crd-publish-openapi-1159-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Dec  9 08:30:33.118: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=crd-publish-openapi-1136 --namespace=crd-publish-openapi-1136 apply -f -'
Dec  9 08:30:33.323: INFO: stderr: ""
Dec  9 08:30:33.323: INFO: stdout: "e2e-test-crd-publish-openapi-1159-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Dec  9 08:30:33.323: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=crd-publish-openapi-1136 --namespace=crd-publish-openapi-1136 delete e2e-test-crd-publish-openapi-1159-crds test-cr'
Dec  9 08:30:33.394: INFO: stderr: ""
Dec  9 08:30:33.394: INFO: stdout: "e2e-test-crd-publish-openapi-1159-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR 12/09/22 08:30:33.394
Dec  9 08:30:33.394: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=crd-publish-openapi-1136 explain e2e-test-crd-publish-openapi-1159-crds'
Dec  9 08:30:33.591: INFO: stderr: ""
Dec  9 08:30:33.591: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-1159-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Dec  9 08:30:35.028: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-1136" for this suite. 12/09/22 08:30:35.037
------------------------------
â€¢ [4.888 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:236

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 08:30:30.155
    Dec  9 08:30:30.155: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename crd-publish-openapi 12/09/22 08:30:30.156
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:30:30.179
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:30:30.183
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for CRD preserving unknown fields in an embedded object [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:236
    Dec  9 08:30:30.188: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 12/09/22 08:30:32.369
    Dec  9 08:30:32.369: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=crd-publish-openapi-1136 --namespace=crd-publish-openapi-1136 create -f -'
    Dec  9 08:30:33.044: INFO: stderr: ""
    Dec  9 08:30:33.044: INFO: stdout: "e2e-test-crd-publish-openapi-1159-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
    Dec  9 08:30:33.044: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=crd-publish-openapi-1136 --namespace=crd-publish-openapi-1136 delete e2e-test-crd-publish-openapi-1159-crds test-cr'
    Dec  9 08:30:33.118: INFO: stderr: ""
    Dec  9 08:30:33.118: INFO: stdout: "e2e-test-crd-publish-openapi-1159-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
    Dec  9 08:30:33.118: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=crd-publish-openapi-1136 --namespace=crd-publish-openapi-1136 apply -f -'
    Dec  9 08:30:33.323: INFO: stderr: ""
    Dec  9 08:30:33.323: INFO: stdout: "e2e-test-crd-publish-openapi-1159-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
    Dec  9 08:30:33.323: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=crd-publish-openapi-1136 --namespace=crd-publish-openapi-1136 delete e2e-test-crd-publish-openapi-1159-crds test-cr'
    Dec  9 08:30:33.394: INFO: stderr: ""
    Dec  9 08:30:33.394: INFO: stdout: "e2e-test-crd-publish-openapi-1159-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR 12/09/22 08:30:33.394
    Dec  9 08:30:33.394: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=crd-publish-openapi-1136 explain e2e-test-crd-publish-openapi-1159-crds'
    Dec  9 08:30:33.591: INFO: stderr: ""
    Dec  9 08:30:33.591: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-1159-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Dec  9 08:30:35.028: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-1136" for this suite. 12/09/22 08:30:35.037
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:230
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 08:30:35.046
Dec  9 08:30:35.046: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename resourcequota 12/09/22 08:30:35.048
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:30:35.063
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:30:35.067
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:230
STEP: Counting existing ResourceQuota 12/09/22 08:30:35.07
STEP: Creating a ResourceQuota 12/09/22 08:30:40.084
STEP: Ensuring resource quota status is calculated 12/09/22 08:30:40.101
STEP: Creating a Pod that fits quota 12/09/22 08:30:42.11
STEP: Ensuring ResourceQuota status captures the pod usage 12/09/22 08:30:42.123
STEP: Not allowing a pod to be created that exceeds remaining quota 12/09/22 08:30:44.128
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources) 12/09/22 08:30:44.131
STEP: Ensuring a pod cannot update its resource requirements 12/09/22 08:30:44.135
STEP: Ensuring attempts to update pod resource requirements did not change quota usage 12/09/22 08:30:44.139
STEP: Deleting the pod 12/09/22 08:30:46.144
STEP: Ensuring resource quota status released the pod usage 12/09/22 08:30:46.154
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Dec  9 08:30:48.158: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-6775" for this suite. 12/09/22 08:30:48.161
------------------------------
â€¢ [SLOW TEST] [13.120 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:230

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 08:30:35.046
    Dec  9 08:30:35.046: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename resourcequota 12/09/22 08:30:35.048
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:30:35.063
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:30:35.067
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a pod. [Conformance]
      test/e2e/apimachinery/resource_quota.go:230
    STEP: Counting existing ResourceQuota 12/09/22 08:30:35.07
    STEP: Creating a ResourceQuota 12/09/22 08:30:40.084
    STEP: Ensuring resource quota status is calculated 12/09/22 08:30:40.101
    STEP: Creating a Pod that fits quota 12/09/22 08:30:42.11
    STEP: Ensuring ResourceQuota status captures the pod usage 12/09/22 08:30:42.123
    STEP: Not allowing a pod to be created that exceeds remaining quota 12/09/22 08:30:44.128
    STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources) 12/09/22 08:30:44.131
    STEP: Ensuring a pod cannot update its resource requirements 12/09/22 08:30:44.135
    STEP: Ensuring attempts to update pod resource requirements did not change quota usage 12/09/22 08:30:44.139
    STEP: Deleting the pod 12/09/22 08:30:46.144
    STEP: Ensuring resource quota status released the pod usage 12/09/22 08:30:46.154
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Dec  9 08:30:48.158: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-6775" for this suite. 12/09/22 08:30:48.161
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events
  should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57
[BeforeEach] [sig-instrumentation] Events
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 08:30:48.17
Dec  9 08:30:48.170: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename events 12/09/22 08:30:48.171
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:30:48.188
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:30:48.191
[BeforeEach] [sig-instrumentation] Events
  test/e2e/framework/metrics/init/init.go:31
[It] should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57
STEP: creating a test event 12/09/22 08:30:48.195
STEP: listing all events in all namespaces 12/09/22 08:30:48.2
STEP: patching the test event 12/09/22 08:30:48.203
STEP: fetching the test event 12/09/22 08:30:48.21
STEP: updating the test event 12/09/22 08:30:48.214
STEP: getting the test event 12/09/22 08:30:48.222
STEP: deleting the test event 12/09/22 08:30:48.225
STEP: listing all events in all namespaces 12/09/22 08:30:48.23
[AfterEach] [sig-instrumentation] Events
  test/e2e/framework/node/init/init.go:32
Dec  9 08:30:48.234: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-instrumentation] Events
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-instrumentation] Events
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-instrumentation] Events
  tear down framework | framework.go:193
STEP: Destroying namespace "events-7580" for this suite. 12/09/22 08:30:48.237
------------------------------
â€¢ [0.072 seconds]
[sig-instrumentation] Events
test/e2e/instrumentation/common/framework.go:23
  should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 08:30:48.17
    Dec  9 08:30:48.170: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename events 12/09/22 08:30:48.171
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:30:48.188
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:30:48.191
    [BeforeEach] [sig-instrumentation] Events
      test/e2e/framework/metrics/init/init.go:31
    [It] should manage the lifecycle of an event [Conformance]
      test/e2e/instrumentation/core_events.go:57
    STEP: creating a test event 12/09/22 08:30:48.195
    STEP: listing all events in all namespaces 12/09/22 08:30:48.2
    STEP: patching the test event 12/09/22 08:30:48.203
    STEP: fetching the test event 12/09/22 08:30:48.21
    STEP: updating the test event 12/09/22 08:30:48.214
    STEP: getting the test event 12/09/22 08:30:48.222
    STEP: deleting the test event 12/09/22 08:30:48.225
    STEP: listing all events in all namespaces 12/09/22 08:30:48.23
    [AfterEach] [sig-instrumentation] Events
      test/e2e/framework/node/init/init.go:32
    Dec  9 08:30:48.234: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-instrumentation] Events
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-instrumentation] Events
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-instrumentation] Events
      tear down framework | framework.go:193
    STEP: Destroying namespace "events-7580" for this suite. 12/09/22 08:30:48.237
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:221
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 08:30:48.243
Dec  9 08:30:48.243: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename webhook 12/09/22 08:30:48.244
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:30:48.262
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:30:48.265
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 12/09/22 08:30:48.283
STEP: Create role binding to let webhook read extension-apiserver-authentication 12/09/22 08:30:48.961
STEP: Deploying the webhook pod 12/09/22 08:30:48.971
STEP: Wait for the deployment to be ready 12/09/22 08:30:48.992
Dec  9 08:30:49.022: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 12/09/22 08:30:51.033
STEP: Verifying the service has paired with the endpoint 12/09/22 08:30:51.043
Dec  9 08:30:52.043: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:221
Dec  9 08:30:52.046: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Registering the custom resource webhook via the AdmissionRegistration API 12/09/22 08:30:52.611
STEP: Creating a custom resource that should be denied by the webhook 12/09/22 08:30:52.665
STEP: Creating a custom resource whose deletion would be denied by the webhook 12/09/22 08:30:54.7
STEP: Updating the custom resource with disallowed data should be denied 12/09/22 08:30:54.707
STEP: Deleting the custom resource should be denied 12/09/22 08:30:54.715
STEP: Remove the offending key and value from the custom resource data 12/09/22 08:30:54.721
STEP: Deleting the updated custom resource should be successful 12/09/22 08:30:54.729
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Dec  9 08:30:55.255: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-8321" for this suite. 12/09/22 08:30:55.338
STEP: Destroying namespace "webhook-8321-markers" for this suite. 12/09/22 08:30:55.35
------------------------------
â€¢ [SLOW TEST] [7.127 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:221

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 08:30:48.243
    Dec  9 08:30:48.243: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename webhook 12/09/22 08:30:48.244
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:30:48.262
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:30:48.265
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 12/09/22 08:30:48.283
    STEP: Create role binding to let webhook read extension-apiserver-authentication 12/09/22 08:30:48.961
    STEP: Deploying the webhook pod 12/09/22 08:30:48.971
    STEP: Wait for the deployment to be ready 12/09/22 08:30:48.992
    Dec  9 08:30:49.022: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 12/09/22 08:30:51.033
    STEP: Verifying the service has paired with the endpoint 12/09/22 08:30:51.043
    Dec  9 08:30:52.043: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny custom resource creation, update and deletion [Conformance]
      test/e2e/apimachinery/webhook.go:221
    Dec  9 08:30:52.046: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Registering the custom resource webhook via the AdmissionRegistration API 12/09/22 08:30:52.611
    STEP: Creating a custom resource that should be denied by the webhook 12/09/22 08:30:52.665
    STEP: Creating a custom resource whose deletion would be denied by the webhook 12/09/22 08:30:54.7
    STEP: Updating the custom resource with disallowed data should be denied 12/09/22 08:30:54.707
    STEP: Deleting the custom resource should be denied 12/09/22 08:30:54.715
    STEP: Remove the offending key and value from the custom resource data 12/09/22 08:30:54.721
    STEP: Deleting the updated custom resource should be successful 12/09/22 08:30:54.729
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Dec  9 08:30:55.255: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-8321" for this suite. 12/09/22 08:30:55.338
    STEP: Destroying namespace "webhook-8321-markers" for this suite. 12/09/22 08:30:55.35
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions
  should check if v1 is in available api versions  [Conformance]
  test/e2e/kubectl/kubectl.go:824
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 08:30:55.386
Dec  9 08:30:55.386: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename kubectl 12/09/22 08:30:55.387
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:30:55.406
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:30:55.411
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if v1 is in available api versions  [Conformance]
  test/e2e/kubectl/kubectl.go:824
STEP: validating api versions 12/09/22 08:30:55.416
Dec  9 08:30:55.418: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=kubectl-7916 api-versions'
Dec  9 08:30:55.564: INFO: stderr: ""
Dec  9 08:30:55.564: INFO: stdout: "admissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling/v1\nautoscaling/v2\nbatch/v1\ncertificates.k8s.io/v1\ncoordination.k8s.io/v1\ncrd.projectcalico.org/v1\ndiscovery.k8s.io/v1\nevents.k8s.io/v1\nflowcontrol.apiserver.k8s.io/v1beta2\nflowcontrol.apiserver.k8s.io/v1beta3\nnetworking.k8s.io/v1\nnode.k8s.io/v1\npolicy/v1\nrbac.authorization.k8s.io/v1\nscheduling.k8s.io/v1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Dec  9 08:30:55.564: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-7916" for this suite. 12/09/22 08:30:55.568
------------------------------
â€¢ [0.220 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl api-versions
  test/e2e/kubectl/kubectl.go:818
    should check if v1 is in available api versions  [Conformance]
    test/e2e/kubectl/kubectl.go:824

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 08:30:55.386
    Dec  9 08:30:55.386: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename kubectl 12/09/22 08:30:55.387
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:30:55.406
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:30:55.411
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if v1 is in available api versions  [Conformance]
      test/e2e/kubectl/kubectl.go:824
    STEP: validating api versions 12/09/22 08:30:55.416
    Dec  9 08:30:55.418: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=kubectl-7916 api-versions'
    Dec  9 08:30:55.564: INFO: stderr: ""
    Dec  9 08:30:55.564: INFO: stdout: "admissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling/v1\nautoscaling/v2\nbatch/v1\ncertificates.k8s.io/v1\ncoordination.k8s.io/v1\ncrd.projectcalico.org/v1\ndiscovery.k8s.io/v1\nevents.k8s.io/v1\nflowcontrol.apiserver.k8s.io/v1beta2\nflowcontrol.apiserver.k8s.io/v1beta3\nnetworking.k8s.io/v1\nnode.k8s.io/v1\npolicy/v1\nrbac.authorization.k8s.io/v1\nscheduling.k8s.io/v1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Dec  9 08:30:55.564: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-7916" for this suite. 12/09/22 08:30:55.568
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 08:30:55.606
Dec  9 08:30:55.607: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename crd-webhook 12/09/22 08:30:55.607
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:30:55.649
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:30:55.652
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:128
STEP: Setting up server cert 12/09/22 08:30:55.667
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 12/09/22 08:30:56.158
STEP: Deploying the custom resource conversion webhook pod 12/09/22 08:30:56.162
STEP: Wait for the deployment to be ready 12/09/22 08:30:56.185
Dec  9 08:30:56.211: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 12/09/22 08:30:58.219
STEP: Verifying the service has paired with the endpoint 12/09/22 08:30:58.229
Dec  9 08:30:59.229: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149
Dec  9 08:30:59.233: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Creating a v1 custom resource 12/09/22 08:31:01.88
STEP: v2 custom resource should be converted 12/09/22 08:31:01.888
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Dec  9 08:31:02.420: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:139
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-webhook-2939" for this suite. 12/09/22 08:31:02.587
------------------------------
â€¢ [SLOW TEST] [7.022 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 08:30:55.606
    Dec  9 08:30:55.607: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename crd-webhook 12/09/22 08:30:55.607
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:30:55.649
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:30:55.652
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:128
    STEP: Setting up server cert 12/09/22 08:30:55.667
    STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 12/09/22 08:30:56.158
    STEP: Deploying the custom resource conversion webhook pod 12/09/22 08:30:56.162
    STEP: Wait for the deployment to be ready 12/09/22 08:30:56.185
    Dec  9 08:30:56.211: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 12/09/22 08:30:58.219
    STEP: Verifying the service has paired with the endpoint 12/09/22 08:30:58.229
    Dec  9 08:30:59.229: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
    [It] should be able to convert from CR v1 to CR v2 [Conformance]
      test/e2e/apimachinery/crd_conversion_webhook.go:149
    Dec  9 08:30:59.233: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Creating a v1 custom resource 12/09/22 08:31:01.88
    STEP: v2 custom resource should be converted 12/09/22 08:31:01.888
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Dec  9 08:31:02.420: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:139
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-webhook-2939" for this suite. 12/09/22 08:31:02.587
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod
  should create a pod from an image when restart is Never  [Conformance]
  test/e2e/kubectl/kubectl.go:1713
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 08:31:02.638
Dec  9 08:31:02.638: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename kubectl 12/09/22 08:31:02.639
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:31:02.756
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:31:02.765
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Kubectl run pod
  test/e2e/kubectl/kubectl.go:1700
[It] should create a pod from an image when restart is Never  [Conformance]
  test/e2e/kubectl/kubectl.go:1713
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 12/09/22 08:31:02.771
Dec  9 08:31:02.771: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=kubectl-8801 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4'
Dec  9 08:31:03.060: INFO: stderr: ""
Dec  9 08:31:03.060: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created 12/09/22 08:31:03.06
[AfterEach] Kubectl run pod
  test/e2e/kubectl/kubectl.go:1704
Dec  9 08:31:03.072: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=kubectl-8801 delete pods e2e-test-httpd-pod'
Dec  9 08:31:05.153: INFO: stderr: ""
Dec  9 08:31:05.153: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Dec  9 08:31:05.153: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-8801" for this suite. 12/09/22 08:31:05.158
------------------------------
â€¢ [2.529 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl run pod
  test/e2e/kubectl/kubectl.go:1697
    should create a pod from an image when restart is Never  [Conformance]
    test/e2e/kubectl/kubectl.go:1713

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 08:31:02.638
    Dec  9 08:31:02.638: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename kubectl 12/09/22 08:31:02.639
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:31:02.756
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:31:02.765
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Kubectl run pod
      test/e2e/kubectl/kubectl.go:1700
    [It] should create a pod from an image when restart is Never  [Conformance]
      test/e2e/kubectl/kubectl.go:1713
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 12/09/22 08:31:02.771
    Dec  9 08:31:02.771: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=kubectl-8801 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4'
    Dec  9 08:31:03.060: INFO: stderr: ""
    Dec  9 08:31:03.060: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: verifying the pod e2e-test-httpd-pod was created 12/09/22 08:31:03.06
    [AfterEach] Kubectl run pod
      test/e2e/kubectl/kubectl.go:1704
    Dec  9 08:31:03.072: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1460426770 --namespace=kubectl-8801 delete pods e2e-test-httpd-pod'
    Dec  9 08:31:05.153: INFO: stderr: ""
    Dec  9 08:31:05.153: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Dec  9 08:31:05.153: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-8801" for this suite. 12/09/22 08:31:05.158
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:37
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 08:31:05.167
Dec  9 08:31:05.167: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename sysctl 12/09/22 08:31:05.17
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:31:05.195
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:31:05.203
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:67
[It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77
STEP: Creating a pod with the kernel.shm_rmid_forced sysctl 12/09/22 08:31:05.209
STEP: Watching for error events or started pod 12/09/22 08:31:05.218
STEP: Waiting for pod completion 12/09/22 08:31:07.225
Dec  9 08:31:07.225: INFO: Waiting up to 3m0s for pod "sysctl-cf6761d3-6ee6-42bc-817b-9300259eeb71" in namespace "sysctl-5847" to be "completed"
Dec  9 08:31:07.228: INFO: Pod "sysctl-cf6761d3-6ee6-42bc-817b-9300259eeb71": Phase="Running", Reason="", readiness=false. Elapsed: 2.657233ms
Dec  9 08:31:09.235: INFO: Pod "sysctl-cf6761d3-6ee6-42bc-817b-9300259eeb71": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009963413s
Dec  9 08:31:09.235: INFO: Pod "sysctl-cf6761d3-6ee6-42bc-817b-9300259eeb71" satisfied condition "completed"
STEP: Checking that the pod succeeded 12/09/22 08:31:09.243
STEP: Getting logs from the pod 12/09/22 08:31:09.243
STEP: Checking that the sysctl is actually updated 12/09/22 08:31:09.257
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Dec  9 08:31:09.258: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "sysctl-5847" for this suite. 12/09/22 08:31:09.261
------------------------------
â€¢ [4.106 seconds]
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
test/e2e/common/node/framework.go:23
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:37
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 08:31:05.167
    Dec  9 08:31:05.167: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename sysctl 12/09/22 08:31:05.17
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:31:05.195
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:31:05.203
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:67
    [It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
      test/e2e/common/node/sysctl.go:77
    STEP: Creating a pod with the kernel.shm_rmid_forced sysctl 12/09/22 08:31:05.209
    STEP: Watching for error events or started pod 12/09/22 08:31:05.218
    STEP: Waiting for pod completion 12/09/22 08:31:07.225
    Dec  9 08:31:07.225: INFO: Waiting up to 3m0s for pod "sysctl-cf6761d3-6ee6-42bc-817b-9300259eeb71" in namespace "sysctl-5847" to be "completed"
    Dec  9 08:31:07.228: INFO: Pod "sysctl-cf6761d3-6ee6-42bc-817b-9300259eeb71": Phase="Running", Reason="", readiness=false. Elapsed: 2.657233ms
    Dec  9 08:31:09.235: INFO: Pod "sysctl-cf6761d3-6ee6-42bc-817b-9300259eeb71": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009963413s
    Dec  9 08:31:09.235: INFO: Pod "sysctl-cf6761d3-6ee6-42bc-817b-9300259eeb71" satisfied condition "completed"
    STEP: Checking that the pod succeeded 12/09/22 08:31:09.243
    STEP: Getting logs from the pod 12/09/22 08:31:09.243
    STEP: Checking that the sysctl is actually updated 12/09/22 08:31:09.257
    [AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Dec  9 08:31:09.258: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sysctl-5847" for this suite. 12/09/22 08:31:09.261
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PreStop
  should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168
[BeforeEach] [sig-node] PreStop
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 08:31:09.273
Dec  9 08:31:09.274: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename prestop 12/09/22 08:31:09.274
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:31:09.314
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:31:09.337
[BeforeEach] [sig-node] PreStop
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] PreStop
  test/e2e/node/pre_stop.go:159
[It] should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168
STEP: Creating server pod server in namespace prestop-5170 12/09/22 08:31:09.344
STEP: Waiting for pods to come up. 12/09/22 08:31:09.371
Dec  9 08:31:09.371: INFO: Waiting up to 5m0s for pod "server" in namespace "prestop-5170" to be "running"
Dec  9 08:31:09.376: INFO: Pod "server": Phase="Pending", Reason="", readiness=false. Elapsed: 4.171755ms
Dec  9 08:31:11.380: INFO: Pod "server": Phase="Running", Reason="", readiness=true. Elapsed: 2.00868993s
Dec  9 08:31:11.380: INFO: Pod "server" satisfied condition "running"
STEP: Creating tester pod tester in namespace prestop-5170 12/09/22 08:31:11.383
Dec  9 08:31:11.390: INFO: Waiting up to 5m0s for pod "tester" in namespace "prestop-5170" to be "running"
Dec  9 08:31:11.394: INFO: Pod "tester": Phase="Pending", Reason="", readiness=false. Elapsed: 3.542792ms
Dec  9 08:31:13.397: INFO: Pod "tester": Phase="Running", Reason="", readiness=true. Elapsed: 2.007118412s
Dec  9 08:31:13.397: INFO: Pod "tester" satisfied condition "running"
STEP: Deleting pre-stop pod 12/09/22 08:31:13.397
Dec  9 08:31:18.415: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod 12/09/22 08:31:18.415
[AfterEach] [sig-node] PreStop
  test/e2e/framework/node/init/init.go:32
Dec  9 08:31:18.452: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] PreStop
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] PreStop
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] PreStop
  tear down framework | framework.go:193
STEP: Destroying namespace "prestop-5170" for this suite. 12/09/22 08:31:18.464
------------------------------
â€¢ [SLOW TEST] [9.202 seconds]
[sig-node] PreStop
test/e2e/node/framework.go:23
  should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PreStop
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 08:31:09.273
    Dec  9 08:31:09.274: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename prestop 12/09/22 08:31:09.274
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:31:09.314
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:31:09.337
    [BeforeEach] [sig-node] PreStop
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] PreStop
      test/e2e/node/pre_stop.go:159
    [It] should call prestop when killing a pod  [Conformance]
      test/e2e/node/pre_stop.go:168
    STEP: Creating server pod server in namespace prestop-5170 12/09/22 08:31:09.344
    STEP: Waiting for pods to come up. 12/09/22 08:31:09.371
    Dec  9 08:31:09.371: INFO: Waiting up to 5m0s for pod "server" in namespace "prestop-5170" to be "running"
    Dec  9 08:31:09.376: INFO: Pod "server": Phase="Pending", Reason="", readiness=false. Elapsed: 4.171755ms
    Dec  9 08:31:11.380: INFO: Pod "server": Phase="Running", Reason="", readiness=true. Elapsed: 2.00868993s
    Dec  9 08:31:11.380: INFO: Pod "server" satisfied condition "running"
    STEP: Creating tester pod tester in namespace prestop-5170 12/09/22 08:31:11.383
    Dec  9 08:31:11.390: INFO: Waiting up to 5m0s for pod "tester" in namespace "prestop-5170" to be "running"
    Dec  9 08:31:11.394: INFO: Pod "tester": Phase="Pending", Reason="", readiness=false. Elapsed: 3.542792ms
    Dec  9 08:31:13.397: INFO: Pod "tester": Phase="Running", Reason="", readiness=true. Elapsed: 2.007118412s
    Dec  9 08:31:13.397: INFO: Pod "tester" satisfied condition "running"
    STEP: Deleting pre-stop pod 12/09/22 08:31:13.397
    Dec  9 08:31:18.415: INFO: Saw: {
    	"Hostname": "server",
    	"Sent": null,
    	"Received": {
    		"prestop": 1
    	},
    	"Errors": null,
    	"Log": [
    		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
    		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
    	],
    	"StillContactingPeers": true
    }
    STEP: Deleting the server pod 12/09/22 08:31:18.415
    [AfterEach] [sig-node] PreStop
      test/e2e/framework/node/init/init.go:32
    Dec  9 08:31:18.452: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] PreStop
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] PreStop
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] PreStop
      tear down framework | framework.go:193
    STEP: Destroying namespace "prestop-5170" for this suite. 12/09/22 08:31:18.464
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-auth] ServiceAccounts
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:531
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 08:31:18.477
Dec  9 08:31:18.477: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename svcaccounts 12/09/22 08:31:18.478
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:31:18.514
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:31:18.525
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:531
Dec  9 08:31:18.565: INFO: created pod
Dec  9 08:31:18.565: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-3284" to be "Succeeded or Failed"
Dec  9 08:31:18.579: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 13.939205ms
Dec  9 08:31:20.584: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018549014s
Dec  9 08:31:22.584: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018649378s
STEP: Saw pod success 12/09/22 08:31:22.584
Dec  9 08:31:22.584: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
Dec  9 08:31:52.586: INFO: polling logs
Dec  9 08:31:52.592: INFO: Pod logs: 
I1209 08:31:19.466990       1 log.go:198] OK: Got token
I1209 08:31:19.467202       1 log.go:198] validating with in-cluster discovery
I1209 08:31:19.467534       1 log.go:198] OK: got issuer https://kubernetes.default.svc.cluster.local
I1209 08:31:19.467631       1 log.go:198] Full, not-validated claims: 
openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-3284:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1670575278, NotBefore:1670574678, IssuedAt:1670574678, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-3284", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"9c5dfce8-fcba-434a-9fe1-ac0a5900b5cd"}}}
I1209 08:31:19.480785       1 log.go:198] OK: Constructed OIDC provider for issuer https://kubernetes.default.svc.cluster.local
I1209 08:31:19.483020       1 log.go:198] OK: Validated signature on JWT
I1209 08:31:19.483137       1 log.go:198] OK: Got valid claims from token!
I1209 08:31:19.483179       1 log.go:198] Full, validated claims: 
&openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-3284:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1670575278, NotBefore:1670574678, IssuedAt:1670574678, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-3284", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"9c5dfce8-fcba-434a-9fe1-ac0a5900b5cd"}}}

Dec  9 08:31:52.592: INFO: completed pod
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Dec  9 08:31:52.602: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-3284" for this suite. 12/09/22 08:31:52.606
------------------------------
â€¢ [SLOW TEST] [34.136 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:531

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 08:31:18.477
    Dec  9 08:31:18.477: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename svcaccounts 12/09/22 08:31:18.478
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:31:18.514
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:31:18.525
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
      test/e2e/auth/service_accounts.go:531
    Dec  9 08:31:18.565: INFO: created pod
    Dec  9 08:31:18.565: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-3284" to be "Succeeded or Failed"
    Dec  9 08:31:18.579: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 13.939205ms
    Dec  9 08:31:20.584: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018549014s
    Dec  9 08:31:22.584: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018649378s
    STEP: Saw pod success 12/09/22 08:31:22.584
    Dec  9 08:31:22.584: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
    Dec  9 08:31:52.586: INFO: polling logs
    Dec  9 08:31:52.592: INFO: Pod logs: 
    I1209 08:31:19.466990       1 log.go:198] OK: Got token
    I1209 08:31:19.467202       1 log.go:198] validating with in-cluster discovery
    I1209 08:31:19.467534       1 log.go:198] OK: got issuer https://kubernetes.default.svc.cluster.local
    I1209 08:31:19.467631       1 log.go:198] Full, not-validated claims: 
    openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-3284:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1670575278, NotBefore:1670574678, IssuedAt:1670574678, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-3284", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"9c5dfce8-fcba-434a-9fe1-ac0a5900b5cd"}}}
    I1209 08:31:19.480785       1 log.go:198] OK: Constructed OIDC provider for issuer https://kubernetes.default.svc.cluster.local
    I1209 08:31:19.483020       1 log.go:198] OK: Validated signature on JWT
    I1209 08:31:19.483137       1 log.go:198] OK: Got valid claims from token!
    I1209 08:31:19.483179       1 log.go:198] Full, validated claims: 
    &openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-3284:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1670575278, NotBefore:1670574678, IssuedAt:1670574678, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-3284", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"9c5dfce8-fcba-434a-9fe1-ac0a5900b5cd"}}}

    Dec  9 08:31:52.592: INFO: completed pod
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Dec  9 08:31:52.602: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-3284" for this suite. 12/09/22 08:31:52.606
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] EmptyDir volumes
  pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:227
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 08:31:52.613
Dec  9 08:31:52.613: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename emptydir 12/09/22 08:31:52.614
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:31:52.643
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:31:52.647
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:227
STEP: Creating Pod 12/09/22 08:31:52.651
Dec  9 08:31:52.658: INFO: Waiting up to 5m0s for pod "pod-sharedvolume-14d437c8-b9d4-43e7-a16d-dda35ea2ec08" in namespace "emptydir-350" to be "running"
Dec  9 08:31:52.662: INFO: Pod "pod-sharedvolume-14d437c8-b9d4-43e7-a16d-dda35ea2ec08": Phase="Pending", Reason="", readiness=false. Elapsed: 3.88463ms
Dec  9 08:31:54.667: INFO: Pod "pod-sharedvolume-14d437c8-b9d4-43e7-a16d-dda35ea2ec08": Phase="Running", Reason="", readiness=false. Elapsed: 2.008365529s
Dec  9 08:31:54.667: INFO: Pod "pod-sharedvolume-14d437c8-b9d4-43e7-a16d-dda35ea2ec08" satisfied condition "running"
STEP: Reading file content from the nginx-container 12/09/22 08:31:54.667
Dec  9 08:31:54.667: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-350 PodName:pod-sharedvolume-14d437c8-b9d4-43e7-a16d-dda35ea2ec08 ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec  9 08:31:54.667: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
Dec  9 08:31:54.668: INFO: ExecWithOptions: Clientset creation
Dec  9 08:31:54.669: INFO: ExecWithOptions: execute(POST https://10.3.0.1:443/api/v1/namespaces/emptydir-350/pods/pod-sharedvolume-14d437c8-b9d4-43e7-a16d-dda35ea2ec08/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
Dec  9 08:31:54.732: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Dec  9 08:31:54.732: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-350" for this suite. 12/09/22 08:31:54.739
------------------------------
â€¢ [2.132 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:227

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 08:31:52.613
    Dec  9 08:31:52.613: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename emptydir 12/09/22 08:31:52.614
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:31:52.643
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:31:52.647
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] pod should support shared volumes between containers [Conformance]
      test/e2e/common/storage/empty_dir.go:227
    STEP: Creating Pod 12/09/22 08:31:52.651
    Dec  9 08:31:52.658: INFO: Waiting up to 5m0s for pod "pod-sharedvolume-14d437c8-b9d4-43e7-a16d-dda35ea2ec08" in namespace "emptydir-350" to be "running"
    Dec  9 08:31:52.662: INFO: Pod "pod-sharedvolume-14d437c8-b9d4-43e7-a16d-dda35ea2ec08": Phase="Pending", Reason="", readiness=false. Elapsed: 3.88463ms
    Dec  9 08:31:54.667: INFO: Pod "pod-sharedvolume-14d437c8-b9d4-43e7-a16d-dda35ea2ec08": Phase="Running", Reason="", readiness=false. Elapsed: 2.008365529s
    Dec  9 08:31:54.667: INFO: Pod "pod-sharedvolume-14d437c8-b9d4-43e7-a16d-dda35ea2ec08" satisfied condition "running"
    STEP: Reading file content from the nginx-container 12/09/22 08:31:54.667
    Dec  9 08:31:54.667: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-350 PodName:pod-sharedvolume-14d437c8-b9d4-43e7-a16d-dda35ea2ec08 ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Dec  9 08:31:54.667: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    Dec  9 08:31:54.668: INFO: ExecWithOptions: Clientset creation
    Dec  9 08:31:54.669: INFO: ExecWithOptions: execute(POST https://10.3.0.1:443/api/v1/namespaces/emptydir-350/pods/pod-sharedvolume-14d437c8-b9d4-43e7-a16d-dda35ea2ec08/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
    Dec  9 08:31:54.732: INFO: Exec stderr: ""
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Dec  9 08:31:54.732: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-350" for this suite. 12/09/22 08:31:54.739
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:276
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 08:31:54.747
Dec  9 08:31:54.747: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename crd-publish-openapi 12/09/22 08:31:54.749
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:31:54.766
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:31:54.771
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:276
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation 12/09/22 08:31:54.775
Dec  9 08:31:54.775: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
Dec  9 08:31:56.686: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Dec  9 08:32:02.733: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-7697" for this suite. 12/09/22 08:32:02.788
------------------------------
â€¢ [SLOW TEST] [8.062 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:276

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 08:31:54.747
    Dec  9 08:31:54.747: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename crd-publish-openapi 12/09/22 08:31:54.749
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:31:54.766
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:31:54.771
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for multiple CRDs of different groups [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:276
    STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation 12/09/22 08:31:54.775
    Dec  9 08:31:54.775: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    Dec  9 08:31:56.686: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Dec  9 08:32:02.733: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-7697" for this suite. 12/09/22 08:32:02.788
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Downward API volume
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:221
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 08:32:02.81
Dec  9 08:32:02.810: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename downward-api 12/09/22 08:32:02.811
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:32:02.848
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:32:02.853
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:221
STEP: Creating a pod to test downward API volume plugin 12/09/22 08:32:02.857
Dec  9 08:32:02.874: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6ff131d2-5da1-4572-88a3-9f3db3f124d5" in namespace "downward-api-5951" to be "Succeeded or Failed"
Dec  9 08:32:02.893: INFO: Pod "downwardapi-volume-6ff131d2-5da1-4572-88a3-9f3db3f124d5": Phase="Pending", Reason="", readiness=false. Elapsed: 19.950416ms
Dec  9 08:32:04.901: INFO: Pod "downwardapi-volume-6ff131d2-5da1-4572-88a3-9f3db3f124d5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027421469s
Dec  9 08:32:06.898: INFO: Pod "downwardapi-volume-6ff131d2-5da1-4572-88a3-9f3db3f124d5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024455425s
STEP: Saw pod success 12/09/22 08:32:06.898
Dec  9 08:32:06.898: INFO: Pod "downwardapi-volume-6ff131d2-5da1-4572-88a3-9f3db3f124d5" satisfied condition "Succeeded or Failed"
Dec  9 08:32:06.901: INFO: Trying to get logs from node ip-10-0-10-179 pod downwardapi-volume-6ff131d2-5da1-4572-88a3-9f3db3f124d5 container client-container: <nil>
STEP: delete the pod 12/09/22 08:32:06.907
Dec  9 08:32:06.921: INFO: Waiting for pod downwardapi-volume-6ff131d2-5da1-4572-88a3-9f3db3f124d5 to disappear
Dec  9 08:32:06.931: INFO: Pod downwardapi-volume-6ff131d2-5da1-4572-88a3-9f3db3f124d5 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Dec  9 08:32:06.932: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-5951" for this suite. 12/09/22 08:32:06.937
------------------------------
â€¢ [4.138 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:221

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 08:32:02.81
    Dec  9 08:32:02.810: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename downward-api 12/09/22 08:32:02.811
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:32:02.848
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:32:02.853
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide container's cpu request [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:221
    STEP: Creating a pod to test downward API volume plugin 12/09/22 08:32:02.857
    Dec  9 08:32:02.874: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6ff131d2-5da1-4572-88a3-9f3db3f124d5" in namespace "downward-api-5951" to be "Succeeded or Failed"
    Dec  9 08:32:02.893: INFO: Pod "downwardapi-volume-6ff131d2-5da1-4572-88a3-9f3db3f124d5": Phase="Pending", Reason="", readiness=false. Elapsed: 19.950416ms
    Dec  9 08:32:04.901: INFO: Pod "downwardapi-volume-6ff131d2-5da1-4572-88a3-9f3db3f124d5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027421469s
    Dec  9 08:32:06.898: INFO: Pod "downwardapi-volume-6ff131d2-5da1-4572-88a3-9f3db3f124d5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024455425s
    STEP: Saw pod success 12/09/22 08:32:06.898
    Dec  9 08:32:06.898: INFO: Pod "downwardapi-volume-6ff131d2-5da1-4572-88a3-9f3db3f124d5" satisfied condition "Succeeded or Failed"
    Dec  9 08:32:06.901: INFO: Trying to get logs from node ip-10-0-10-179 pod downwardapi-volume-6ff131d2-5da1-4572-88a3-9f3db3f124d5 container client-container: <nil>
    STEP: delete the pod 12/09/22 08:32:06.907
    Dec  9 08:32:06.921: INFO: Waiting for pod downwardapi-volume-6ff131d2-5da1-4572-88a3-9f3db3f124d5 to disappear
    Dec  9 08:32:06.931: INFO: Pod downwardapi-volume-6ff131d2-5da1-4572-88a3-9f3db3f124d5 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Dec  9 08:32:06.932: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-5951" for this suite. 12/09/22 08:32:06.937
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:129
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 08:32:06.967
Dec  9 08:32:06.968: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename security-context 12/09/22 08:32:06.97
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:32:07.049
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:32:07.059
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:129
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 12/09/22 08:32:07.063
Dec  9 08:32:07.072: INFO: Waiting up to 5m0s for pod "security-context-3f89a8d0-809b-4f8d-a897-e946a18b69ab" in namespace "security-context-1328" to be "Succeeded or Failed"
Dec  9 08:32:07.079: INFO: Pod "security-context-3f89a8d0-809b-4f8d-a897-e946a18b69ab": Phase="Pending", Reason="", readiness=false. Elapsed: 7.35047ms
Dec  9 08:32:09.084: INFO: Pod "security-context-3f89a8d0-809b-4f8d-a897-e946a18b69ab": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012239576s
Dec  9 08:32:11.083: INFO: Pod "security-context-3f89a8d0-809b-4f8d-a897-e946a18b69ab": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011443076s
STEP: Saw pod success 12/09/22 08:32:11.083
Dec  9 08:32:11.083: INFO: Pod "security-context-3f89a8d0-809b-4f8d-a897-e946a18b69ab" satisfied condition "Succeeded or Failed"
Dec  9 08:32:11.086: INFO: Trying to get logs from node ip-10-0-10-179 pod security-context-3f89a8d0-809b-4f8d-a897-e946a18b69ab container test-container: <nil>
STEP: delete the pod 12/09/22 08:32:11.092
Dec  9 08:32:11.105: INFO: Waiting for pod security-context-3f89a8d0-809b-4f8d-a897-e946a18b69ab to disappear
Dec  9 08:32:11.109: INFO: Pod security-context-3f89a8d0-809b-4f8d-a897-e946a18b69ab no longer exists
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Dec  9 08:32:11.109: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-1328" for this suite. 12/09/22 08:32:11.113
------------------------------
â€¢ [4.152 seconds]
[sig-node] Security Context
test/e2e/node/framework.go:23
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 08:32:06.967
    Dec  9 08:32:06.968: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename security-context 12/09/22 08:32:06.97
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:32:07.049
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:32:07.059
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
      test/e2e/node/security_context.go:129
    STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 12/09/22 08:32:07.063
    Dec  9 08:32:07.072: INFO: Waiting up to 5m0s for pod "security-context-3f89a8d0-809b-4f8d-a897-e946a18b69ab" in namespace "security-context-1328" to be "Succeeded or Failed"
    Dec  9 08:32:07.079: INFO: Pod "security-context-3f89a8d0-809b-4f8d-a897-e946a18b69ab": Phase="Pending", Reason="", readiness=false. Elapsed: 7.35047ms
    Dec  9 08:32:09.084: INFO: Pod "security-context-3f89a8d0-809b-4f8d-a897-e946a18b69ab": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012239576s
    Dec  9 08:32:11.083: INFO: Pod "security-context-3f89a8d0-809b-4f8d-a897-e946a18b69ab": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011443076s
    STEP: Saw pod success 12/09/22 08:32:11.083
    Dec  9 08:32:11.083: INFO: Pod "security-context-3f89a8d0-809b-4f8d-a897-e946a18b69ab" satisfied condition "Succeeded or Failed"
    Dec  9 08:32:11.086: INFO: Trying to get logs from node ip-10-0-10-179 pod security-context-3f89a8d0-809b-4f8d-a897-e946a18b69ab container test-container: <nil>
    STEP: delete the pod 12/09/22 08:32:11.092
    Dec  9 08:32:11.105: INFO: Waiting for pod security-context-3f89a8d0-809b-4f8d-a897-e946a18b69ab to disappear
    Dec  9 08:32:11.109: INFO: Pod security-context-3f89a8d0-809b-4f8d-a897-e946a18b69ab no longer exists
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Dec  9 08:32:11.109: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-1328" for this suite. 12/09/22 08:32:11.113
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should update a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:810
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 08:32:11.121
Dec  9 08:32:11.121: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename svcaccounts 12/09/22 08:32:11.122
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:32:11.142
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:32:11.146
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should update a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:810
STEP: Creating ServiceAccount "e2e-sa-d47z5"  12/09/22 08:32:11.149
Dec  9 08:32:11.154: INFO: AutomountServiceAccountToken: false
STEP: Updating ServiceAccount "e2e-sa-d47z5"  12/09/22 08:32:11.155
Dec  9 08:32:11.165: INFO: AutomountServiceAccountToken: true
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Dec  9 08:32:11.165: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-2979" for this suite. 12/09/22 08:32:11.169
------------------------------
â€¢ [0.055 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should update a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:810

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 08:32:11.121
    Dec  9 08:32:11.121: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename svcaccounts 12/09/22 08:32:11.122
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:32:11.142
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:32:11.146
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should update a ServiceAccount [Conformance]
      test/e2e/auth/service_accounts.go:810
    STEP: Creating ServiceAccount "e2e-sa-d47z5"  12/09/22 08:32:11.149
    Dec  9 08:32:11.154: INFO: AutomountServiceAccountToken: false
    STEP: Updating ServiceAccount "e2e-sa-d47z5"  12/09/22 08:32:11.155
    Dec  9 08:32:11.165: INFO: AutomountServiceAccountToken: true
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Dec  9 08:32:11.165: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-2979" for this suite. 12/09/22 08:32:11.169
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test when starting a container that exits
  should run with the expected status [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:52
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 08:32:11.18
Dec  9 08:32:11.180: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename container-runtime 12/09/22 08:32:11.181
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:32:11.209
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:32:11.214
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should run with the expected status [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:52
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount' 12/09/22 08:32:11.229
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase' 12/09/22 08:32:28.343
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition 12/09/22 08:32:28.348
STEP: Container 'terminate-cmd-rpa': should get the expected 'State' 12/09/22 08:32:28.355
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance] 12/09/22 08:32:28.356
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount' 12/09/22 08:32:28.38
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase' 12/09/22 08:32:31.398
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition 12/09/22 08:32:33.412
STEP: Container 'terminate-cmd-rpof': should get the expected 'State' 12/09/22 08:32:33.42
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance] 12/09/22 08:32:33.42
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount' 12/09/22 08:32:33.442
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase' 12/09/22 08:32:34.449
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition 12/09/22 08:32:37.467
STEP: Container 'terminate-cmd-rpn': should get the expected 'State' 12/09/22 08:32:37.472
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance] 12/09/22 08:32:37.472
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Dec  9 08:32:37.491: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-8734" for this suite. 12/09/22 08:32:37.494
------------------------------
â€¢ [SLOW TEST] [26.319 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    when starting a container that exits
    test/e2e/common/node/runtime.go:45
      should run with the expected status [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:52

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 08:32:11.18
    Dec  9 08:32:11.180: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename container-runtime 12/09/22 08:32:11.181
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:32:11.209
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:32:11.214
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should run with the expected status [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:52
    STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount' 12/09/22 08:32:11.229
    STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase' 12/09/22 08:32:28.343
    STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition 12/09/22 08:32:28.348
    STEP: Container 'terminate-cmd-rpa': should get the expected 'State' 12/09/22 08:32:28.355
    STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance] 12/09/22 08:32:28.356
    STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount' 12/09/22 08:32:28.38
    STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase' 12/09/22 08:32:31.398
    STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition 12/09/22 08:32:33.412
    STEP: Container 'terminate-cmd-rpof': should get the expected 'State' 12/09/22 08:32:33.42
    STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance] 12/09/22 08:32:33.42
    STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount' 12/09/22 08:32:33.442
    STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase' 12/09/22 08:32:34.449
    STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition 12/09/22 08:32:37.467
    STEP: Container 'terminate-cmd-rpn': should get the expected 'State' 12/09/22 08:32:37.472
    STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance] 12/09/22 08:32:37.472
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Dec  9 08:32:37.491: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-8734" for this suite. 12/09/22 08:32:37.494
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:68
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 08:32:37.5
Dec  9 08:32:37.500: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename downward-api 12/09/22 08:32:37.501
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:32:37.518
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:32:37.522
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:68
STEP: Creating a pod to test downward API volume plugin 12/09/22 08:32:37.526
Dec  9 08:32:37.536: INFO: Waiting up to 5m0s for pod "downwardapi-volume-99245323-c7be-427f-9cd4-8825b62d238e" in namespace "downward-api-5906" to be "Succeeded or Failed"
Dec  9 08:32:37.541: INFO: Pod "downwardapi-volume-99245323-c7be-427f-9cd4-8825b62d238e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.722457ms
Dec  9 08:32:39.546: INFO: Pod "downwardapi-volume-99245323-c7be-427f-9cd4-8825b62d238e": Phase="Running", Reason="", readiness=false. Elapsed: 2.009747836s
Dec  9 08:32:41.546: INFO: Pod "downwardapi-volume-99245323-c7be-427f-9cd4-8825b62d238e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010195935s
STEP: Saw pod success 12/09/22 08:32:41.546
Dec  9 08:32:41.547: INFO: Pod "downwardapi-volume-99245323-c7be-427f-9cd4-8825b62d238e" satisfied condition "Succeeded or Failed"
Dec  9 08:32:41.550: INFO: Trying to get logs from node ip-10-0-10-179 pod downwardapi-volume-99245323-c7be-427f-9cd4-8825b62d238e container client-container: <nil>
STEP: delete the pod 12/09/22 08:32:41.556
Dec  9 08:32:41.568: INFO: Waiting for pod downwardapi-volume-99245323-c7be-427f-9cd4-8825b62d238e to disappear
Dec  9 08:32:41.571: INFO: Pod downwardapi-volume-99245323-c7be-427f-9cd4-8825b62d238e no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Dec  9 08:32:41.571: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-5906" for this suite. 12/09/22 08:32:41.576
------------------------------
â€¢ [4.083 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 08:32:37.5
    Dec  9 08:32:37.500: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename downward-api 12/09/22 08:32:37.501
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:32:37.518
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:32:37.522
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:68
    STEP: Creating a pod to test downward API volume plugin 12/09/22 08:32:37.526
    Dec  9 08:32:37.536: INFO: Waiting up to 5m0s for pod "downwardapi-volume-99245323-c7be-427f-9cd4-8825b62d238e" in namespace "downward-api-5906" to be "Succeeded or Failed"
    Dec  9 08:32:37.541: INFO: Pod "downwardapi-volume-99245323-c7be-427f-9cd4-8825b62d238e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.722457ms
    Dec  9 08:32:39.546: INFO: Pod "downwardapi-volume-99245323-c7be-427f-9cd4-8825b62d238e": Phase="Running", Reason="", readiness=false. Elapsed: 2.009747836s
    Dec  9 08:32:41.546: INFO: Pod "downwardapi-volume-99245323-c7be-427f-9cd4-8825b62d238e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010195935s
    STEP: Saw pod success 12/09/22 08:32:41.546
    Dec  9 08:32:41.547: INFO: Pod "downwardapi-volume-99245323-c7be-427f-9cd4-8825b62d238e" satisfied condition "Succeeded or Failed"
    Dec  9 08:32:41.550: INFO: Trying to get logs from node ip-10-0-10-179 pod downwardapi-volume-99245323-c7be-427f-9cd4-8825b62d238e container client-container: <nil>
    STEP: delete the pod 12/09/22 08:32:41.556
    Dec  9 08:32:41.568: INFO: Waiting for pod downwardapi-volume-99245323-c7be-427f-9cd4-8825b62d238e to disappear
    Dec  9 08:32:41.571: INFO: Pod downwardapi-volume-99245323-c7be-427f-9cd4-8825b62d238e no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Dec  9 08:32:41.571: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-5906" for this suite. 12/09/22 08:32:41.576
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:47
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 12/09/22 08:32:41.584
Dec  9 08:32:41.584: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
STEP: Building a namespace api object, basename configmap 12/09/22 08:32:41.585
STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:32:41.605
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:32:41.619
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:47
STEP: Creating configMap with name configmap-test-volume-1acee234-cd9b-4c77-9426-11cab702952d 12/09/22 08:32:41.624
STEP: Creating a pod to test consume configMaps 12/09/22 08:32:41.637
Dec  9 08:32:41.650: INFO: Waiting up to 5m0s for pod "pod-configmaps-3906ff7b-9f0d-4899-b915-08f92a30c464" in namespace "configmap-7858" to be "Succeeded or Failed"
Dec  9 08:32:41.656: INFO: Pod "pod-configmaps-3906ff7b-9f0d-4899-b915-08f92a30c464": Phase="Pending", Reason="", readiness=false. Elapsed: 5.580556ms
Dec  9 08:32:43.660: INFO: Pod "pod-configmaps-3906ff7b-9f0d-4899-b915-08f92a30c464": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009813195s
Dec  9 08:32:45.662: INFO: Pod "pod-configmaps-3906ff7b-9f0d-4899-b915-08f92a30c464": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011015201s
STEP: Saw pod success 12/09/22 08:32:45.662
Dec  9 08:32:45.663: INFO: Pod "pod-configmaps-3906ff7b-9f0d-4899-b915-08f92a30c464" satisfied condition "Succeeded or Failed"
Dec  9 08:32:45.666: INFO: Trying to get logs from node ip-10-0-10-179 pod pod-configmaps-3906ff7b-9f0d-4899-b915-08f92a30c464 container agnhost-container: <nil>
STEP: delete the pod 12/09/22 08:32:45.671
Dec  9 08:32:45.681: INFO: Waiting for pod pod-configmaps-3906ff7b-9f0d-4899-b915-08f92a30c464 to disappear
Dec  9 08:32:45.684: INFO: Pod pod-configmaps-3906ff7b-9f0d-4899-b915-08f92a30c464 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Dec  9 08:32:45.684: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-7858" for this suite. 12/09/22 08:32:45.687
------------------------------
â€¢ [4.108 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:47

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 12/09/22 08:32:41.584
    Dec  9 08:32:41.584: INFO: >>> kubeConfig: /tmp/kubeconfig-1460426770
    STEP: Building a namespace api object, basename configmap 12/09/22 08:32:41.585
    STEP: Waiting for a default service account to be provisioned in namespace 12/09/22 08:32:41.605
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/09/22 08:32:41.619
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:47
    STEP: Creating configMap with name configmap-test-volume-1acee234-cd9b-4c77-9426-11cab702952d 12/09/22 08:32:41.624
    STEP: Creating a pod to test consume configMaps 12/09/22 08:32:41.637
    Dec  9 08:32:41.650: INFO: Waiting up to 5m0s for pod "pod-configmaps-3906ff7b-9f0d-4899-b915-08f92a30c464" in namespace "configmap-7858" to be "Succeeded or Failed"
    Dec  9 08:32:41.656: INFO: Pod "pod-configmaps-3906ff7b-9f0d-4899-b915-08f92a30c464": Phase="Pending", Reason="", readiness=false. Elapsed: 5.580556ms
    Dec  9 08:32:43.660: INFO: Pod "pod-configmaps-3906ff7b-9f0d-4899-b915-08f92a30c464": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009813195s
    Dec  9 08:32:45.662: INFO: Pod "pod-configmaps-3906ff7b-9f0d-4899-b915-08f92a30c464": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011015201s
    STEP: Saw pod success 12/09/22 08:32:45.662
    Dec  9 08:32:45.663: INFO: Pod "pod-configmaps-3906ff7b-9f0d-4899-b915-08f92a30c464" satisfied condition "Succeeded or Failed"
    Dec  9 08:32:45.666: INFO: Trying to get logs from node ip-10-0-10-179 pod pod-configmaps-3906ff7b-9f0d-4899-b915-08f92a30c464 container agnhost-container: <nil>
    STEP: delete the pod 12/09/22 08:32:45.671
    Dec  9 08:32:45.681: INFO: Waiting for pod pod-configmaps-3906ff7b-9f0d-4899-b915-08f92a30c464 to disappear
    Dec  9 08:32:45.684: INFO: Pod pod-configmaps-3906ff7b-9f0d-4899-b915-08f92a30c464 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Dec  9 08:32:45.684: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-7858" for this suite. 12/09/22 08:32:45.687
  << End Captured GinkgoWriter Output
------------------------------
[SynchronizedAfterSuite] 
test/e2e/e2e.go:88
[SynchronizedAfterSuite] TOP-LEVEL
  test/e2e/e2e.go:88
[SynchronizedAfterSuite] TOP-LEVEL
  test/e2e/e2e.go:88
Dec  9 08:32:45.703: INFO: Running AfterSuite actions on node 1
Dec  9 08:32:45.703: INFO: Skipping dumping logs from cluster
------------------------------
[SynchronizedAfterSuite] PASSED [0.001 seconds]
[SynchronizedAfterSuite] 
test/e2e/e2e.go:88

  Begin Captured GinkgoWriter Output >>
    [SynchronizedAfterSuite] TOP-LEVEL
      test/e2e/e2e.go:88
    [SynchronizedAfterSuite] TOP-LEVEL
      test/e2e/e2e.go:88
    Dec  9 08:32:45.703: INFO: Running AfterSuite actions on node 1
    Dec  9 08:32:45.703: INFO: Skipping dumping logs from cluster
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterSuite] Kubernetes e2e suite report
test/e2e/e2e_test.go:153
[ReportAfterSuite] TOP-LEVEL
  test/e2e/e2e_test.go:153
------------------------------
[ReportAfterSuite] PASSED [0.000 seconds]
[ReportAfterSuite] Kubernetes e2e suite report
test/e2e/e2e_test.go:153

  Begin Captured GinkgoWriter Output >>
    [ReportAfterSuite] TOP-LEVEL
      test/e2e/e2e_test.go:153
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterSuite] Kubernetes e2e JUnit report
test/e2e/framework/test_context.go:529
[ReportAfterSuite] TOP-LEVEL
  test/e2e/framework/test_context.go:529
------------------------------
[ReportAfterSuite] PASSED [0.147 seconds]
[ReportAfterSuite] Kubernetes e2e JUnit report
test/e2e/framework/test_context.go:529

  Begin Captured GinkgoWriter Output >>
    [ReportAfterSuite] TOP-LEVEL
      test/e2e/framework/test_context.go:529
  << End Captured GinkgoWriter Output
------------------------------

Ran 368 of 7069 Specs in 5649.864 seconds
SUCCESS! -- 368 Passed | 0 Failed | 0 Pending | 6701 Skipped
PASS

Ginkgo ran 1 suite in 1h34m10.410716649s
Test Suite Passed
[38;5;228mYou're using deprecated Ginkgo functionality:[0m
[38;5;228m=============================================[0m
  [38;5;11m--noColor is deprecated, use --no-color instead[0m
  [1mLearn more at:[0m [38;5;14m[4mhttps://onsi.github.io/ginkgo/MIGRATING_TO_V2#changed-command-line-flags[0m

[38;5;243mTo silence deprecations that can be silenced set the following environment variable:[0m
  [38;5;243mACK_GINKGO_DEPRECATIONS=2.4.0[0m

